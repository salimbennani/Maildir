Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from pop3.zju.edu.cn (61.164.42.155:110) by
  likexu-MOBL1.ccr.corp.intel.com with POP3; 14 Nov 2018 14:45:03 -0000
Received: from icoremail.net (unknown [209.85.214.171])
	by mail-app2 (Coremail) with SMTP id by_KCgDHH8II7etbYwSPAQ--.42778S3;
	Wed, 14 Nov 2018 17:38:17 +0800 (CST)
Received: from mail-pl1-f171.google.com (unknown [209.85.214.171])
	by mx2.icoremail.net (Coremail) with SMTP id AQAAfwBnSkYG7etb7tI9AA--.3060S3;
	Wed, 14 Nov 2018 17:38:14 +0800 (CST)
Received: by mail-pl1-f171.google.com with SMTP id p4-v6so7518040plo.5
        for <xuliker@zju.edu.cn>; Wed, 14 Nov 2018 01:38:14 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-original-authentication-results:x-gm-message-state:delivered-to
         :date:from:to:cc:subject:message-id:references:mime-version
         :content-disposition:in-reply-to:user-agent:sender:precedence
         :list-id;
        bh=vyU1+BGM69ij/GoSjpvGk5OZb/MlIGYjKPEdbfQqmE8=;
        b=hLS6FclGyXvZfgD0iiI7xV8EO3AnUIKrr97wrwSVW1okSv2ntSMB2yquPDgvRjjSKo
         2IuAfGIN1ODftS0R0A6fl1Q8In9Wv7K4kOl/BTLVicgQ3ORJmQUJILBf/6DE1YeXpmMJ
         LTM5FGEgW60OFVaF3ySlik2M6a3ZWNyNaU8Ct9grOaW+BADIEqz7fIt1FyIyjRj6gBLg
         DQcn5ixeTXMGunEeB36cVAhpQSR33QkqyKQ2I77oaEgDw2yewIW7DwpxXpc3+lrmx9m5
         t61gO/gux4XNDhdAXBT1EO//poiywIrI6RHoOxhvBGRyny740rLF0/U57vfFwLyn9SQ0
         nGIw==
X-Original-Authentication-Results: mx.google.com;       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org;       dmarc=fail (p=NONE sp=NONE dis=NONE) header.from=kernel.org
X-Gm-Message-State: AGRZ1gLo/VlVadlKTinwhu6dtXmXbEPMzh+RU9WA/jVJ/iyzJdZLeq+t
	qQYZXUgMBYiMYWJLreM0Lcqi+BO0kwVX4Lge0h0cMwh+Xbuk73brwg==
X-Received: by 2002:a17:902:887:: with SMTP id 7-v6mr1177996pll.283.1542188293905;
        Wed, 14 Nov 2018 01:38:13 -0800 (PST)
X-Forwarded-To: xuliker@zju.edu.cn
X-Forwarded-For: liker.xu@gmail.com xuliker@zju.edu.cn
Delivered-To: liker.xu@gmail.com
Received: by 2002:a17:90a:c304:0:0:0:0 with SMTP id g4-v6csp5581994pjt;
        Wed, 14 Nov 2018 01:38:13 -0800 (PST)
X-Google-Smtp-Source: AJdET5e2EGMNCIgogznwp5i9CJgaOw2ouqDcH3s+W618fKWoVbgTv8uSS26xHzX3coi607gbXPvz
X-Received: by 2002:a17:902:8a89:: with SMTP id p9-v6mr1189286plo.183.1542188293087;
        Wed, 14 Nov 2018 01:38:13 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1542188293; cv=none;
        d=google.com; s=arc-20160816;
        b=V+283x4Fygz0yOSCt/gNRa19uWPHdYOpR9YmIvX9q7iSfbVi0sGiqgybnXyObRyONF
         PBFNd9tpdFDuX6aX/C/vakZbLWh6NGu0Knc2i8ABOmJB/hJUOobveHdGXOUGLpguqsMa
         gG4xCQd+dW8QthtShykaAo2G2jPIRycQjGrDkpPAqIEUGTEdU45pWb4pDQ9M0SiTnwGq
         CeDBFtAyVa7cq0hQn+TIT+LatIKOeanqdivVKd4rqAp4wobWcqfVYfJ416mie0xsrv4r
         Jz/a6+j/ey8ZUOr1WvSZiJ1+o5OVhbZpDlYbLK+ex2srn4eYnIAyHnds2Y8f38nt6w8N
         De3g==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=list-id:precedence:sender:user-agent:in-reply-to
         :content-disposition:mime-version:references:message-id:subject:cc
         :to:from:date;
        bh=vyU1+BGM69ij/GoSjpvGk5OZb/MlIGYjKPEdbfQqmE8=;
        b=X4JPV/Z2mn4ygFr0vtV/o5YZ21NM6163uUvWCpy64jWjMQZm+lYrXIPniX2Um3MmCh
         I7rJs+CM2Zb6dXhCgG0YPo8s3lvCHcUJBxgBk8aopjJZ9CmR5BIUq7UKvNeHI9dytLdO
         0cUIbyUub5dY67NV6qsBtp4L7q61E0tUpb7VGTxjJ8clo75Y4rwE/T7GVaBCULmYHPZj
         xyX87VSD/ZUyOTZwPugIYYp9WxQTyG0ubgElG/jaZaP1ROfoLEAOeUDD1/2cJG/WJZJJ
         ND8i1GOZsYJ0jQx1tPCrfitsGvxol3KKoVZplKEySDvbg4OgOB192IimIy0NVwWYfuuV
         xPAQ==
ARC-Authentication-Results: i=1; mx.google.com;
       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org;
       dmarc=fail (p=NONE sp=NONE dis=NONE) header.from=kernel.org
Received: from vger.kernel.org (vger.kernel.org. [209.132.180.67])
        by mx.google.com with ESMTP id ba2-v6si23357077plb.88.2018.11.14.01.37.57;
        Wed, 14 Nov 2018 01:38:13 -0800 (PST)
Received-SPF: pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) client-ip=209.132.180.67;
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1732312AbeKNTjv (ORCPT <rfc822;docz2a@gmail.com> + 99 others);
        Wed, 14 Nov 2018 14:39:51 -0500
Received: from mx2.suse.de ([195.135.220.15]:37070 "EHLO mx1.suse.de"
        rhost-flags-OK-OK-OK-FAIL) by vger.kernel.org with ESMTP
        id S1727558AbeKNTjv (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Wed, 14 Nov 2018 14:39:51 -0500
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay2.suse.de (unknown [195.135.220.254])
        by mx1.suse.de (Postfix) with ESMTP id 44C4FAE30;
        Wed, 14 Nov 2018 09:37:21 +0000 (UTC)
Date: Wed, 14 Nov 2018 10:37:20 +0100
From: Michal Hocko <mhocko@kernel.org>
To: David Hildenbrand <david@redhat.com>
Cc: Baoquan He <bhe@redhat.com>, linux-mm@kvack.org,
        linux-kernel@vger.kernel.org, akpm@linux-foundation.org,
        aarcange@redhat.com
Subject: Re: Memory hotplug softlock issue
Message-ID: <20181114093720.GI23419@dhcp22.suse.cz>
References: <20181114070909.GB2653@MiWiFi-R3L-srv>
 <5a6c6d6b-ebcd-8bfa-d6e0-4312bfe86586@redhat.com>
 <20181114090134.GG23419@dhcp22.suse.cz>
 <4449a0a2-be72-02bb-9f02-ed2484b160f8@redhat.com>
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: <4449a0a2-be72-02bb-9f02-ed2484b160f8@redhat.com>
User-Agent: Mutt/1.10.1 (2018-07-13)
Sender: liker.xu+caf_=xuliker=zju.edu.cn@gmail.com
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-CM-TRANSID: AQAAfwBnSkYG7etb7tI9AA--.3060S3
Authentication-Results: mail-app2; spf=pass smtp.mail=liker.xu+caf_=xu
	liker=zju.edu.cn@gmail.com;
X-Coremail-Antispam: 1UD129KBjvJXoWxCF1fZFyfGr43Xry5ArWxtFb_yoW5ur4kpF
	WfKa18KF4kKFZ5Gw1xAw1kJryFywn7GF45Jrn3Jryqv398GF1Ivr4Ivr4j9F9IkryfJw4q
	qFWIya47WFn0v37anT9S1TB71UUUUUUqnTZGkaVYY2UrUUUU0bIjqfuFe4nvWSU8nxnvy2
	9KBjDU0xBIdaVrnRJUUUm2b7Iv0xC_Kw4lb4IE77IF4wAFF20E14v26r1j6r4UM7CIcVAF
	z4kK6r1j6r18M28lY4IEw2IIxxk0rwA2F7IY1VAKz4vEj48ve4kI8wA2z4x0Y4vE2Ix0cI
	8IcVAFwI0_Ar0_tr1l84ACjcxK6xIIjxv20xvEc7CjxVAFwI0_Cr0_Gr1UM28EF7xvwVC2
	z280aVAFwI0_GcCE3s1l84ACjcxK6I8E87Iv6xkF7I0E14v26rxl6s0DM2AIxVAIcxkEcV
	Aq07x20xvEncxIr21l5I8CrVACY4xI64kE6c02F40Ex7xfMcIj6xIIjxv20xvE14v26r1j
	6r18McIj6I8E87Iv67AKxVW8JVWxJwAm72CE4IkC6x0Yz7v_Jr0_Gr1lF7xvr2IY64vIr4
	1l7I0Y6sxI4wCY1x0264kExVAvwVAq07x20xylc7Ca8VAvwVCFzxkY4VCF77xAMxkIecxE
	wVCI4VW8uwCY0x0Ix7I2Y4AK64vIr41lcIIF0xvE2Ix0cI8IcVAFwI0_Gr0_Xr1lcIIF0x
	vE2Ix0cI8IcVCY1x0267AKxVW8JVWxJwCYIxAIcVC2z280aVAFwI0_Cr1j6rxdMxvI42IY
	6I8E87Iv6xkF7I0E14v26F4UJVW0owCF04k20xvY0x0EwIxGrwCF04k20xvEw4C26cxK6c
	8Ij28IcwCF72vE77IF4wCFx2IqxVCFs4IE7xkEbVWUJVW8JwCFI7km07C267AKxVWUAVWU
	twC20s026c02F40E14v26r1j6r18MI8I3I0E7480Y4vE14v26r106r1rMI8E67AF67kF1V
	AFwI0_JF0_Jw1lIxkGc2Ij64vIr41lIxAIcVCF04k26cxKx2IYs7xG6r4j6FyUYxBIdaVF
	xhVjvjDU0xZFpf9x07bHgA7UUUUU=

On Wed 14-11-18 10:22:31, David Hildenbrand wrote:
> >>
> >> The real question is, however, why offlining of the last block doesn't
> >> succeed. In __offline_pages() we basically have an endless loop (while
> >> holding the mem_hotplug_lock in write). Now I consider this piece of
> >> code very problematic (we should automatically fail after X
> >> attempts/after X seconds, we should not ignore -ENOMEM), and we've had
> >> other BUGs whereby we would run into an endless loop here (e.g. related
> >> to hugepages I guess).
> > 
> > We used to have number of retries previous and it was too fragile. If
> > you need a timeout then you can easily do that from userspace. Just do
> > timeout $TIME echo 0 > $MEM_PATH/online
> 
> I agree that number of retries is not a good measure.
> 
> But as far as I can see this happens from the kernel via an ACPI event.
> E.g. failing to offline a block after X seconds would still make sense.
> (if something takes 120seconds to offline 128MB/2G there is something
> very bad going on, we could set the default limit to e.g. 30seconds),
> however ...

I disagree. THis is pulling policy into the kernel and that just
generates problems. What might look like a reasonable timeout to some
workloads might be wrong for others.

> > I have seen an issue when the migration cannot make a forward progress
> > because of a glibc page with a reference count bumping up and down. Most
> > probable explanation is the faultaround code. I am working on this and
> > will post a patch soon. In any case the migration should converge and if
> > it doesn't do then there is a bug lurking somewhere.
> 
> ... I also agree that this should converge. And if we detect a serious
> issue that we can't handle/where we can't converge (e.g. -ENOMEM) we
> should abort.

As I've said ENOMEM can be considered a hard failure. We do not trigger
OOM killer when allocating migration target so we only rely on somebody
esle making a forward progress for us and that is suboptimal. Yet I
haven't seen this happening in hotplug scenarios so far. Making
hotremove while the memory is really under pressure is a bad idea in the
first place most of the time. It is quite likely that somebody else just
triggers the oom killer and the offlining part will eventually make a
forward progress.
> 
> > 
> > Failing on ENOMEM is a questionable thing. I haven't seen that happening
> > wildly but if it is a case then I wouldn't be opposed.
> > 
> >> You mentioned memory pressure, if our host is under memory pressure we
> >> can easily trigger running into an endless loop there, because we
> >> basically ignore -ENOMEM e.g. when we cannot get a page to migrate some
> >> memory to be offlined. I assume this is the case here.
> >> do_migrate_range() could be the bad boy if it keeps failing forever and
> >> we keep retrying.
> 
> I've seen quite some issues while playing with virtio-mem, but didn't
> have the time to look into the details. Still on my long list of things
> to look into.

Memory hotplug is really far away from being optimal and robust. This
has always been the case. Issues used to be workaround by retry limits
etc. If we ever want to make it more robust we have to bite a bullet and
actually chase all the issues that might be basically anywhere and fix
them. This is just a nature of a pony that memory hotplug is.
-- 
Michal Hocko
SUSE Labs
