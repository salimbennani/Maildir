Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 29 Nov 2018 19:48:49 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga007.fm.intel.com (fmsmga007.fm.intel.com [10.253.24.52])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 449FB5803ED;
	Thu, 29 Nov 2018 02:46:34 -0800 (PST)
Received: from fmsmga103.fm.intel.com ([10.1.193.90])
  by fmsmga007-1.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 29 Nov 2018 02:46:34 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AR2/pkRfIHp8FgUUopvxGo5vglGMj4u6mDksu8pMi?=
 =?us-ascii?q?zoh2WeGdxc6+bB2N2/xhgRfzUJnB7Loc0qyK6/CmATRIyK3CmUhKSIZLWR4BhJ?=
 =?us-ascii?q?detC0bK+nBN3fGKuX3ZTcxBsVIWQwt1Xi6NU9IBJS2PAWK8TW94jEIBxrwKxd+?=
 =?us-ascii?q?KPjrFY7OlcS30P2594HObwlSizexfbB/IA+qoQnNq8IbnZZsJqEtxxXTv3BGYf?=
 =?us-ascii?q?5WxWRmJVKSmxbz+MK994N9/ipTpvws6ddOXb31cKokQ7NYCi8mM30u683wqRbD?=
 =?us-ascii?q?VwqP6WACXWgQjxFFHhLK7BD+Xpf2ryv6qu9w0zSUMMHqUbw5Xymp4rx1QxH0li?=
 =?us-ascii?q?gIKz858HnWisNuiqJbvAmhrAF7z4LNfY2ZKOZycqbbcNgHR2ROQ9xRWjRODYOy?=
 =?us-ascii?q?bYQBD+QPM+VFoYfju1QOtgO+CAu3CePz1jNFnGP60bEm3+kjFwzNwQwuH8gJsH?=
 =?us-ascii?q?TRtNj6O6YSUeapw6bWyzXDc+5d1zbg6IjJbhAhvfaMXa5tesfW10kvFgXFgUmO?=
 =?us-ascii?q?pozjIzOZzOsNs3Wa7+p8SeKvjHInphp1ojiuwMcjkJPJhoUPxlDD7yV5z584KN?=
 =?us-ascii?q?ulQ0B4ed6pCIVcuz2eOodsX88vTX9ktDwnxrAFpZK3ZikHxZY/yxLBd/CKd5KE?=
 =?us-ascii?q?7xHjWeqLPDt1hXNodKiiixu2/0WtzPD3WNOu31ZQtCVFl8HBtnAT2BzX7ciKUu?=
 =?us-ascii?q?V9/ki/1jaVzQzT6f9LIVoylaXFL54t2LkwloAcsUjbHy/2nlv5jLOOe0k65uSl?=
 =?us-ascii?q?7/7rbqjoq5OCLYN4lwLzPrg0lsG+A+k0Kg0OUHKa+eS42r3j50r5QLBSg/0yk6?=
 =?us-ascii?q?nZto3aJMsCqq66HQBVyIAj5Ai7Dzu/19QZk38HI0xfeB+ckYjpNE/BIOriAfe8?=
 =?us-ascii?q?nVusijFryO7CPrH7BZXNNHfDnK/7fblh805c1BYzzddH6pJQC7EBI+z8VlX+td?=
 =?us-ascii?q?zFFRI5Nw20w+D6CNRyzI8eWGSPArOHP6PWq1OH+uUvI+yUbo8PpDn9M+Ql5+Lp?=
 =?us-ascii?q?jXIhmV8SZ6ip3YcNZ3C/BPhmI1iZbmDqgtcOCmoKugs+TOr3iFyNSzJTZnCyX7?=
 =?us-ascii?q?4i6TE/Eo6pEYDDRoW1irybwCi7BoFWZnxBCl2UE3focJuLV+0PaCKVJM9hlDsE?=
 =?us-ascii?q?WKOlS48g0xGuqQD7x6BmLurS5i0Xq5bj2MJp6O3UkBE47SZ0ANiF02GRU2F0mX?=
 =?us-ascii?q?sFSCUt3KB/pkx9yU2P0bJijPxaDtFT4/JJUgEnNZ/T1eB6CtbyWh7fcdeNUlqp?=
 =?us-ascii?q?XtKmATQpRNIr39AOe1p9G8mljh3b3CqlGbkVm6aPBJw16K3c2XfxKt15y3bH0q?=
 =?us-ascii?q?khklYnTtFONW2gmq5w6QzTC5TVnEWekqagbb4c0zLV9Gef0WqOu1lVXxNqXqXb?=
 =?us-ascii?q?Q38TfEvWos7/5kPZUbCuD7MrMg9Cyc6HLqtHcdnpjVRARPf+N9XSeWOxm2GsBR?=
 =?us-ascii?q?mWwrOAdpble2IY3C/FEkgLjxgT/WqaNQg5HiquvnjRDCJwGl71Y0Pj6+9+qGil?=
 =?us-ascii?q?QU8y1AyKa0xh17yo+h8an/CcSvUT3q4atyclsTl7AFG939fOAdqauwVhZLlcYc?=
 =?us-ascii?q?864FpfyWLZtgl9Ppu8L6Bihl8SaRh3s1np1xVtDoVAkM4qrHwxwQp2KKKY1k5B?=
 =?us-ascii?q?djyC0ZDxPL3XNnf9/BS1Z6HK3VHe1c6c+r0T5/Qgt1XjoAapG1I4/HVjzdZU3G?=
 =?us-ascii?q?WT55XQAAUJTJL+T1w49x55p7HdfCkw/IfU1XxqMampvT7OwdMpBO05yhm+e9dT?=
 =?us-ascii?q?Kr+LFAj3E8cCHcihNPQqm0S1bhIDJO1T9LM0M9m6ePec2a+rPPxvnDSpjWlc5I?=
 =?us-ascii?q?B900SM9zdzS+LS3pYFxe2Y0RWDVzvmkFihtcX3k5heZT4OBmq/1TTkBIlJa61o?=
 =?us-ascii?q?fIYEFX2hI9eqydV5nZLtXWBX9ESiB18fxMCmYx6SYEHj0g1K0kQXp2eqmS+5zz?=
 =?us-ascii?q?xyjjEoobCT3C3Iw+T+ahUHPnRHS3VljVfpOYK0lcwVXFC0bwg1kxuo/Ub7x6lB?=
 =?us-ascii?q?qKV/NWXTWlpIfy7tImFmU6uwsKeCYsFV5JMptyVXTPqzYVSARrHhpBsa1jvpH3?=
 =?us-ascii?q?FCyzAjazGqppL5kgR4iG2HNnZzr3nZecZqyRfE/tPcRv1R3jsARCZmjznaHVy8?=
 =?us-ascii?q?P9iv/dWJmJbPqOG+V2S9VpJNdSnn15+PtCy+5WdyGx2wg+izmsH7EQg9ySL618?=
 =?us-ascii?q?NlVSLSoBb+Y4nr0b+3MeZmfkluGV/95NB2GoB4kossmp4Q3WIWiYmS/XoCiW3z?=
 =?us-ascii?q?K8lU2bribHoRQj4G293V7xLk2EF5LnKJ2pj2VnOSwsZ6Y9m6Y2UW2j8y7sxQCa?=
 =?us-ascii?q?eU6qBEkjVxolaisQ3RZv19lC8HyfQy8H4an/0JuA01wyWYA7ASHlNXMTbilhuW?=
 =?us-ascii?q?9NC+sLtYZHy0freuzkp+ksusDLWDogFaRXb4dY0uHS527sVjLl3M1Gf/5Z3jeN?=
 =?us-ascii?q?nVddgTrAGbkw/cj+hJL5I8juYKhS1iOW7nvHwq0eg7jQF13ZGhvYiKMGFt/KO/?=
 =?us-ascii?q?Ah5FOTz5fcIT+jfxjalAmsaaxZygHpJkGj8TRpvnUeqoEC4OtfTgLwuBCyczqn?=
 =?us-ascii?q?CfGbrCBwOf7FpmomnLE5CqMXGXOXYYwc9jRBmbOExQngQUUC8mkZ4+EwCg3Nbh?=
 =?us-ascii?q?f1th5jAN+l74rQNByuByOBn6TGjfvx2kaisuRJicMRpW7RxC50HPPsyF9e9zGy?=
 =?us-ascii?q?BY/pu8rA2CMGCbZgJIDX0XVUyAHVzsIr6u5dzY+eiCGuW+N+fOYamJqeFGV/aI?=
 =?us-ascii?q?wo+v0pJ7/zmWMMWDJGJiD/o92kpMRn15H8XZmzMSSy0YjS7NbsibpAui9S1ztM?=
 =?us-ascii?q?yw7PPrWAf374uVF7RSKclv+wyxgaqbN+6fmid5KTVb1pMN33PIy6If3Fkdiy5w?=
 =?us-ascii?q?bTmtDK8AuDXJTKLRnK9XEhEaZzlyNMtO86IzwA1NNdTHhdPy075yluQ1BEtdVV?=
 =?us-ascii?q?z9hsGpYtQHLHugNFzcHkaEKrSHKSfPw8H2eq68TbxQjONJtxy/ozqbEknjPiid?=
 =?us-ascii?q?mDntTRygLeZMjCSDNhxEpI69agptCXTkTN/+dh27Mdp3gScqzrEumnzKNXATMT?=
 =?us-ascii?q?5nfkNJr72Q6z5Yg/plF2xA6HplMfeLmyKD4+bELZYWtONhAj5omOJC/HQ617xV?=
 =?us-ascii?q?4TlERfNvnivSq99uo1e+neiOyjpoShxOqjlQiYKPvEViP7jZ95ZaVXbF+hIN8X?=
 =?us-ascii?q?ufCxAQq9R5Dd3vvvMY9t+arKvvMjZJ8Jr+8M8YDtPVYJafOWY7MBHqXjTZCA8M?=
 =?us-ascii?q?Vj+DPHvawUdalafB2GeSq80Eq57nlZwSApBGVUApEe9SXlpkGtcPJosxUS4lja?=
 =?us-ascii?q?KckOYM5Hy3th6XQ99V6MOUHsmOCOnifW7KxYJPYAEFlPahddwe?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0BYAAChwv9bh0O0hNFlHAEBAQQBAQcEA?=
 =?us-ascii?q?QGBVAQBAQsBAYNqJ4N5lCCCDRRollqBYikTAYdyIjcGDQEDAQEBAQEBAgETAQE?=
 =?us-ascii?q?BCA0JCCkjDEIBEAGBYiQBgmEBAQEBAgEBAgsVHQEBIxQBBQkBAQoLDQICERUCA?=
 =?us-ascii?q?gMfEgEFARwGEwUUgwiBeggFmnQ8ih1wgS+CdgEBBYcoCBJ5iwuBVz+BEYIUSTW?=
 =?us-ascii?q?EPBFzgkWCV4kFBEyBRpUKBwKCHQSPERiBWo9CiXuLZy6CLTCBO4F3MxowdAaCN?=
 =?us-ascii?q?YIbDBeIXoVAPjOBBQEBhmaGUgEB?=
X-IPAS-Result: =?us-ascii?q?A0BYAAChwv9bh0O0hNFlHAEBAQQBAQcEAQGBVAQBAQsBAYN?=
 =?us-ascii?q?qJ4N5lCCCDRRollqBYikTAYdyIjcGDQEDAQEBAQEBAgETAQEBCA0JCCkjDEIBE?=
 =?us-ascii?q?AGBYiQBgmEBAQEBAgEBAgsVHQEBIxQBBQkBAQoLDQICERUCAgMfEgEFARwGEwU?=
 =?us-ascii?q?UgwiBeggFmnQ8ih1wgS+CdgEBBYcoCBJ5iwuBVz+BEYIUSTWEPBFzgkWCV4kFB?=
 =?us-ascii?q?EyBRpUKBwKCHQSPERiBWo9CiXuLZy6CLTCBO4F3MxowdAaCNYIbDBeIXoVAPjO?=
 =?us-ascii?q?BBQEBhmaGUgEB?=
X-IronPort-AV: E=Sophos;i="5.56,294,1539673200"; 
   d="scan'208";a="53819735"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 29 Nov 2018 02:46:32 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1727787AbeK2VsU (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Thu, 29 Nov 2018 16:48:20 -0500
Received: from mail-it1-f193.google.com ([209.85.166.193]:33392 "EHLO
        mail-it1-f193.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726879AbeK2VsU (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Thu, 29 Nov 2018 16:48:20 -0500
Received: by mail-it1-f193.google.com with SMTP id m8so7091068itk.0
        for <linux-kernel@vger.kernel.org>; Thu, 29 Nov 2018 02:43:23 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=linaro.org; s=google;
        h=mime-version:references:in-reply-to:from:date:message-id:subject:to
         :cc;
        bh=W43hL2jxdV8AviS5wnWHYTM9XjIWPe0lzZEOvGfc1wE=;
        b=Ua1Pr3tSIgzmkZSoujH70qmcJOcIvA3QzeNR0GqoXVl8FbIJyyiBdP1k5KQNpvNRBZ
         wlYZklPBe/IlTHOXFXo8WQlGIapQUPBOalR8Eq+Gikh8Xf3oE5iAnuhLuofhTj+5ITF7
         vbuHTpKlGt1lbIHV521bUlUmd/u3SnCS9udsE=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:mime-version:references:in-reply-to:from:date
         :message-id:subject:to:cc;
        bh=W43hL2jxdV8AviS5wnWHYTM9XjIWPe0lzZEOvGfc1wE=;
        b=dd/rHa72BdyHHOXlbx/zKex8sG7qkg25OuzF7FYDL8YGZEcqN0gaESgIr7jRZDyQRe
         tSKp9RfSb346MwFEkdzGDUnT/MPFrZANTWUyY1DpTvvG7Zfc3+HABot+zQkxDQZ4RK9l
         VJ5odHJXWNQBQkccPDltrcATZGjGIPsXNM9/z8TQGaj46nmkN2DOn7CRfeNNHjC/EUbH
         g9sLnaLqJ724/Xt2FJziDCmkeT0Eo09kKjAL7EyaeXAxJMHtNu9MobRUVYFcTQ+EWxtR
         bI3zVsmozgLFrWKRVud/6I18Ui1wD28dO3dSB7iOfwzC2k6MtZCT+SiC8Or6iR2ixUmo
         lhFQ==
X-Gm-Message-State: AA+aEWYd/FpmgwAmdfdZ52GB7M9aY2bPEdnN1CG/lfMLbIrLrZcBwAPz
        TCs4cB3q1F102+JQBpjyPy7IaTwO+5hVdBwJ49WUIg==
X-Google-Smtp-Source: AFSGD/UQ9RxgYaZGivOr3mJrEfoYd4l+d2efCy5VqAqLzyFeFWXRdDmSCBWwcNHkr94P5rMk6WeGcmJRqQTqL3933HU=
X-Received: by 2002:a24:6f42:: with SMTP id x63mr979083itb.152.1543488203263;
 Thu, 29 Nov 2018 02:43:23 -0800 (PST)
MIME-Version: 1.0
References: <1542711308-25256-1-git-send-email-vincent.guittot@linaro.org>
 <1542711308-25256-3-git-send-email-vincent.guittot@linaro.org>
 <CAKfTPtD=sV3zJiZMfCFi92_f6j-jTO9D5RsEBAXHVa6VN3Urwg@mail.gmail.com>
 <20181128100241.GA2131@hirez.programming.kicks-ass.net> <20181128115336.GB23094@e110439-lin>
 <CAKfTPtBsKc7v5gc=XUrzO-_4kahGfdNteo=t9W5fLv0Ee8co_w@mail.gmail.com>
 <20181128144039.GC23094@e110439-lin> <CAKfTPtAR7otTTwKYbg5OWbgrUYNKBNsUnOcMS9CfQtbYspvO5A@mail.gmail.com>
 <20181128152133.GD23094@e110439-lin> <CAKfTPtDTSXiVsPepiJDFT9ZCXfUVYTQhRvgfZbF=_w87girPAQ@mail.gmail.com>
 <20181128163545.GE23094@e110439-lin>
In-Reply-To: <20181128163545.GE23094@e110439-lin>
From: Vincent Guittot <vincent.guittot@linaro.org>
Date: Thu, 29 Nov 2018 11:43:11 +0100
Message-ID: <CAKfTPtAd8V=Qst82HPb1HW7ZdvrHGgaVhZ=rAdzbHNednp+ztg@mail.gmail.com>
Subject: Re: [PATCH v7 2/2] sched/fair: update scale invariance of PELT
To: Patrick Bellasi <patrick.bellasi@arm.com>
Cc: Peter Zijlstra <peterz@infradead.org>,
        Ingo Molnar <mingo@kernel.org>,
        linux-kernel <linux-kernel@vger.kernel.org>,
        "Rafael J. Wysocki" <rjw@rjwysocki.net>,
        Dietmar Eggemann <dietmar.eggemann@arm.com>,
        Morten Rasmussen <Morten.Rasmussen@arm.com>,
        Paul Turner <pjt@google.com>, Ben Segall <bsegall@google.com>,
        Thara Gopinath <thara.gopinath@linaro.org>,
        pkondeti@codeaurora.org, Quentin Perret <quentin.perret@arm.com>,
        Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
Content-Type: text/plain; charset="UTF-8"
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On Wed, 28 Nov 2018 at 17:35, Patrick Bellasi <patrick.bellasi@arm.com> wrote:
>
> On 28-Nov 16:42, Vincent Guittot wrote:
> > On Wed, 28 Nov 2018 at 16:21, Patrick Bellasi <patrick.bellasi@arm.com> wrote:
> > >
> > > On 28-Nov 15:55, Vincent Guittot wrote:
> > > > On Wed, 28 Nov 2018 at 15:40, Patrick Bellasi <patrick.bellasi@arm.com> wrote:
> > > > >
> > > > > On 28-Nov 14:33, Vincent Guittot wrote:
> > > > > > On Wed, 28 Nov 2018 at 12:53, Patrick Bellasi <patrick.bellasi@arm.com> wrote:
> > > > > > >
> > > > > > > On 28-Nov 11:02, Peter Zijlstra wrote:
> > > > > > > > On Wed, Nov 28, 2018 at 10:54:13AM +0100, Vincent Guittot wrote:
> > > > > > > >
> > > > > > > > > Is there anything else that I should do for these patches ?
> > > > > > > >
> > > > > > > > IIRC, Morten mention they break util_est; Patrick was going to explain.
> > > > > > >
> > > > > > > I guess the problem is that, once we cross the current capacity,
> > > > > > > strictly speaking util_avg does not represent anymore a utilization.
> > > > > > >
> > > > > > > With the new signal this could happen and we end up storing estimated
> > > > > > > utilization samples which will overestimate the task requirements.
> > > > > > >
> > > > > > > We will have a spike in estimated utilization at next wakeup, since we
> > > > > > > use MAX(util_avg@dequeue_time, ewma). Potentially we also inflate the EWMA in
> > > > > > > case we collect multiple samples above the current capacity.
> > > > > >
> > > > > > TBH I don't see how it's different from current implementation with a
> > > > > > task that was scheduled on big core and now wakes up on little core.
> > > > > > The util_est is overestimated as well.
> > > > >
> > > > > While running below the capacity of a CPU, either big or LITTLE, we
> > > > > can still measure the actual used bandwidth as long as we have idle
> > > > > time. If the task is then moved into a lower capacity core, I think
> > > > > it's still safe to assume that, likely, it would need more capacity.
> > > > >
> > > > > Why do you say it's the same ?
> > > >
> > > > In the example of a task that runs 39ms in period of 80ms that we used
> > > > during previous version,
> > > > the utilization on the big core will reach 709 so will util_est too
> > > > When the task migrates on little core (512), util_est is higher than
> > > > current cpu capacity
> > >
> > > Right, and what's the problem ?
> >
> > you worry about an util_est being higher than capacity which is the case there
>
> I worry about util_est being higher then the capacity the task WAS
> running... not the capacity the task IS running... if that value does
> not correspond to what the task really need... (more on that at the
> end).
>
> > > 1) We know that PELT is calibrated to 32ms period task and in your
> > >    example, since the runtime is higher then the half-life, it's
> > >    correct to estimate a utilization higher then 50%.
> > >
> > >    PELT utilization is defined _based on the half-life_: thus
> > >    your task having a 50% duty cycle does not mean we are not correct
> > >    if report a utilization != 50%.
> > >    It would be as broken as reporting 10% utilization for a task
> > >    running 100ms every 1s.
> > >
> > > 2) If it was a 70% task on a previous activation, once it's moved into
> > >    a lower capacity CPU it's still correct to assume that it's likely
> > >    going to require the same bandwidth and thus will be
> > >    under-provisioned.
> > >
> > > I still don't see where we are wrong in this case :/
> > >
> > > To me it looks different then the problem I described.
> > >
> > > > > With your new signal instead, once we cross the current capacity,
> > > > > utilization is just not anymore utilization. Thus, IMHO it make sense
> > > > > avoid to accumulate a sample for what we call "estimated utilization".
> >
> > This is not true. With the example above, the util_est will be exactly the same
> >  on big and little cores with the new signal
>
> ... AFAIU only if we have idle time...
>
> > > > > I would also say that, with the current implementation which caps
> > > > > utilization to the current capacity, we get better estimation in
> > > > > general. At least we can say with absolute precision:
> > > > >
> > > > >    "the task needs _at least_ that amount of capacity".
> > > > >
> > > > > Potentially we can also flag the task as being under-provisioned, in
> > > > > case there was not idle time, and _let a policy_ decide what to do
> > > > > with it and the granted information we have.
> > > > >
> > > > > While, with your new signal, once we are over the current capacity,
> > > > > the "utilization" is just a sort of "random" number at best useful to
> > > > > drive some conclusions about how long the task has been delayed.
> >
> > see my comment above
> >
> > > > >
> > > > > IOW, I fear that we are embedding a policy within a signal which is
> > > > > currently representing something very well defined: how much cpu
> > > > > bandwidth a task used. While, latency/under-provisioning policies
> > > > > perhaps should be better placed somewhere else.
> > > > >
> > > > > Perhaps I've missed it in some of the previous discussions:
> > > > > have we have considered/discussed this signal-vs-policy aspect ?
> > >
> > > What's your opinion on the above instead ?
> >
> > It's not a policy but it gives better knowledge about the amount a work done
> > I have put below discussion on the  subject on previous version
>
> Thanks, I think I've skimmed through it, but it's sill useful...
>
> > > > With contribution scaling the PELT utilization of a task is a _minimum_
> > > > utilization. Regardless of where the task is currently/was running (and
> > > > provided that it doesn't change behaviour) its PELT utilization will
> > > > approximate its _minimum_ utilization on an idle 1024 capacity CPU.
> > >
> > > The main drawback is that the _minimum_ utilization depends on the CPU
> > > capacity on which the task runs. The two 25% tasks on a 256 capacity
> > > CPU will have an utilization of 128 as an example
> > >
> > > >
> > > > With time scaling the PELT utilization doesn't really have a meaning on
> > > > its own. It has to be compared to the capacity of the CPU where it
> > > > is/was running to know what the its current PELT utilization means. When
> > >
> > > I would have said the opposite. The utilization of the task will
> > > always reflect the same amount of work that has been already done
> > > whatever the CPU capacity.
> > > In fact, the new scaling mechanism uses the real amount of work that
> > > has been already done to compute the utilization signal which is not
> > > the case currently. This gives more information about the real amount
> > > of worked that has been computed in the over utilization case.
> > >
> > > > the utilization over-shoots the capacity its value is no longer
> > > > represents utilization, it just means that it has a higher compute
> > > > demand than is offered on its current CPU and a high value means that it
> > > > has been suffering longer. It can't be used to predict the actual
> > > > utilization on an idle 1024 capacity any better than contribution scaled
> > > > PELT utilization.
> > >
> > > I think that it provides earlier detection of over utilization and
> > > more accurate signal for a longer time duration which can help the
> > > load balance
> > > Coming back to 50% task example . I will use a 50ms running time
> > > during a 100ms period for the example below to make it easier
> > >
> > > Starting from 0, the evolution of the utilization is:
> > >
> > > With contribution scaling:
> > >          time  0ms  50ms  100ms  150ms  200ms
> > > capacity
> > > 1024           0    666
> > > 512            0    333   453
> > > When the CPU start to be over utilized (@100ms), the utilization is
> > > already too low (453 instead of 666) and scheduler doesn't detect yet
> > > that we are over utilized
> > > 256            0    169   226    246    252
> > > That's even worse with this lower capacity
> > >
> > > With time scaling,
> > >          time  0ms  50ms  100ms  150ms  200ms
> > > capacity
> > > 1024           0    666
> > > 512            0    428   677
> > > We know that the current capacity is not enough and the utilization
> > > reflect the correct utilization level compare to 1024 capacity (the
> > > 666 vs 677 difference comes from the 1024us window so the last window
> > > is not full in the case of max capacity)
> > > 256            0    234   468    564    677
> > > At 100ms, we know that there is not enough capacity. (In fact we know
> > > that at 56ms). And even at time 200ms, the amount of work is exactly
> > > what would have been executed on a CPU 4x faster
> > >
> > > >
> > > > This change might not be a showstopper, but it is something to be aware
> > > > off and take into account wherever PELT utilization is used.
> > >
> > > The point above is clearly a big difference between the 2 approaches
> > > of the no spare cycle case but I think it will help by giving more
> > > information in the over utilization case
>
> I like the idea that we ramp up faster and always get to the same
> value. I like also the idea that we always reach the same value on
> both LITTLE and big.
>
> As long as there is idle time this is working fine, in these cases we
> should probably also collect util_est samples.
>
> But what happens when we don't have idle time ?

As shown above, the utilization stays correct for a longer time frame
even after the over utilization point and provides better over
utilization detection

>
> Let say we have 2 15% tasks, co-scheduled on a cpu with <300 capacity.
>
> Are not these two tasks being reported as 50% tasks (after a while) ?

Yes they will but similarly to above they will stay correct for longer
time even when they become higher than current cpu capacity


>
> If that's the case, these are samples we should not store...
>
> --
> #include <best/regards.h>
>
> Patrick Bellasi
