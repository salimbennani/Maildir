Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 29 Nov 2018 08:38:05 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga004.jf.intel.com (orsmga004.jf.intel.com [10.7.209.38])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 3E36A5802E4;
	Wed, 28 Nov 2018 07:42:39 -0800 (PST)
Received: from fmsmga105.fm.intel.com ([10.1.193.10])
  by orsmga004-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 28 Nov 2018 07:42:38 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3A2a1TgRGdqjk0nDIrNxAoKJ1GYnF86YWxBRYc798d?=
 =?us-ascii?q?s5kLTJ75o8SzbnLW6fgltlLVR4KTs6sC17KG9fi4EUU7or+5+EgYd5JNUxJXwe?=
 =?us-ascii?q?43pCcHRPC/NEvgMfTxZDY7FskRHHVs/nW8LFQHUJ2mPw6arXK99yMdFQviPgRp?=
 =?us-ascii?q?OOv1BpTSj8Oq3Oyu5pHfeQpFiCa+bL9oMBm6sRjau9ULj4dlNqs/0AbCrGFSe+?=
 =?us-ascii?q?RRy2NoJFaTkAj568yt4pNt8Dletuw4+cJYXqr0Y6o3TbpDDDQ7KG81/9HktQPC?=
 =?us-ascii?q?TQSU+HQRVHgdnwdSDAjE6BH6WYrxsjf/u+Fg1iSWIdH6QLYpUjm58axlVAHnhz?=
 =?us-ascii?q?sGNz4h8WHYlMpwjL5AoBm8oxBz2pPYbJ2JOPZ7eK7WYNEUSndbXstJWCNBDIGz?=
 =?us-ascii?q?YYsBAeQCIOhWsZXyp0AWrRa8HgSsGP/jxyVUinPqwaE30eIsGhzG0gw6GNIOtW?=
 =?us-ascii?q?zZo9r0NKcUTe+60q/IzSneZP1XxDf96ZTIcgwmofGQQLl9dtTRyUgpFwzZkFqQ?=
 =?us-ascii?q?r5DlMyma1uQQsmib8/ZgVeWzi2M8rwFxoz6vyd02ionOnI4VzUrE9SpgzYszON?=
 =?us-ascii?q?a2S1Z7bMa6HJdMsyyWLZZ6T808T21ypSo3yaEKtYS6cSUI0Jgr2QLTZvidf4WL?=
 =?us-ascii?q?4h/vTvudLDZ5iX5/Zb6ygxe//E69wePmTMa0ykxFri9dn9nMqH8N0xvT59CZSv?=
 =?us-ascii?q?ty4EihwyyD1wPN5eFeJ0A7i67bJ4Qmwr4qmZofqUXDHinol0XqlKKaaFko9+yy?=
 =?us-ascii?q?5+j6bLjquIWQO5J3hw3iKKgjm86yDfw9MgcUXmib/eq81Kfk/U38WLhKivw2kq?=
 =?us-ascii?q?/EsJHVPMgbpbC2AxVT0ok97xazFjCm0doenXYZNlJIYwyHj4f3NFHUOvz4Dumw?=
 =?us-ascii?q?g06qkDh1w/DKJLrhAo/CLnTbirfuYa5961JAyAo01d1f45NUCrIfL/7pVU7xqc?=
 =?us-ascii?q?fVDhs4Mwyy3ubmB89x1oIYWWKTHKCZNLnevkOP5uIqO+OMfpMauC7hK/g54P7j?=
 =?us-ascii?q?lX85lkUcfam1x5QXb2q0HvR7I0qDZ3rsjcwMEWMLvgo4Uezrh0eOUT9VZ3auQa?=
 =?us-ascii?q?084is3B56hDYfGXoqtmqCO3D+nHp1KYWBLElKMEXD2eImeWPcMbySSIslmkjEf?=
 =?us-ascii?q?UbihSokh1QyhtQPgyrpnKPbU9TMctZ75yNd14OjTnwko9TNoF8Sdz32NT2Zsk2?=
 =?us-ascii?q?wSXD823Kd/oU9nxleZy6d4gedVFdhS5/NPTwc7OoTQz+18C9DuRA3Bes2FR0qh?=
 =?us-ascii?q?QtWjGTsxVM4+w8cSY0ZhHNWvlhPD0DCsA7MPk7yLBYY78qTT33XqI8Z9ynDG1L?=
 =?us-ascii?q?QujlU8Q8tPM3GmibB79wTJG4HJlECZnb6wdasAxC7N6HuDzW2WsU5FSgFwT7vK?=
 =?us-ascii?q?UWofZkTMq9T5/V3NT7mpBbQjMQtBzMqCJ7BOat3oi1VGWfjiNM7fY2K3h2e/Gx?=
 =?us-ascii?q?KIyqmQY4rtfmUXxD/dB1QckwAP4XaGMhAzCTq7rGLAEjNiD1LvbFnq8elltny7?=
 =?us-ascii?q?SFQ5zwWLb01nyrq09QQZhf2aS/MPwL0EvD0tpClzHFa4x9jWEcaPpxJ9fKVAZt?=
 =?us-ascii?q?Mw+EtH1WPctwx6IpygL6BjikQCcwhtuEPuzRF3CoRGkcglt34qyAtyKaSF0FJO?=
 =?us-ascii?q?bT+Y3JbwOqHJJWn25hygd6nW2lTG2taM5qgP8Og4q0nkvAyxDEUi92to0tZL3H?=
 =?us-ascii?q?qc/JnFFxcSXo/rXUYx9Bh6oK/abzI554PV031sLKa1viXD29IvGOsq1BKgc81D?=
 =?us-ascii?q?P6ODEQ/4C9caCNS2KOw2h1ipaQoJPeBP9KIuI8yqbfqH2K6xM+ZmkzKrlmBH4I?=
 =?us-ascii?q?F70kKR+CtwUO/I35AZw/6G2guLTSvzjFCkssrvg4BLeSkSHnajySjjHINRfKxy?=
 =?us-ascii?q?cpoRBme0PsK3wc9yh5jzVH5C9V6vHk8J19WteRWPcVP9xwpQ1UIMrHyjmCu4yS?=
 =?us-ascii?q?F0kj4zoqqe2izO3/rtdB4dNmFXQ2liiE/mIZKogNACQEiocw8pmQOl5Eb73aRa?=
 =?us-ascii?q?pL5zL3LOQUdOZCX2K2BiUq2ttruNecJP6ZUosTlJX+S4e1yVVrn9ox4C2SP5A2?=
 =?us-ascii?q?Re3Cw7dy2tupjhnRx1knidLHVwrHreY81w3g3Q5N/fRf5Q2DoJWi94hCLTBli9?=
 =?us-ascii?q?Odmp4NqVm43CsuC4S2KuSJlTfTP3woOHsSux/XdqDgGnn/Cvht3nFhA30C/h2N?=
 =?us-ascii?q?ltTynIrhf8Yo/w2qS+MOJneFRoBVDm58p7HIF+jpU/hJUK1XcGgZWV+GINkX3v?=
 =?us-ascii?q?PtVDxaL+cH0NSCYIw9HP4Qjpwk1jLm+TyIL/WXWQ2c9hZ9i8YmML1SMx9cFKCK?=
 =?us-ascii?q?GI7LNamSt5uEa3rQXUYfJlhDcS1eMu6GIGg+EOoActyySdArMIEkVCMy3jiQ+I?=
 =?us-ascii?q?78ygo6VNf2mgb6Kw1EVlkNClDbGCpBxcWXnjdpcjGy9w8ttwMFbW3HLv7YHkfc?=
 =?us-ascii?q?HabcgPuR2MjxfAk+9VJYotlvoNgCprI2P8smcjy+4mlhxuxpC6sZODK2Vs+qK5?=
 =?us-ascii?q?HxFZOif0Z8MV5jHik6JektyK0ICoG5VrAi8LU4fwTfK0DDISsuzqOB2UHz07rn?=
 =?us-ascii?q?ebBKDTHQuC6Ep9q3LPEparN2yYJXUDzNViQgWdK1JbgAwOQDo6mZs5HBiwxMP9?=
 =?us-ascii?q?aEd5+iwR5lngpxtO0O1oMgfwUnzFqAe0cDs0S4WfLBlL7gFE/UjVKteT7uZyHy?=
 =?us-ascii?q?Fe452goxaBKm2dZwRUE24JXlaICEzkPrmr/dPA6fSXBvKiL/vSZrWDsexfV/CV?=
 =?us-ascii?q?xZK2z4tp5TCMOt+UPnllCf071VFOXXR4G8TfhjUORDYblyPLb86Huhi8/jd7od?=
 =?us-ascii?q?y48PTuQAjv/5eAC6NOMdVz/BC7maeDOPCRhCZ6KjZY14kAxXzSyLgY014dlTtu?=
 =?us-ascii?q?dyS2EbkbsS7NTaTQmrJYDhIBaiNzMtdI4Lw43gVXJcHbjdb13KZijvEpE1dFSU?=
 =?us-ascii?q?DhmsaxaMMQPmGyL0nHC1iLNbicIT3Lwtr6Yae9Sb1WkeVVuAe8uTedE0//IDuD?=
 =?us-ascii?q?kyPlWAyoMeFJlCubJgBRuJmhchZxDmjuVM7mahy+MNNtkTI626E7hnPUOm4aKj?=
 =?us-ascii?q?V8d0JNrruN7SJXmPl/GmpB7mZ7IumAgSqW8+7YKpMOu/txHit0j/5a4Gg9y7ZN?=
 =?us-ascii?q?7CBLXvp1mC7Trt5ou16nk+mPxSBhUBpBsTtLgIOLvUN/OaTW7JVAWHDE/A4T4m?=
 =?us-ascii?q?WUERgFu9xlCti885xXn+DGiLjyIzQK2d/Q+cIHBoCAMMubKnsqMlzjFTrQBRAE?=
 =?us-ascii?q?ZTexMCfUgEkLw9+I8XjAkJk8oZXqhNIhV75HSFspXqcEC0JuEdoeZphqXy84lq?=
 =?us-ascii?q?KziM8O7GC56h7LS5MJ7dj8SvuODKC3e36ihr5eak5NmOugIA=3D=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0BAAADAtv5bh0O0hNFkHAEBAQQBAQcEA?=
 =?us-ascii?q?QGBVAQBAQsBgVqCESeDeZQhgg0UaJZagV8sEwGHbiI3Bg0BAwEBAQEBAQIBEwE?=
 =?us-ascii?q?BAQgNCQgpIwyCNiQBgmEBAQEBAgEBAgsVHQEBIxQBBQkBAQoLDQICERUCAgMfE?=
 =?us-ascii?q?gEFARwGEwUUgwiBeggFmyU8ih1wgS+CdgEBBYc3CBJ5iwuBVz+BEYIUSTWEPBF?=
 =?us-ascii?q?zgkWCV4kDBEyBRpUEBwKCHASPERiBWo87iXmLZS6CLTCBO4F3MxowdAaCNYIbD?=
 =?us-ascii?q?Bd/AQ6HUIVAPjOBBQEBjTMBAQ?=
X-IPAS-Result: =?us-ascii?q?A0BAAADAtv5bh0O0hNFkHAEBAQQBAQcEAQGBVAQBAQsBgVq?=
 =?us-ascii?q?CESeDeZQhgg0UaJZagV8sEwGHbiI3Bg0BAwEBAQEBAQIBEwEBAQgNCQgpIwyCN?=
 =?us-ascii?q?iQBgmEBAQEBAgEBAgsVHQEBIxQBBQkBAQoLDQICERUCAgMfEgEFARwGEwUUgwi?=
 =?us-ascii?q?BeggFmyU8ih1wgS+CdgEBBYc3CBJ5iwuBVz+BEYIUSTWEPBFzgkWCV4kDBEyBR?=
 =?us-ascii?q?pUEBwKCHASPERiBWo87iXmLZS6CLTCBO4F3MxowdAaCNYIbDBd/AQ6HUIVAPjO?=
 =?us-ascii?q?BBQEBjTMBAQ?=
X-IronPort-AV: E=Sophos;i="5.56,291,1539673200"; 
   d="scan'208";a="139927655"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 28 Nov 2018 07:42:37 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1728798AbeK2Coh (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Wed, 28 Nov 2018 21:44:37 -0500
Received: from mail-it1-f196.google.com ([209.85.166.196]:52647 "EHLO
        mail-it1-f196.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1728217AbeK2Cof (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Wed, 28 Nov 2018 21:44:35 -0500
Received: by mail-it1-f196.google.com with SMTP id i7so5032693iti.2
        for <linux-kernel@vger.kernel.org>; Wed, 28 Nov 2018 07:42:30 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=linaro.org; s=google;
        h=mime-version:references:in-reply-to:from:date:message-id:subject:to
         :cc;
        bh=0uUjT4ItIRh9y4vgEIOefwEK1rUf8/o5NSvJYCDiNqg=;
        b=QpI5fRTgwcA6NW7hv+ad1nIkjnv/E07MUMhqrYOupClaUNVqBfqhQhiw8h1wDDaTg7
         cYCtAkJ5Nu75HeSCnGf1/VxPCJaU3Fu1wGWMOViprECUJckNpSrEKAs2+0o26Ym9mtQT
         tNZ9CwCy+lxtMSde5bh27KnSAiqT1rJqDhXag=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:mime-version:references:in-reply-to:from:date
         :message-id:subject:to:cc;
        bh=0uUjT4ItIRh9y4vgEIOefwEK1rUf8/o5NSvJYCDiNqg=;
        b=EScK9OTPhEcYPwwXIhUfDfiS5eCg8quE6Tx6EhIbfRPts8EX+lT2aQh4Tfu4UU/dhs
         Iv0gu5nQy2v/y5SBHRsAQom3iEWgp1Zl/O9MilFkdNTMKJGPX7N2VDXq8MlVbu+qAbNK
         Maknvt1SwleBz/1iYC7r5tvrsYKLimkz+185ye5JbQXRiC2uY083pn7MvInIHfuP7nGu
         vE2RfhSrjJR3A2RYJeNV7VSbEdQCFHAd6HPNEkEThWC+UbHvzbJBMnq9J5u8x/4gpHjx
         pSQjFE60ouOoFoHYtrPlktvufVoqFa1Sm12Sy0Hh0HMEtu8zfFa6G+KPvMPBN/6l7KZl
         ghmA==
X-Gm-Message-State: AA+aEWYs+FPppEkIbJwrIXJNUgkxkUls+e4qwpCOy15qqk1t49HIs5yj
        UMTR0S5Vfknw4XGDvlfN7X+IfCdNmQw5jyJrMlZBRA==
X-Google-Smtp-Source: AJdET5c38JVFLP6Qfaw7SlhN35A+IKNZSWmquSH+ppYBloqNRAzRG0Nb/J+ZOY8PYHb5QxKHHRV4bsW9t4BveVj2lVc=
X-Received: by 2002:a02:6019:: with SMTP id i25mr34247138jac.137.1543419749929;
 Wed, 28 Nov 2018 07:42:29 -0800 (PST)
MIME-Version: 1.0
References: <1542711308-25256-1-git-send-email-vincent.guittot@linaro.org>
 <1542711308-25256-3-git-send-email-vincent.guittot@linaro.org>
 <CAKfTPtD=sV3zJiZMfCFi92_f6j-jTO9D5RsEBAXHVa6VN3Urwg@mail.gmail.com>
 <20181128100241.GA2131@hirez.programming.kicks-ass.net> <20181128115336.GB23094@e110439-lin>
 <CAKfTPtBsKc7v5gc=XUrzO-_4kahGfdNteo=t9W5fLv0Ee8co_w@mail.gmail.com>
 <20181128144039.GC23094@e110439-lin> <CAKfTPtAR7otTTwKYbg5OWbgrUYNKBNsUnOcMS9CfQtbYspvO5A@mail.gmail.com>
 <20181128152133.GD23094@e110439-lin>
In-Reply-To: <20181128152133.GD23094@e110439-lin>
From: Vincent Guittot <vincent.guittot@linaro.org>
Date: Wed, 28 Nov 2018 16:42:18 +0100
Message-ID: <CAKfTPtDTSXiVsPepiJDFT9ZCXfUVYTQhRvgfZbF=_w87girPAQ@mail.gmail.com>
Subject: Re: [PATCH v7 2/2] sched/fair: update scale invariance of PELT
To: Patrick Bellasi <patrick.bellasi@arm.com>
Cc: Peter Zijlstra <peterz@infradead.org>,
        Ingo Molnar <mingo@kernel.org>,
        linux-kernel <linux-kernel@vger.kernel.org>,
        "Rafael J. Wysocki" <rjw@rjwysocki.net>,
        Dietmar Eggemann <dietmar.eggemann@arm.com>,
        Morten Rasmussen <Morten.Rasmussen@arm.com>,
        Paul Turner <pjt@google.com>, Ben Segall <bsegall@google.com>,
        Thara Gopinath <thara.gopinath@linaro.org>,
        pkondeti@codeaurora.org, Quentin Perret <quentin.perret@arm.com>,
        Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
Content-Type: text/plain; charset="UTF-8"
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On Wed, 28 Nov 2018 at 16:21, Patrick Bellasi <patrick.bellasi@arm.com> wrote:
>
> On 28-Nov 15:55, Vincent Guittot wrote:
> > On Wed, 28 Nov 2018 at 15:40, Patrick Bellasi <patrick.bellasi@arm.com> wrote:
> > >
> > > On 28-Nov 14:33, Vincent Guittot wrote:
> > > > On Wed, 28 Nov 2018 at 12:53, Patrick Bellasi <patrick.bellasi@arm.com> wrote:
> > > > >
> > > > > On 28-Nov 11:02, Peter Zijlstra wrote:
> > > > > > On Wed, Nov 28, 2018 at 10:54:13AM +0100, Vincent Guittot wrote:
> > > > > >
> > > > > > > Is there anything else that I should do for these patches ?
> > > > > >
> > > > > > IIRC, Morten mention they break util_est; Patrick was going to explain.
> > > > >
> > > > > I guess the problem is that, once we cross the current capacity,
> > > > > strictly speaking util_avg does not represent anymore a utilization.
> > > > >
> > > > > With the new signal this could happen and we end up storing estimated
> > > > > utilization samples which will overestimate the task requirements.
> > > > >
> > > > > We will have a spike in estimated utilization at next wakeup, since we
> > > > > use MAX(util_avg@dequeue_time, ewma). Potentially we also inflate the EWMA in
> > > > > case we collect multiple samples above the current capacity.
> > > >
> > > > TBH I don't see how it's different from current implementation with a
> > > > task that was scheduled on big core and now wakes up on little core.
> > > > The util_est is overestimated as well.
> > >
> > > While running below the capacity of a CPU, either big or LITTLE, we
> > > can still measure the actual used bandwidth as long as we have idle
> > > time. If the task is then moved into a lower capacity core, I think
> > > it's still safe to assume that, likely, it would need more capacity.
> > >
> > > Why do you say it's the same ?
> >
> > In the example of a task that runs 39ms in period of 80ms that we used
> > during previous version,
> > the utilization on the big core will reach 709 so will util_est too
> > When the task migrates on little core (512), util_est is higher than
> > current cpu capacity
>
> Right, and what's the problem ?

you worry about an util_est being higher than capacity which is the case there

>
> 1) We know that PELT is calibrated to 32ms period task and in your
>    example, since the runtime is higher then the half-life, it's
>    correct to estimate a utilization higher then 50%.
>
>    PELT utilization is defined _based on the half-life_: thus
>    your task having a 50% duty cycle does not mean we are not correct
>    if report a utilization != 50%.
>    It would be as broken as reporting 10% utilization for a task
>    running 100ms every 1s.
>
> 2) If it was a 70% task on a previous activation, once it's moved into
>    a lower capacity CPU it's still correct to assume that it's likely
>    going to require the same bandwidth and thus will be
>    under-provisioned.
>
> I still don't see where we are wrong in this case :/
>
> To me it looks different then the problem I described.
>
> > > With your new signal instead, once we cross the current capacity,
> > > utilization is just not anymore utilization. Thus, IMHO it make sense
> > > avoid to accumulate a sample for what we call "estimated utilization".

This is not true. With the example above, the util_est will be exactly the same
 on big and little cores with the new signal

> > >
> > > I would also say that, with the current implementation which caps
> > > utilization to the current capacity, we get better estimation in
> > > general. At least we can say with absolute precision:
> > >
> > >    "the task needs _at least_ that amount of capacity".
> > >
> > > Potentially we can also flag the task as being under-provisioned, in
> > > case there was not idle time, and _let a policy_ decide what to do
> > > with it and the granted information we have.
> > >
> > > While, with your new signal, once we are over the current capacity,
> > > the "utilization" is just a sort of "random" number at best useful to
> > > drive some conclusions about how long the task has been delayed.

see my comment above

> > >
> > > IOW, I fear that we are embedding a policy within a signal which is
> > > currently representing something very well defined: how much cpu
> > > bandwidth a task used. While, latency/under-provisioning policies
> > > perhaps should be better placed somewhere else.
> > >
> > > Perhaps I've missed it in some of the previous discussions:
> > > have we have considered/discussed this signal-vs-policy aspect ?
>
> What's your opinion on the above instead ?

It's not a policy but it gives better knowledge about the amount a work done
I have put below discussion on the  subject on previous version

> >
> > With contribution scaling the PELT utilization of a task is a _minimum_
> > utilization. Regardless of where the task is currently/was running (and
> > provided that it doesn't change behaviour) its PELT utilization will
> > approximate its _minimum_ utilization on an idle 1024 capacity CPU.
>
> The main drawback is that the _minimum_ utilization depends on the CPU
> capacity on which the task runs. The two 25% tasks on a 256 capacity
> CPU will have an utilization of 128 as an example
>
> >
> > With time scaling the PELT utilization doesn't really have a meaning on
> > its own. It has to be compared to the capacity of the CPU where it
> > is/was running to know what the its current PELT utilization means. When
>
> I would have said the opposite. The utilization of the task will
> always reflect the same amount of work that has been already done
> whatever the CPU capacity.
> In fact, the new scaling mechanism uses the real amount of work that
> has been already done to compute the utilization signal which is not
> the case currently. This gives more information about the real amount
> of worked that has been computed in the over utilization case.
>
> > the utilization over-shoots the capacity its value is no longer
> > represents utilization, it just means that it has a higher compute
> > demand than is offered on its current CPU and a high value means that it
> > has been suffering longer. It can't be used to predict the actual
> > utilization on an idle 1024 capacity any better than contribution scaled
> > PELT utilization.
>
> I think that it provides earlier detection of over utilization and
> more accurate signal for a longer time duration which can help the
> load balance
> Coming back to 50% task example . I will use a 50ms running time
> during a 100ms period for the example below to make it easier
>
> Starting from 0, the evolution of the utilization is:
>
> With contribution scaling:
>          time  0ms  50ms  100ms  150ms  200ms
> capacity
> 1024           0    666
> 512            0    333   453
> When the CPU start to be over utilized (@100ms), the utilization is
> already too low (453 instead of 666) and scheduler doesn't detect yet
> that we are over utilized
> 256            0    169   226    246    252
> That's even worse with this lower capacity
>
> With time scaling,
>          time  0ms  50ms  100ms  150ms  200ms
> capacity
> 1024           0    666
> 512            0    428   677
> We know that the current capacity is not enough and the utilization
> reflect the correct utilization level compare to 1024 capacity (the
> 666 vs 677 difference comes from the 1024us window so the last window
> is not full in the case of max capacity)
> 256            0    234   468    564    677
> At 100ms, we know that there is not enough capacity. (In fact we know
> that at 56ms). And even at time 200ms, the amount of work is exactly
> what would have been executed on a CPU 4x faster
>
> >
> > This change might not be a showstopper, but it is something to be aware
> > off and take into account wherever PELT utilization is used.
>
> The point above is clearly a big difference between the 2 approaches
> of the no spare cycle case but I think it will help by giving more
> information in the over utilization case
>
> Vincent
> >
> > Morten

>
> --
> #include <best/regards.h>
>
> Patrick Bellasi
