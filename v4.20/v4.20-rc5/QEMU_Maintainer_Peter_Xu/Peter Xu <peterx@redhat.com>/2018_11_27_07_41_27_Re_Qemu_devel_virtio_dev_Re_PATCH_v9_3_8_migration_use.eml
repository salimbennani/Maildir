Return-Path: <qemu-devel-bounces+like.xu=linux.intel.com@nongnu.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 27 Nov 2018 16:16:02 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga005.fm.intel.com (fmsmga005.fm.intel.com [10.253.24.32])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id B62855803C2
	for <like.xu@linux.intel.com>; Mon, 26 Nov 2018 23:42:08 -0800 (PST)
Received: from orsmga106.jf.intel.com ([10.7.208.65])
  by fmsmga005-1.fm.intel.com with ESMTP; 26 Nov 2018 23:42:08 -0800
IronPort-PHdr: =?us-ascii?q?9a23=3A9PGGdhEEw0CT9Op2xM3ApZ1GYnF86YWxBRYc798d?=
 =?us-ascii?q?s5kLTJ7yosywAkXT6L1XgUPTWs2DsrQY07qQ6/iocFdDyK7JiGoFfp1IWk1Nou?=
 =?us-ascii?q?QttCtkPvS4D1bmJuXhdS0wEZcKflZk+3amLRodQ56mNBXdrXKo8DEdBAj0OxZr?=
 =?us-ascii?q?KeTpAI7SiNm82/yv95HJbAhEmDmwbaluIBmqsA7cqtQYjYx+J6gr1xDHuGFIe+?=
 =?us-ascii?q?NYxWNpIVKcgRPx7dqu8ZBg7ipdpesv+9ZPXqvmcas4S6dYDCk9PGAu+MLrrxjD?=
 =?us-ascii?q?QhCR6XYaT24bjwBHAwnB7BH9Q5fxri73vfdz1SWGIcH7S60/VC+85Kl3VhDnlC?=
 =?us-ascii?q?YHNyY48G7JjMxwkLlbqw+lqxBm3oLYfJ2ZOP94c6jAf90VWHBBU95fWSJBHI2y?=
 =?us-ascii?q?bIUPAOUdMulFrYbyqUYArQO8CAeuC+7j1zFFimPo0q0hyOkhDR3K0RY8E94Sqn?=
 =?us-ascii?q?nZrtP4P7oSX+Cvy6nIyC3OYelI1jfh9ofIaA0qrvCRXbltdsfR0VcgFx/bgVWK?=
 =?us-ascii?q?po3oJCmV2foQvGib9eVgSfijhHIgqwF0uzWiwNonhIrRho8N1FzI6SZ0zJwoKd?=
 =?us-ascii?q?GlS0N3e8CoHZVQui2AKod7QNsuT390tCs+0LEKpJC2cDYQxJg6xBPSZeaLf5aL?=
 =?us-ascii?q?7x/lSe2fOy13hGh/d7K6nxuy8Vavyun7VsSs1FZKrzFFksXXtnwX2BzT7NWHRu?=
 =?us-ascii?q?F6/ke71jaDzwHT6udaLkAojafXNYItz70qmpYOrEjOHTX6lFv4gaKWbEko5+ql?=
 =?us-ascii?q?5/ziYrr8p5+cM4F0ihv5MqQrgsG/GPo3Mg0TX2SC5OuzyqPj8lP9QLlTlfI2lb?=
 =?us-ascii?q?TZsJbGKssFva61BAtV0ocg6xmhFTun38kYkGEDLFJEfhKHkofoN0vPIPD+Efew?=
 =?us-ascii?q?nVCsnC13yPDBO73sGZPNLn/FkLfue7Z99lRQyA0pzdBQ/55UEK0OIOrvWk/ts9?=
 =?us-ascii?q?zVFhs5Mw23w+n5Etl82Z4eVHmLAq+YNqPSrFCJ6vguI+mKeI8apjL9J+I56P7p?=
 =?us-ascii?q?iH9q0WIbZrSjiJsLdGijTLMhJ0SCfWGqhNAHHmEX+A0kQ6vvgVyGVDdVIHGqQ6?=
 =?us-ascii?q?M74Cp8EY+jEMLPS56ghO+82jymFMhTb2FCFlfeCHrtasCIVukBbGeIL9Z8nycY?=
 =?us-ascii?q?fb6mTYAnyFeprgCt0KdtLOff5ngFs4n+3sN+/ezZmEIO8mlLAsXV7GiLQCkggW?=
 =?us-ascii?q?4JASBww6l5rmR8zEuO1e5zhPkORvJJ4PYcagomOISU//EyX8j/Rg/bbv+TRVqm?=
 =?us-ascii?q?S8ngCjY0GIFii+QSalpwTo3xxivI2DCnVvpMz+SG?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0APAACD9PxbhxHrdtBkHAEBAQQBAQcEA?=
 =?us-ascii?q?QGBUQcBAQsBgTCCYoN5iBhfiyWCDZJGhHeBcxQYFIhwIjQJDQEDAQEBAQEBAgE?=
 =?us-ascii?q?TAQEBCgsJCBsOIwyCNgUCAxoBBoJcAQIDAQIgBB8KKQMDAQIGAQEKGAICBRoDB?=
 =?us-ascii?q?AICAwELBUkTBYMcggIBBKU1fDOKMIELin4XgUA/hCOIAjGCJgKQFY8gVQkNghO?=
 =?us-ascii?q?PBSMKgU+HeYc3LIlBjkiBRoINcBWDJ4InFxKLZYIyNDEBgQYci0+BdwEB?=
X-IPAS-Result: =?us-ascii?q?A0APAACD9PxbhxHrdtBkHAEBAQQBAQcEAQGBUQcBAQsBgTC?=
 =?us-ascii?q?CYoN5iBhfiyWCDZJGhHeBcxQYFIhwIjQJDQEDAQEBAQEBAgETAQEBCgsJCBsOI?=
 =?us-ascii?q?wyCNgUCAxoBBoJcAQIDAQIgBB8KKQMDAQIGAQEKGAICBRoDBAICAwELBUkTBYM?=
 =?us-ascii?q?cggIBBKU1fDOKMIELin4XgUA/hCOIAjGCJgKQFY8gVQkNghOPBSMKgU+HeYc3L?=
 =?us-ascii?q?IlBjkiBRoINcBWDJ4InFxKLZYIyNDEBgQYci0+BdwEB?=
X-IronPort-AV: E=Sophos;i="5.56,286,1539673200"; 
   d="scan'208";a="41687429"
X-Amp-Result: UNSCANNABLE
X-Amp-File-Uploaded: False
Unscannable: 2
Received: from lists.gnu.org ([208.118.235.17])
  by mtab.intel.com with ESMTP/TLS/AES256-SHA; 26 Nov 2018 23:42:03 -0800
Received: from localhost ([::1]:40498 helo=lists.gnu.org)
	by lists.gnu.org with esmtp (Exim 4.71)
	(envelope-from <qemu-devel-bounces+like.xu=linux.intel.com@nongnu.org>)
	id 1gRY0R-0007lb-0v
	for like.xu@linux.intel.com; Tue, 27 Nov 2018 02:42:03 -0500
Received: from eggs.gnu.org ([2001:4830:134:3::10]:48839)
	by lists.gnu.org with esmtp (Exim 4.71)
	(envelope-from <peterx@redhat.com>) id 1gRY08-0007lK-6F
	for qemu-devel@nongnu.org; Tue, 27 Nov 2018 02:41:45 -0500
Received: from Debian-exim by eggs.gnu.org with spam-scanned (Exim 4.71)
	(envelope-from <peterx@redhat.com>) id 1gRY04-0000FY-88
	for qemu-devel@nongnu.org; Tue, 27 Nov 2018 02:41:44 -0500
Received: from mx1.redhat.com ([209.132.183.28]:59176)
	by eggs.gnu.org with esmtps (TLS1.0:DHE_RSA_AES_256_CBC_SHA1:32)
	(Exim 4.71) (envelope-from <peterx@redhat.com>) id 1gRY03-0000FH-Vr
	for qemu-devel@nongnu.org; Tue, 27 Nov 2018 02:41:40 -0500
Received: from smtp.corp.redhat.com (int-mx07.intmail.prod.int.phx2.redhat.com
	[10.5.11.22])
	(using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
	(No client certificate requested)
	by mx1.redhat.com (Postfix) with ESMTPS id C763D7F40E;
	Tue, 27 Nov 2018 07:41:38 +0000 (UTC)
Received: from xz-x1 (dhcp-14-128.nay.redhat.com [10.66.14.128])
	by smtp.corp.redhat.com (Postfix) with ESMTPS id D49FE100191D;
	Tue, 27 Nov 2018 07:41:29 +0000 (UTC)
Date: Tue, 27 Nov 2018 15:41:27 +0800
From: Peter Xu <peterx@redhat.com>
To: Wei Wang <wei.w.wang@intel.com>
Message-ID: <20181127074127.GD3205@xz-x1>
References: <1542276484-25508-1-git-send-email-wei.w.wang@intel.com>
	<1542276484-25508-4-git-send-email-wei.w.wang@intel.com>
	<20181127054056.GA3205@xz-x1> <5BFCDE07.20707@intel.com>
	<5BFCE052.8070705@intel.com>
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Disposition: inline
In-Reply-To: <5BFCE052.8070705@intel.com>
User-Agent: Mutt/1.10.1 (2018-07-13)
X-Scanned-By: MIMEDefang 2.84 on 10.5.11.22
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16
	(mx1.redhat.com [10.5.110.25]);
	Tue, 27 Nov 2018 07:41:39 +0000 (UTC)
X-detected-operating-system: by eggs.gnu.org: GNU/Linux 2.2.x-3.x [generic]
	[fuzzy]
X-Received-From: 209.132.183.28
Subject: Re: [Qemu-devel] [virtio-dev] Re: [PATCH v9 3/8] migration: use
 bitmap_mutex in migration_bitmap_clear_dirty
X-BeenThere: qemu-devel@nongnu.org
X-Mailman-Version: 2.1.21
Precedence: list
List-Id: <qemu-devel.nongnu.org>
List-Unsubscribe: <https://lists.nongnu.org/mailman/options/qemu-devel>,
	<mailto:qemu-devel-request@nongnu.org?subject=unsubscribe>
List-Archive: <http://lists.nongnu.org/archive/html/qemu-devel/>
List-Post: <mailto:qemu-devel@nongnu.org>
List-Help: <mailto:qemu-devel-request@nongnu.org?subject=help>
List-Subscribe: <https://lists.nongnu.org/mailman/listinfo/qemu-devel>,
	<mailto:qemu-devel-request@nongnu.org?subject=subscribe>
Cc: virtio-dev@lists.oasis-open.org, quintela@redhat.com,
	liliang.opensource@gmail.com, mst@redhat.com,
	qemu-devel@nongnu.org, dgilbert@redhat.com, pbonzini@redhat.com,
	nilal@redhat.com
Errors-To: qemu-devel-bounces+like.xu=linux.intel.com@nongnu.org
Sender: "Qemu-devel" <qemu-devel-bounces+like.xu=linux.intel.com@nongnu.org>

On Tue, Nov 27, 2018 at 02:12:34PM +0800, Wei Wang wrote:
> On 11/27/2018 02:02 PM, Wei Wang wrote:
> > On 11/27/2018 01:40 PM, Peter Xu wrote:
> > > On Thu, Nov 15, 2018 at 06:07:59PM +0800, Wei Wang wrote:
> > > > The bitmap mutex is used to synchronize threads to update the dirty
> > > > bitmap and the migration_dirty_pages counter. For example, the free
> > > > page optimization clears bits of free pages from the bitmap in an
> > > > iothread context. This patch makes migration_bitmap_clear_dirty update
> > > > the bitmap and counter under the mutex.
> > > > 
> > > > Signed-off-by: Wei Wang <wei.w.wang@intel.com>
> > > > CC: Dr. David Alan Gilbert <dgilbert@redhat.com>
> > > > CC: Juan Quintela <quintela@redhat.com>
> > > > CC: Michael S. Tsirkin <mst@redhat.com>
> > > > CC: Peter Xu <peterx@redhat.com>
> > > > ---
> > > >   migration/ram.c | 3 +++
> > > >   1 file changed, 3 insertions(+)
> > > > 
> > > > diff --git a/migration/ram.c b/migration/ram.c
> > > > index 7e7deec..ef69dbe 100644
> > > > --- a/migration/ram.c
> > > > +++ b/migration/ram.c
> > > > @@ -1556,11 +1556,14 @@ static inline bool
> > > > migration_bitmap_clear_dirty(RAMState *rs,
> > > >   {
> > > >       bool ret;
> > > >   +    qemu_mutex_lock(&rs->bitmap_mutex);
> > > >       ret = test_and_clear_bit(page, rb->bmap);
> > > >         if (ret) {
> > > >           rs->migration_dirty_pages--;
> > > >       }
> > > > +    qemu_mutex_unlock(&rs->bitmap_mutex);
> > > > +
> > > >       return ret;
> > > >   }
> > > It seems fine to me, but have you thought about
> > > test_and_clear_bit_atomic()?  Note that we just had
> > > test_and_set_bit_atomic() a few months ago.
> > 
> > Thanks for sharing. I think we might also need to
> > mutex migration_dirty_pages.
> > 
> > > 
> > > And not related to this patch: I'm unclear on why we have had
> > > bitmap_mutex before, since it seems unnecessary.
> > 
> > OK. This is because with the optimization we have a thread
> > which clears bits (of free pages) from the bitmap and updates
> > migration_dirty_pages. So we need to synchronization between
> > the migration thread and the optimization thread.
> > 
> 
> And before this feature, I think yes, that bitmap_mutex is not needed.
> It was left there due to some historical reasons.
> I remember Dave previous said he was about to remove it. But the new
> feature will need it again.

Ok then I'm fine with it.  Though you could update the comments too if
you like:

    /* protects modification of the bitmap and migration_dirty_pages */
    QemuMutex bitmap_mutex;

And it's tricky that sometimes we don't take the lock when reading
this variable "migration_dirty_pages".  I don't see obvious issue so
far, hope it's true (at least I skipped the colo ones...).

ram_bytes_remaining[333]       return ram_state ? (ram_state->migration_dirty_pages * TARGET_PAGE_SIZE) :
migration_bitmap_clear_dirty[1562] rs->migration_dirty_pages--;
migration_bitmap_sync_range[1570] rs->migration_dirty_pages +=
postcopy_chunk_hostpages_pass[2809] rs->migration_dirty_pages += !test_and_set_bit(page, bitmap);
ram_state_init[3037]           (*rsp)->migration_dirty_pages = ram_bytes_total() >> TARGET_PAGE_BITS;
ram_state_resume_prepare[3112] rs->migration_dirty_pages = pages;
ram_save_pending[3344]         remaining_size = rs->migration_dirty_pages * TARGET_PAGE_SIZE;
ram_save_pending[3353]         remaining_size = rs->migration_dirty_pages * TARGET_PAGE_SIZE;
colo_cache_from_block_offset[3468] ram_state->migration_dirty_pages++;
colo_init_ram_cache[3716]      ram_state->migration_dirty_pages = 0;
colo_flush_ram_cache[3997]     trace_colo_flush_ram_cache_begin(ram_state->migration_dirty_pages);

Regards,

-- 
Peter Xu

