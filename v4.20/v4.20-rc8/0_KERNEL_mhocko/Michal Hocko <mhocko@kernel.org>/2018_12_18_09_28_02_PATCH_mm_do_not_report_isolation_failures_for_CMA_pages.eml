Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by i7-8700 with POP3-SSL;
  18 Dec 2018 18:09:50 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga002.fm.intel.com (fmsmga002.fm.intel.com [10.253.24.26])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 838B25805F0;
	Tue, 18 Dec 2018 01:28:19 -0800 (PST)
Received: from orsmga102-1.jf.intel.com (HELO mga09.intel.com) ([10.7.208.27])
  by fmsmga002-1.fm.intel.com with ESMTP; 18 Dec 2018 01:28:18 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AGiXByh//OI+1av9uRHKM819IXTAuvvDOBiVQ1KB9?=
 =?us-ascii?q?1+IfIJqq85mqBkHD//Il1AaPAd2Lraocw8Pt8InYEVQa5piAtH1QOLdtbDQizf?=
 =?us-ascii?q?ssogo7HcSeAlf6JvO5JwYzHcBFSUM3tyrjaRsdF8nxfUDdrWOv5jAOBBr/KRB1?=
 =?us-ascii?q?JuPoEYLOksi7ze+/94HQbglSmDaxfa55IQmrownWqsQYm5ZpJLwryhvOrHtIeu?=
 =?us-ascii?q?BWyn1tKFmOgRvy5dq+8YB6/ShItP0v68BPUaPhf6QlVrNYFygpM3o05MLwqxbO?=
 =?us-ascii?q?SxaE62YGXWUXlhpIBBXF7A3/U5zsvCb2qvZx1S+HNsDtU7s6RSqt4LtqSB/wiS?=
 =?us-ascii?q?cIKTg58H3MisdtiK5XuQ+tqwBjz4LRZoyeKfhwcb7Hfd4CRWRPQNtfWSJCDI27?=
 =?us-ascii?q?YIQBAPEBPf5aooTmu1cDrgGzCRW2Ce/z1jNFgGL9060g0+QmFAHLxAsuH9MSv3?=
 =?us-ascii?q?TUttr1MLoZX+KyzKbUzjXDaO5d1DD46IjIdRAuvfKMXbVrfMrS10YvDQXFgUuU?=
 =?us-ascii?q?qYD/ODOVzOsNvnGd4uF9W+yvjGsnpBtwojip3sosiZfGhpgTyl/a8SV12oE1Jc?=
 =?us-ascii?q?e3SEN9fNWqE4NQujmEO4dqRs4uWXxktSgkxrEcpJK2fzQGxI4myhPdc/CLbZWE?=
 =?us-ascii?q?7xz5WOqMITp0mWhpdba/ihqo7EStxPHwW8+p21hQtCVFiMPDtnUV2hzT9MeHTv?=
 =?us-ascii?q?x981+/2TaAyQ/T8PtIIUMqmqrBLZ4u3LowmoAUsUTZES/6gET2jKmIeUU44uWk?=
 =?us-ascii?q?9fjrb7H8qpOBOYJ4lBvyPrktl8CjG+g1MwoDU3Ce+eum1b3j+UP5QK9Njv0ziq?=
 =?us-ascii?q?TZtJHaJcIGpq+2GgNV0Zgs6wy5Dzi41NQUhH4HLVxDeB2ZlYjkIE/OIP/mAvel?=
 =?us-ascii?q?mViskylkx+rAPrL/BpXBNH/DkK3ufbpl8U5T1BIzzcxD55JTErwBJPPzWkzvu9?=
 =?us-ascii?q?DCAR45Lhe5w+LmCNV7y4MfVniDAq6fMKPOr1CI4vgjLPWLZI8QoDz9MeQq5+by?=
 =?us-ascii?q?jX8lnl8QZamp0oEWaHCkBPtmJF+VYXrxgtgbF2cKsREzTOjriF2ETD5SaGy+X6?=
 =?us-ascii?q?M65jEnFo2mCZ3PSZyqgLyExC27BIFZZnhaClCQFnflb4eEW/ASZy6IIc9hlToE?=
 =?us-ascii?q?Vb6mS4I60RGutQn6y6doL+bO+y0Ys47j28Zx5+HJiR4y8jl0BdyH026RV2F0gn?=
 =?us-ascii?q?8IRzgu0a9lukx9ylCD0atij/xCD9NT5ehEUgM7NZ7a0ux7BMr+WgPHfteVVlmm?=
 =?us-ascii?q?Rs+qDi02TtI029UOeVpyG82+jhDf2CqnG7wVmKaNBJAu9qLc3mL+J8Byy3vd0K?=
 =?us-ascii?q?khjl8mQtZANGG8h65/8RTTCJDNk0mDi6mqcqEc1jbX9Gif1WqOoF1YUAloXKrY?=
 =?us-ascii?q?R38feFXZoc755kzYSb+uEqooMg1Gxc6GKatKb9npgE5CRPfiPtTefm2wl32xBR?=
 =?us-ascii?q?aO2rODco7qd38B0yXaDUgOixoT8mqeNQgiGiehpHrTAyF0GlLxfUzg6+l+p2m9?=
 =?us-ascii?q?Tk8v0Q6KaVZs1760+h4TmPydROkf3rMCuCc9tTp0GEyx0M7RC9qFvwBhZrlTYc?=
 =?us-ascii?q?sh4Fdb0mLUrxB9Pp2lL6Blml4ecwR2s1ny1xltDYVAi8wqrHIszAp9Mq+Y1FJB?=
 =?us-ascii?q?dzWF3ZH/ILHXK2/y/AyxZK7SwF3RzNGW+qIX4vQit1rjpB2pFlYl83h/ydZV12?=
 =?us-ascii?q?WT55rUAwsSS57+SVs3+AVgqLHcYSk94J3U2GZoMam1tD/Cxt0oCPEkyhamY9dQ?=
 =?us-ascii?q?LqeEGBXuHM0dAsikMPYqlESxbhIYIOBS87Y5PsO7d/ecxKGnJudhnDK8gmRB74?=
 =?us-ascii?q?BwyUaM9yt6Su7V0JcJ2fCY3g2bVzjiiFetqNz4mYdBZTsKBGqw1TDkBJJNZq11?=
 =?us-ascii?q?ZYsLE2auLNGtydVkmpHtXGRU9Fi9CFMc2c+lYAadYEb53QJNy0sXu3unmS2jwj?=
 =?us-ascii?q?xwkjEpqLef3SPUz+TjchoHJnBERG14gVjwJoi0isgQXFK0YAgxiBul+UH6yrBb?=
 =?us-ascii?q?paR+NWXSQV1EfzPrL2FkSaawsruCY8hS6JInqylXUeK8YUyERb75uRcVzyTjH2?=
 =?us-ascii?q?5GzjAhaz6qoon5nwB9iG+FLnZ8tmDZecJzxRfY/tDcXuRe3jkFRCl5jznYGF68?=
 =?us-ascii?q?MsKt/dWVkZfDr++/W3igVp1VbSnk04eAuDGn6m1tBB21h+qzlcH/EQgmzS/70M?=
 =?us-ascii?q?FnVCXSoxb7eIXr1aW6MeR8c0lsBV/87dd6G45knosxgpEQxWYVhpGP8XUblmfz?=
 =?us-ascii?q?NM1R2bjiY3oVWT4L39nV7RDm2EJ5L3KF3YL5VmiHzctnaNm3eWcW2iM778BXB6?=
 =?us-ascii?q?aY9r1EnS1polWmqQLde+RynjAYyfE28n4Vn/kJuBYxziWaGr0SGEhYPS/2mBiS?=
 =?us-ascii?q?4dGxsr5XZHqxfriqz0Vxg8qhDL6ZrwFYWXb5fIoiHCBq4sV+NlLMzGP86oX+dN?=
 =?us-ascii?q?bMatITswWekw3cgOhNNJIxiv0KiDJ7OWL6uH0p0e86ggZo3ZGnp4iHMGRt8bm9?=
 =?us-ascii?q?Ah5ZMD31eswS9ivsjaZYgsaZwYSvEo99FTUMWZviVeioHy4KtfT7KwaOFyUxpW?=
 =?us-ascii?q?yBFrrYGQ+f9V1qr3bSE52wM3GXJX8ZzchtRRWHJUxfhhwUUys+npIjCg+qw8nh?=
 =?us-ascii?q?el9j5j8N/l74tgdMyuVwOhn9SGjfoRmnaiwuRJiDNhZW7RxC50TIMcyY9O1zBD?=
 =?us-ascii?q?pV/pmgrAyLN2yaaB5EDWAPWkyYGV/jOqOi6sXH8+idHuC+NefBYa2SqexCUPeF?=
 =?us-ascii?q?3ZKu0ox7/zaVK8WAJH9iA+cg2kpEW3B5FN/UmzEOSywRiiLMYNSXpBa6+i1rsM?=
 =?us-ascii?q?+/9O7nVx7o5YuKE7FSK8lg+wiqgaefMO6dnCZ5JihZ1pMPxn/IyaIT3F0IiyFp?=
 =?us-ascii?q?eDmiD64AtTPWTK/Lnq9XDhgbaz58NcdS7qI82BVNNtDfit/vyrF4ifs1AU9fVV?=
 =?us-ascii?q?P9gsGpedAKI2alOVzdGUaEL66GKiPLws3tZaO8SKZdjOFVtx22pDaaHFXvPjWF?=
 =?us-ascii?q?lzn1SR+vNftAgz2cPBxboIu9aApiCXD/TNL6bR22KMN3jTwzwbEumnzGL3ITMT?=
 =?us-ascii?q?hifENLtbCQ6SJYgvNiG21O9HZlLO+EmzqH4OncMJoZrfxrAiFsneJA/Hs606dV?=
 =?us-ascii?q?7D1DRPFtmCrdtNhuo1SnkumJ0jVmURpOpSxNhIKEpkhiPaTZ9p9dWXfL5h4N7G?=
 =?us-ascii?q?OQCwgUqNthENHgp6dQyt2c3J70fR5E/8jZ4oM5ANLIL8TPZHMiKR/BHD/SERtA?=
 =?us-ascii?q?Qzm2M23Wm01alreV7HLD/bYgrZ25vZsFSrZfHHwyHfAdQhBgHNsJI5BfXT4+l7?=
 =?us-ascii?q?OfyskS6izt/1HqWMxGs8WfBbqpCvL1JWPc1OEcag=3D=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0ATAAA6vRhch0O0hNFkHQEBBQEHBQGBU?=
 =?us-ascii?q?QgBCwGBVYEUgQInjBVfixqCIZdGgXMTAQEYCwgBh18iNAkNAQMBAQEBAQECARM?=
 =?us-ascii?q?BAQEKCwkIKSMMgjYkAYJpAi8BFDIGCQIYBzEDMQEFASITBYMdAYIABAEKnBg8j?=
 =?us-ascii?q?EiDKoEDAYYNEodshEEXgUA/hUmJOgKJNIIGlWcJhw+KRAwYgiqHS4deAYs0gwO?=
 =?us-ascii?q?LEAIEAgQFAgUPIYElgg4zGggoCIMnCQqCFBeIXoVAPzIBAYEDAQGNeAEB?=
X-IPAS-Result: =?us-ascii?q?A0ATAAA6vRhch0O0hNFkHQEBBQEHBQGBUQgBCwGBVYEUgQI?=
 =?us-ascii?q?njBVfixqCIZdGgXMTAQEYCwgBh18iNAkNAQMBAQEBAQECARMBAQEKCwkIKSMMg?=
 =?us-ascii?q?jYkAYJpAi8BFDIGCQIYBzEDMQEFASITBYMdAYIABAEKnBg8jEiDKoEDAYYNEod?=
 =?us-ascii?q?shEEXgUA/hUmJOgKJNIIGlWcJhw+KRAwYgiqHS4deAYs0gwOLEAIEAgQFAgUPI?=
 =?us-ascii?q?YElgg4zGggoCIMnCQqCFBeIXoVAPzIBAYEDAQGNeAEB?=
X-IronPort-AV: E=Sophos;i="5.56,367,1539673200"; 
   d="scan'208";a="57914908"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 18 Dec 2018 01:28:16 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726474AbeLRJ2N (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Tue, 18 Dec 2018 04:28:13 -0500
Received: from mail-pf1-f193.google.com ([209.85.210.193]:43735 "EHLO
        mail-pf1-f193.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726364AbeLRJ2N (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Tue, 18 Dec 2018 04:28:13 -0500
Received: by mail-pf1-f193.google.com with SMTP id w73so7848199pfk.10
        for <linux-kernel@vger.kernel.org>; Tue, 18 Dec 2018 01:28:12 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:mime-version
         :content-transfer-encoding;
        bh=wPvGr/wNcmr4Tb5YHQNZJ/6FEIYgCd53/p3Q1S9g57k=;
        b=WXvsUfktY2Yetg0Z2PHLIORXZf1zYurEejZQmuGtua0pik3lYlxSt2JPbDMTptNEmh
         J6ZQ5uSS1f68mdTF4e1yX+YXmRSJudN5+mYqe2RC+ixtJESwpYJx+pb5OrnHMVureNds
         YDM70T5tfn0jAqtMgoogLXEc7+vEmVmBeo+K5/2iZAJqQOpLuIObNBHkQ4G2zRfcAae6
         mKitHuYiIccsI8Nvf9Eya5Mco2mIIfuTuw7S1CLp2memd+hBTLbof37GqJdp4ofXgtX/
         a3mt68bpwD5r/ZX9P3nXHM8w0/jwf0rAEuW1m72/8S+/XHiufvySjir7/L5pPz5478wi
         iBCQ==
X-Gm-Message-State: AA+aEWbD4Hpa0cNjAxs/dlSTM6qfmY41D3JfJlihcVRl7SigZKYJbZ1y
        DM6VaQQ+5JvXGNSu+JYt9vN0tkoZ
X-Google-Smtp-Source: AFSGD/Xaz9uMk7WsmcIJdbAQH9qQtYEhhAXaorM6ebFAjvod2kBDUwADFe1b9pLyHaTLUZ7cmDeOvw==
X-Received: by 2002:a62:30c3:: with SMTP id w186mr16023767pfw.39.1545125291692;
        Tue, 18 Dec 2018 01:28:11 -0800 (PST)
Received: from tiehlicka.microfocus.com (prg-ext-pat.suse.com. [213.151.95.130])
        by smtp.gmail.com with ESMTPSA id h9sm19484073pgd.53.2018.12.18.01.28.08
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Tue, 18 Dec 2018 01:28:10 -0800 (PST)
From: Michal Hocko <mhocko@kernel.org>
To: Andrew Morton <akpm@linux-foundation.org>
Cc: Oscar Salvador <OSalvador@suse.com>,
        Anshuman Khandual <anshuman.khandual@arm.com>,
        Heiko Carstens <heiko.carstens@de.ibm.com>,
        <linux-mm@kvack.org>, LKML <linux-kernel@vger.kernel.org>,
        Michal Hocko <mhocko@suse.com>
Subject: [PATCH] mm: do not report isolation failures for CMA pages
Date: Tue, 18 Dec 2018 10:28:02 +0100
Message-Id: <20181218092802.31429-1-mhocko@kernel.org>
X-Mailer: git-send-email 2.19.2
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

From: Michal Hocko <mhocko@suse.com>

Heiko has complained that his log is swamped by warnings from has_unmovable_pages
[   20.536664] page dumped because: has_unmovable_pages
[   20.536792] page:000003d081ff4080 count:1 mapcount:0 mapping:000000008ff88600 index:0x0 compound_mapcount: 0
[   20.536794] flags: 0x3fffe0000010200(slab|head)
[   20.536795] raw: 03fffe0000010200 0000000000000100 0000000000000200 000000008ff88600
[   20.536796] raw: 0000000000000000 0020004100000000 ffffffff00000001 0000000000000000
[   20.536797] page dumped because: has_unmovable_pages
[   20.536814] page:000003d0823b0000 count:1 mapcount:0 mapping:0000000000000000 index:0x0
[   20.536815] flags: 0x7fffe0000000000()
[   20.536817] raw: 07fffe0000000000 0000000000000100 0000000000000200 0000000000000000
[   20.536818] raw: 0000000000000000 0000000000000000 ffffffff00000001 0000000000000000

which are not triggered by the memory hotplug but rather CMA allocator.
The original idea behind dumping the page state for all call paths was
that these messages will be helpful debugging failures. From the above
it seems that this is not the case for the CMA path because we are
lacking much more context. E.g the second reported page might be a CMA
allocated page. It is still interesting to see a slab page in the CMA
area but it is hard to tell whether this is bug from the above output
alone.

Address this issue by dumping the page state only on request. Both
start_isolate_page_range and has_unmovable_pages already have an
argument to ignore hwpoison pages so make this argument more generic and
turn it into flags and allow callers to combine non-default modes into a
mask. While we are at it, has_unmovable_pages call from is_pageblock_removable_nolock
(sysfs removable file) is questionable to report the failure so drop it
from there as well.

Reported-by: Heiko Carstens <heiko.carstens@de.ibm.com>
Signed-off-by: Michal Hocko <mhocko@suse.com>
---

Hi,
this is triggered by [1]. I think it should go as a separate patch
rathen than folded in to [2] because it gives a more context for future
reference but I will not insist of course.

Implementation wise I went with the simplest patch but if there is a
strong feeling that we need a dedicated enum then I will do that. The
API is quite low level so I didn't feel an urge to do that myself.

[1] http://lkml.kernel.org/r/20181217155922.GC3560@osiris
[2] http://lkml.kernel.org/r/20181116083020.20260-6-mhocko@kernel.org

 include/linux/page-isolation.h | 11 +++++++++--
 mm/memory_hotplug.c            |  5 +++--
 mm/page_alloc.c                | 11 +++++------
 mm/page_isolation.c            | 10 ++++------
 4 files changed, 21 insertions(+), 16 deletions(-)

diff --git a/include/linux/page-isolation.h b/include/linux/page-isolation.h
index 4ae347cbc36d..4eb26d278046 100644
--- a/include/linux/page-isolation.h
+++ b/include/linux/page-isolation.h
@@ -30,8 +30,11 @@ static inline bool is_migrate_isolate(int migratetype)
 }
 #endif
 
+#define SKIP_HWPOISON	0x1
+#define REPORT_FAILURE	0x2
+
 bool has_unmovable_pages(struct zone *zone, struct page *page, int count,
-			 int migratetype, bool skip_hwpoisoned_pages);
+			 int migratetype, int flags);
 void set_pageblock_migratetype(struct page *page, int migratetype);
 int move_freepages_block(struct zone *zone, struct page *page,
 				int migratetype, int *num_movable);
@@ -44,10 +47,14 @@ int move_freepages_block(struct zone *zone, struct page *page,
  * For isolating all pages in the range finally, the caller have to
  * free all pages in the range. test_page_isolated() can be used for
  * test it.
+ *
+ * The following flags are allowed (they can be combined in a bit mask)
+ * SKIP_HWPOISON - ignore hwpoison pages
+ * REPORT_FAILURE - report details about the failure to isolate the range
  */
 int
 start_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
-			 unsigned migratetype, bool skip_hwpoisoned_pages);
+			 unsigned migratetype, int flags);
 
 /*
  * Changes MIGRATE_ISOLATE to MIGRATE_MOVABLE.
diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c
index c82193db4be6..8537429d33a6 100644
--- a/mm/memory_hotplug.c
+++ b/mm/memory_hotplug.c
@@ -1226,7 +1226,7 @@ static bool is_pageblock_removable_nolock(struct page *page)
 	if (!zone_spans_pfn(zone, pfn))
 		return false;
 
-	return !has_unmovable_pages(zone, page, 0, MIGRATE_MOVABLE, true);
+	return !has_unmovable_pages(zone, page, 0, MIGRATE_MOVABLE, SKIP_HWPOISON);
 }
 
 /* Checks if this range of memory is likely to be hot-removable. */
@@ -1577,7 +1577,8 @@ static int __ref __offline_pages(unsigned long start_pfn,
 
 	/* set above range as isolated */
 	ret = start_isolate_page_range(start_pfn, end_pfn,
-				       MIGRATE_MOVABLE, true);
+				       MIGRATE_MOVABLE,
+				       SKIP_HWPOISON | REPORT_FAILURE);
 	if (ret) {
 		mem_hotplug_done();
 		reason = "failure to isolate range";
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index ec2c7916dc2d..ee4043419791 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -7754,8 +7754,7 @@ void *__init alloc_large_system_hash(const char *tablename,
  * race condition. So you can't expect this function should be exact.
  */
 bool has_unmovable_pages(struct zone *zone, struct page *page, int count,
-			 int migratetype,
-			 bool skip_hwpoisoned_pages)
+			 int migratetype, int flags)
 {
 	unsigned long pfn, iter, found;
 
@@ -7818,7 +7817,7 @@ bool has_unmovable_pages(struct zone *zone, struct page *page, int count,
 		 * The HWPoisoned page may be not in buddy system, and
 		 * page_count() is not 0.
 		 */
-		if (skip_hwpoisoned_pages && PageHWPoison(page))
+		if ((flags & SKIP_HWPOISON) && PageHWPoison(page))
 			continue;
 
 		if (__PageMovable(page))
@@ -7845,7 +7844,8 @@ bool has_unmovable_pages(struct zone *zone, struct page *page, int count,
 	return false;
 unmovable:
 	WARN_ON_ONCE(zone_idx(zone) == ZONE_MOVABLE);
-	dump_page(pfn_to_page(pfn+iter), "unmovable page");
+	if (flags & REPORT_FAILURE)
+		dump_page(pfn_to_page(pfn+iter), "unmovable page");
 	return true;
 }
 
@@ -7972,8 +7972,7 @@ int alloc_contig_range(unsigned long start, unsigned long end,
 	 */
 
 	ret = start_isolate_page_range(pfn_max_align_down(start),
-				       pfn_max_align_up(end), migratetype,
-				       false);
+				       pfn_max_align_up(end), migratetype, 0);
 	if (ret)
 		return ret;
 
diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 43e085608846..ce323e56b34d 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -15,8 +15,7 @@
 #define CREATE_TRACE_POINTS
 #include <trace/events/page_isolation.h>
 
-static int set_migratetype_isolate(struct page *page, int migratetype,
-				bool skip_hwpoisoned_pages)
+static int set_migratetype_isolate(struct page *page, int migratetype, int isol_flags)
 {
 	struct zone *zone;
 	unsigned long flags, pfn;
@@ -60,8 +59,7 @@ static int set_migratetype_isolate(struct page *page, int migratetype,
 	 * FIXME: Now, memory hotplug doesn't call shrink_slab() by itself.
 	 * We just check MOVABLE pages.
 	 */
-	if (!has_unmovable_pages(zone, page, arg.pages_found, migratetype,
-				 skip_hwpoisoned_pages))
+	if (!has_unmovable_pages(zone, page, arg.pages_found, migratetype, flags))
 		ret = 0;
 
 	/*
@@ -185,7 +183,7 @@ __first_valid_page(unsigned long pfn, unsigned long nr_pages)
  * prevents two threads from simultaneously working on overlapping ranges.
  */
 int start_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
-			     unsigned migratetype, bool skip_hwpoisoned_pages)
+			     unsigned migratetype, int flags)
 {
 	unsigned long pfn;
 	unsigned long undo_pfn;
@@ -199,7 +197,7 @@ int start_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
 	     pfn += pageblock_nr_pages) {
 		page = __first_valid_page(pfn, pageblock_nr_pages);
 		if (page &&
-		    set_migratetype_isolate(page, migratetype, skip_hwpoisoned_pages)) {
+		    set_migratetype_isolate(page, migratetype, flags)) {
 			undo_pfn = pfn;
 			goto undo;
 		}
-- 
2.19.2

