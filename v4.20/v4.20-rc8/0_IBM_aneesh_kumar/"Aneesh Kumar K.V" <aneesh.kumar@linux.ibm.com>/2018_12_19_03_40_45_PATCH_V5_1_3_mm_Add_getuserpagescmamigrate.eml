Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by i7-8700 with POP3-SSL;
  19 Dec 2018 12:42:24 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga008.fm.intel.com (fmsmga008.fm.intel.com [10.253.24.58])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 4DD4C5805FC;
	Tue, 18 Dec 2018 19:41:14 -0800 (PST)
Received: from fmsmga104.fm.intel.com ([10.1.193.100])
  by fmsmga008-1.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 18 Dec 2018 19:41:14 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3A44KoNhZInGeeDr+TduiiBrX/LSx+4OfEezUN459i?=
 =?us-ascii?q?sYplN5qZpcq5Zh7h7PlgxGXEQZ/co6odzbaO4+a4ASQp2tWoiDg6aptCVhsI24?=
 =?us-ascii?q?09vjcLJ4q7M3D9N+PgdCcgHc5PBxdP9nC/NlVJSo6lPwWB6nK94iQPFRrhKAF7?=
 =?us-ascii?q?Ovr6GpLIj8Swyuu+54Dfbx9HiTahYr5+Ngm6oRnMvcQKnIVuLbo8xAHUqXVSYe?=
 =?us-ascii?q?RWwm1oJVOXnxni48q74YBu/SdNtf8/7sBMSar1cbg2QrxeFzQmLns65Nb3uhnZ?=
 =?us-ascii?q?TAuA/WUTX2MLmRdVGQfF7RX6XpDssivms+d2xSeXMdHqQb0yRD+v9LlgRgP2hy?=
 =?us-ascii?q?gbNj456GDXhdJ2jKJHuxKquhhzz5fJbI2JKPZye6XQds4YS2VcRMZcTyxPDJ2h?=
 =?us-ascii?q?YYUBCOQOP+hYoIbyqVUNthu+HQuhCfjzxjJLnHL6wbc33/g9HQzAwQcuH8gOsH?=
 =?us-ascii?q?PRrNjtOqsfTea1w7fWwjXYb/NdxDLz6JXNch87u/GHQLd+fdDexkUhCgjIiU+f?=
 =?us-ascii?q?qYr7MDyLzeQNs3KX7+l9VeKqkWEnsRp8ojyuxscql4nIiZgZylHf+iV82oo6Od?=
 =?us-ascii?q?q4SEtibNOiDZBetDmaOpNoTs8+R2xkojs2x7MYtZKhYSQHy4grywTeZvGFa4SF?=
 =?us-ascii?q?5gjvWeWRLDtihn9pZaiziwuu/UWuzOD3S9O630xQriVfl9nBrnAN2ALX6siAUv?=
 =?us-ascii?q?Z94Eih1iiV1wzJ6eFLP1o0lazFJJ4l2LIwkYATsUvbEi/3nkX5krOWe1069uS0?=
 =?us-ascii?q?7+nreKjqq5GCO4Nulw3zMbgilta+DOk6KgQOWnKU+eW41L3t5035R7BKg+Uykq?=
 =?us-ascii?q?nYtpDaOMsaqre6AwBLyIYj7QiwDzO/3NQfk3gHKkxKeAicgoj3NFHBPur4Ae28?=
 =?us-ascii?q?g1uyijdrwe7JPrn7DpXKNHjDn6/tfaxh5E5E1Aoz0ddf6opQCrEAI/L8RFX9td?=
 =?us-ascii?q?PFDhIiNwy0wuDnCMhy148EWGKPBLOZP73WsVOS+u0vJOyMbpcPuDnhM/gl++Lu?=
 =?us-ascii?q?jXghlF8dZ6ap3IcXZ2q/Hvh8I0WZfGDjgtEOEWoRugo+TerqiECNUDJJZnayWb?=
 =?us-ascii?q?486S8/CI68EYjDQYWtiqSb3CinBp1WenxGCleUHHfqcIWLRe0AaCGVIs9nlDwE?=
 =?us-ascii?q?UqOsS4sg1RGoqQ/7xKBrLuvS+i0Eq53j0MJ56PHUlRE37TZ0FdiS03mRT2FomW?=
 =?us-ascii?q?MFXyU53Lt/oUx6yVePy7J4jOZaFdFI4/NJUwE6NYPTzuBgCtDyXB7BccmNSFq8?=
 =?us-ascii?q?XtqmBjQxRMorw9ASe0Z9B8mijhfb0iqpGbAVkaaHBJg18q3G2XjxKN1wy3LH1K?=
 =?us-ascii?q?knklknTdFDNWyghq5j6QfTA5TFnFmel6avba4cxjLC9H+fzWqSu0FVSBN/Xr/b?=
 =?us-ascii?q?XX8BfEfWrc725kXZT7CwD7QrNQ9Byc2HKqtOcdDpiVRGRPH+ONXReW6xmmGwBQ?=
 =?us-ascii?q?qWybOIdoblZ2Id3CDFAkgejw8T5WqGNRQ5Biq5vm3RFiJuGkz1b0Ps6+Z+rmi7?=
 =?us-ascii?q?QVEyzwyRa01h1ry1+gMahPCGSvMT2K4EtzklqzluAFm92NfWAcKapwV9ZKVcfc?=
 =?us-ascii?q?894FBf2GLFtgx9O5ugL7xihl8eaQh3o1ni1xJtCoVEkMgqqnwqwRF2KaKZ1lNB?=
 =?us-ascii?q?ajyZ0YrxOr3RNmn94hSvZ7TK1VHZ1dac4r0P5+ggq1X/oAGpEVIv/G9j09ZL3H?=
 =?us-ascii?q?qT+JXLABAJXpLsT0k47R56p7LdYikj/I7U0XxsMa+psj7Nwd4pBe0lygq+cNdb?=
 =?us-ascii?q?Kq+LCAjyE8gCDci0NOMqg0Spbg4DPO1K9K80ItmqeOec1K+qPOZvhjSmjWtc7Y?=
 =?us-ascii?q?B500KM8Td8S+HS05YExfGYwhWIVzPmgFi9tcD3nJhOZSsOEWqn1SjkGIlRa7Vo?=
 =?us-ascii?q?fYYKFWihOde3ych5h5L3XX5X6kSjB1If1MC1YxWSa0Hy0hNK1UQQp3yqgi+4zz?=
 =?us-ascii?q?1ykzE0oauTxi3Ow+L+dBUZPm5HXnVtjVDpIYKsldAVQFCobxQ1lBui/Uv7x6lb?=
 =?us-ascii?q?qL5/LmXJWkdIYi72InpmUquxsLqCfsFO5IkpsSVRTOSzf1SaRqThrBsd1iPpB3?=
 =?us-ascii?q?FeyywjdzG2ppX5mAR3iGCHI3Zpr3rZesZwyQ3E5NPGRv5R3TsGRC9mhjnRHVW8?=
 =?us-ascii?q?O9ip/dOJl5bMqOy+VmShVoFNfinv14+PqCy75WhyCx2lg/+zgsHnERQ90SLj19?=
 =?us-ascii?q?hlTyLIoAz+Yonq0aS3KuZnfkhuBF/h5Mt2AIB+ko0shJ4O3XgWnIma/X0CkW3r?=
 =?us-ascii?q?K9VUxbr+bGYRRT4M29PV4xLq2Ex5InKJ2oL2THOdwsR6atm+Y2MW3D897s9QBK?=
 =?us-ascii?q?eV6rxEgTV6ol6ioQ3NZvh9my8XyeEy534Cn+EJpA0twz2YArATHklXJzbglhqW?=
 =?us-ascii?q?4NClsKVYenyvfqOu20pkktCsF7WCogBaWHbkdZYuBy5w7sNjMF3S1H3/8J3reN?=
 =?us-ascii?q?7VbdgLrB2bjw/Aj/RJKJI2jvcKmS1nOWfnsXwk0eE7iwFu3YqhvIiGMGht+KO5?=
 =?us-ascii?q?AhhFNjz6fc8T+zftjbpAkcaSxYygApJhGjATVpvyUf2oCC4StejgNwuWCzIzsH?=
 =?us-ascii?q?CbGb7CHQOF7EdmsmnCE5SqN3GROXkYws9uRBibJExDng8UWC82kYI+FgCv3Mbh?=
 =?us-ascii?q?alt25igN5l7krRtB0uBoOAP6UmjBvwekcCs0RIKcLBpL7QFC+kHVPtaF7u9oGy?=
 =?us-ascii?q?FY/5uhrBGCK2CBZgRIC30JVVKAB1z5Irau4tzA+fCCBuWiN/vOfamOqetGWvaI?=
 =?us-ascii?q?wpKvz5Jm/yuWOcWJJHViFPo72kxMXX1iH8TZmjMPSzEYli7Xbs6bogu89TNzrs?=
 =?us-ascii?q?yl7PvrXwfv75OVC7ROKdVv5wy2gaCbOu6Qmil5KDVY1pANxXPSy7gfxlkSiy5w?=
 =?us-ascii?q?eDm3DLQArjXATKbRmq9REh4aZDl/NMpO76IgwAZNPdTXhc/y1r59lvQ1EUtKVU?=
 =?us-ascii?q?T9msG1YswHO3uyNE7cC0aRKruHJSfHw8X2Ya6nTb1QjeNUtwC/uDqBEk/jOCiD?=
 =?us-ascii?q?mCftVxy1Le5MiySbNgREuI6hahZtFXTjTNX+Zx2nMd93iCc6zqEuinzWNW4TLz?=
 =?us-ascii?q?58flhJrr2R6yNYn/p+F3ZA7npjMemLhSKZ4/PEJZYRtPthGj50mP5C4HQm17tV?=
 =?us-ascii?q?6zlJRfxvlyvUq95uolemnfGOyzpnShVOrDlLiZmPvUVjP6XZ65ZBVWzF/BIL8W?=
 =?us-ascii?q?WfFRAKq8F5Bd3ovqADguTIwavwMDJq99PS4NtZCc/JLs6OLHsmN1zuAjGQRC4E?=
 =?us-ascii?q?Rj6iMSn/gEdel7nG832SpZs9gpvhgpwDTvlcTlNjUrszDkl1Ec1KA55tUjo/kb?=
 =?us-ascii?q?OFl80J9DLqpRjSbMJIupnNTbSIHKOrYDOBgJFJYx8F27bxK8ISOsvyxggqclBn?=
 =?us-ascii?q?mJnRM1HZWNpE5Cp7YUt8jExT8XQ2YWw5wEXjIleh6WESGda7nxQtjQ04bf4q7D?=
 =?us-ascii?q?3h6k1xJ1aMrTZmw2crntCwyxmXfTPqLO+TQIRfEWLX8XZ7esfyRQt4dg30nlFh?=
 =?us-ascii?q?PS3sQ7NXkqsmdGpuzgTbvM0cSrZnUaRYbUpIlrmsbPIy3AEZ833/yA=3D=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AUAAA8vRlch0O0hNFkHAEBAQQBAQcEA?=
 =?us-ascii?q?QGBUgYBAQsBg2snjHWNPmiRY4R9gXMTAQEYEwGHHSI1CA0BAwEBAQEBAQIBEwE?=
 =?us-ascii?q?BAQoLCQgpL4I2JAGCYgIBAwECJAsBRgYJAQE+EgNUBgEKCAWDHYICAwKlQAEBA?=
 =?us-ascii?q?YFqM4oxh36EQYFXP4ERgl2LFQKJPIcTUpAFBwICkVIMGJFWLIkakAkCBAYFAhM?=
 =?us-ascii?q?BgUgDgglwgzyCJxeOJWoBgQQBAY1gAQE?=
X-IPAS-Result: =?us-ascii?q?A0AUAAA8vRlch0O0hNFkHAEBAQQBAQcEAQGBUgYBAQsBg2s?=
 =?us-ascii?q?njHWNPmiRY4R9gXMTAQEYEwGHHSI1CA0BAwEBAQEBAQIBEwEBAQoLCQgpL4I2J?=
 =?us-ascii?q?AGCYgIBAwECJAsBRgYJAQE+EgNUBgEKCAWDHYICAwKlQAEBAYFqM4oxh36EQYF?=
 =?us-ascii?q?XP4ERgl2LFQKJPIcTUpAFBwICkVIMGJFWLIkakAkCBAYFAhMBgUgDgglwgzyCJ?=
 =?us-ascii?q?xeOJWoBgQQBAY1gAQE?=
X-IronPort-AV: E=Sophos;i="5.56,371,1539673200"; 
   d="scan'208";a="55740703"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 18 Dec 2018 19:41:12 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1727383AbeLSDlK (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Tue, 18 Dec 2018 22:41:10 -0500
Received: from mx0a-001b2d01.pphosted.com ([148.163.156.1]:49942 "EHLO
        mx0a-001b2d01.pphosted.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S1726537AbeLSDlK (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Tue, 18 Dec 2018 22:41:10 -0500
Received: from pps.filterd (m0098393.ppops.net [127.0.0.1])
        by mx0a-001b2d01.pphosted.com (8.16.0.22/8.16.0.22) with SMTP id wBJ3dSb7050694
        for <linux-kernel@vger.kernel.org>; Tue, 18 Dec 2018 22:41:08 -0500
Received: from e34.co.us.ibm.com (e34.co.us.ibm.com [32.97.110.152])
        by mx0a-001b2d01.pphosted.com with ESMTP id 2pf988awbj-1
        (version=TLSv1.2 cipher=AES256-GCM-SHA384 bits=256 verify=NOT)
        for <linux-kernel@vger.kernel.org>; Tue, 18 Dec 2018 22:41:08 -0500
Received: from localhost
        by e34.co.us.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
        for <linux-kernel@vger.kernel.org> from <aneesh.kumar@linux.ibm.com>;
        Wed, 19 Dec 2018 03:41:07 -0000
Received: from b03cxnp08028.gho.boulder.ibm.com (9.17.130.20)
        by e34.co.us.ibm.com (192.168.1.134) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
        (version=TLSv1/SSLv3 cipher=AES256-GCM-SHA384 bits=256/256)
        Wed, 19 Dec 2018 03:41:03 -0000
Received: from b03ledav004.gho.boulder.ibm.com (b03ledav004.gho.boulder.ibm.com [9.17.130.235])
        by b03cxnp08028.gho.boulder.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id wBJ3f2O511731048
        (version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256 verify=FAIL);
        Wed, 19 Dec 2018 03:41:02 GMT
Received: from b03ledav004.gho.boulder.ibm.com (unknown [127.0.0.1])
        by IMSVA (Postfix) with ESMTP id 1A9737806A;
        Wed, 19 Dec 2018 03:41:02 +0000 (GMT)
Received: from b03ledav004.gho.boulder.ibm.com (unknown [127.0.0.1])
        by IMSVA (Postfix) with ESMTP id 662BD78068;
        Wed, 19 Dec 2018 03:40:58 +0000 (GMT)
Received: from skywalker.ibmuc.com (unknown [9.85.71.103])
        by b03ledav004.gho.boulder.ibm.com (Postfix) with ESMTP;
        Wed, 19 Dec 2018 03:40:58 +0000 (GMT)
From: "Aneesh Kumar K.V" <aneesh.kumar@linux.ibm.com>
To: akpm@linux-foundation.org, Michal Hocko <mhocko@kernel.org>,
        Alexey Kardashevskiy <aik@ozlabs.ru>, mpe@ellerman.id.au,
        paulus@samba.org, David Gibson <david@gibson.dropbear.id.au>
Cc: linux-mm@kvack.org, linux-kernel@vger.kernel.org,
        linuxppc-dev@lists.ozlabs.org,
        "Aneesh Kumar K.V" <aneesh.kumar@linux.ibm.com>
Subject: [PATCH V5 1/3] mm: Add get_user_pages_cma_migrate
Date: Wed, 19 Dec 2018 09:10:45 +0530
X-Mailer: git-send-email 2.19.2
In-Reply-To: <20181219034047.16305-1-aneesh.kumar@linux.ibm.com>
References: <20181219034047.16305-1-aneesh.kumar@linux.ibm.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
X-TM-AS-GCONF: 00
x-cbid: 18121903-0016-0000-0000-000009665F90
X-IBM-SpamModules-Scores: 
X-IBM-SpamModules-Versions: BY=3.00010246; HX=3.00000242; KW=3.00000007;
 PH=3.00000004; SC=3.00000271; SDB=6.01133803; UDB=6.00589449; IPR=6.00913977;
 MB=3.00024741; MTD=3.00000008; XFM=3.00000015; UTC=2018-12-19 03:41:06
X-IBM-AV-DETECTION: SAVI=unused REMOTE=unused XFE=unused
x-cbparentid: 18121903-0017-0000-0000-000041759891
Message-Id: <20181219034047.16305-2-aneesh.kumar@linux.ibm.com>
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10434:,, definitions=2018-12-19_02:,,
 signatures=0
X-Proofpoint-Spam-Details: rule=outbound_notspam policy=outbound score=0 priorityscore=1501
 malwarescore=0 suspectscore=0 phishscore=0 bulkscore=0 spamscore=0
 clxscore=1015 lowpriorityscore=0 mlxscore=0 impostorscore=0
 mlxlogscore=999 adultscore=0 classifier=spam adjust=0 reason=mlx
 scancount=1 engine=8.0.1-1810050000 definitions=main-1812190029
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

This helper does a get_user_pages_fast and if it find pages in the CMA area
it will try to migrate them before taking page reference. This makes sure that
we don't keep non-movable pages (due to page reference count) in the CMA area.
Not able to move pages out of CMA area result in CMA allocation failures.

Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
---
 include/linux/hugetlb.h |   2 +
 include/linux/migrate.h |   3 +
 mm/hugetlb.c            |   4 +-
 mm/migrate.c            | 139 ++++++++++++++++++++++++++++++++++++++++
 4 files changed, 146 insertions(+), 2 deletions(-)

diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
index 087fd5f48c91..1eed0cdaec0e 100644
--- a/include/linux/hugetlb.h
+++ b/include/linux/hugetlb.h
@@ -371,6 +371,8 @@ struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,
 				nodemask_t *nmask);
 struct page *alloc_huge_page_vma(struct hstate *h, struct vm_area_struct *vma,
 				unsigned long address);
+struct page *alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,
+				     int nid, nodemask_t *nmask);
 int huge_add_to_page_cache(struct page *page, struct address_space *mapping,
 			pgoff_t idx);
 
diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index f2b4abbca55e..d82b35afd2eb 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -286,6 +286,9 @@ static inline int migrate_vma(const struct migrate_vma_ops *ops,
 }
 #endif /* IS_ENABLED(CONFIG_MIGRATE_VMA_HELPER) */
 
+extern int get_user_pages_cma_migrate(unsigned long start, int nr_pages, int write,
+				      struct page **pages);
+
 #endif /* CONFIG_MIGRATION */
 
 #endif /* _LINUX_MIGRATE_H */
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 7f2a28ab46d5..faf3102ae45e 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -1585,8 +1585,8 @@ static struct page *alloc_surplus_huge_page(struct hstate *h, gfp_t gfp_mask,
 	return page;
 }
 
-static struct page *alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,
-		int nid, nodemask_t *nmask)
+struct page *alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,
+				     int nid, nodemask_t *nmask)
 {
 	struct page *page;
 
diff --git a/mm/migrate.c b/mm/migrate.c
index f7e4bfdc13b7..d564558fba03 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -2946,3 +2946,142 @@ int migrate_vma(const struct migrate_vma_ops *ops,
 }
 EXPORT_SYMBOL(migrate_vma);
 #endif /* defined(MIGRATE_VMA_HELPER) */
+
+static struct page *new_non_cma_page(struct page *page, unsigned long private)
+{
+	/*
+	 * We want to make sure we allocate the new page from the same node
+	 * as the source page.
+	 */
+	int nid = page_to_nid(page);
+	/*
+	 * Trying to allocate a page for migration. Ignore allocation
+	 * failure warnings
+	 */
+	gfp_t gfp_mask = GFP_USER | __GFP_THISNODE | __GFP_NOWARN;
+
+	if (PageHighMem(page))
+		gfp_mask |= __GFP_HIGHMEM;
+
+#ifdef CONFIG_HUGETLB_PAGE
+	if (PageHuge(page)) {
+		struct hstate *h = page_hstate(page);
+		/*
+		 * We don't want to dequeue from the pool because pool pages will
+		 * mostly be from the CMA region.
+		 */
+		return alloc_migrate_huge_page(h, gfp_mask, nid, NULL);
+	}
+#endif
+	if (PageTransHuge(page)) {
+		struct page *thp;
+		/*
+		 * ignore allocation failure warnings
+		 */
+		gfp_t thp_gfpmask = GFP_TRANSHUGE | __GFP_THISNODE | __GFP_NOWARN;
+
+		/*
+		 * Remove the movable mask so that we don't allocate from
+		 * CMA area again.
+		 */
+		thp_gfpmask &= ~__GFP_MOVABLE;
+		thp = __alloc_pages_node(nid, thp_gfpmask, HPAGE_PMD_ORDER);
+		if (!thp)
+			return NULL;
+		prep_transhuge_page(thp);
+		return thp;
+	}
+
+	return __alloc_pages_node(nid, gfp_mask, 0);
+}
+
+/**
+ * get_user_pages_cma_migrate() - pin user pages in memory by migrating pages in CMA region
+ * @start:	starting user address
+ * @nr_pages:	number of pages from start to pin
+ * @write:	whether pages will be written to
+ * @pages:	array that receives pointers to the pages pinned.
+ *		Should be at least nr_pages long.
+ *
+ * Attempt to pin user pages in memory without taking mm->mmap_sem.
+ * If not successful, it will fall back to taking the lock and
+ * calling get_user_pages().
+ *
+ * If the pinned pages are backed by CMA region, we migrate those pages out,
+ * allocating new pages from non-CMA region. This helps in avoiding keeping
+ * pages pinned in the CMA region for a long time thereby resulting in
+ * CMA allocation failures.
+ *
+ * Returns number of pages pinned. This may be fewer than the number
+ * requested. If nr_pages is 0 or negative, returns 0. If no pages
+ * were pinned, returns -errno.
+ */
+
+int get_user_pages_cma_migrate(unsigned long start, int nr_pages, int write,
+			       struct page **pages)
+{
+	int i, ret;
+	bool drain_allow = true;
+	bool migrate_allow = true;
+	LIST_HEAD(cma_page_list);
+
+get_user_again:
+	ret = get_user_pages_fast(start, nr_pages, write, pages);
+	if (ret <= 0)
+		return ret;
+
+	for (i = 0; i < ret; ++i) {
+		/*
+		 * If we get a page from the CMA zone, since we are going to
+		 * be pinning these entries, we might as well move them out
+		 * of the CMA zone if possible.
+		 */
+		if (is_migrate_cma_page(pages[i]) && migrate_allow) {
+
+			struct page *head = compound_head(pages[i]);
+
+			if (PageHuge(head))
+				isolate_huge_page(head, &cma_page_list);
+			else {
+				if (!PageLRU(head) && drain_allow) {
+					lru_add_drain_all();
+					drain_allow = false;
+				}
+
+				if (!isolate_lru_page(head)) {
+					list_add_tail(&head->lru, &cma_page_list);
+					mod_node_page_state(page_pgdat(head),
+							    NR_ISOLATED_ANON +
+							    page_is_file_cache(head),
+							    hpage_nr_pages(head));
+				}
+			}
+		}
+	}
+	if (!list_empty(&cma_page_list)) {
+		/*
+		 * drop the above get_user_pages reference.
+		 */
+		for (i = 0; i < ret; ++i)
+			put_page(pages[i]);
+
+		if (migrate_pages(&cma_page_list, new_non_cma_page,
+				  NULL, 0, MIGRATE_SYNC, MR_CONTIG_RANGE)) {
+			/*
+			 * some of the pages failed migration. Do get_user_pages
+			 * without migration.
+			 */
+			migrate_allow = false;
+
+			if (!list_empty(&cma_page_list))
+				putback_movable_pages(&cma_page_list);
+		}
+		/*
+		 * We did migrate all the pages, Try to get the page references again
+		 * migrating any new CMA pages which we failed to isolate earlier.
+		 */
+		drain_allow = true;
+		goto get_user_again;
+	}
+	return ret;
+}
-- 
2.19.2

