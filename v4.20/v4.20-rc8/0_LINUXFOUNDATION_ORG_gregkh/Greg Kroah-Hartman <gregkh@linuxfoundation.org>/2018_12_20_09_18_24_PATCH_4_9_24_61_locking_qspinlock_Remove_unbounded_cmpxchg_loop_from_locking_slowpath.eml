Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by i7-8700 with POP3-SSL;
  20 Dec 2018 19:15:12 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga006.fm.intel.com (fmsmga006.fm.intel.com [10.253.24.20])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 5E67C580522;
	Thu, 20 Dec 2018 01:40:11 -0800 (PST)
Received: from fmsmga101.fm.intel.com ([10.1.193.65])
  by fmsmga006-1.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 20 Dec 2018 01:40:10 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AKeggAxYpf6ECzvA1PTpAIUD/LSx+4OfEezUN459i?=
 =?us-ascii?q?sYplN5qZpci/bR7h7PlgxGXEQZ/co6odzbaO4+a4ASQp2tWoiDg6aptCVhsI24?=
 =?us-ascii?q?09vjcLJ4q7M3D9N+PgdCcgHc5PBxdP9nC/NlVJSo6lPwWB6nK94iQPFRrhKAF7?=
 =?us-ascii?q?Ovr6GpLIj8Swyuu+54Dfbx9HiTahYr5+Ngm6oRnMvcQKnIVuLbo8xAHUqXVSYe?=
 =?us-ascii?q?RWwm1oJVOXnxni48q74YBu/SdNtf8/7sBMSar1cbg2QrxeFzQmLns65Nb3uhnZ?=
 =?us-ascii?q?TAuA/WUTX2MLmRdVGQfF7RX6XpDssivms+d2xSeXMdHqQb0yRD+v9LlgRgP2hy?=
 =?us-ascii?q?gbNj456GDXhdJ2jKJHuxKquhhzz5fJbI2JKPZye6XQds4YS2VcRMZcTyxPDJ2h?=
 =?us-ascii?q?YYUBDOQPOuRXr4fzqFQBsRSwChKsBPvtxzJTmn/73qM33/g7HAzc3gEtGc8Fvn?=
 =?us-ascii?q?TOrNXyMacfSeG7zLPWwjXMcvhY3C396I/SfRAhuP2CX6h/cdDNyUkqDQzFiEib?=
 =?us-ascii?q?pIvqPzOPyOsNvGub7+p7WOKrim4nrRxxozehxscrl4nJgJ8axUrY9SV4x4Y1I8?=
 =?us-ascii?q?e0SElhYd6rCZZdsTyROYVxQsMnWW5ouSA6x6UFuZGlZigKzoooxxrFZ/yAaYiI?=
 =?us-ascii?q?7RTuX/uSLzdgnH9pZq6zihKo/US9xODwSNO43EtJoyZZiNXBt3IA2wTQ58WJUP?=
 =?us-ascii?q?dw/Uas1SyM2g3X8O1IPFw4mKjdJpU82LA/jIATvl7GHiLumEX5kquWdkI89+i2?=
 =?us-ascii?q?5OTofK/mqoWfN4BqkAH+NLohmsilDeQ/KAgOUHCX+eW61LL94U30WKtGguEyn6?=
 =?us-ascii?q?XDrZzXK9oXqrSkDwJWzoov8ReyAjW+3NQdh3YHLVZFeBydj4juPlHDOPT4Dfa5?=
 =?us-ascii?q?g1SxnzZn3vPGMaP7ApXLMHfDlK7tfbFz6k5a1gUz18tS54xbCr4fOvL/QEzxu8?=
 =?us-ascii?q?LCDh8/LQO0x/zrCNJn1oMRQW6PGLOWMLvOsV+U4eIiO+2MZI4WuDnjMfQk6OPu?=
 =?us-ascii?q?gGQ9mV8ce6mpwJQWZGq5HvRgP0WWf37sjs0dHmcNuwo0VPbqh0GaUT5Pe3ayWL?=
 =?us-ascii?q?ox5jEhB4KnEYfMXIetjKaB3CemBJJWYG9GB0uIEXfpcYWERvgNZDiTIs9njjwL?=
 =?us-ascii?q?S7yhR5U92hGpsQ/w06BnIfbM+i0EqZLj08B46PfIlREy8jx0DN6R03uXQGF2hW?=
 =?us-ascii?q?4IQz423KZioU1y0FuD0K54g+BGGtxX/f9GTgA6NZvExexgF9/yQh7BfsuOSFu+?=
 =?us-ascii?q?QdWpGzcxQsg1w98PeUl9HdqigwvH3yqrBb8VirOKCIY18qLaw3j+OcJ9x2za26?=
 =?us-ascii?q?kmilksWtFPOnG+hq5j6wjTAJbEnFiDmKa0a6sQxi7N+32FzWqVok5YVgl8UaHG?=
 =?us-ascii?q?XXAaYkvbttD55kLET7+zBrUrKApBycieKqRUbt3ll0lJRPDmON7GeWK+h3+wBQ?=
 =?us-ascii?q?qUxrOLdIflZn8S3DvDB0QekwAc53CGNRMgCSenuG/eCD1uFVTyY0Lj6+V+qXW7?=
 =?us-ascii?q?TlMqwAGOdUFuy721+hsNj/yGV/wTxq4EuDsmqzhsAFa93tfWC92cpwphfKRQe8?=
 =?us-ascii?q?897E1A1WLDswx9P5qgL695i14acgR3uV7u1hptBoVBl8gqsG0lzA5oJa2E11NB?=
 =?us-ascii?q?ciuS3YrsNb3PNmny4BevZrbM1VHaztmX9bkA6fQip1r4oQGmC1At83Nk09lSzX?=
 =?us-ascii?q?uR/ZHKDAsUUZLsXUc77Rl6p7fGYiYj44PYz2FjMa6xsjXawdImGPMlygq8f9dY?=
 =?us-ascii?q?KK6FFBLyH9cGCMS0Ke0qm0KmbhQLPO1J8K40PsWmd+aJ2aKxPeZgmi6mgnpD4I?=
 =?us-ascii?q?xnzk2M8C98QPbS35kZ2/GYwheHVzDkgVe7s8D4hZpLZSsPEWq40yTkApBeZrdz?=
 =?us-ascii?q?fYoSFWihOcm3ychgiJ73XH5Y8kWuB1cH2M+vZBqTYEbx3QxW1UQLv3OnnTG0wC?=
 =?us-ascii?q?BzkzEstqCfxjDBw/z+dBobPW5GXGljjVb2LYm0ldwaR1Wobw4ymRuh5Eb6wbVb?=
 =?us-ascii?q?patlI2nSR0dIYzb5L2V4XqSssbqCZtZF6Ik0viVPTOS8fVeaR6b/oxQAySPjHG?=
 =?us-ascii?q?hexDchezGxoJr5nB96iGObLHltqnrUYsVwxRbZ5NzBSv9dxDsGRC9kiTbJAliw?=
 =?us-ascii?q?JcWm/dKRl53bqOCxS3qhVoFPcSns1Y6BtDG05WpwDR29nvCznMbqEQw70S/9yt?=
 =?us-ascii?q?lrWj/ErBf6Yon3yau6NfhrcVVvBF/588B6AJ1xkpMshJEM3ngXnpaV8mAGkWvp?=
 =?us-ascii?q?MdVbxLjxbHwCRTMQx97V4Q7l2FBsL36TxoL5UGmdzdVlZ9WgfmwW3Sc95dhQCK?=
 =?us-ascii?q?iI9LxEgTd1ol2goALUYPh9nS0SxeEg6XEEmOEJpBQizj+GDb8MB0lYMjfhlxCJ?=
 =?us-ascii?q?79C4saVWa3yjcbm21Epig9+hCKuOrR1bWHb8Yp0iBzN/7t1jMFLQ133+8p3keN?=
 =?us-ascii?q?7Vbd4JtxybiRHAj/VOKJIql/oHni5nOWP7vX04xO83lx1u3ZemvIeZL2Vh5r62?=
 =?us-ascii?q?AhldNjftfcMc5intjbpCnsaRx42gBY9uGjIPXJv1V/6oFC8SuO/jNwaPFj08t3?=
 =?us-ascii?q?iaFaDeHQ+Z9EdpsXbPH4q3OHGQIXkT1c9iSwWFJExDnAAUWy03npwjGQCv2sPh?=
 =?us-ascii?q?a1115iwL6l77txtMyf9oNx/lXWfEvwqobjY0SJ6CLBtZ9A1C5kHVMdCA4eJ3BS?=
 =?us-ascii?q?1X4pqhrAmVIGyBewtIFX0JWlCDB137Priu4sTA8/GFBuWkKfvCe66OqetYV/eH?=
 =?us-ascii?q?3p+v1opm/zCRNsSAJHViDvs72lZdUnB9AcjWhzIPSykPnSLXc8GbvAu8+jFwrs?=
 =?us-ascii?q?2n8PTkRgPv5YiMC7tTK9lv+Aq2gaCMN+GOniZ5NC1Y24gIxX/JzrgfwVEThztv?=
 =?us-ascii?q?dzmrDbQPqyrNQLjMla9QCh4RczlzO9dQ76IgwglNPtbWhcno1rFjlP46FVdEWU?=
 =?us-ascii?q?blmsGofsEKJ2C9NFXaBEeELriGJDvLw93pbqO4U7FfkOJUtxioszaBD0DjJiiD?=
 =?us-ascii?q?lyXuVx23LeFMjSSbMAZCt42nbhltCXbsTNT9Zx2/Md93iyA2wLIuinPLM24cLS?=
 =?us-ascii?q?Zzc0dXorKM6iNYh+11G3Zd4Xp9MemEhyGZ4vHYK5kMt/trByV0l+RC73U7y7ta?=
 =?us-ascii?q?9idERPNulSvWr95upUymk+aVxjpmVhpOtihEhIaRsUp+PqXZ88oIZXGR+BML8H?=
 =?us-ascii?q?XVCBkQodZhIsPgtroWydXVkq/3bjBY/IH658wZUufOJc3PE2AmOBqsODfOEA4M?=
 =?us-ascii?q?S3b/OnvSiE9duPWT8GCFoJ8nrJTlhJsJTPlcTlNjRaBSMVhsANFXeMQ/ZTgji7?=
 =?us-ascii?q?POyZdQvXc=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0ANAAAnYhtch0O0hNFbCBwBAQEEAQEHB?=
 =?us-ascii?q?AEBgVEHAQELAYFVgRSBAieDfYgZX4sdgiGJAYUwiRgUgV8VGA0GAYctIjQJDQE?=
 =?us-ascii?q?DAQEBAQEBAgETAQEBCA0JCCkjDII2JAGCYgMDAQIgBAsBDQEBNwEFCQEBJAIFI?=
 =?us-ascii?q?QICAwwSBx4RGQUagwMBgWkDFQQBCqZkcHwMJ4J2AQEFgTABg1QNghUIgQuGc4M?=
 =?us-ascii?q?lgRwXgX+GeoF3AQ0FAYMngleJKSAEgXeVMBgzCYcRhjJwgyYkgV9NhFKKXotAg?=
 =?us-ascii?q?weBEooygUaBHXEzGggoCIMnCQqCCAwXEohMhUA/MgEBgQADAQEhE4pugj4BAQ?=
X-IPAS-Result: =?us-ascii?q?A0ANAAAnYhtch0O0hNFbCBwBAQEEAQEHBAEBgVEHAQELAYF?=
 =?us-ascii?q?VgRSBAieDfYgZX4sdgiGJAYUwiRgUgV8VGA0GAYctIjQJDQEDAQEBAQEBAgETA?=
 =?us-ascii?q?QEBCA0JCCkjDII2JAGCYgMDAQIgBAsBDQEBNwEFCQEBJAIFIQICAwwSBx4RGQU?=
 =?us-ascii?q?agwMBgWkDFQQBCqZkcHwMJ4J2AQEFgTABg1QNghUIgQuGc4MlgRwXgX+GeoF3A?=
 =?us-ascii?q?Q0FAYMngleJKSAEgXeVMBgzCYcRhjJwgyYkgV9NhFKKXotAgweBEooygUaBHXE?=
 =?us-ascii?q?zGggoCIMnCQqCCAwXEohMhUA/MgEBgQADAQEhE4pugj4BAQ?=
X-IronPort-AV: E=Sophos;i="5.56,376,1539673200"; 
   d="scan'208";a="67674767"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mga01b.intel.com with ESMTP; 20 Dec 2018 01:40:10 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1731939AbeLTJZO (ORCPT <rfc822;like.xu@linux.intel.com>
        + 22 others); Thu, 20 Dec 2018 04:25:14 -0500
Received: from mail.kernel.org ([198.145.29.99]:41818 "EHLO mail.kernel.org"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1730017AbeLTJZH (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Thu, 20 Dec 2018 04:25:07 -0500
Received: from localhost (5356596B.cm-6-7b.dynamic.ziggo.nl [83.86.89.107])
        (using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
        (No client certificate requested)
        by mail.kernel.org (Postfix) with ESMTPSA id 665E5217D9;
        Thu, 20 Dec 2018 09:25:06 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=kernel.org;
        s=default; t=1545297906;
        bh=UbNsHsbYJVbOmpI+pe4996Tja8FvHzTIrhaqSPBg9OI=;
        h=From:To:Cc:Subject:Date:In-Reply-To:References:From;
        b=ZPc2wIWLb4fcskJvPUlTn4YEUYgMjeRfr9HDDJPv6Lir1859cQIed/eHSgcfCUgtG
         2Jsf7UM7scD95ckxCt6SikLf1KioQI70+YpTSRe2CzwR+KP+xZd/6xEgNbhD0TgbZC
         6JmoSTMiKL26evLEyP6joRyrr+HD3r0ZmV5RxjN4=
From: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
To: linux-kernel@vger.kernel.org
Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>,
        stable@vger.kernel.org, Will Deacon <will.deacon@arm.com>,
        "Peter Zijlstra (Intel)" <peterz@infradead.org>,
        Waiman Long <longman@redhat.com>,
        Linus Torvalds <torvalds@linux-foundation.org>,
        Thomas Gleixner <tglx@linutronix.de>, boqun.feng@gmail.com,
        linux-arm-kernel@lists.infradead.org, paulmck@linux.vnet.ibm.com,
        Ingo Molnar <mingo@kernel.org>,
        Sebastian Andrzej Siewior <bigeasy@linutronix.de>,
        Sasha Levin <sashal@kernel.org>
Subject: [PATCH 4.9 24/61] locking/qspinlock: Remove unbounded cmpxchg() loop from locking slowpath
Date: Thu, 20 Dec 2018 10:18:24 +0100
Message-Id: <20181220085844.710354700@linuxfoundation.org>
X-Mailer: git-send-email 2.20.1
In-Reply-To: <20181220085843.743900603@linuxfoundation.org>
References: <20181220085843.743900603@linuxfoundation.org>
User-Agent: quilt/0.65
X-stable: review
X-Patchwork-Hint: ignore
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

4.9-stable review patch.  If anyone has any objections, please let me know.

------------------

commit 59fb586b4a07b4e1a0ee577140ab4842ba451acd upstream.

The qspinlock locking slowpath utilises a "pending" bit as a simple form
of an embedded test-and-set lock that can avoid the overhead of explicit
queuing in cases where the lock is held but uncontended. This bit is
managed using a cmpxchg() loop which tries to transition the uncontended
lock word from (0,0,0) -> (0,0,1) or (0,0,1) -> (0,1,1).

Unfortunately, the cmpxchg() loop is unbounded and lockers can be starved
indefinitely if the lock word is seen to oscillate between unlocked
(0,0,0) and locked (0,0,1). This could happen if concurrent lockers are
able to take the lock in the cmpxchg() loop without queuing and pass it
around amongst themselves.

This patch fixes the problem by unconditionally setting _Q_PENDING_VAL
using atomic_fetch_or, and then inspecting the old value to see whether
we need to spin on the current lock owner, or whether we now effectively
hold the lock. The tricky scenario is when concurrent lockers end up
queuing on the lock and the lock becomes available, causing us to see
a lockword of (n,0,0). With pending now set, simply queuing could lead
to deadlock as the head of the queue may not have observed the pending
flag being cleared. Conversely, if the head of the queue did observe
pending being cleared, then it could transition the lock from (n,0,0) ->
(0,0,1) meaning that any attempt to "undo" our setting of the pending
bit could race with a concurrent locker trying to set it.

We handle this race by preserving the pending bit when taking the lock
after reaching the head of the queue and leaving the tail entry intact
if we saw pending set, because we know that the tail is going to be
updated shortly.

Signed-off-by: Will Deacon <will.deacon@arm.com>
Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Acked-by: Waiman Long <longman@redhat.com>
Cc: Linus Torvalds <torvalds@linux-foundation.org>
Cc: Thomas Gleixner <tglx@linutronix.de>
Cc: boqun.feng@gmail.com
Cc: linux-arm-kernel@lists.infradead.org
Cc: paulmck@linux.vnet.ibm.com
Link: http://lkml.kernel.org/r/1524738868-31318-6-git-send-email-will.deacon@arm.com
Signed-off-by: Ingo Molnar <mingo@kernel.org>
Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Signed-off-by: Sasha Levin <sashal@kernel.org>
---
 kernel/locking/qspinlock.c          | 102 ++++++++++++++++------------
 kernel/locking/qspinlock_paravirt.h |   5 --
 2 files changed, 58 insertions(+), 49 deletions(-)

diff --git a/kernel/locking/qspinlock.c b/kernel/locking/qspinlock.c
index cc98050e8bec..e7ab99a1f438 100644
--- a/kernel/locking/qspinlock.c
+++ b/kernel/locking/qspinlock.c
@@ -126,6 +126,17 @@ static inline __pure struct mcs_spinlock *decode_tail(u32 tail)
 #define _Q_LOCKED_PENDING_MASK (_Q_LOCKED_MASK | _Q_PENDING_MASK)
 
 #if _Q_PENDING_BITS == 8
+/**
+ * clear_pending - clear the pending bit.
+ * @lock: Pointer to queued spinlock structure
+ *
+ * *,1,* -> *,0,*
+ */
+static __always_inline void clear_pending(struct qspinlock *lock)
+{
+	WRITE_ONCE(lock->pending, 0);
+}
+
 /**
  * clear_pending_set_locked - take ownership and clear the pending bit.
  * @lock: Pointer to queued spinlock structure
@@ -161,6 +172,17 @@ static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)
 
 #else /* _Q_PENDING_BITS == 8 */
 
+/**
+ * clear_pending - clear the pending bit.
+ * @lock: Pointer to queued spinlock structure
+ *
+ * *,1,* -> *,0,*
+ */
+static __always_inline void clear_pending(struct qspinlock *lock)
+{
+	atomic_andnot(_Q_PENDING_VAL, &lock->val);
+}
+
 /**
  * clear_pending_set_locked - take ownership and clear the pending bit.
  * @lock: Pointer to queued spinlock structure
@@ -382,7 +404,7 @@ EXPORT_SYMBOL(queued_spin_unlock_wait);
 void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 {
 	struct mcs_spinlock *prev, *next, *node;
-	u32 new, old, tail;
+	u32 old, tail;
 	int idx;
 
 	BUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));
@@ -405,59 +427,51 @@ void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 					       (VAL != _Q_PENDING_VAL) || !cnt--);
 	}
 
+	/*
+	 * If we observe any contention; queue.
+	 */
+	if (val & ~_Q_LOCKED_MASK)
+		goto queue;
+
 	/*
 	 * trylock || pending
 	 *
 	 * 0,0,0 -> 0,0,1 ; trylock
 	 * 0,0,1 -> 0,1,1 ; pending
 	 */
-	for (;;) {
+	val = atomic_fetch_or_acquire(_Q_PENDING_VAL, &lock->val);
+	if (!(val & ~_Q_LOCKED_MASK)) {
 		/*
-		 * If we observe any contention; queue.
+		 * We're pending, wait for the owner to go away.
+		 *
+		 * *,1,1 -> *,1,0
+		 *
+		 * this wait loop must be a load-acquire such that we match the
+		 * store-release that clears the locked bit and create lock
+		 * sequentiality; this is because not all
+		 * clear_pending_set_locked() implementations imply full
+		 * barriers.
 		 */
-		if (val & ~_Q_LOCKED_MASK)
-			goto queue;
-
-		new = _Q_LOCKED_VAL;
-		if (val == new)
-			new |= _Q_PENDING_VAL;
+		if (val & _Q_LOCKED_MASK) {
+			smp_cond_load_acquire(&lock->val.counter,
+					      !(VAL & _Q_LOCKED_MASK));
+		}
 
 		/*
-		 * Acquire semantic is required here as the function may
-		 * return immediately if the lock was free.
+		 * take ownership and clear the pending bit.
+		 *
+		 * *,1,0 -> *,0,1
 		 */
-		old = atomic_cmpxchg_acquire(&lock->val, val, new);
-		if (old == val)
-			break;
-
-		val = old;
-	}
-
-	/*
-	 * we won the trylock
-	 */
-	if (new == _Q_LOCKED_VAL)
+		clear_pending_set_locked(lock);
 		return;
+	}
 
 	/*
-	 * we're pending, wait for the owner to go away.
-	 *
-	 * *,1,1 -> *,1,0
-	 *
-	 * this wait loop must be a load-acquire such that we match the
-	 * store-release that clears the locked bit and create lock
-	 * sequentiality; this is because not all clear_pending_set_locked()
-	 * implementations imply full barriers.
-	 */
-	smp_cond_load_acquire(&lock->val.counter, !(VAL & _Q_LOCKED_MASK));
-
-	/*
-	 * take ownership and clear the pending bit.
-	 *
-	 * *,1,0 -> *,0,1
+	 * If pending was clear but there are waiters in the queue, then
+	 * we need to undo our setting of pending before we queue ourselves.
 	 */
-	clear_pending_set_locked(lock);
-	return;
+	if (!(val & _Q_PENDING_MASK))
+		clear_pending(lock);
 
 	/*
 	 * End of pending bit optimistic spinning and beginning of MCS
@@ -561,15 +575,15 @@ locked:
 	 * claim the lock:
 	 *
 	 * n,0,0 -> 0,0,1 : lock, uncontended
-	 * *,0,0 -> *,0,1 : lock, contended
+	 * *,*,0 -> *,*,1 : lock, contended
 	 *
-	 * If the queue head is the only one in the queue (lock value == tail),
-	 * clear the tail code and grab the lock. Otherwise, we only need
-	 * to grab the lock.
+	 * If the queue head is the only one in the queue (lock value == tail)
+	 * and nobody is pending, clear the tail code and grab the lock.
+	 * Otherwise, we only need to grab the lock.
 	 */
 	for (;;) {
 		/* In the PV case we might already have _Q_LOCKED_VAL set */
-		if ((val & _Q_TAIL_MASK) != tail) {
+		if ((val & _Q_TAIL_MASK) != tail || (val & _Q_PENDING_MASK)) {
 			set_locked(lock);
 			break;
 		}
diff --git a/kernel/locking/qspinlock_paravirt.h b/kernel/locking/qspinlock_paravirt.h
index 3e33336911ff..9c07c72fb10e 100644
--- a/kernel/locking/qspinlock_paravirt.h
+++ b/kernel/locking/qspinlock_paravirt.h
@@ -88,11 +88,6 @@ static __always_inline void set_pending(struct qspinlock *lock)
 	WRITE_ONCE(lock->pending, 1);
 }
 
-static __always_inline void clear_pending(struct qspinlock *lock)
-{
-	WRITE_ONCE(lock->pending, 0);
-}
-
 /*
  * The pending bit check in pv_queued_spin_steal_lock() isn't a memory
  * barrier. Therefore, an atomic cmpxchg() is used to acquire the lock
-- 
2.19.1



