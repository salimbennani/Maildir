Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by i7-8700 with POP3-SSL;
  20 Dec 2018 19:15:06 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga006.fm.intel.com (fmsmga006.fm.intel.com [10.253.24.20])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 3575C580522;
	Thu, 20 Dec 2018 01:38:49 -0800 (PST)
Received: from fmsmga102.fm.intel.com ([10.1.193.69])
  by fmsmga006-1.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 20 Dec 2018 01:38:48 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AWlIC1RxbqIgSA/XXCy+O+j09IxM/srCxBDY+r6Qd?=
 =?us-ascii?q?0e4TI/ad9pjvdHbS+e9qxAeQG9mDu7Qc06L/iOPJYSQ4+5GPsXQPItRndiQuro?=
 =?us-ascii?q?EopTEmG9OPEkbhLfTnPGQQFcVGU0J5rTngaRAGUMnxaEfPrXKs8DUcBgvwNRZv?=
 =?us-ascii?q?JuTyB4Xek9m72/q99pHPYAhEniaxba9vJxiqsAvdsdUbj5F/Iagr0BvJpXVIe+?=
 =?us-ascii?q?VSxWx2IF+Yggjx6MSt8pN96ipco/0u+dJOXqX8ZKQ4UKdXDC86PGAv5c3krgfM?=
 =?us-ascii?q?QA2S7XYBSGoWkx5IAw/Y7BHmW5r6ryX3uvZh1CScIMb7Vq4/Vyi84Kh3SR/okC?=
 =?us-ascii?q?YHOCA/8GHLkcx7kaZXrAu8qxBj34LYZYeYP+d8cKzAZ9MXXWpPUNhMWSJPAY2y?=
 =?us-ascii?q?aIkAD+QPMulXs4bzqEAOrQO8CAS3GOPiyTFFimPs0KAg0eksFxzN0gw6H9IJtX?=
 =?us-ascii?q?TZtNT7NL0MXuC60aLGyC/Db/RM1jf98YTGcAouoeuQXbJ1a8XRz1QkGgTKjlWK?=
 =?us-ascii?q?t4PlMDCV1uQWvmif7upgU/+vimEpqwF2vzivwNojhZPVhoIUzVDE8z91wIEvJd?=
 =?us-ascii?q?23UUN2Z8OvHpVXtyGfLYR2Q8UiTnlruCkk0L0Gv4C0fCwQxJQg3R7fZPqKeJWL?=
 =?us-ascii?q?7BL7TOudPyt0iXZ/dL6iiRu+71KsxvD/W8WoylpHryhInsHPu30DzRDe6cmKRu?=
 =?us-ascii?q?F+80qlwzqDyhzf5+NCLEspj6TUMYQhzaQ1lpcLsUTMACv2mELuga+IeUUr5PKo?=
 =?us-ascii?q?5/7kYrr4vJ+cMZF7igXkPqQpgMy/Dvw0MgkIX2eF5eSxzKPv8VH9TblQk/E7nL?=
 =?us-ascii?q?fVvIrHKckYuqK1GQ5Y34Q75xa6FTim0dAYnXcdLFJCfRKKl4zpO1DIIPDlAvaz?=
 =?us-ascii?q?mlesnylxx/DAILLhBozBLn/NkbfnY7l98VVRyBQ8zd9B/ZJYELIBL+zpWk/3qt?=
 =?us-ascii?q?PYCgU1Mwuuw+boENl9zJ8RWXqTAq+FN6PfqVuI5uMsI+aSfoMUtyv9JuMh5/7v?=
 =?us-ascii?q?i385hFAccbOo3ZsRdHC3APBmL1+Fbnrrh9cLCX0KsRYmTOz2lF2CViZeZ3aoUK?=
 =?us-ascii?q?I9+jE0EoWmAZ3DRoCwmrOB2ii7E4ZSZmBHDFCMDHjpe5+FW/cKdCKdPMthniYY?=
 =?us-ascii?q?WrimTo8rzQuuuxPiy7p7MurU/TUVuoj41Nh14O3Tlgs+9SZuAMSfyGyNS2B0nm?=
 =?us-ascii?q?UVRz45xqx/oEp9ykud3qh8mfBXCdtT5/ZRWAcgKZHc1/B6C8z1Wg/ZfdeGUlCm?=
 =?us-ascii?q?Ts+iATEwVN0xxdAObl15G9WjiBDDwiWrD6UUl7yNGJw77Kbc02LtKMZ6znbMzL?=
 =?us-ascii?q?MhgEU+QstTKW2mgbZy+BXJCI7XjUqVjaaqer4a3C7W6miDy22CvEVbUA51VaXI?=
 =?us-ascii?q?RnQfZkrQrdTk6ULOVb6uCbI7MgRfzc6OMLdFatrsjV9eXvfsJMzeY36tm2e3HR?=
 =?us-ascii?q?uH26mDbJT0dGkH3CXSEk4EkxsN8naALgU+Aiaho2TDDD1hD17vYkXs8fVgp3O/?=
 =?us-ascii?q?VEM70waKb0h53bqv5hEVneCcS+8U3r8cpCchqjB0HFGh39LWC9uMvRZhcL9bYd?=
 =?us-ascii?q?Mn5FdH1GTZtwNmM5ykLqBigEMecgtts0Pv0RV3FptPkcwwoHw2ywpyLLqS0Eld?=
 =?us-ascii?q?eDOAwZDwJrrXJ3Ho8xCrdaHX1U/R0daM9qgU9fQ3tk/svAeqFkol7XVqyN1V03?=
 =?us-ascii?q?qa5pXXAwseS5PxUkAr9xdko7HWeDUy54TR1Xd0K6m7rifC2841BOsi0huhf8pf?=
 =?us-ascii?q?PLmYGwPoEswaB9KhKOolm1WyahIEPeZS9LM7Ps+8dvuG3rKrM/hknD68kWtH54?=
 =?us-ascii?q?V92FqW9yVgUu7Iw4oFw/aA0wudVjfzkE2ustrqloBDfz0SGHSwyTLlBIJIeqJ9?=
 =?us-ascii?q?Z4ILBnqwLM2twdV+gYXgW3pZ9F6lGlMH19WleRuUb1zhwwJQ0V4brmCgmSu91z?=
 =?us-ascii?q?Z0iS0mrrKD3CzSxOTvbBoGOm9RSGhil1vsOpW0j9YBUUisbggpkgal5Engy6ha?=
 =?us-ascii?q?oqR/M3fcQUNScyfqKGFiV7O6tqCebM5X9JMorSJXXfy+YV+AUL79oBga0yT5E2?=
 =?us-ascii?q?tF3j87dDKqupT/nxNkjmKQN3JzrHvfecFtyhbT/t3cRfhN3jUYQCl0kyXYBl+5?=
 =?us-ascii?q?P9Ox59Wbi4/DsvyiV2KmTpBScTPkzYSauCu55G1qBwayn/Symt3hDAg73jX319?=
 =?us-ascii?q?hsVSXUshn8ZpPn2Li9MeJiZkNoHkPz69JmGoFilYs9nJIQ1mIbhpmP/XoHjH38?=
 =?us-ascii?q?MdNU2a/laHoNRDgLw8Pa4QT/2U1jKG6JyJz9VnmH3sRhYNy6aHsM2i0h98BKFL?=
 =?us-ascii?q?uU7LtckCtwo1q4rhjebeJznzgD0vsu9GAVg/sStwUz1CWdDasfHU1ZPSzqihSJ?=
 =?us-ascii?q?4MqyrKRRZGazb7ew0FByksymDLGHugtcQmr2eo8+HS9s6cVyKE7M0Hr26o3+ed?=
 =?us-ascii?q?jfd9QTtgCPkxfbjuhYM5YxlvsMhSp6NmPxp3wly+gnjRNw2ZG2ppSIK2Jo/Kih?=
 =?us-ascii?q?GB5XKiX1Z98P+jHqlatRhdyZ34erHpV8ADkLWIboQOmsEDITs/TnKgmPHCc9qn?=
 =?us-ascii?q?edBbrQAwuf5F16oHLIFpChL2uXK2UBzdV+WBmdI1RSgAIOUzU/gJE5DRqmxND7?=
 =?us-ascii?q?fEd6+zAR5UP4qh1WxeJzLBT/VmbfpAG1ajY7UpSfLRxW7h1c6EfRK8CR8uVzHy?=
 =?us-ascii?q?RA9J27sAONMnCbZxhPDWwRWk2EAE3jPqCz5dba9eiUHPG+L/zIYbWBsuFeU/aI?=
 =?us-ascii?q?xZSy0opp5TqMN8OPPmV8AP0/wEZMQXd5G8HBkTUVVyMXjz7Nb9KcpBqk+i14sM?=
 =?us-ascii?q?G/8PfoWALu/4ePCKFSMdJg+x+om6eDKvWQiT19KTZZ0JMM2HDJxKIe3F4UlyFh?=
 =?us-ascii?q?aT2tHa4cui7KSaLagrVXAAIDayNvKMtI6Lox0RNQOcHAkNP6yL55guQxC1dETl?=
 =?us-ascii?q?HhnsCpZcoXI2CyLl/HBUCLNKiYKj3P2c34faS8SbhIhuVOqxKwoSqbE1PkPjmb?=
 =?us-ascii?q?iznpVhWvPftQgy2BIBNeuJ+ychB2BGjnTdLmbAC7MdBtgT03x700mm3FNWoGPT?=
 =?us-ascii?q?dgdENNq6Wa7TlEjfVnB2xB8n1lIPGEmimD6enYL5cWsf1zDSV1l+JV+nI6y7RO?=
 =?us-ascii?q?4SFARfx1njbSr9F0r1GnlOmP1iRoUB5UpjlXg4KLuBYqBaKM8phGRGaB/x8X62?=
 =?us-ascii?q?iUIwoFqsEjCdD1va1UjN/Vm/HdMjBHpv3O8MRUJNLTIcLCEHM7LR/gHnaAAxEI?=
 =?us-ascii?q?Qj2rHWXegVFNnvaP8HGcspk9rN7rgpVYGewTb0A8Cv5PUhctJ9cFOpoiG2p8yb?=
 =?us-ascii?q?M=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0ANAABoYhtch0O0hNFbCBwBAQEEAQEHB?=
 =?us-ascii?q?AEBgVEHAQELAYJpgQIng32IGV+LHYIhiQGFMIkYFIFiEhgNBgGHLSI0CQ0BAwE?=
 =?us-ascii?q?BAQEBAQIBEwEBAQgNCQgpIwyCNiQBgmIDAwECIAQLAQ0BATcBBQkBASQCBQwVA?=
 =?us-ascii?q?gIDDBIHHhEZBYMdAYFpAxUEAQqmZHB8DCeCdgEBBYEwAYNUDYIVCIELhnODJYE?=
 =?us-ascii?q?cF4F/gRGFaYFlEgENBQFegkmCV4lNgXeEGpEWGDMJhxGGMnCDJiSBX02EUopeL?=
 =?us-ascii?q?IsUgweBEooygUaBHXEzGggoCIMnCQqCCAsBFxKITIVAPzIBAYEAAwEBIROKboI?=
 =?us-ascii?q?+AQE?=
X-IPAS-Result: =?us-ascii?q?A0ANAABoYhtch0O0hNFbCBwBAQEEAQEHBAEBgVEHAQELAYJ?=
 =?us-ascii?q?pgQIng32IGV+LHYIhiQGFMIkYFIFiEhgNBgGHLSI0CQ0BAwEBAQEBAQIBEwEBA?=
 =?us-ascii?q?QgNCQgpIwyCNiQBgmIDAwECIAQLAQ0BATcBBQkBASQCBQwVAgIDDBIHHhEZBYM?=
 =?us-ascii?q?dAYFpAxUEAQqmZHB8DCeCdgEBBYEwAYNUDYIVCIELhnODJYEcF4F/gRGFaYFlE?=
 =?us-ascii?q?gENBQFegkmCV4lNgXeEGpEWGDMJhxGGMnCDJiSBX02EUopeLIsUgweBEooygUa?=
 =?us-ascii?q?BHXEzGggoCIMnCQqCCAsBFxKITIVAPzIBAYEAAwEBIROKboI+AQE?=
X-IronPort-AV: E=Sophos;i="5.56,376,1539673200"; 
   d="scan'208";a="57668995"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 20 Dec 2018 01:38:47 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1733308AbeLTJi2 (ORCPT <rfc822;like.xu@linux.intel.com>
        + 22 others); Thu, 20 Dec 2018 04:38:28 -0500
Received: from mail.kernel.org ([198.145.29.99]:52406 "EHLO mail.kernel.org"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1732050AbeLTJ0N (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Thu, 20 Dec 2018 04:26:13 -0500
Received: from localhost (5356596B.cm-6-7b.dynamic.ziggo.nl [83.86.89.107])
        (using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
        (No client certificate requested)
        by mail.kernel.org (Postfix) with ESMTPSA id 4B9F92186A;
        Thu, 20 Dec 2018 09:26:11 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=kernel.org;
        s=default; t=1545297971;
        bh=Z8M1l/y9REiXZAWKjKa2E7LcnPn+MPKkomdbNdFqe0M=;
        h=From:To:Cc:Subject:Date:In-Reply-To:References:From;
        b=KfhCIjeRN6umKOyKJ7K73C8GouBJj+XwZ7BoobZR7I3fPhXirIeJLlds62OMRjFQD
         7ilPz+PKY2WiVG4II6E6rshz8gzLX1Na6Rf9lT2BDZosJAEo/Jjxe55C0FphvC51KB
         qW9arnRo4y3heIXAl/BChgYXulnd9nhFc8g5fkAo=
From: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
To: linux-kernel@vger.kernel.org
Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>,
        stable@vger.kernel.org, Will Deacon <will.deacon@arm.com>,
        "Peter Zijlstra (Intel)" <peterz@infradead.org>,
        Waiman Long <longman@redhat.com>,
        Boqun Feng <boqun.feng@gmail.com>,
        Linus Torvalds <torvalds@linux-foundation.org>,
        Thomas Gleixner <tglx@linutronix.de>,
        linux-arm-kernel@lists.infradead.org, paulmck@linux.vnet.ibm.com,
        Ingo Molnar <mingo@kernel.org>,
        Sebastian Andrzej Siewior <bigeasy@linutronix.de>,
        Sasha Levin <sashal@kernel.org>
Subject: [PATCH 4.14 23/72] locking/qspinlock: Merge struct __qspinlock into struct qspinlock
Date: Thu, 20 Dec 2018 10:18:22 +0100
Message-Id: <20181220085923.251181210@linuxfoundation.org>
X-Mailer: git-send-email 2.20.1
In-Reply-To: <20181220085922.332225035@linuxfoundation.org>
References: <20181220085922.332225035@linuxfoundation.org>
User-Agent: quilt/0.65
X-stable: review
X-Patchwork-Hint: ignore
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

4.14-stable review patch.  If anyone has any objections, please let me know.

------------------

commit 625e88be1f41b53cec55827c984e4a89ea8ee9f9 upstream.

'struct __qspinlock' provides a handy union of fields so that
subcomponents of the lockword can be accessed by name, without having to
manage shifts and masks explicitly and take endianness into account.

This is useful in qspinlock.h and also potentially in arch headers, so
move the 'struct __qspinlock' into 'struct qspinlock' and kill the extra
definition.

Signed-off-by: Will Deacon <will.deacon@arm.com>
Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Acked-by: Waiman Long <longman@redhat.com>
Acked-by: Boqun Feng <boqun.feng@gmail.com>
Cc: Linus Torvalds <torvalds@linux-foundation.org>
Cc: Thomas Gleixner <tglx@linutronix.de>
Cc: linux-arm-kernel@lists.infradead.org
Cc: paulmck@linux.vnet.ibm.com
Link: http://lkml.kernel.org/r/1524738868-31318-3-git-send-email-will.deacon@arm.com
Signed-off-by: Ingo Molnar <mingo@kernel.org>
Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Signed-off-by: Sasha Levin <sashal@kernel.org>
---
 arch/x86/include/asm/qspinlock.h          |  2 +-
 arch/x86/include/asm/qspinlock_paravirt.h |  3 +-
 include/asm-generic/qspinlock_types.h     | 32 +++++++++++++++-
 kernel/locking/qspinlock.c                | 46 ++---------------------
 kernel/locking/qspinlock_paravirt.h       | 34 ++++++-----------
 5 files changed, 46 insertions(+), 71 deletions(-)

diff --git a/arch/x86/include/asm/qspinlock.h b/arch/x86/include/asm/qspinlock.h
index 9982dd96f093..cf4cdf508ef4 100644
--- a/arch/x86/include/asm/qspinlock.h
+++ b/arch/x86/include/asm/qspinlock.h
@@ -15,7 +15,7 @@
  */
 static inline void native_queued_spin_unlock(struct qspinlock *lock)
 {
-	smp_store_release((u8 *)lock, 0);
+	smp_store_release(&lock->locked, 0);
 }
 
 #ifdef CONFIG_PARAVIRT_SPINLOCKS
diff --git a/arch/x86/include/asm/qspinlock_paravirt.h b/arch/x86/include/asm/qspinlock_paravirt.h
index 923307ea11c7..9ef5ee03d2d7 100644
--- a/arch/x86/include/asm/qspinlock_paravirt.h
+++ b/arch/x86/include/asm/qspinlock_paravirt.h
@@ -22,8 +22,7 @@ PV_CALLEE_SAVE_REGS_THUNK(__pv_queued_spin_unlock_slowpath);
  *
  * void __pv_queued_spin_unlock(struct qspinlock *lock)
  * {
- *	struct __qspinlock *l = (void *)lock;
- *	u8 lockval = cmpxchg(&l->locked, _Q_LOCKED_VAL, 0);
+ *	u8 lockval = cmpxchg(&lock->locked, _Q_LOCKED_VAL, 0);
  *
  *	if (likely(lockval == _Q_LOCKED_VAL))
  *		return;
diff --git a/include/asm-generic/qspinlock_types.h b/include/asm-generic/qspinlock_types.h
index 034acd0c4956..0763f065b975 100644
--- a/include/asm-generic/qspinlock_types.h
+++ b/include/asm-generic/qspinlock_types.h
@@ -29,13 +29,41 @@
 #endif
 
 typedef struct qspinlock {
-	atomic_t	val;
+	union {
+		atomic_t val;
+
+		/*
+		 * By using the whole 2nd least significant byte for the
+		 * pending bit, we can allow better optimization of the lock
+		 * acquisition for the pending bit holder.
+		 */
+#ifdef __LITTLE_ENDIAN
+		struct {
+			u8	locked;
+			u8	pending;
+		};
+		struct {
+			u16	locked_pending;
+			u16	tail;
+		};
+#else
+		struct {
+			u16	tail;
+			u16	locked_pending;
+		};
+		struct {
+			u8	reserved[2];
+			u8	pending;
+			u8	locked;
+		};
+#endif
+	};
 } arch_spinlock_t;
 
 /*
  * Initializier
  */
-#define	__ARCH_SPIN_LOCK_UNLOCKED	{ ATOMIC_INIT(0) }
+#define	__ARCH_SPIN_LOCK_UNLOCKED	{ .val = ATOMIC_INIT(0) }
 
 /*
  * Bitfields in the atomic value:
diff --git a/kernel/locking/qspinlock.c b/kernel/locking/qspinlock.c
index 18161264227a..e60e618287b4 100644
--- a/kernel/locking/qspinlock.c
+++ b/kernel/locking/qspinlock.c
@@ -126,40 +126,6 @@ static inline __pure struct mcs_spinlock *decode_tail(u32 tail)
 
 #define _Q_LOCKED_PENDING_MASK (_Q_LOCKED_MASK | _Q_PENDING_MASK)
 
-/*
- * By using the whole 2nd least significant byte for the pending bit, we
- * can allow better optimization of the lock acquisition for the pending
- * bit holder.
- *
- * This internal structure is also used by the set_locked function which
- * is not restricted to _Q_PENDING_BITS == 8.
- */
-struct __qspinlock {
-	union {
-		atomic_t val;
-#ifdef __LITTLE_ENDIAN
-		struct {
-			u8	locked;
-			u8	pending;
-		};
-		struct {
-			u16	locked_pending;
-			u16	tail;
-		};
-#else
-		struct {
-			u16	tail;
-			u16	locked_pending;
-		};
-		struct {
-			u8	reserved[2];
-			u8	pending;
-			u8	locked;
-		};
-#endif
-	};
-};
-
 #if _Q_PENDING_BITS == 8
 /**
  * clear_pending_set_locked - take ownership and clear the pending bit.
@@ -171,9 +137,7 @@ struct __qspinlock {
  */
 static __always_inline void clear_pending_set_locked(struct qspinlock *lock)
 {
-	struct __qspinlock *l = (void *)lock;
-
-	WRITE_ONCE(l->locked_pending, _Q_LOCKED_VAL);
+	WRITE_ONCE(lock->locked_pending, _Q_LOCKED_VAL);
 }
 
 /*
@@ -188,13 +152,11 @@ static __always_inline void clear_pending_set_locked(struct qspinlock *lock)
  */
 static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)
 {
-	struct __qspinlock *l = (void *)lock;
-
 	/*
 	 * Use release semantics to make sure that the MCS node is properly
 	 * initialized before changing the tail code.
 	 */
-	return (u32)xchg_release(&l->tail,
+	return (u32)xchg_release(&lock->tail,
 				 tail >> _Q_TAIL_OFFSET) << _Q_TAIL_OFFSET;
 }
 
@@ -249,9 +211,7 @@ static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)
  */
 static __always_inline void set_locked(struct qspinlock *lock)
 {
-	struct __qspinlock *l = (void *)lock;
-
-	WRITE_ONCE(l->locked, _Q_LOCKED_VAL);
+	WRITE_ONCE(lock->locked, _Q_LOCKED_VAL);
 }
 
 
diff --git a/kernel/locking/qspinlock_paravirt.h b/kernel/locking/qspinlock_paravirt.h
index 15b6a39366c6..1435ba7954c3 100644
--- a/kernel/locking/qspinlock_paravirt.h
+++ b/kernel/locking/qspinlock_paravirt.h
@@ -70,10 +70,8 @@ struct pv_node {
 #define queued_spin_trylock(l)	pv_queued_spin_steal_lock(l)
 static inline bool pv_queued_spin_steal_lock(struct qspinlock *lock)
 {
-	struct __qspinlock *l = (void *)lock;
-
 	if (!(atomic_read(&lock->val) & _Q_LOCKED_PENDING_MASK) &&
-	    (cmpxchg_acquire(&l->locked, 0, _Q_LOCKED_VAL) == 0)) {
+	    (cmpxchg_acquire(&lock->locked, 0, _Q_LOCKED_VAL) == 0)) {
 		qstat_inc(qstat_pv_lock_stealing, true);
 		return true;
 	}
@@ -88,16 +86,12 @@ static inline bool pv_queued_spin_steal_lock(struct qspinlock *lock)
 #if _Q_PENDING_BITS == 8
 static __always_inline void set_pending(struct qspinlock *lock)
 {
-	struct __qspinlock *l = (void *)lock;
-
-	WRITE_ONCE(l->pending, 1);
+	WRITE_ONCE(lock->pending, 1);
 }
 
 static __always_inline void clear_pending(struct qspinlock *lock)
 {
-	struct __qspinlock *l = (void *)lock;
-
-	WRITE_ONCE(l->pending, 0);
+	WRITE_ONCE(lock->pending, 0);
 }
 
 /*
@@ -107,10 +101,8 @@ static __always_inline void clear_pending(struct qspinlock *lock)
  */
 static __always_inline int trylock_clear_pending(struct qspinlock *lock)
 {
-	struct __qspinlock *l = (void *)lock;
-
-	return !READ_ONCE(l->locked) &&
-	       (cmpxchg_acquire(&l->locked_pending, _Q_PENDING_VAL,
+	return !READ_ONCE(lock->locked) &&
+	       (cmpxchg_acquire(&lock->locked_pending, _Q_PENDING_VAL,
 				_Q_LOCKED_VAL) == _Q_PENDING_VAL);
 }
 #else /* _Q_PENDING_BITS == 8 */
@@ -355,7 +347,6 @@ static void pv_wait_node(struct mcs_spinlock *node, struct mcs_spinlock *prev)
 static void pv_kick_node(struct qspinlock *lock, struct mcs_spinlock *node)
 {
 	struct pv_node *pn = (struct pv_node *)node;
-	struct __qspinlock *l = (void *)lock;
 
 	/*
 	 * If the vCPU is indeed halted, advance its state to match that of
@@ -384,7 +375,7 @@ static void pv_kick_node(struct qspinlock *lock, struct mcs_spinlock *node)
 	 * the hash table later on at unlock time, no atomic instruction is
 	 * needed.
 	 */
-	WRITE_ONCE(l->locked, _Q_SLOW_VAL);
+	WRITE_ONCE(lock->locked, _Q_SLOW_VAL);
 	(void)pv_hash(lock, pn);
 }
 
@@ -399,7 +390,6 @@ static u32
 pv_wait_head_or_lock(struct qspinlock *lock, struct mcs_spinlock *node)
 {
 	struct pv_node *pn = (struct pv_node *)node;
-	struct __qspinlock *l = (void *)lock;
 	struct qspinlock **lp = NULL;
 	int waitcnt = 0;
 	int loop;
@@ -450,13 +440,13 @@ pv_wait_head_or_lock(struct qspinlock *lock, struct mcs_spinlock *node)
 			 *
 			 * Matches the smp_rmb() in __pv_queued_spin_unlock().
 			 */
-			if (xchg(&l->locked, _Q_SLOW_VAL) == 0) {
+			if (xchg(&lock->locked, _Q_SLOW_VAL) == 0) {
 				/*
 				 * The lock was free and now we own the lock.
 				 * Change the lock value back to _Q_LOCKED_VAL
 				 * and unhash the table.
 				 */
-				WRITE_ONCE(l->locked, _Q_LOCKED_VAL);
+				WRITE_ONCE(lock->locked, _Q_LOCKED_VAL);
 				WRITE_ONCE(*lp, NULL);
 				goto gotlock;
 			}
@@ -464,7 +454,7 @@ pv_wait_head_or_lock(struct qspinlock *lock, struct mcs_spinlock *node)
 		WRITE_ONCE(pn->state, vcpu_hashed);
 		qstat_inc(qstat_pv_wait_head, true);
 		qstat_inc(qstat_pv_wait_again, waitcnt);
-		pv_wait(&l->locked, _Q_SLOW_VAL);
+		pv_wait(&lock->locked, _Q_SLOW_VAL);
 
 		/*
 		 * Because of lock stealing, the queue head vCPU may not be
@@ -489,7 +479,6 @@ gotlock:
 __visible void
 __pv_queued_spin_unlock_slowpath(struct qspinlock *lock, u8 locked)
 {
-	struct __qspinlock *l = (void *)lock;
 	struct pv_node *node;
 
 	if (unlikely(locked != _Q_SLOW_VAL)) {
@@ -518,7 +507,7 @@ __pv_queued_spin_unlock_slowpath(struct qspinlock *lock, u8 locked)
 	 * Now that we have a reference to the (likely) blocked pv_node,
 	 * release the lock.
 	 */
-	smp_store_release(&l->locked, 0);
+	smp_store_release(&lock->locked, 0);
 
 	/*
 	 * At this point the memory pointed at by lock can be freed/reused,
@@ -544,7 +533,6 @@ __pv_queued_spin_unlock_slowpath(struct qspinlock *lock, u8 locked)
 #ifndef __pv_queued_spin_unlock
 __visible void __pv_queued_spin_unlock(struct qspinlock *lock)
 {
-	struct __qspinlock *l = (void *)lock;
 	u8 locked;
 
 	/*
@@ -552,7 +540,7 @@ __visible void __pv_queued_spin_unlock(struct qspinlock *lock)
 	 * unhash. Otherwise it would be possible to have multiple @lock
 	 * entries, which would be BAD.
 	 */
-	locked = cmpxchg_release(&l->locked, _Q_LOCKED_VAL, 0);
+	locked = cmpxchg_release(&lock->locked, _Q_LOCKED_VAL, 0);
 	if (likely(locked == _Q_LOCKED_VAL))
 		return;
 
-- 
2.19.1



