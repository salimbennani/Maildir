Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 10 Dec 2018 09:00:24 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga008.jf.intel.com (orsmga008.jf.intel.com [10.7.209.65])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id BA77458079D;
	Sun,  9 Dec 2018 16:32:22 -0800 (PST)
Received: from orsmga106.jf.intel.com ([10.7.208.65])
  by orsmga008-1.jf.intel.com with ESMTP; 09 Dec 2018 16:32:22 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3ATX/n2ReqqHvkFjIBFVeucZRalGMj4u6mDksu8pMi?=
 =?us-ascii?q?zoh2WeGdxc6/ZByN2/xhgRfzUJnB7Loc0qyK6/CmATRIyK3CmUhKSIZLWR4BhJ?=
 =?us-ascii?q?detC0bK+nBN3fGKuX3ZTcxBsVIWQwt1Xi6NU9IBJS2PAWK8TW94jEIBxrwKxd+?=
 =?us-ascii?q?KPjrFY7OlcS30P2594HObwlSizexfbB/IA+qoQnNq8IbnZZsJqEtxxXTv3BGYf?=
 =?us-ascii?q?5WxWRmJVKSmxbz+MK994N9/ipTpvws6ddOXb31cKokQ7NYCi8mM30u683wqRbD?=
 =?us-ascii?q?VwqP6WACXWgQjxFFHhLK7BD+Xpf2ryv6qu9w0zSUMMHqUbw5Xymp4rx1QxH0li?=
 =?us-ascii?q?gIKz858HnWisNuiqJbvAmhrAF7z4LNfY2ZKOZycqbbcNgHR2ROQ9xRWjRPDI28?=
 =?us-ascii?q?cYUBEugOM+VWr4bzqFQBsQC+BRGuCe701j9EmmX70bEm3+k7Dw3L2hErEdIUsH?=
 =?us-ascii?q?TTqdX4LKkcXvqrzKnJ0DrIcu9Y2Tbj54jObhAho++DXaltesfW00kvFh3KjlOM?=
 =?us-ascii?q?qYznITyVzf8As2ec7+pnWuKvj3AopxttrTiow8chk4/EjZ8WxFDc7Sh13po5KN?=
 =?us-ascii?q?miREJmb9OoDoFcuzyZOodqWM8vQmNltD4kxrAHp5K3YC0HxIknyhHDbvGLboqF?=
 =?us-ascii?q?7xftWeuULzp3nnJodbehixu860etyOjxW8mq31tEsCZIl9bBu3AJ2hHQ7MWMV+?=
 =?us-ascii?q?Fz8V272TmV0gDe8uFELl4wlarcM5Mh3LEwmYQJsUjZHS/5hl/2jKmIeUUg4OSo?=
 =?us-ascii?q?7P7nYrr+qp+dMY97lB3+P7wwlsCjBek0KBUCUmaF9eimybHu/lH1TK9Lg/A0iq?=
 =?us-ascii?q?XZtYrVJcUfpq63GQ9V1YMj5g67Dzen1tQYgHYGIEtGeB2ZlYjpPU/BIPThAfe4?=
 =?us-ascii?q?jVWslilkx+rdM73/DZXCMGLDnK3ifblj8U5czhQ8zdRF65JTELEBL+r/WlXtu9?=
 =?us-ascii?q?zAEh85Lwu0zv7jCNV81YMRR3iDA6CEMK7JtV+I5+QvI/SDZYMPuTb9LeQl6ODq?=
 =?us-ascii?q?jXMjhVAdeqyp14MNaH+kBvRmP1mZYX30j9cBC2gKvxY+Q/btiFGYUT5Temy9X6?=
 =?us-ascii?q?Qz5jwgDIKmDIHDRp2igbCb3Se7GIFWaX5CClyWDXjocICEUe8WaC2OOs9hjiAE?=
 =?us-ascii?q?Vb+5Ro8j0hGhqhX2y7lgLurS/C0Ysonu1Nx05+3ViBEz+iZ4D8Wb02GRUW50mn?=
 =?us-ascii?q?kESCMx3KB6uUZ90EuM0bBkg/xEEtxe/+lGUhw6NZ7bzOx2EcryVRjDftqSTFam?=
 =?us-ascii?q?Q9OmASw+T94rwt8OZVp9FMumjhzZwyWqBLoVnaSRBJMo6qLcw2TxJ8FlxnbFzq?=
 =?us-ascii?q?YhiUMqQs9ONWK8gK5/+BPeB4rIk0Wfiqarer4Q3C/L9Gef02WOuFtUXxJ3UaXA?=
 =?us-ascii?q?RXoffFfZrczl5kPeSL+jEakoPRFfycGcMKdKasfmjVNdRPj9PtTSZGaxm2S1BR?=
 =?us-ascii?q?aM3b6MaIvqe2MA3CTSEkQEkgYT/WqYOgg6HCuuv2XeDDl2H1L1f0zs6fV+qG+8?=
 =?us-ascii?q?TkIs0g6FdVNh2KSv9h4Vn/OcTegT0awCuCo6rzV0HVC938/ZCtaapgpherlcbs?=
 =?us-ascii?q?054FtdyW3ZsAl9NIS6L69+nl4ebxh3v0T22hVrC4VAlM8qrG8qzAtyM66YzElN?=
 =?us-ascii?q?dzSC3ZD0O73XLGby8Qura67X3FHezdmX9r0O6PQ+t1XsogWpGlA+/HVg1tlfy2?=
 =?us-ascii?q?Gc6YnSDAoOTZLxVV46+AJ+p77EbSgy+YPV2Wd3PqmprzDNwdQpBOgiyha+ZNpf?=
 =?us-ascii?q?NKKEFAnvE8wVHcSuKeoqm0S3YRIAJuxd6Kk0P8a+ffucxKGrJPpgnC6hjWlf4I?=
 =?us-ascii?q?F9z1iA+DZmRu/J35YK2feY3gScWjf4jVehtN33mI9eaTETGGq/1TbrBIpLaqJu?=
 =?us-ascii?q?eoYLDH+kI9erydVmm57tR3lY+Ua5B1MHxMCmYwCSYEH63QFK00QYv2anmSqhwz?=
 =?us-ascii?q?NoiT4pqreQ0zLUw+TlchoKIWpLRGhkjVfxLom4ldEaXE60bwc3kBup/1r1x69e?=
 =?us-ascii?q?pK5nNWncXV9IfzTqL2FlSqaxtruCb9RP6Z8yqiVXTeK8bEueSr78pRsaziziE3?=
 =?us-ascii?q?FfxDA9azGlpJH5kwZmh2KaKXZ5tGDZdt1oxRfD+NzcQuZc3jgcSyliijnXBV+8?=
 =?us-ascii?q?M8Oy/dqOlJfDseG+V2S/WZ1VaiTryYKAtC2m5WxlGxG/nvazmsH5Hgg+yyP0y9?=
 =?us-ascii?q?5qVSDQphbmfobrz7i6Mf5gfkRwBF7z8c16Fpt8kosxgpEdwnwahpST/Xoanmb/?=
 =?us-ascii?q?K9Rb2aTibHUTQT4H2cLa4A/g2Ed7NHKG25r5VmmBwstmf9S6YXkZ2iMn48BKFa?=
 =?us-ascii?q?eb9rpEnSRur1q8rALRZ+V9nzgHxfsv7n4an/8GuA43wiqBBbASGFFSPTbwmBSQ?=
 =?us-ascii?q?89C+sKJXaX6qcbix10pxh8qtDb+crQFHRHb2ZI0iEjRu4cVlP1LBy3nz6oDieN?=
 =?us-ascii?q?nNYtMfrByUkxHcj+dLLJI9jOYFhS1iOWjlp30q1/Y7jQBy3ZG9pIWHN2Jt/Kej?=
 =?us-ascii?q?Dh9ZLDL1Yd4T+ir2jaZfhcuW24GvHpN8GjQERpfoTPSoEC4MuvTjLQqBDDo8qn?=
 =?us-ascii?q?KDE7rFAQCf8Ftmr27IE52zLH6YPn4ZzdF/RBWHIExfnRsZXDE7np4/CwCry9bt?=
 =?us-ascii?q?cEZ/5jAN+FH4rgFAxf5vNxn6Sm3fvhunai8oSJiDKxpb9gNC6F3QMcOA9eJ/BT?=
 =?us-ascii?q?1Y8oeirAyQKWybegJIAnsSWkGfA1DjP7+u5cTP8uSCB+q+KefObquKqeBES/iI?=
 =?us-ascii?q?wpev2JN8/zmQLsWPImViD/oj10tDWnB5Bt3ZmygVRCwRjS7NdMmbpBGz+iBsqs?=
 =?us-ascii?q?C/8fLrWB/g5IeVCrtSN8lv9A6ygauZK+GQgyN5Iy5C1pwQ3X/I1KQf3FkKhiFu?=
 =?us-ascii?q?aTatELcAtS3MTK7Kma9XFRkbayxtO8tS8qI8xRJAOcraitPzy755geQ5C1ZDVV?=
 =?us-ascii?q?z9hM6pYdYGLH26NFPCHEyLLqiJJSXXw8HrZqOxUadfjPhPuB2qpzmaE1XvPjCC?=
 =?us-ascii?q?lzTyUxCvMOdMjDyUPRBEuYG9dAptBnbnTN78dhK7N9p3hyUswbIonnPKKXIcMT?=
 =?us-ascii?q?9kfkNRr72Q6DlUjellF2Nd7npqM++Elj2d7+nZLJYWrPRqDj51l+Jc/HQ11b9V?=
 =?us-ascii?q?4DtYS/xynSvYtsRurE2+kumT1jpnVwJDqzZRi4KNuEViOKPZ+YFBWHbE5h0N62?=
 =?us-ascii?q?qQBg8Op9tkDN3vpq9RxsLOlKL1NDdN7dbU8dEACMjTLcLUeEYmZD7oHiPPRDcC?=
 =?us-ascii?q?VyCiMW3EzxhBl+uM/3mZtbA+q57xiN8ATaJdWFUpF/QcTEN/E4pRDo1wW2YImK?=
 =?us-ascii?q?CagIYy5He4sBvcX40OpJHKUO6QDO7HJzGegr1JIRAPxOWrfswoKoTn1hk6ORFB?=
 =?us-ascii?q?l4PQFh+VBIgVrw=3D=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AKAQCosw1ch0O0hNFkHAEBAQQBAQcEA?=
 =?us-ascii?q?QGBZYExJYIWJ5gggWglFJhhA14BASsBh10iOBIBAwEBAQEBAQIBEwEBAQoLCQg?=
 =?us-ascii?q?pIwyCNiQBgmIBAgIBAQIkEwYBATcBBAEJAQEKRgMxAQUBHAYYFgODA4F6CAEEm?=
 =?us-ascii?q?h48ih2BbDOCdgEBBYcbCBKKc4EcF4F/gREzgl+BQQGBXAKBGyqFd4lpgUKFEY9?=
 =?us-ascii?q?mVQmFOoUJgyCDbBiBf4dnh1eZCQYCCQcPIYE8gXdNIxWDJ4IbCw0Lg0qKUiIfM?=
 =?us-ascii?q?oEFAQEhigWCPgEB?=
X-IPAS-Result: =?us-ascii?q?A0AKAQCosw1ch0O0hNFkHAEBAQQBAQcEAQGBZYExJYIWJ5g?=
 =?us-ascii?q?ggWglFJhhA14BASsBh10iOBIBAwEBAQEBAQIBEwEBAQoLCQgpIwyCNiQBgmIBA?=
 =?us-ascii?q?gIBAQIkEwYBATcBBAEJAQEKRgMxAQUBHAYYFgODA4F6CAEEmh48ih2BbDOCdgE?=
 =?us-ascii?q?BBYcbCBKKc4EcF4F/gREzgl+BQQGBXAKBGyqFd4lpgUKFEY9mVQmFOoUJgyCDb?=
 =?us-ascii?q?BiBf4dnh1eZCQYCCQcPIYE8gXdNIxWDJ4IbCw0Lg0qKUiIfMoEFAQEhigWCPgE?=
 =?us-ascii?q?B?=
X-IronPort-AV: E=Sophos;i="5.56,336,1539673200"; 
   d="scan'208";a="43364302"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 09 Dec 2018 16:32:20 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726434AbeLJA3S (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Sun, 9 Dec 2018 19:29:18 -0500
Received: from mail-pf1-f195.google.com ([209.85.210.195]:41564 "EHLO
        mail-pf1-f195.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726268AbeLJA3S (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Sun, 9 Dec 2018 19:29:18 -0500
Received: by mail-pf1-f195.google.com with SMTP id b7so4501074pfi.8
        for <linux-kernel@vger.kernel.org>; Sun, 09 Dec 2018 16:29:17 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=google.com; s=20161025;
        h=date:from:to:cc:subject:in-reply-to:message-id:references
         :user-agent:mime-version;
        bh=zmdKOnAe97IQjsjN+LZw0dQZYH9ZjtseI/M5pBXkouI=;
        b=Qh4C5JTgT90TKqncfhCcemqRvRglhJmY6tPR4ovt4GfIvC9snAABPeul41VHaErrrs
         QDcoKrgDGqbKgxeB50nghumb21u4PhXrjHbpew8F1F0OQ46sm+nYp3D46mwESQxzK4q0
         404v4QMeE7AoFiET8IKJfy4+LC5QfAK5VYRcOL4NkPf08aPGGc2G4ocxhGwRKUt9KLm3
         Hg8Uz+CxK7NBeflZMMgHYe4AmzXdHSh22x4dxhPYkGYPDj7d7S2SWtRm4qTJ6zFvTJId
         XqtclgE79F5beNBqgEGHfaHcI7EsUbnvz3PR7eNYqpEq7V72hSt8okQc8FgdVpwlWej8
         r0Ug==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:date:from:to:cc:subject:in-reply-to:message-id
         :references:user-agent:mime-version;
        bh=zmdKOnAe97IQjsjN+LZw0dQZYH9ZjtseI/M5pBXkouI=;
        b=Ux22VWeRPqFf7UWDazW9PkVb5QZA26uHeNQtmzvhpykyT2Ac8AEahX7mXyqf0QKQUs
         YrgDx6FjIZe0BswBjJJzpSlG5pQq5h/qDX3p96oo0s0rRYVxYjbWMGgkLvGmm+HvlEPV
         YEJhqgWn3IAH0Uutvd6EM8B45aFyLfuPlsNEnaNGyi7D+pgOb3mxLS4CavFzinWbyWdo
         oI/lk5jq7n3Z/X/+GoX7n/2h3j/ld/lj4U4gf5F6/eiSrEk1Zenla224xgxg1IYWPVWk
         xQcCwsqimd6GqG5m16q+g7lW95ch4pNQUNkhiZQUWKRikvi58+1kvmwpHS7hhNa3Tw3P
         /reQ==
X-Gm-Message-State: AA+aEWYN0WbLTImLuHTu+XunuT53tPd9O1qJqZlICwwokNYPremejBGb
        PTIt8K1CAZVlTbS1XBugC4GN7Q==
X-Google-Smtp-Source: AFSGD/W/5jQVoL8KRsK+rAJatVKuhyhtEEIq30OYuJK7Y2hvl0LpKoQMQrFwQXqJEL4voWL7XyjbKQ==
X-Received: by 2002:a63:b17:: with SMTP id 23mr9145144pgl.423.1544401756635;
        Sun, 09 Dec 2018 16:29:16 -0800 (PST)
Received: from [2620:15c:17:3:3a5:23a7:5e32:4598] ([2620:15c:17:3:3a5:23a7:5e32:4598])
        by smtp.gmail.com with ESMTPSA id v9sm13588936pfg.144.2018.12.09.16.29.14
        (version=TLS1_2 cipher=ECDHE-RSA-CHACHA20-POLY1305 bits=256/256);
        Sun, 09 Dec 2018 16:29:14 -0800 (PST)
Date: Sun, 9 Dec 2018 16:29:13 -0800 (PST)
From: David Rientjes <rientjes@google.com>
X-X-Sender: rientjes@chino.kir.corp.google.com
To: Linus Torvalds <torvalds@linux-foundation.org>
cc: Andrea Arcangeli <aarcange@redhat.com>,
        mgorman@techsingularity.net, Vlastimil Babka <vbabka@suse.cz>,
        Michal Hocko <mhocko@kernel.org>, ying.huang@intel.com,
        s.priebe@profihost.ag,
        Linux List Kernel Mailing <linux-kernel@vger.kernel.org>,
        alex.williamson@redhat.com, lkp@01.org, kirill@shutemov.name,
        Andrew Morton <akpm@linux-foundation.org>,
        zi.yan@cs.rutgers.edu
Subject: Re: [LKP] [mm] ac5b2c1891: vm-scalability.throughput -61.3%
 regression
In-Reply-To: <CAHk-=wjVuLjZ1Wr52W=hNqh=_8gbzuKA+YpsVb4NBHCJsE6cyA@mail.gmail.com>
Message-ID: <alpine.DEB.2.21.1812091538310.215735@chino.kir.corp.google.com>
References: <CAHk-=wgVL_sxXSbjYTiGhxp6+9wLQ9ZmSN+0R5PCF6_a9pQgWw@mail.gmail.com> <20181203185954.GM31738@dhcp22.suse.cz> <CAHk-=wiNKLH2Pbnr9z2SvmDhf7XT==U6NPRkQNX13Sg-FRk0Yw@mail.gmail.com> <20181203201214.GB3540@redhat.com>
 <CAHk-=wg=6uxAJMbvGJC-5CSikC8OdqsjE1vw+DsCMj=2SNSnZg@mail.gmail.com> <CAHk-=whDg5+e2-eXYo-jwC1spt2r7JjLQSaLm4OyfGMQHLTrdw@mail.gmail.com> <64a4aec6-3275-a716-8345-f021f6186d9b@suse.cz> <20181204104558.GV23260@techsingularity.net> <20181205204034.GB11899@redhat.com>
 <CAHk-=whi8Ju-cTDL4cYtwuLA7BYgGJYyy6HVMoknZaLHnRc83g@mail.gmail.com> <20181205233632.GE11899@redhat.com> <CAHk-=wguXjkbK8BUU998s7HD7AXJgBkuc9JmuNxiN7uGQyfSfQ@mail.gmail.com> <CAHk-=wjm9V843eg0uesMrxKnCCq7UfWn8VJ+z-cNztb_0fVW6A@mail.gmail.com>
 <alpine.DEB.2.21.1812061505010.162675@chino.kir.corp.google.com> <CAHk-=wjVuLjZ1Wr52W=hNqh=_8gbzuKA+YpsVb4NBHCJsE6cyA@mail.gmail.com>
User-Agent: Alpine 2.21 (DEB 202 2017-01-01)
MIME-Version: 1.0
Content-Type: text/plain; charset=US-ASCII
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On Thu, 6 Dec 2018, Linus Torvalds wrote:

> > On Broadwell, the access latency to local small pages was +5.6%, remote
> > hugepages +16.4%, and remote small pages +19.9%.
> >
> > On Naples, the access latency to local small pages was +4.9%, intrasocket
> > hugepages +10.5%, intrasocket small pages +19.6%, intersocket small pages
> > +26.6%, and intersocket hugepages +29.2%
> 
> Are those two last numbers transposed?
> 
> Or why would small page accesses be *faster* than hugepages for the
> intersocket case?
> 
> Of course, depending on testing, maybe the page itself was remote, but
> the page tables were random, and you happened to get a remote page
> table for the hugepage case?
> 

Yes, looks like that was the case, if the page tables were from the same 
node as the intersocket remote hugepage it looks like a ~0.1% increase 
accessing small pages, so basically unchanged.  So this complicates the 
allocation strategy somewhat; on this platform, at least, hugepages are 
preferred on the same socket but there isn't a significant benefit from 
getting a cross socket hugepage over small page.

The typical way this is resolved is based on the SLIT and how the kernel 
defines RECLAIM_DISTANCE.  I'm not sure that we can expect the distances 
between proximity domains to be defined according to this value for a 
one-size-fits-all solution.  I've always thought that RECLAIM_DISTANCE 
should be configurable so that initscripts can actually determine its 
ideal value when using vm.zone_reclaim_mode.

> > So it *appears* from the x86 platforms that NUMA matters much more
> > significantly than hugeness, but remote hugepages are a slight win over
> > remote small pages.  PPC appeared the same wrt the local node but then
> > prefers hugeness over affinity when it comes to remote pages.
> 
> I do think POWER at least historically has much weaker TLB fills, but
> also very costly page table creation/teardown. Constant-time O(1)
> arguments about hash lookups are only worth so much when the constant
> time is pretty big. They've been working on it.
> 
> So at least on POWER, afaik one issue is literally that hugepages made
> the hash setup and teardown situation much better.
> 

I'm still working on the more elaborate test case that will generate these 
results because I think I can use it at boot to determine an ideal 
RECLAIM_DISTANCE.  I can also get numbers for hash vs radix MMU if you're 
interested.

> One thing that might be worth looking at is whether the process itself
> is all that node-local. Maybe we could aim for a policy that says
> "prefer local memory, but if we notice that the accesses to this vma
> aren't all that local, then who cares?".
> 
> IOW, the default could be something more dynamic than just "always use
> __GFP_THISNODE". It could be more along the lines of "start off using
> __GFP_THISNODE, but for longer-lived processes that bounce around
> across nodes, maybe relax it?"
> 

It would allow the use of MPOL_PREFERRED for an exact preference if they 
are known to not be bounced around.  This would be required for processes 
that are bound to the cpus of a single node through cpuset or 
sched_setaffinity() but unconstrained as far as memory is concerned.

The goal of __GFP_THISNODE being the default for thp, however, is that we 
*know* we're going to be accessing it locally at least in the short term, 
perhaps forever.  Any other default would assume the remotely allocated 
hugepage would eventually be accessed locally, otherwise we would have 
been much better off just failing the hugepage allocation and accessing 
small pages.  You could make an assumption that's the case iff the process 
does not fit in its local node, and I think that would be the minority of 
applications.

I guess there could be some heuristic that could determine this based on 
MM_ANONPAGES of Andrea's qemu and zone->zone_pgdat->node_present_pages.  
It feels like something that should be more exactly defined, though, for 
the application to say that it prefers remote hugepages over local 
small pages because it can't access either locally forever anyway.

This was where I suggested a new prctl() mode so that an application can 
prefer remote hugepages because it knows it's larger than the single node 
and that requires no change to the binary itself because it is inherited 
across fork.

The sane default, though, seems to always prefer local allocation, whether 
hugepages or small pages, for the majority of workloads since that's where 
the lowest access latency is.

> Honestly, I think things like vm_policy etc should not be the solution
> - yes, some people may know *exactly* what access patterns they want,
> but for most situations, I think the policy should be that defaults
> "just work".
> 
> In fact, I wish even MADV_HUGEPAGE itself were to approach being a
> no-op with THP.
> 

Besides the NUMA locality of the allocations, we still have the allocation 
latency concern that MADV_HUGEPAGE changes.  The madvise mode has taken on 
two meanings: (1) prefer to fault hugepages when the thp enabled setting 
is "madvise" so other applications don't blow up their rss unexpectedly, 
and (2) try synchronous compaction/reclaim at fault for thp defrag 
settings of "madvise" or "defer+madvise".

 [ It was intended to take on an additional meaning through the 
   now-reverted patch, which was (3) relax the NUMA locality preference. ]

My binaries that remap their text segment to be backed by transparent 
hugepages and qemu both share the same preference to try hard to fault 
hugepages through compaction because we don't necessarily care about 
allocation latency, we care about access latency later.  Smaller binaries, 
those that do not have strict NUMA locality requirements, and short-lived 
allocations are not going to want to incur the performance penalty of 
synchronous compaction.

So I think today's semantics for MADV_HUGEPAGE make sense, but I'd like to 
explore other areas that could improve this, both for the default case and 
the specialized cases:

 - prctl() mode to readily allow remote allocations rather than reclaiming
   or compacting memory locally (affects more than just hugepages if the
   system has a non-zero vm.zone_reclaim_mode),

 - better feedback loop after the first compaction attempt in the page 
   allocator slowpath to determine if reclaim is actually worthwhile for
   high-order allocations, and

 - configurable vm.reclaim_distance (use RECLAIM_DISTANCE as the default)
   which can be defined since there is not a one-size-fits-all strategy
   for allocations (there's no benefit to allocating hugepages cross 
   socket on Naples, for example),
