Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 07 Dec 2018 08:54:14 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga005.jf.intel.com (orsmga005.jf.intel.com [10.7.209.41])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 3379E5804F9;
	Thu,  6 Dec 2018 13:39:13 -0800 (PST)
Received: from fmsmga103.fm.intel.com ([10.1.193.90])
  by orsmga005-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 06 Dec 2018 13:39:12 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AC1OrjR3QI+kU27JhsmDT+DRfVm0co7zxezQtwd8Z?=
 =?us-ascii?q?segSKfjxwZ3uMQTl6Ol3ixeRBMOHs6IC07KempujcFRI2YyGvnEGfc4EfD4+ou?=
 =?us-ascii?q?JSoTYdBtWYA1bwNv/gYn9yNs1DUFh44yPzahANS47xaFLIv3K98yMZFAnhOgpp?=
 =?us-ascii?q?POT1HZPZg9iq2+yo9JDffwZFiCChbb9uMR67sRjfus4KjIV4N60/0AHJonxGe+?=
 =?us-ascii?q?RXwWNnO1eelAvi68mz4ZBu7T1et+ou+MBcX6r6eb84TaFDAzQ9L281/szrugLd?=
 =?us-ascii?q?QgaJ+3ART38ZkhtMAwjC8RH6QpL8uTb0u+ZhxCWXO9D9QLYpUjqg8qhrUgflhi?=
 =?us-ascii?q?cZOTAk/m/Zict+g6BVoBK6vxxywZXZbJ2JOPdkYq/QZ88WSXZHU81MVyJBGIS8?=
 =?us-ascii?q?b44XAuQcIeZXsZf9qEUTphWjBAmsH//vxSVShnDowKY31P8hER3Y0ww+Ad0Otm?=
 =?us-ascii?q?7YrM70NKcJTeC61rPIwivYYvNRwzfy8pTHchQ/rv2WQb1wds/RxFApGgjYjVuQ?=
 =?us-ascii?q?sZToMy2J2ukJqWSX8uRtWfy1h2I6qAx9viKjy8Yuh4XRm44YxU3I+T9kzIs6JN?=
 =?us-ascii?q?C0UlN3bcOlHZdKqi2WK417Sd44TW5yoiY10LgGtIa7fCcUzJQnwAbSa+KIc4eW?=
 =?us-ascii?q?+BLvTuWRLilihHJjZr2/gwy+8U+6yu3zTsW00VBKoTRZktTUqHwByxje5tKaRv?=
 =?us-ascii?q?Zz4EutwyuD2gPP5u1eIE05l7LXK5s7zb4xkpoTv17DHijzmEjuiK+Wd0Mk+vWn?=
 =?us-ascii?q?6uj+YbXpuIWcN4lqhQH6K6guncK+AeImPQgURGWb5+u826P5/UHjQ7VFkOc2kq?=
 =?us-ascii?q?/Hv5DeP8gbobS5AwBN3oY59xm/Fyum0MgfnXQfKFJFeRGHgJbzN1DBPfD1FvO/?=
 =?us-ascii?q?g1WqkDd2yPHKJLzhApPRLnfdlLftZ6py60lZyAAr19BQ+4pUCq0dIPL0QkLxtM?=
 =?us-ascii?q?bXDh4lMwOuxObrEtV91p4EVmKJDa+UK6fSsV6O5uIyLOiAfo4VuDDhK/c74/7i?=
 =?us-ascii?q?l2M2mVgYfaOxx5sYdGi4Huh6I0WeeXfsgNABHnkQsgo9S+zqjluCUTlIana2Xq?=
 =?us-ascii?q?I84Cw7CY28AYfCQICtnKKO3COhEpJKYWBGD0iGEW30eIWcR/cMdCWSL9d7nTwf?=
 =?us-ascii?q?VbihTIwh2Qu0tADgybpqNe7U+iwetZL+29l5/ezTlRcu9TNqC8SRyX2CT2Zxnm?=
 =?us-ascii?q?kQXT85wLh/oVBhyleEyaV4gOZXFdpJ6/NNUwc1L5jcz+NhBtD2WwLBeMqJSVm8?=
 =?us-ascii?q?TtWnBzExUsw+w9sUb0lhHNWiiwjJ3zC2DL8Ni7yLGJs0/7rB0HfrOcZy1WzK1K?=
 =?us-ascii?q?k7gFkgWctAK2umiql79wjQAo7Jl16Ul6KrdaQawS7M+32PzWuIvEFETgFwVb/J?=
 =?us-ascii?q?UmwYZkvTtd75/F/NT6eyCbQ7NQtM0dONJbFUatL3l1lGRO3sONLFY22vnWe9Hh?=
 =?us-ascii?q?KIxrKKbIr3dGQRxiTdCE4Ykw8N+XaKLxQxBiCko2jGFjxhCUrvY1/w8el5sH67?=
 =?us-ascii?q?Uk40zwSNb01917q0+gQZheCGR/MUxLIEvCYhqzN7HFum29LWCtyApxdufalGYN?=
 =?us-ascii?q?M95ktH2nzdtwBnIpOgKKVijEYEcwtrp0Puywl3CoJYnMcwqHMl0gVzJryY0V9b?=
 =?us-ascii?q?bDyYwI3/NafRKmnx+xCvdbXb2lXf0NaQ56cO5+40q1TlvAG1CEUi929r3MVS03?=
 =?us-ascii?q?uZ/p/KFhYdUYrtUkYr8Bh3v7HbYjQn64zIz3FtMKm0vSXE29ImH+Ylzhegf9FC?=
 =?us-ascii?q?MKKLDgPyEssaB9SwJ+wugVSmchUEPOVK/q4uI8ymb+eG2LKsPOt4hjKmjHhI75?=
 =?us-ascii?q?pn0k2R8CpwUOjI0Igfw/GZxweISy3zjFO8vc/pg4BEYjcSE3G7ySjlAo5Re6Jz?=
 =?us-ascii?q?cZwKCWeoP8242NF+i4TxVH5f8V6pH0kG19OxeRqOc1z92hVd1EQWoXC9mSq01S?=
 =?us-ascii?q?d0kywvrqeE2CzOwuLidAcIO2JRRWlii0vsLpawj9wAQEeoaA0pngO/5Unm36hb?=
 =?us-ascii?q?uLh/L27LTEdKZSf2NWJiUqi3traYeM5A8pAosSZWUOS6f1+aTKXwox8b0yPlAm?=
 =?us-ascii?q?tfyyo3dzCsupXlgRN6jHiRI2p0rHrcYctw3wvQ5MTARf5N2ToLXCl5hiPQBlSm?=
 =?us-ascii?q?Pdmp/NOUmozHsuC/UWKhS5JSfTPqzYOGqCu0+2lqDQejkPC0n93tCRI63jPj19?=
 =?us-ascii?q?l2SSXIqw7xYorx2KS7K+5nflRoC0X668p1AYx+loowhJcN2XkVnJmV/HwHkXvt?=
 =?us-ascii?q?PtVfw67xcH0NRTsTyd7P/AflwFFjLm6Ox4/hU3Wdw8hhaMOgbmIYxC099NxKB7?=
 =?us-ascii?q?mO7LNfgyR1uFW4oBnVYfh8mDcd1PQv5GQbg+EPpAoi0CGdDqoOEklfOCzmjw6I?=
 =?us-ascii?q?4MymrKVLeGavdqC91FZ5ndCkFr2DoxtTWHDkepc5Bi9w795yMFbN0H3194Hld8?=
 =?us-ascii?q?PcbdMVth2IjRjAi/JZJ442lvoPnSBnI37yvWU5y+4nihxjxYq1s5KAK2Vp4aK1?=
 =?us-ascii?q?GBpYNiDuasMX+zHti7ten8mM04CuGJVhBisEXJ/yQf20FzISsOztNxySHz0ktn?=
 =?us-ascii?q?ebBb3fEBed6Eh8qnLADYurO2uLJHUDztViWRqdJFFZgA8ORzU3hZo5FgGsxMz8?=
 =?us-ascii?q?f0Z1/DER5ljkqhRSzuJkLQXwUmDapA2wcDc7VICfLAZK7gFF/0rUMcue7v9qHy?=
 =?us-ascii?q?FF5JKhqhaBKnecZwROAmEEQUiEB1HlPrmz6tjM6emYBuyiL/TQZbWCs/BRV/CN?=
 =?us-ascii?q?xZi3yItp4y6MNtmTPnllF/A72lRMXXF6G8TamjUDUS8XlzjKb86Uuhi84DB3rt?=
 =?us-ascii?q?uk/fTvWQLv45aPCrRIPdVu/RC2nbmMN+qKiCllLjZY04sGxWXUx7gHwF4SlyZu?=
 =?us-ascii?q?eiG3HrQdriHNVrzfm69NAx4faiN+L89I76M63glQNs/XkNL11rhkjvErD1dJT0?=
 =?us-ascii?q?DumsasZcYSOWGyKEvHBFqXNLSBPTDL38D3YaamRbJMlulbqx2wtiidE0L4OjSD?=
 =?us-ascii?q?lj/pVw2gMO1WjSGbOgBet5+5chp3FWfjS9fmYAWhMNBrlT023aE0hnTSOG8cKz?=
 =?us-ascii?q?d8dUZNrryW7SxAg/R/G3ZO7nxqLeSfnyaZ7u/YKosZsPdxAyR0kf5a72o+y7dP?=
 =?us-ascii?q?8C5EQ/l1y2PuqYtroleplcGVxzZnWQYIoTFOwMqTsFlvI7fx959OQ3/I8RsBq2?=
 =?us-ascii?q?KKBEckvdxgX+bztr5dguPOj776MjoKp8nI8NURAY7PKdmbOWAsGR3zETXQAU0O?=
 =?us-ascii?q?Sjv9ZjKXvFBUjPzHri7dlZM9sJW5wJc=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0ADAAA4lglch0O0hNFjGgEBAQEBAgEBA?=
 =?us-ascii?q?QEHAgEBAQGBUgQBAQEBCwGDayeMcoswgg0UkkGEeYFzEgEBGBMBh1ciNQgNAQM?=
 =?us-ascii?q?BAQEBAQECARMBAQEIDQkIKSMMgjYigmUDAwECJBkBATcBBQkBAR8xA1QHEgWDH?=
 =?us-ascii?q?IICBAGlUYFsM4J2AQEFhyUIh3CEL4IWgRGNbpA1kCwHAoIkjyICFpE2LIhij1c?=
 =?us-ascii?q?CBAIEBQITAYFHAYILMxoEH1CCbIIbDBeDSop0UYEFAQGKOgEB?=
X-IPAS-Result: =?us-ascii?q?A0ADAAA4lglch0O0hNFjGgEBAQEBAgEBAQEHAgEBAQGBUgQ?=
 =?us-ascii?q?BAQEBCwGDayeMcoswgg0UkkGEeYFzEgEBGBMBh1ciNQgNAQMBAQEBAQECARMBA?=
 =?us-ascii?q?QEIDQkIKSMMgjYigmUDAwECJBkBATcBBQkBAR8xA1QHEgWDHIICBAGlUYFsM4J?=
 =?us-ascii?q?2AQEFhyUIh3CEL4IWgRGNbpA1kCwHAoIkjyICFpE2LIhij1cCBAIEBQITAYFHA?=
 =?us-ascii?q?YILMxoEH1CCbIIbDBeDSop0UYEFAQGKOgEB?=
X-IronPort-AV: E=Sophos;i="5.56,323,1539673200"; 
   d="scan'208";a="54887923"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 06 Dec 2018 13:39:10 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726113AbeLFViv (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Thu, 6 Dec 2018 16:38:51 -0500
Received: from aserp2130.oracle.com ([141.146.126.79]:33740 "EHLO
        aserp2130.oracle.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726007AbeLFVip (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Thu, 6 Dec 2018 16:38:45 -0500
Received: from pps.filterd (aserp2130.oracle.com [127.0.0.1])
        by aserp2130.oracle.com (8.16.0.22/8.16.0.22) with SMTP id wB6LYiJh159474;
        Thu, 6 Dec 2018 21:38:18 GMT
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=oracle.com; h=from : to : cc :
 subject : date : message-id : in-reply-to : references; s=corp-2018-07-02;
 bh=Q92XsM559slBxkunNq5q8saE7LteeOstRPtlDtl+k4g=;
 b=rx29F5om0uUK7q2Fpinp6be8Han7DhVVPpd1N1ffmy3VScO/Vmi8Br4xN+jU+Xzc/Dcv
 gGlUQyvRrGnm+M6rmUazysbD9xVddAm6V7exZQ8rBljGehEYJTELVs1EIUwwNiv+t9d/
 kxfek3kYfzLYOVoZ83/bW8Pa1A7mvH3GY6F9BXbLtZu7s2pCWlmS0K0CaCE4CPeaR/iX
 lzCe53jmCrOyB/3WW+4y/dX/9DWi/ukZMZRuIXaxCLnKfcYm7aa+jEFKFGpN1BiE2uT3
 ORWKWWHSu0BpWM4XKrc5aqg6Xcf15+b37gCrADjA9l0pcmr+UfQJh787CuFr1gZJmqdI yw== 
Received: from aserv0022.oracle.com (aserv0022.oracle.com [141.146.126.234])
        by aserp2130.oracle.com with ESMTP id 2p3ftfenfw-1
        (version=TLSv1.2 cipher=ECDHE-RSA-AES256-GCM-SHA384 bits=256 verify=OK);
        Thu, 06 Dec 2018 21:38:18 +0000
Received: from userv0121.oracle.com (userv0121.oracle.com [156.151.31.72])
        by aserv0022.oracle.com (8.14.4/8.14.4) with ESMTP id wB6LcI2d007204
        (version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256 verify=OK);
        Thu, 6 Dec 2018 21:38:18 GMT
Received: from abhmp0004.oracle.com (abhmp0004.oracle.com [141.146.116.10])
        by userv0121.oracle.com (8.14.4/8.13.8) with ESMTP id wB6LcHEj001035;
        Thu, 6 Dec 2018 21:38:17 GMT
Received: from ca-dev63.us.oracle.com (/10.211.8.221)
        by default (Oracle Beehive Gateway v4.0)
        with ESMTP ; Thu, 06 Dec 2018 21:38:17 +0000
From: Steve Sistare <steven.sistare@oracle.com>
To: mingo@redhat.com, peterz@infradead.org
Cc: subhra.mazumdar@oracle.com, dhaval.giani@oracle.com,
        daniel.m.jordan@oracle.com, pavel.tatashin@microsoft.com,
        matt@codeblueprint.co.uk, umgwanakikbuti@gmail.com,
        riel@redhat.com, jbacik@fb.com, juri.lelli@redhat.com,
        valentin.schneider@arm.com, vincent.guittot@linaro.org,
        quentin.perret@arm.com, steven.sistare@oracle.com,
        linux-kernel@vger.kernel.org
Subject: [PATCH v4 03/10] sched/topology: Provide cfs_overload_cpus bitmap
Date: Thu,  6 Dec 2018 13:28:09 -0800
Message-Id: <1544131696-2888-4-git-send-email-steven.sistare@oracle.com>
X-Mailer: git-send-email 1.8.3.1
In-Reply-To: <1544131696-2888-1-git-send-email-steven.sistare@oracle.com>
References: <1544131696-2888-1-git-send-email-steven.sistare@oracle.com>
X-Proofpoint-Virus-Version: vendor=nai engine=5900 definitions=9099 signatures=668679
X-Proofpoint-Spam-Details: rule=notspam policy=default score=0 suspectscore=2 malwarescore=0
 phishscore=0 bulkscore=0 spamscore=0 mlxscore=0 mlxlogscore=988
 adultscore=0 classifier=spam adjust=0 reason=mlx scancount=1
 engine=8.0.1-1810050000 definitions=main-1812060181
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

From: Steve Sistare <steve.sistare@oracle.com>

Define and initialize a sparse bitmap of overloaded CPUs, per
last-level-cache scheduling domain, for use by the CFS scheduling class.
Save a pointer to cfs_overload_cpus in the rq for efficient access.

Signed-off-by: Steve Sistare <steven.sistare@oracle.com>
---
 include/linux/sched/topology.h |  1 +
 kernel/sched/sched.h           |  2 ++
 kernel/sched/topology.c        | 25 +++++++++++++++++++++++--
 3 files changed, 26 insertions(+), 2 deletions(-)

diff --git a/include/linux/sched/topology.h b/include/linux/sched/topology.h
index 6b99761..b173a77 100644
--- a/include/linux/sched/topology.h
+++ b/include/linux/sched/topology.h
@@ -72,6 +72,7 @@ struct sched_domain_shared {
 	atomic_t	ref;
 	atomic_t	nr_busy_cpus;
 	int		has_idle_cores;
+	struct sparsemask *cfs_overload_cpus;
 };
 
 struct sched_domain {
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 618577f..eacf5db 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -81,6 +81,7 @@
 
 struct rq;
 struct cpuidle_state;
+struct sparsemask;
 
 /* task_struct::on_rq states: */
 #define TASK_ON_RQ_QUEUED	1
@@ -812,6 +813,7 @@ struct rq {
 	struct cfs_rq		cfs;
 	struct rt_rq		rt;
 	struct dl_rq		dl;
+	struct sparsemask	*cfs_overload_cpus;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/* list of leaf cfs_rq on this CPU: */
diff --git a/kernel/sched/topology.c b/kernel/sched/topology.c
index 3e72ce0..89a78ce 100644
--- a/kernel/sched/topology.c
+++ b/kernel/sched/topology.c
@@ -3,6 +3,7 @@
  * Scheduler topology setup/handling methods
  */
 #include "sched.h"
+#include "sparsemask.h"
 
 DEFINE_MUTEX(sched_domains_mutex);
 
@@ -410,7 +411,9 @@ static void destroy_sched_domains(struct sched_domain *sd)
 
 static void update_top_cache_domain(int cpu)
 {
+	struct sparsemask *cfs_overload_cpus = NULL;
 	struct sched_domain_shared *sds = NULL;
+	struct rq *rq = cpu_rq(cpu);
 	struct sched_domain *sd;
 	int id = cpu;
 	int size = 1;
@@ -420,8 +423,10 @@ static void update_top_cache_domain(int cpu)
 		id = cpumask_first(sched_domain_span(sd));
 		size = cpumask_weight(sched_domain_span(sd));
 		sds = sd->shared;
+		cfs_overload_cpus = sds->cfs_overload_cpus;
 	}
 
+	rcu_assign_pointer(rq->cfs_overload_cpus, cfs_overload_cpus);
 	rcu_assign_pointer(per_cpu(sd_llc, cpu), sd);
 	per_cpu(sd_llc_size, cpu) = size;
 	per_cpu(sd_llc_id, cpu) = id;
@@ -1621,7 +1626,22 @@ static void __sdt_free(const struct cpumask *cpu_map)
 
 static int sd_llc_alloc(struct sched_domain *sd)
 {
-	/* Allocate sd->shared data here. Empty for now. */
+	struct sched_domain_shared *sds = sd->shared;
+	struct cpumask *span = sched_domain_span(sd);
+	int nid = cpu_to_node(cpumask_first(span));
+	int flags = __GFP_ZERO | GFP_KERNEL;
+	struct sparsemask *mask;
+
+	/*
+	 * Allocate the bitmap if not already allocated.  This is called for
+	 * every CPU in the LLC but only allocates once per sd_llc_shared.
+	 */
+	if (!sds->cfs_overload_cpus) {
+		mask = sparsemask_alloc_node(nr_cpu_ids, 3, flags, nid);
+		if (!mask)
+			return 1;
+		sds->cfs_overload_cpus = mask;
+	}
 
 	return 0;
 }
@@ -1633,7 +1653,8 @@ static void sd_llc_free(struct sched_domain *sd)
 	if (!sds)
 		return;
 
-	/* Free data here. Empty for now. */
+	sparsemask_free(sds->cfs_overload_cpus);
+	sds->cfs_overload_cpus = NULL;
 }
 
 static int sd_llc_alloc_all(const struct cpumask *cpu_map, struct s_data *d)
-- 
1.8.3.1

