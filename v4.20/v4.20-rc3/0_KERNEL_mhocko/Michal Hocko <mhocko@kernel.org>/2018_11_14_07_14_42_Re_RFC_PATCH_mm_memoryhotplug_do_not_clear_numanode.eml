Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from pop3.zju.edu.cn (61.164.42.155:110) by
  likexu-MOBL1.ccr.corp.intel.com with POP3; 14 Nov 2018 14:43:57 -0000
Received: from icoremail.net (unknown [209.85.210.174])
	by mail-app2 (Coremail) with SMTP id by_KCgCn36SLy+tb4DuNAQ--.42127S3;
	Wed, 14 Nov 2018 15:15:24 +0800 (CST)
Received: from mail-pf1-f174.google.com (unknown [209.85.210.174])
	by mx2.icoremail.net (Coremail) with SMTP id AQAAfwC3l0OJy+tb9Rk9AA--.1678S3;
	Wed, 14 Nov 2018 15:15:21 +0800 (CST)
Received: by mail-pf1-f174.google.com with SMTP id b81-v6so6940098pfe.11
        for <xuliker@zju.edu.cn>; Tue, 13 Nov 2018 23:15:21 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-original-authentication-results:x-gm-message-state:delivered-to
         :date:from:to:cc:subject:message-id:references:mime-version
         :content-disposition:in-reply-to:user-agent:sender:precedence
         :list-id;
        bh=j/yioWykOFC7bcHSIZhIrJui/qeYo1yh+Dha+pSoJgQ=;
        b=HpwvVraeIZzIQiKzPJ27T5AfRPnfHU5F2gLF4/TUAFPCHfnupzxZOXbwmubQDt3mcE
         2jg0KtFgmSSeSTSk8saZIzAaKTb2oDIYIXtoIAyftLVYtNbWFmSNBuGg3cZqi9ATq0XM
         0mYzUDrTV6qf3TRcQvBZuSiU4ABNwwkrDEbwYqhvVLN0xuckSDy+fsce/8IRsPC5tBlP
         5rjWJpOsIg25BKnqYAkDFnbPtCv24v68UwDmulE4mMMledd7sbg9dFKHIr0LCJa00Qob
         Kp4lyn2KNR3AuSO4FqcGdbqao8Fu3gsva/+0YsL3ARw3SQzmOVbslyCB+oJqjq7vkUXo
         ah1A==
X-Original-Authentication-Results: mx.google.com;       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org;       dmarc=fail (p=NONE sp=NONE dis=NONE) header.from=kernel.org
X-Gm-Message-State: AGRZ1gJcvZQ5t4XRkO68e0t/Bzvt3/v+hZMHawdTvI1f95s928/2LICI
	WWYLLd7jkH6+I0YI2w8y1UCwN9FNh2wiK6FSly/M8N/o4idTj9azOQ==
X-Received: by 2002:a62:3687:: with SMTP id d129-v6mr823228pfa.56.1542179720978;
        Tue, 13 Nov 2018 23:15:20 -0800 (PST)
X-Forwarded-To: xuliker@zju.edu.cn
X-Forwarded-For: liker.xu@gmail.com xuliker@zju.edu.cn
Delivered-To: liker.xu@gmail.com
Received: by 2002:a17:90a:c304:0:0:0:0 with SMTP id g4-v6csp5469696pjt;
        Tue, 13 Nov 2018 23:15:19 -0800 (PST)
X-Google-Smtp-Source: AJdET5cMJT5k/nE2h0cdouiBrfjBqI2Lbr0egE9TqeSRtw8RpbQAOyPTKt2EoQV0QKrG58Q8Hmlj
X-Received: by 2002:a17:902:108a:: with SMTP id c10-v6mr753151pla.171.1542179719890;
        Tue, 13 Nov 2018 23:15:19 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1542179719; cv=none;
        d=google.com; s=arc-20160816;
        b=fbuCZVzwOZCL10Lf6psvSXOHTrxEKGD7s4QN9qEAM6fcTLDqP0cGUS56yudvNEiLdt
         yjfKyN7OIo6XSLC4YVKOLAQ0Ee8Z+i2lnl7vtB+wtGyCjJyOYKabRtKlekgk85C0D04k
         ISu5QC4qD5fuJOjSOPjUbyvmsu8jDrFnHPLnCjOP/g+GjUeW1fp17/z5IQxHteNVhj4N
         azBs+9asfHS+LGZSkTEdC9CCQIW9wrFzWv26KDQIkJjZVyg2HoeyjEfUzefJvP26ddca
         9mArNFPZrVPYKZMzu7rJi6Kioke96DarVFb3SxNMp2Gn88fhtMdTIEkJBImfhsTqu3rc
         gC2Q==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=list-id:precedence:sender:user-agent:in-reply-to
         :content-disposition:mime-version:references:message-id:subject:cc
         :to:from:date;
        bh=j/yioWykOFC7bcHSIZhIrJui/qeYo1yh+Dha+pSoJgQ=;
        b=PwNmbBWJP6zPNk3VKpm7KybIY2m/0YZohaoFgFo/higY8pR4ef6CtzQ9XVgZqIr/5D
         GLNOn2rFfXthAyXQnsjXGeV635cCzotbKORHPDyMCGW3OwYzA+OdTvm54YKnNT5fqFy0
         q++gQ2wEqwO4hlIAO8X12TR0WSsxjqK3TTxJuGwoB3D1lo2ctB8JVJI4znbrkLrSe6Qa
         +GL8JJsaM/UEZkJ+EQ+bBay+nz1uKhIkhGxNEsSU6bkw1VZzK6iiq/fm1J8/rs9ffRsp
         arpsQdLOYHRYRYsdSoqDcB/RELol3Hdad1+9qSVHlMXMoQzgaCfA4LW40bIRsfj7+myQ
         Jj5A==
ARC-Authentication-Results: i=1; mx.google.com;
       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org;
       dmarc=fail (p=NONE sp=NONE dis=NONE) header.from=kernel.org
Received: from vger.kernel.org (vger.kernel.org. [209.132.180.67])
        by mx.google.com with ESMTP id o22si12928716pgb.584.2018.11.13.23.15.04;
        Tue, 13 Nov 2018 23:15:19 -0800 (PST)
Received-SPF: pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) client-ip=209.132.180.67;
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1731811AbeKNRQo (ORCPT <rfc822;zhaoweiliew@gmail.com>
        + 99 others); Wed, 14 Nov 2018 12:16:44 -0500
Received: from mx2.suse.de ([195.135.220.15]:44606 "EHLO mx1.suse.de"
        rhost-flags-OK-OK-OK-FAIL) by vger.kernel.org with ESMTP
        id S1726927AbeKNRQo (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Wed, 14 Nov 2018 12:16:44 -0500
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay2.suse.de (unknown [195.135.220.254])
        by mx1.suse.de (Postfix) with ESMTP id 9C7C0AAAE;
        Wed, 14 Nov 2018 07:14:43 +0000 (UTC)
Date: Wed, 14 Nov 2018 08:14:42 +0100
From: Michal Hocko <mhocko@kernel.org>
To: Andrew Morton <akpm@linux-foundation.org>, linux-mm@kvack.org
Cc: Oscar Salvador <OSalvador@suse.com>,
        LKML <linux-kernel@vger.kernel.org>,
        Wen Congyang <tangchen@cn.fujitsu.com>,
        Tang Chen <wency@cn.fujitsu.com>,
        Miroslav Benes <mbenes@suse.cz>,
        Vlastimil Babka <vbabka@suse.cz>
Subject: Re: [RFC PATCH] mm, memory_hotplug: do not clear numa_node
 association after hot_remove
Message-ID: <20181114071442.GB23419@dhcp22.suse.cz>
References: <20181108100413.966-1-mhocko@kernel.org>
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: <20181108100413.966-1-mhocko@kernel.org>
User-Agent: Mutt/1.10.1 (2018-07-13)
Sender: liker.xu+caf_=xuliker=zju.edu.cn@gmail.com
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-CM-TRANSID: AQAAfwC3l0OJy+tb9Rk9AA--.1678S3
Authentication-Results: mail-app2; spf=pass smtp.mail=liker.xu+caf_=xu
	liker=zju.edu.cn@gmail.com;
X-Coremail-Antispam: 1UD129KBjvJXoW3JF1rJrW8ZryUZw1rCw47XFb_yoWxGryxpr
	WUW3y5Cr4ktr1UWw18t3y5JryUXr4xAa1UJw1xuF1kZF15Gr1kAr18tr4UWryDJrWrXF17
	JFs8Ja1Iga45JaDanT9S1TB71UUUUUUqnTZGkaVYY2UrUUUU0bIjqfuFe4nvWSU8nxnvy2
	9KBjDU0xBIdaVrnRJUUUP0b7Iv0xC_Kw4lb4IE77IF4wAFF20E14v26r1j6r4UM7CIcVAF
	z4kK6r1j6r18M28lY4IEw2IIxxk0rwA2F7IY1VAKz4vEj48ve4kI8wA2z4x0Y4vE2Ix0cI
	8IcVAFwI0_Xr0_Ar1l84ACjcxK6xIIjxv20xvEc7CjxVAFwI0_Gr0_Cr1l84ACjcxK6I8E
	87Iv67AKxVW0oVCq3wA2z4x0Y4vEx4A2jsIEc7CjxVAFwI0_GcCE3s1le2I262IYc4CY6c
	8Ij28IcVAaY2xG8wAqx4xG64xvF2IEw4CE5I8CrVC2j2WlYx0E2Ix0cI8IcVAFwI0_Jr0_
	Jr4lYx0Ex4A2jsIE14v26r4j6F4UMcvjeVCFs4IE7xkEbVWUJVW8JwACjcxG0xvEwIxGrw
	CjxxvEa2IrMxkF7I0Ew4C26cxK6c8Ij28IcwCY1Ik26cxK6xAEc7vF6xCjj44lc2xSY4AK
	6IIF6r4xMxkI7II2jI8vz4vEwIxGrwCYIxAIcVC0I7IYx2IY67AKxVWUCVW8JwCYIxAIcV
	C0I7IYx2IY6xkF7I0E14v26r1j6r4UMxvI42IY6I8E87Iv67AKxVWxJr0_GcWlcIIF0xvE
	x4A2jsIEc7CjxVAFwI0_Cr1j6rxdMxAIw28IcxkI7VAKI48JMxAIw28IcVAKzI0EY4vE52
	x082I5MxCjnVCjjxCrMxC20s026xCaFVCjc4AY6r1j6r4UMI8I3I0E5I8CrVAFwI0_Jr0_
	Jr4lx2IqxVCjr7xvwVAFwI0_JrI_JrWlx4CE17CEb7AF67AKxVWUtVW8ZwCIc40Y0x0EwI
	xGrwCI42IY6xAIw20EY4v20xvaj40_Gr0_ZrUvcSsGvfC2KfnxnUUI43ZEXa7IUOgiStUU
	UUU==

It seems there were no objections here. So can we have it in linux-next
for a wider testing a possibly target the next merge window?

On Thu 08-11-18 11:04:13, Michal Hocko wrote:
> From: Michal Hocko <mhocko@suse.com>
> 
> Per-cpu numa_node provides a default node for each possible cpu. The
> association gets initialized during the boot when the architecture
> specific code explores cpu->NUMA affinity. When the whole NUMA node is
> removed though we are clearing this association
> 
> try_offline_node
>   check_and_unmap_cpu_on_node
>     unmap_cpu_on_node
>       numa_clear_node
>         numa_set_node(cpu, NUMA_NO_NODE)
> 
> This means that whoever calls cpu_to_node for a cpu associated with such
> a node will get NUMA_NO_NODE. This is problematic for two reasons. First
> it is fragile because __alloc_pages_node would simply blow up on an
> out-of-bound access. We have encountered this when loading kvm module
> BUG: unable to handle kernel paging request at 00000000000021c0
> IP: [<ffffffff8119ccb3>] __alloc_pages_nodemask+0x93/0xb70
> PGD 800000ffe853e067 PUD 7336bbc067 PMD 0
> Oops: 0000 [#1] SMP
> [...]
> CPU: 88 PID: 1223749 Comm: modprobe Tainted: G        W          4.4.156-94.64-default #1
> task: ffff88727eff1880 ti: ffff887354490000 task.ti: ffff887354490000
> RIP: 0010:[<ffffffff8119ccb3>]  [<ffffffff8119ccb3>] __alloc_pages_nodemask+0x93/0xb70
> RSP: 0018:ffff887354493b40  EFLAGS: 00010202
> RAX: 00000000000021c0 RBX: 0000000000000000 RCX: 0000000000000000
> RDX: 0000000000000000 RSI: 0000000000000002 RDI: 00000000014000c0
> RBP: 00000000014000c0 R08: ffffffffffffffff R09: 0000000000000000
> R10: ffff88fffc89e790 R11: 0000000000014000 R12: 0000000000000101
> R13: ffffffffa0772cd4 R14: ffffffffa0769ac0 R15: 0000000000000000
> FS:  00007fdf2f2f1700(0000) GS:ffff88fffc880000(0000) knlGS:0000000000000000
> CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
> CR2: 00000000000021c0 CR3: 00000077205ee000 CR4: 0000000000360670
> DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
> DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
> Stack:
>  0000000000000086 014000c014d20400 ffff887354493bb8 ffff882614d20f4c
>  0000000000000000 0000000000000046 0000000000000046 ffffffff810ac0c9
>  ffff88ffe78c0000 ffffffff0000009f ffffe8ffe82d3500 ffff88ff8ac55000
> Call Trace:
>  [<ffffffffa07476cd>] alloc_vmcs_cpu+0x3d/0x90 [kvm_intel]
>  [<ffffffffa0772c0c>] hardware_setup+0x781/0x849 [kvm_intel]
>  [<ffffffffa04a1c58>] kvm_arch_hardware_setup+0x28/0x190 [kvm]
>  [<ffffffffa04856fc>] kvm_init+0x7c/0x2d0 [kvm]
>  [<ffffffffa0772cf2>] vmx_init+0x1e/0x32c [kvm_intel]
>  [<ffffffff8100213a>] do_one_initcall+0xca/0x1f0
>  [<ffffffff81193886>] do_init_module+0x5a/0x1d7
>  [<ffffffff81112083>] load_module+0x1393/0x1c90
>  [<ffffffff81112b30>] SYSC_finit_module+0x70/0xa0
>  [<ffffffff8161cbc3>] entry_SYSCALL_64_fastpath+0x1e/0xb7
> DWARF2 unwinder stuck at entry_SYSCALL_64_fastpath+0x1e/0xb7
> 
> on an older kernel but the code is basically the same in the current
> Linus tree as well. alloc_vmcs_cpu could use alloc_pages_nodemask which
> would recognize NUMA_NO_NODE and use alloc_pages_node which would translate
> it to numa_mem_id but that is wrong as well because it would use a cpu
> affinity of the local CPU which might be quite far from the original node.
> It is also reasonable to expect that cpu_to_node will provide a sane value
> and there might be many more callers like that.
> 
> The second problem is that __register_one_node relies on cpu_to_node
> to properly associate cpus back to the node when it is onlined. We do
> not want to lose that link as there is no arch independent way to get it
> from the early boot time AFAICS.
> 
> Drop the whole check_and_unmap_cpu_on_node machinery and keep the
> association to fix both issues. The NODE_DATA(nid) is not deallocated
> so it will stay in place and if anybody wants to allocate from that node
> then a fallback node will be used.
> 
> Thanks to Vlastimil Babka for his live system debugging skills that
> helped debugging the issue.
> 
> Debugged-by: Vlastimil Babka <vbabka@suse.cz>
> Reported-by: Miroslav Benes <mbenes@suse.cz>
> Fixes: e13fe8695c57 ("cpu-hotplug,memory-hotplug: clear cpu_to_node() when offlining the node")
> Cc: Wen Congyang <tangchen@cn.fujitsu.com>
> Cc: Tang Chen <wency@cn.fujitsu.com>
> Signed-off-by: Michal Hocko <mhocko@suse.com>
> ---
> 
> Hi,
> please note that I am sending this as an RFC even though this has been
> confirmed to fix the oops in kvm_intel module because I cannot simply
> tell that there are no other side effect that I do not see from the code
> reading. I would appreciate some background from people who have
> introduced this code e13fe8695c57 ("cpu-hotplug,memory-hotplug: clear
> cpu_to_node() when offlining the node") because the changelog doesn't
> really explain the motivation much.
> 
>  mm/memory_hotplug.c | 30 +-----------------------------
>  1 file changed, 1 insertion(+), 29 deletions(-)
> 
> diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c
> index 2b2b3ccbbfb5..87aeafac54ee 100644
> --- a/mm/memory_hotplug.c
> +++ b/mm/memory_hotplug.c
> @@ -1753,34 +1753,6 @@ static int check_cpu_on_node(pg_data_t *pgdat)
>  	return 0;
>  }
>  
> -static void unmap_cpu_on_node(pg_data_t *pgdat)
> -{
> -#ifdef CONFIG_ACPI_NUMA
> -	int cpu;
> -
> -	for_each_possible_cpu(cpu)
> -		if (cpu_to_node(cpu) == pgdat->node_id)
> -			numa_clear_node(cpu);
> -#endif
> -}
> -
> -static int check_and_unmap_cpu_on_node(pg_data_t *pgdat)
> -{
> -	int ret;
> -
> -	ret = check_cpu_on_node(pgdat);
> -	if (ret)
> -		return ret;
> -
> -	/*
> -	 * the node will be offlined when we come here, so we can clear
> -	 * the cpu_to_node() now.
> -	 */
> -
> -	unmap_cpu_on_node(pgdat);
> -	return 0;
> -}
> -
>  /**
>   * try_offline_node
>   * @nid: the node ID
> @@ -1813,7 +1785,7 @@ void try_offline_node(int nid)
>  		return;
>  	}
>  
> -	if (check_and_unmap_cpu_on_node(pgdat))
> +	if (check_cpu_on_node(pgdat))
>  		return;
>  
>  	/*
> -- 
> 2.19.1

-- 
Michal Hocko
SUSE Labs
