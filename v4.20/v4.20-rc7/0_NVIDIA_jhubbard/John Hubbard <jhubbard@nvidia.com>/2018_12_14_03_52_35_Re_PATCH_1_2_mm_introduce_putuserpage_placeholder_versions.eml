Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by i7-8700 with POP3-SSL;
  14 Dec 2018 20:24:04 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga004.jf.intel.com (orsmga004.jf.intel.com [10.7.209.38])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id BA25158079D;
	Thu, 13 Dec 2018 19:52:43 -0800 (PST)
Received: from orsmga101.jf.intel.com ([10.7.208.22])
  by orsmga004-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 13 Dec 2018 19:52:43 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3A0WcYrxdiZeyk7GldpeaVb6lUlGMj4u6mDksu8pMi?=
 =?us-ascii?q?zoh2WeGdxc6/YRyN2/xhgRfzUJnB7Loc0qyK6/CmATRIyK3CmUhKSIZLWR4BhJ?=
 =?us-ascii?q?detC0bK+nBN3fGKuX3ZTcxBsVIWQwt1Xi6NU9IBJS2PAWK8TW94jEIBxrwKxd+?=
 =?us-ascii?q?KPjrFY7OlcS30P2594HObwlSizexfbB/IA+qoQnNq8IbnZZsJqEtxxXTv3BGYf?=
 =?us-ascii?q?5WxWRmJVKSmxbz+MK994N9/ipTpvws6ddOXb31cKokQ7NYCi8mM30u683wqRbD?=
 =?us-ascii?q?VwqP6WACXWgQjxFFHhLK7BD+Xpf2ryv6qu9w0zSUMMHqUbw5Xymp4rx1QxH0li?=
 =?us-ascii?q?gIKz858HnWisNuiqJbvAmhrAF7z4LNfY2ZKOZycqbbcNgHR2ROQ9xRWjRcDI2i?=
 =?us-ascii?q?YYsBD+kPM+hWoIbypVQOrAexCwa3BOP3yDJFnWP20K8g3ug9DQ3L0g4tEtQTu3?=
 =?us-ascii?q?rUttX1M6ISXPixwqnPzTXDae5d1zXg6IfTaR8uu+uMUq9tesfWy0kvFx7FgU6L?=
 =?us-ascii?q?poP/JTOay/8As26F7+phSO2vinQopxttrTiow8chk4/EjZ8bxFDD8CV22oc1Jd?=
 =?us-ascii?q?ugRU54f9GkCp1QuD+eN4dsRcMiWW5otSAnwbMFoZ62ZDYGxIgjyhLFdvCKfZaE?=
 =?us-ascii?q?7gj+WOuSPTt0nnNodbCnixqv80Ws1PfwWteo3FpQsyZInMPAu34J2hDL9MSLV/?=
 =?us-ascii?q?pw8l2/1TqR1A3f8PxILV4pmabBKpMszbg9nYcJv0vZBC/5gkD2gbeWdko6/uio?=
 =?us-ascii?q?7PzqYqvpppCCLY94kAL+Pbo0msy5H+s4NhICX2+B+eSzzLHj/Ev5T6tWjvAujK?=
 =?us-ascii?q?XVrJTXKd4GqqO3HQNZyJsv5hWjAzu80dkVn2ELLFdfdxKGi4jpNUvOIPf9Dfqn?=
 =?us-ascii?q?h1SskTFrx+3JP7H4AZXCMGLDkLH/crZ58kJczwQyzdZB6JJOEbwBPv3zVVHrtN?=
 =?us-ascii?q?DCDR82LRa0w+D5B9V5zI8eXniPAqCBPKPIrVCI/v4vI/WLZIINvDb9Kvsl6OD0?=
 =?us-ascii?q?gX42hF8QZq2p3ZoRaHClEfVqOUSZYXzwgtgfFWcGpBYxTOvviFeaSz5ce26yX7?=
 =?us-ascii?q?4g5jE8EI+mDZ3MRoGxgLOb2ye3BJ1WZn1cBVCKHnflbIGEW/YKaCKPLc5tiD0E?=
 =?us-ascii?q?Vb69S4A/0RGirhP1y71iLuDM4C0XqYrj1MRp5+3UjRwy9zt0ANqH32GOSGF0mG?=
 =?us-ascii?q?UIRzgt0aB7oEx9zEqD0Kdij/xZE9xT++1GUgMgOZHAyOx6Dsj4WhjdcdeRVFam?=
 =?us-ascii?q?XtKmDCkrQdIqw98OZEV9F8+4jh/Z3SqnGLsVl72NBJwp/aPQxXnxJ8Bhy3nY0K?=
 =?us-ascii?q?ktlUUpQsxKNWe+nK5w6xDTB5LVk0Wej6uqdr4T3CjX+GeHzGqBpkdYUAFrXKXB?=
 =?us-ascii?q?XHAfYFbWrNvj6kPDSb+uFaooMg9bxcGeLatKb8XjjU9aS/f7JNTef2Wxln+tCh?=
 =?us-ascii?q?mS2LODcpDme2UH0yXbE0gLjQYT8XGCNQg9Ayehp3nTDDhvFVLpfkPt/vNyqHK9?=
 =?us-ascii?q?Tk8o0Q6Ka1dt2Kay+h4QnfacUe8c3qoYuCc9rDV5BEuy0MjIC9WevQZhfL9TYd?=
 =?us-ascii?q?Um4FhZ02LUrAh9Pp2mL6B/iV8SaQV3v0Xy1xppDoVMi9QlrHQvzABqM6KXzEtB?=
 =?us-ascii?q?dy+E3ZD3IrDXNmjy/BWoa67K2lHf0Mya+rsV5PQ/sVXjuACpFkwt83h819lV0n?=
 =?us-ascii?q?2c5ojFDQYIUJLxVFo3+AZ+p73AfiY94IbU32V2Maaoqj/Cx84pBOw9xxm6e9dQ?=
 =?us-ascii?q?LqyFGxHyEsEAHMeuNfEllEKvbhIHO+BS6rU5P8end/uAxa6qM/xsnDOgjWRb/o?=
 =?us-ascii?q?991liA+DZ7Su7Nx5wF2e2X3hObVzfgi1esqsD2mZ1eaT4OBGa+yCjkC5RXZq19?=
 =?us-ascii?q?ZosLDWauI8uqxtRxnZLtWnhY9EK9CFMCwsOmZR2Sb1nl1w1KyUsXuWCnmTe/zz?=
 =?us-ascii?q?FsiTEpr7aQ0zbUw+v/cxoLIGhLS3d4jVftOIS7k8oVXEy1YAc3jhul4kD6yrNf?=
 =?us-ascii?q?pKR+KWnTXEhJczL3L2FkTqu/qL6Cb9RT55MvtCVdSP68bkyCSr7hvxsa1DvuHm?=
 =?us-ascii?q?tfxDwhdzCmoI75nwF8iG+GKHZzrXzZedx/xBvF5dzcQ+JR0SQCRCVilTbXAV28?=
 =?us-ascii?q?NcGz/dqIj5fDrvy+V2W5W51Raybr14CAtCi85WFwGh2whfOzmt7mEQg8zyD70c?=
 =?us-ascii?q?JnVSHJrBb6f4nq2L62MeNhfkl0GlD879B2FZ15kos1nJsQw2QVho2J/Xoblmf+?=
 =?us-ascii?q?KdVa1rj5bHYXRz4LwtjV7RPh2E1iKHKJ2o34Wm+cwstne9m1fGcW1jgh4MBNDa?=
 =?us-ascii?q?ee9KZEkjdtolqksQLRZuBwnjQHxvsv8nIag/wJtxArziWSGb0SGUhYPSrxlxWH?=
 =?us-ascii?q?9dy+raNXZHqxfri0zkZxgdehDLSaqAFGRHn5YosiHTN37shnNVLM0X7z5Zv+dN?=
 =?us-ascii?q?jec90TrQGUnAnaj+dONp0xjPUKiDFjOWL8u30l1uE6gQZv3ZG8oIiINWFt8Lil?=
 =?us-ascii?q?DR5fMz3/f9kT9S31jaZCgsaW2JiiHolmGjUORpfpTOinEDQPtfT8LAaOESYxqn?=
 =?us-ascii?q?OaGbrZAA+e511qr3PJE5C3KX6XIGMVwsllRBmYPEZfmhwbXC0mnp4lEQCn3Nfu?=
 =?us-ascii?q?f11+5jAV+170sB9Myv9zOhn7U2ffohqoazguRJieKhpW8h9N50PPPcOC6eJzGj?=
 =?us-ascii?q?lS/oe9owyVNmybewNIAHkLWkOeAlDsIKKu6cPc8+SCBeq+MvjOYaiIqeNETPeF?=
 =?us-ascii?q?3pav0opg/zaROcSDJHhiD/sn2kVdWXB1AdjWmzIKSyYPjSLCc9abpAug+i1wts?=
 =?us-ascii?q?2+8PPrWB/25YuSEbRSN89j+wuxgaeFOO6dnyJ5KTde1pMRyn7E0rkf3FgOiy5w?=
 =?us-ascii?q?czmhC6gPtSnITKjIgK9YEwYbaz9vNMtP96883hdCOc/YitP21750lP01C01CVV?=
 =?us-ascii?q?zuhM6pf9EKI3qmOVPDBUaLMqmGJDLRz8H2Z6O8VaNfjOFOuxKsvjabFlfpPi6f?=
 =?us-ascii?q?mDnxSxCvLeZMgTmBMxxDo469aAhiCGj5QNLiax22K9t3jTwwwb0piXLGL28cMT?=
 =?us-ascii?q?5gc0xTqr2c9z9XgvJ6G2ZZ9HpqMfGEmzqF7+nfMpsWrfprAiFum+1G+ng117tV?=
 =?us-ascii?q?4z9CRPFunCvSr9huo0ypk+WVyzpnVgZOpShPhI6RoUpiPqDZ/IFaWXnY5BIN8X?=
 =?us-ascii?q?mQCxMSqtplENLvvaVQytnJlK3rKTZC88jb/c0TB8XPLMKHMXwhMQfmGTLODQsF?=
 =?us-ascii?q?SyKrOn/bh0BHjP6S8XiV/dAGrc3AkYQDApRbVVozG+lSXkhsAt0OCJltWTYji7?=
 =?us-ascii?q?SakIgD4n/o6FHzSd9du52PefaTGu7iYGKbgr1FfAAF6an1IYQaKsvw3Ek0OXdg?=
 =?us-ascii?q?m4GfOU3QW5htvydrZxU46BFP8GV0RGQp12rjdAKh4XZVHvmxyE1lwjBiaPggoW?=
 =?us-ascii?q?+/q2w8IUDH8W5pyBE8?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AnAABKKBNch0O0hNFjGQEBAQEBAQEBA?=
 =?us-ascii?q?QEBAQcBAQEBAQGBZYJqgQInCoNylCqBYC0UmTQUGAsIAYdFIjgSAQMBAQEBAQE?=
 =?us-ascii?q?CARMBAQEIDQkIKSMMgjYkAYJiAQIDAQIgBBkBATcBBQkBAQoOCgICJgICA0QQB?=
 =?us-ascii?q?gEMAQUCAQEBFgODAwGCAAUKpiFwfDOCdgEBBYEwAYEShGcDBYELiheBHBeBQD+?=
 =?us-ascii?q?BOIFtfoRNgziCV4kfIAGBdoQVRzePdlYJhw2KRgYYiX2HTiyJBIR2iyqBXYF3M?=
 =?us-ascii?q?xoIGxU7gmwTgggMF4NKhRSFYB8BMQGBBAEBihorgQGBHwEB?=
X-IPAS-Result: =?us-ascii?q?A0AnAABKKBNch0O0hNFjGQEBAQEBAQEBAQEBAQcBAQEBAQG?=
 =?us-ascii?q?BZYJqgQInCoNylCqBYC0UmTQUGAsIAYdFIjgSAQMBAQEBAQECARMBAQEIDQkIK?=
 =?us-ascii?q?SMMgjYkAYJiAQIDAQIgBBkBATcBBQkBAQoOCgICJgICA0QQBgEMAQUCAQEBFgO?=
 =?us-ascii?q?DAwGCAAUKpiFwfDOCdgEBBYEwAYEShGcDBYELiheBHBeBQD+BOIFtfoRNgziCV?=
 =?us-ascii?q?4kfIAGBdoQVRzePdlYJhw2KRgYYiX2HTiyJBIR2iyqBXYF3MxoIGxU7gmwTggg?=
 =?us-ascii?q?MF4NKhRSFYB8BMQGBBAEBihorgQGBHwEB?=
X-IronPort-AV: E=Sophos;i="5.56,351,1539673200"; 
   d="scan'208";a="44934068"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 13 Dec 2018 19:52:41 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726919AbeLNDwj (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Thu, 13 Dec 2018 22:52:39 -0500
Received: from hqemgate15.nvidia.com ([216.228.121.64]:15481 "EHLO
        hqemgate15.nvidia.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726445AbeLNDwi (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Thu, 13 Dec 2018 22:52:38 -0500
Received: from hqpgpgate102.nvidia.com (Not Verified[216.228.121.13]) by hqemgate15.nvidia.com (using TLS: TLSv1.2, DES-CBC3-SHA)
        id <B5c1329000000>; Thu, 13 Dec 2018 19:52:32 -0800
Received: from hqmail.nvidia.com ([172.20.161.6])
  by hqpgpgate102.nvidia.com (PGP Universal service);
  Thu, 13 Dec 2018 19:52:35 -0800
X-PGP-Universal: processed;
        by hqpgpgate102.nvidia.com on Thu, 13 Dec 2018 19:52:35 -0800
Received: from [10.110.48.28] (172.20.13.39) by HQMAIL101.nvidia.com
 (172.20.187.10) with Microsoft SMTP Server (TLS) id 15.0.1395.4; Fri, 14 Dec
 2018 03:52:35 +0000
Subject: Re: [PATCH 1/2] mm: introduce put_user_page*(), placeholder versions
To: Dave Chinner <david@fromorbit.com>,
        Jerome Glisse <jglisse@redhat.com>
CC: Jan Kara <jack@suse.cz>, Matthew Wilcox <willy@infradead.org>,
        Dan Williams <dan.j.williams@intel.com>,
        John Hubbard <john.hubbard@gmail.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Linux MM <linux-mm@kvack.org>, <tom@talpey.com>,
        Al Viro <viro@zeniv.linux.org.uk>, <benve@cisco.com>,
        Christoph Hellwig <hch@infradead.org>,
        Christopher Lameter <cl@linux.com>,
        "Dalessandro, Dennis" <dennis.dalessandro@intel.com>,
        Doug Ledford <dledford@redhat.com>,
        Jason Gunthorpe <jgg@ziepe.ca>,
        Michal Hocko <mhocko@kernel.org>, <mike.marciniszyn@intel.com>,
        <rcampbell@nvidia.com>,
        Linux Kernel Mailing List <linux-kernel@vger.kernel.org>,
        linux-fsdevel <linux-fsdevel@vger.kernel.org>
References: <20181205014441.GA3045@redhat.com>
 <59ca5c4b-fd5b-1fc6-f891-c7986d91908e@nvidia.com>
 <7b4733be-13d3-c790-ff1b-ac51b505e9a6@nvidia.com>
 <20181207191620.GD3293@redhat.com>
 <3c4d46c0-aced-f96f-1bf3-725d02f11b60@nvidia.com>
 <20181208022445.GA7024@redhat.com> <20181210102846.GC29289@quack2.suse.cz>
 <20181212150319.GA3432@redhat.com> <20181212214641.GB29416@dastard>
 <20181212215931.GG5037@redhat.com> <20181213005119.GD29416@dastard>
X-Nvconfidentiality: public
From: John Hubbard <jhubbard@nvidia.com>
Message-ID: <05a68829-6e6d-b766-11b4-99e1ba4bc87b@nvidia.com>
Date: Thu, 13 Dec 2018 19:52:35 -0800
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101
 Thunderbird/60.3.3
MIME-Version: 1.0
In-Reply-To: <20181213005119.GD29416@dastard>
X-Originating-IP: [172.20.13.39]
X-ClientProxiedBy: HQMAIL106.nvidia.com (172.18.146.12) To
 HQMAIL101.nvidia.com (172.20.187.10)
Content-Type: text/plain; charset="utf-8"
Content-Language: en-US-large
Content-Transfer-Encoding: 7bit
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=nvidia.com; s=n1;
        t=1544759552; bh=6xYVY0tXIY9tCF5suFjsryy8ZqsydFEfNMSOISwBRQg=;
        h=X-PGP-Universal:Subject:To:CC:References:X-Nvconfidentiality:From:
         Message-ID:Date:User-Agent:MIME-Version:In-Reply-To:
         X-Originating-IP:X-ClientProxiedBy:Content-Type:Content-Language:
         Content-Transfer-Encoding;
        b=B2tzWU6Gx7arMwhBcjbutix2zab1z0Ae7JSjOkWdmXSamYVVriZkWw+GmMcKf6i+6
         m6HVTB9/NQrHzpH3szkxWMSBotBF32FtZnMRY+mfWAkxuVMwJpZHSRLBEMI7wvE0zR
         qA+52Ip+HvUzbeKwsDHlLXbeZ9ZlAEf0RXDz+CjrccVhX2h/B8lVgKbYNZA6u3l9hB
         6CxJPTxrOKlk6kob/inxF8xlbnXc3J+fcpQbcF/TA0oeGCtH7ARjFUTyisbxJurUg/
         GB7BFsBMHklFFWQrGD6OOUMeNPTjqTeFk+JlGZ0SQHUbb/rKCnSxFL73h3NrOeuqmR
         NJhv03ESra1Kg==
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On 12/12/18 4:51 PM, Dave Chinner wrote:
> On Wed, Dec 12, 2018 at 04:59:31PM -0500, Jerome Glisse wrote:
>> On Thu, Dec 13, 2018 at 08:46:41AM +1100, Dave Chinner wrote:
>>> On Wed, Dec 12, 2018 at 10:03:20AM -0500, Jerome Glisse wrote:
>>>> On Mon, Dec 10, 2018 at 11:28:46AM +0100, Jan Kara wrote:
>>>>> On Fri 07-12-18 21:24:46, Jerome Glisse wrote:
>>>>> So this approach doesn't look like a win to me over using counter in struct
>>>>> page and I'd rather try looking into squeezing HMM public page usage of
>>>>> struct page so that we can fit that gup counter there as well. I know that
>>>>> it may be easier said than done...
>>>>

Agreed. After all the discussion this week, I'm thinking that the original idea
of a per-struct-page counter is better. Fortunately, we can do the moral equivalent 
of that, unless I'm overlooking something: Jerome had another proposal that he
described, off-list, for doing that counting, and his idea avoids the problem of 
finding space in struct page. (And in fact, when I responded yesterday, I initially 
thought that's where he was going with this.)

So how about this hybrid solution:

1. Stay with the basic RFC approach of using a per-page counter, but actually
store the counter(s) in the mappings instead of the struct page. We can use
!PageAnon and page_mapping to look up all the mappings, stash the dma_pinned_count
there. So the total pinned count is scattered across mappings. Probably still need
a PageDmaPinned bit.

Thanks again to Jerome for coming up with that idea, and I hope I haven't missed
a critical point or misrepresented it.

2. put_user_page() would still restrict itself to managing PageDmaPinned and
dma_pinned_count, as before. No messing with page_mkwrite or anything that
requires lock_page():

void put_user_page(struct page *page)
{
	if (PageAnon(page))
		put_page(page);
	else {
		/* Approximately: Check PageDmaPinned, look up dma_pinned_count
		 * via page_mapping's, decrement the appropriate
		 * mapping's dma_pinned_count. Clear PageDmaPinned
		 * if dma_pinned_count hits zero.
		 */

	...
}

I'm not sure how tricky finding the "appropriate" mapping is, but it seems 
like just comparing current->mm information with the mappings should do it.

3. And as before, use PageDmaPinned to decide what to do in page_mkclean() and
try_to_unmap().

Maybe here is the part where someone says, "you should have created the actual
patchset, instead of typing all those words". But I'm still hoping to get some
consensus first. :)

one more note below...

>>>> So i want back to the drawing board and first i would like to ascertain
>>>> that we all agree on what the objectives are:
>>>>
>>>>     [O1] Avoid write back from a page still being written by either a
>>>>          device or some direct I/O or any other existing user of GUP.
> 
> IOWs, you need to mark pages being written to by a GUP as
> PageWriteback, so all attempts to write the page will block on
> wait_on_page_writeback() before trying to write the dirty page.
> 
>>>>          This would avoid possible file system corruption.
> 
> This isn't a filesystem corruption vector. At worst, it could cause
> torn data writes due to updating the page while it is under IO. We
> have a name for this: "stable pages". This is designed to prevent
> updates to pages via mmap writes from causing corruption of things
> like MD RAID due to modification of the data during RAID parity
> calculations. Hence we have wait_for_stable_page() calls in all
> ->page_mkwrite implementations so that new mmap writes block until
> writeback IO is complete on the devices that require stable pages
> to prevent corruption.
> 
> IOWs, we already deal with this "delay new modification while
> writeback is in progress" problem in the mmap/filesystem world and
> have infrastructure to handle it. And the ->page_mkwrite code
> already deals with it.
> 
>>>>
>>>>     [O2] Avoid crash when set_page_dirty() is call on a page that is
>>>>          considered clean by core mm (buffer head have been remove and
>>>>          with some file system this turns into an ugly mess).
>>>
>>> I think that's wrong. This isn't an "avoid a crash" case, this is a
>>> "prevent data and/or filesystem corruption" case. The primary goal
>>> we have here is removing our exposure to potential corruption, which
>>> has the secondary effect of avoiding the crash/panics that currently
>>> occur as a result of inconsistent page/filesystem state.
>>
>> This is O1 avoid corruption is O1
> 
> It's "avoid a specific instance of data corruption", not a general
> mechanism for avoiding data/filesystem corruption.
> 
> Calling set_page_dirty() on a file backed page which has not been
> correctly prepared can cause data corruption, filesystem coruption
> and shutdowns, etc because we have dirty data over a region that is
> not correctly mapped. Yes, it can also cause a crash (because we
> really, really suck at validation and error handling in generic code
> paths), but there's so, so much more that can go wrong than crash
> the kernel when we do stupid shit like this.
> 
>>> i.e. The goal is to have ->page_mkwrite() called on the clean page
>>> /before/ the file-backed page is marked dirty, and hence we don't
>>> expose ourselves to potential corruption or crashes that are a
>>> result of inappropriately calling set_page_dirty() on clean
>>> file-backed pages.
>>
>> Yes and this would be handle by put_user_page ie:
> 
> No, put_user_page() is too late - it's after the DMA has completed,
> but we have to ensure the file has backing store allocated and the
> pages are in the correct state /before/ the DMA is done.
> 
> Think ENOSPC - that has to be handled before we do the DMA, not
> after. Before the DMA it is a recoverable error, after the DMA it is
> data loss/corruption failure.
> 
>> put_user_page(struct page *page, bool dirty)
>> {
>>     if (!PageAnon(page)) {
>>         if (dirty) {
>>             // Do the whole dance ie page_mkwrite and all before
>>             // calling set_page_dirty()
>>         }
>>         ...
>>     }
>>     ...
>> }
> 
> Essentially, doing this would require a whole new "dirty a page"
> infrastructure because it is in the IO path, not the page fault
> path.
> 
> And, for hardware that does it's own page faults for DMA, this whole
> post-DMA page setup is broken because the pages will have already
> gone through ->page_mkwrite() and be set up correctly already.
> 
>>>> For [O2] i believe we can handle that case in the put_user_page()
>>>> function to properly dirty the page without causing filesystem
>>>> freak out.
>>>
>>> I'm pretty sure you can't call ->page_mkwrite() from
>>> put_user_page(), so I don't think this is workable at all.
>>
>> Hu why ? i can not think of any reason whike you could not. User of
> 
> It's not a fault path, you can't safely lock pages, you can't take
> fault-path only locks in the IO path (mmap_sem inversion problems),
> etc.
> 

Yes, I looked closer at ->page_mkwrite (ext4_page_mkwrite, for example),
and it's clearly doing lock_page(), so it does seem like this particular
detail (calling page_mkwrite from put_user_page) is dead.

> /me has a nagging feeling this was all explained in a previous
> discussions of this patchset...
> 

Yes, lots of related discussion definitely happened already, for example
this October thread covered page_mkwrite and interactions with gup:

https://lore.kernel.org/r/20181001061127.GQ31060@dastard

...but so far, this is the first time I recall seeing a proposal to call
page_mkwrite from put_user_page. 


thanks,
-- 
John Hubbard
NVIDIA
