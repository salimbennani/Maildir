Return-Path: <virtualization-bounces@lists.linux-foundation.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by i7-8700 with POP3-SSL;
  14 Dec 2018 20:25:12 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga002.fm.intel.com (fmsmga002.fm.intel.com [10.253.24.26])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 973FE5803DC
	for <like.xu@linux.intel.com>; Thu, 13 Dec 2018 23:41:59 -0800 (PST)
Received: from fmsmga102.fm.intel.com ([10.1.193.69])
  by fmsmga002-1.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 13 Dec 2018 23:41:59 -0800
IronPort-PHdr: =?us-ascii?q?9a23=3A6ZCWERHRQSDOtXt9DarLop1GYnF86YWxBRYc798d?=
 =?us-ascii?q?s5kLTJ76psy/bnLW6fgltlLVR4KTs6sC17KG9fi4EUU7or+5+EgYd5JNUxJXwe?=
 =?us-ascii?q?43pCcHRPC/NEvgMfTxZDY7FskRHHVs/nW8LFQHUJ2mPw6arXK99yMdFQviPgRp?=
 =?us-ascii?q?OOv1BpTSj8Oq3Oyu5pHfeQpFiCa+bL9oMBm6sRjau9ULj4dlNqs/0AbCrGFSe+?=
 =?us-ascii?q?RRy2NoJFaTkAj568yt4pNt8Dletuw4+cJYXqr0Y6o3TbpDDDQ7KG81/9HktQPC?=
 =?us-ascii?q?TQSU+HQRVHgdnwdSDAjE6BH6WYrxsjf/u+Fg1iSWIdH6QLYpUjm58axlVAHnhz?=
 =?us-ascii?q?sGNz4h8WHYlMpwjL5AoBm8oxBz2pPYbJ2JOPZ7eK7WYNEUSndbXstJSiJPHI28?=
 =?us-ascii?q?YYsMAeQPM+lXoIvyp1oWrRa8BwehC/7jxzFUinDq0qM6yPouHBra3AEiBd0CrG?=
 =?us-ascii?q?jYodvrOKoUTOu7zLPIzTLGb/5O2zf96ZLHchYuofCMXLJxf9TeyU8yHA7CjFWQ?=
 =?us-ascii?q?qJbqPzWa1uUNsmia4fRvVeS0hm4ntgF+uDauydktioXTmo0VzVXE+Dx/zY0oK9?=
 =?us-ascii?q?O4T0t7bsSlEJtWryyaM4p2QsU/Q2BntiY6zaAGuZimcycQ1JQnxhnfa/qdf4iP?=
 =?us-ascii?q?+BLjW+CcKip7inJ9YL+zmhm//VS6xuHiS8W4zUxGojdEn9TIrHwBygLf5tCaRv?=
 =?us-ascii?q?dh5EutxDSC2gLJ5u1ZIE04j7fXJp8iz7IomZcesV7PEjH5lUjylqOaaFgo9vay?=
 =?us-ascii?q?5+npYrjroIKXOZVuhQHkKKsun9SyAeQmPQgKWGiW4eG826fi/U39WrlKivw2kq?=
 =?us-ascii?q?/EsJHVK8QbobO5AwlI3Yk59xa/DjCm0NICkXkANlJFdwqLj4nvO17QPPD1Feqz?=
 =?us-ascii?q?jluwnDtx2fzKI6DtDo/QInXClLrtZ6tx51BExAo2199f5pZUCr8bIPL0X0/8rM?=
 =?us-ascii?q?LYDh4jMwyo3uboEtF91oIfWG2VHq+ZMaTSsVmR6u00JOmMeYkVtyrjJPg+/PPu?=
 =?us-ascii?q?iX45mUQBfamyx5cXbHG4HvJ7I0SWeHbsjNABEXoMvgUjVuPqlFyCUTlVZ3qoWK?=
 =?us-ascii?q?I8/D47BJq8DYjfXoCtnKCB3CCjE51SZ2BGCU6DHW3ndoWZQPoMbCOSIsl8kj0L?=
 =?us-ascii?q?T7ShSokh1Q2wuw/+0bZoMu3U+ihL/a/lz8V/su3PiQkpp3szC8WGz3rLS2Byg3?=
 =?us-ascii?q?MGQCIw0KljoEt7jFCZ3u99iv1cENVVoPRRTgY9M4Wb0e1gF837XgPTd8uIT1D1?=
 =?us-ascii?q?f9O9HDtkS9swx8MJMVh8Acm/hxTCzSO2ArgT0qaGHYE56b7023nqO907z3fAyb?=
 =?us-ascii?q?lkgV47RMdGKWyhgOh47QeEHJPDkUiSi/O3c78B1jXG7maJwDmyuxQSdQ97UazI?=
 =?us-ascii?q?WTgkIAPqttXj5ULEBff6DK87GhFMxc6LNu1Bbdi/3ntcQ/K2OtXXZW2gmni9A1?=
 =?us-ascii?q?7cxLKFaoX7enoQ2g3dFUEbngFV9nGDY1ttThy9qn7TWWQ9XWnkZFnhpKwn8Cu2?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0ApAAA/XRNchwyp04xjHQEBBQEHBQGBU?=
 =?us-ascii?q?QgBCwGCaQOBJgqMC1+LMoINgSGWNIF2ERgLCYN6g0siNAkNAQMBAQEBAQECARM?=
 =?us-ascii?q?BAQEKCwkIKSMMgjYFAgMaAQaCWwEBAQECAQEBAQsZEwwKEAUHAggCAQMCAQECB?=
 =?us-ascii?q?gEBCgkPCRUBAwQIAwEFAwIBAgEVAS8GDQYCAQEBgxwBgXgIBQqnRSUOhUCEawW?=
 =?us-ascii?q?MPheBP0CBEYMSgx4BBIFDhXYCiR6GdTaPeQ9HCYMSg3uGLYQgGIFdiBASJocoj?=
 =?us-ascii?q?imEP4ZwgUaCDnBQgmyCJxeIXoVMMgEBMYEEEYl7AQMiMFiBHwEB?=
X-IPAS-Result: =?us-ascii?q?A0ApAAA/XRNchwyp04xjHQEBBQEHBQGBUQgBCwGCaQOBJgq?=
 =?us-ascii?q?MC1+LMoINgSGWNIF2ERgLCYN6g0siNAkNAQMBAQEBAQECARMBAQEKCwkIKSMMg?=
 =?us-ascii?q?jYFAgMaAQaCWwEBAQECAQEBAQsZEwwKEAUHAggCAQMCAQECBgEBCgkPCRUBAwQ?=
 =?us-ascii?q?IAwEFAwIBAgEVAS8GDQYCAQEBgxwBgXgIBQqnRSUOhUCEawWMPheBP0CBEYMSg?=
 =?us-ascii?q?x4BBIFDhXYCiR6GdTaPeQ9HCYMSg3uGLYQgGIFdiBASJocojimEP4ZwgUaCDnB?=
 =?us-ascii?q?QgmyCJxeIXoVMMgEBMYEEEYl7AQMiMFiBHwEB?=
X-IronPort-AV: E=Sophos;i="5.56,352,1539673200"; 
   d="scan'208";a="56876600"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from mail.linuxfoundation.org ([140.211.169.12])
  by mtab.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 13 Dec 2018 23:41:58 -0800
Received: from mail.linux-foundation.org (localhost [127.0.0.1])
	by mail.linuxfoundation.org (Postfix) with ESMTP id 71DB18CC;
	Fri, 14 Dec 2018 07:41:55 +0000 (UTC)
X-Original-To: virtualization@lists.linux-foundation.org
Delivered-To: virtualization@mail.linuxfoundation.org
Received: from smtp1.linuxfoundation.org (smtp1.linux-foundation.org
	[172.17.192.35])
	by mail.linuxfoundation.org (Postfix) with ESMTPS id 1EBB049D
	for <virtualization@lists.linux-foundation.org>;
	Fri, 14 Dec 2018 07:41:54 +0000 (UTC)
X-Greylist: domain auto-whitelisted by SQLgrey-1.7.6
Received: from huawei.com (szxga07-in.huawei.com [45.249.212.35])
	by smtp1.linuxfoundation.org (Postfix) with ESMTPS id C7CF72C4
	for <virtualization@lists.linux-foundation.org>;
	Fri, 14 Dec 2018 07:41:52 +0000 (UTC)
Received: from DGGEMS402-HUB.china.huawei.com (unknown [172.30.72.58])
	by Forcepoint Email with ESMTP id 4E1A6E048EB3B;
	Fri, 14 Dec 2018 15:41:49 +0800 (CST)
Received: from [127.0.0.1] (10.177.16.168) by DGGEMS402-HUB.china.huawei.com
	(10.3.19.202) with Microsoft SMTP Server id 14.3.408.0; Fri, 14 Dec 2018
	15:41:45 +0800
Subject: Re: [PATCH v2 2/5] VSOCK: support fill data to mergeable rx buffer in
	host
To: "Michael S. Tsirkin" <mst@redhat.com>
References: <5C10D4FB.6070009@huawei.com>
	<20181212103138-mutt-send-email-mst@kernel.org>
	<5C11CD14.3040809@huawei.com>
	<20181213094615-mutt-send-email-mst@kernel.org>
From: jiangyiwen <jiangyiwen@huawei.com>
Message-ID: <5C135EB8.1000208@huawei.com>
Date: Fri, 14 Dec 2018 15:41:44 +0800
User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:38.0) Gecko/20100101
	Thunderbird/38.5.1
MIME-Version: 1.0
In-Reply-To: <20181213094615-mutt-send-email-mst@kernel.org>
X-Originating-IP: [10.177.16.168]
X-CFilter-Loop: Reflected
X-Spam-Status: No, score=-2.6 required=5.0 tests=BAYES_00,RCVD_IN_DNSWL_LOW
	autolearn=ham version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	smtp1.linux-foundation.org
Cc: netdev@vger.kernel.org, kvm@vger.kernel.org,
	Stefan Hajnoczi <stefanha@redhat.com>,
	virtualization@lists.linux-foundation.org
X-BeenThere: virtualization@lists.linux-foundation.org
X-Mailman-Version: 2.1.12
Precedence: list
List-Id: Linux virtualization <virtualization.lists.linux-foundation.org>
List-Unsubscribe: <https://lists.linuxfoundation.org/mailman/options/virtualization>,
	<mailto:virtualization-request@lists.linux-foundation.org?subject=unsubscribe>
List-Archive: <http://lists.linuxfoundation.org/pipermail/virtualization/>
List-Post: <mailto:virtualization@lists.linux-foundation.org>
List-Help: <mailto:virtualization-request@lists.linux-foundation.org?subject=help>
List-Subscribe: <https://lists.linuxfoundation.org/mailman/listinfo/virtualization>,
	<mailto:virtualization-request@lists.linux-foundation.org?subject=subscribe>
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: 7bit
Sender: virtualization-bounces@lists.linux-foundation.org
Errors-To: virtualization-bounces@lists.linux-foundation.org

On 2018/12/13 22:48, Michael S. Tsirkin wrote:
> On Thu, Dec 13, 2018 at 11:08:04AM +0800, jiangyiwen wrote:
>> On 2018/12/12 23:37, Michael S. Tsirkin wrote:
>>> On Wed, Dec 12, 2018 at 05:29:31PM +0800, jiangyiwen wrote:
>>>> When vhost support VIRTIO_VSOCK_F_MRG_RXBUF feature,
>>>> it will merge big packet into rx vq.
>>>>
>>>> Signed-off-by: Yiwen Jiang <jiangyiwen@huawei.com>
>>>
>>> I feel this approach jumps into making interface changes for
>>> optimizations too quickly. For example, what prevents us
>>> from taking a big buffer, prepending each chunk
>>> with the header and writing it out without
>>> host/guest interface changes?
>>>
>>> This should allow optimizations such as vhost_add_used_n
>>> batching.
>>>
>>> I realize a header in each packet does have a cost,
>>> but it also has advantages such as improved robustness,
>>> I'd like to see more of an apples to apples comparison
>>> of the performance gain from skipping them.
>>>
>>>
>>
>> Hi Michael,
>>
>> I don't fully understand what you mean, do you want to
>> see a performance comparison that before performance and
>> only use batching?
>>
>> In my opinion, guest don't fill big buffer in rx vq because
>> the balance performance and guest memory pressure, add
>> mergeable feature can improve big packets performance,
>> as for small packets, I try to find out the reason, may be
>> the fluctuation of test results, or in mergeable mode, when
>> Host send a 4k packet to Guest, we should call vhost_get_vq_desc()
>> twice in host(hdr + 4k data), and in guest we also should call
>> virtqueue_get_buf() twice.
>>
>> Thanks,
>> Yiwen.
> 
> What I mean is that at least some of the gain here is because
> larger skbs are passed up the stack.
> 

Yes, the main gain is from larger skbs.

> You do not need larger packets to build larger skbs though.
> Just check that the addresses match and you can combine
> multiple fragments in a single skb.
> 
> 

I understand what you mean, if use batching send that the
performance also can be improved, I can test the real
performance result only use batching.

Thanks,
Yiwen.

> 
>>>> ---
>>>>  drivers/vhost/vsock.c             | 111 ++++++++++++++++++++++++++++++--------
>>>>  include/linux/virtio_vsock.h      |   1 +
>>>>  include/uapi/linux/virtio_vsock.h |   5 ++
>>>>  3 files changed, 94 insertions(+), 23 deletions(-)
>>>>
>>>> diff --git a/drivers/vhost/vsock.c b/drivers/vhost/vsock.c
>>>> index 34bc3ab..dc52b0f 100644
>>>> --- a/drivers/vhost/vsock.c
>>>> +++ b/drivers/vhost/vsock.c
>>>> @@ -22,7 +22,8 @@
>>>>  #define VHOST_VSOCK_DEFAULT_HOST_CID	2
>>>>
>>>>  enum {
>>>> -	VHOST_VSOCK_FEATURES = VHOST_FEATURES,
>>>> +	VHOST_VSOCK_FEATURES = VHOST_FEATURES |
>>>> +			(1ULL << VIRTIO_VSOCK_F_MRG_RXBUF),
>>>>  };
>>>>
>>>>  /* Used to track all the vhost_vsock instances on the system. */
>>>> @@ -80,6 +81,69 @@ static struct vhost_vsock *vhost_vsock_get(u32 guest_cid)
>>>>  	return vsock;
>>>>  }
>>>>
>>>> +/* This segment of codes are copied from drivers/vhost/net.c */
>>>> +static int get_rx_bufs(struct vhost_virtqueue *vq,
>>>> +		struct vring_used_elem *heads, int datalen,
>>>> +		unsigned *iovcount, unsigned int quota)
>>>> +{
>>>> +	unsigned int out, in;
>>>> +	int seg = 0;
>>>> +	int headcount = 0;
>>>> +	unsigned d;
>>>> +	int ret;
>>>> +	/*
>>>> +	 * len is always initialized before use since we are always called with
>>>> +	 * datalen > 0.
>>>> +	 */
>>>> +	u32 uninitialized_var(len);
>>>> +
>>>> +	while (datalen > 0 && headcount < quota) {
>>>> +		if (unlikely(seg >= UIO_MAXIOV)) {
>>>> +			ret = -ENOBUFS;
>>>> +			goto err;
>>>> +		}
>>>> +
>>>> +		ret = vhost_get_vq_desc(vq, vq->iov + seg,
>>>> +				ARRAY_SIZE(vq->iov) - seg, &out,
>>>> +				&in, NULL, NULL);
>>>> +		if (unlikely(ret < 0))
>>>> +			goto err;
>>>> +
>>>> +		d = ret;
>>>> +		if (d == vq->num) {
>>>> +			ret = 0;
>>>> +			goto err;
>>>> +		}
>>>> +
>>>> +		if (unlikely(out || in <= 0)) {
>>>> +			vq_err(vq, "unexpected descriptor format for RX: "
>>>> +					"out %d, in %d\n", out, in);
>>>> +			ret = -EINVAL;
>>>> +			goto err;
>>>> +		}
>>>> +
>>>> +		heads[headcount].id = cpu_to_vhost32(vq, d);
>>>> +		len = iov_length(vq->iov + seg, in);
>>>> +		heads[headcount].len = cpu_to_vhost32(vq, len);
>>>> +		datalen -= len;
>>>> +		++headcount;
>>>> +		seg += in;
>>>> +	}
>>>> +
>>>> +	heads[headcount - 1].len = cpu_to_vhost32(vq, len + datalen);
>>>> +	*iovcount = seg;
>>>> +
>>>> +	/* Detect overrun */
>>>> +	if (unlikely(datalen > 0)) {
>>>> +		ret = UIO_MAXIOV + 1;
>>>> +		goto err;
>>>> +	}
>>>> +	return headcount;
>>>> +err:
>>>> +	vhost_discard_vq_desc(vq, headcount);
>>>> +	return ret;
>>>> +}
>>>> +
>>>>  static void
>>>>  vhost_transport_do_send_pkt(struct vhost_vsock *vsock,
>>>>  			    struct vhost_virtqueue *vq)
>>>> @@ -87,22 +151,34 @@ static struct vhost_vsock *vhost_vsock_get(u32 guest_cid)
>>>>  	struct vhost_virtqueue *tx_vq = &vsock->vqs[VSOCK_VQ_TX];
>>>>  	bool added = false;
>>>>  	bool restart_tx = false;
>>>> +	int mergeable;
>>>> +	size_t vsock_hlen;
>>>>
>>>>  	mutex_lock(&vq->mutex);
>>>>
>>>>  	if (!vq->private_data)
>>>>  		goto out;
>>>>
>>>> +	mergeable = vhost_has_feature(vq, VIRTIO_VSOCK_F_MRG_RXBUF);
>>>> +	/*
>>>> +	 * Guest fill page for rx vq in mergeable case, so it will not
>>>> +	 * allocate pkt structure, we should reserve size of pkt in advance.
>>>> +	 */
>>>> +	if (likely(mergeable))
>>>> +		vsock_hlen = sizeof(struct virtio_vsock_pkt);
>>>> +	else
>>>> +		vsock_hlen = sizeof(struct virtio_vsock_hdr);
>>>> +
>>>>  	/* Avoid further vmexits, we're already processing the virtqueue */
>>>>  	vhost_disable_notify(&vsock->dev, vq);
>>>>
>>>>  	for (;;) {
>>>>  		struct virtio_vsock_pkt *pkt;
>>>>  		struct iov_iter iov_iter;
>>>> -		unsigned out, in;
>>>> +		unsigned out = 0, in = 0;
>>>>  		size_t nbytes;
>>>>  		size_t len;
>>>> -		int head;
>>>> +		s16 headcount;
>>>>
>>>>  		spin_lock_bh(&vsock->send_pkt_list_lock);
>>>>  		if (list_empty(&vsock->send_pkt_list)) {
>>>> @@ -116,16 +192,9 @@ static struct vhost_vsock *vhost_vsock_get(u32 guest_cid)
>>>>  		list_del_init(&pkt->list);
>>>>  		spin_unlock_bh(&vsock->send_pkt_list_lock);
>>>>
>>>> -		head = vhost_get_vq_desc(vq, vq->iov, ARRAY_SIZE(vq->iov),
>>>> -					 &out, &in, NULL, NULL);
>>>> -		if (head < 0) {
>>>> -			spin_lock_bh(&vsock->send_pkt_list_lock);
>>>> -			list_add(&pkt->list, &vsock->send_pkt_list);
>>>> -			spin_unlock_bh(&vsock->send_pkt_list_lock);
>>>> -			break;
>>>> -		}
>>>> -
>>>> -		if (head == vq->num) {
>>>> +		headcount = get_rx_bufs(vq, vq->heads, vsock_hlen + pkt->len,
>>>> +				&in, likely(mergeable) ? UIO_MAXIOV : 1);
>>>> +		if (headcount <= 0) {
>>>>  			spin_lock_bh(&vsock->send_pkt_list_lock);
>>>>  			list_add(&pkt->list, &vsock->send_pkt_list);
>>>>  			spin_unlock_bh(&vsock->send_pkt_list_lock);
>>>> @@ -133,24 +202,20 @@ static struct vhost_vsock *vhost_vsock_get(u32 guest_cid)
>>>>  			/* We cannot finish yet if more buffers snuck in while
>>>>  			 * re-enabling notify.
>>>>  			 */
>>>> -			if (unlikely(vhost_enable_notify(&vsock->dev, vq))) {
>>>> +			if (!headcount && unlikely(vhost_enable_notify(&vsock->dev, vq))) {
>>>>  				vhost_disable_notify(&vsock->dev, vq);
>>>>  				continue;
>>>>  			}
>>>>  			break;
>>>>  		}
>>>>
>>>> -		if (out) {
>>>> -			virtio_transport_free_pkt(pkt);
>>>> -			vq_err(vq, "Expected 0 output buffers, got %u\n", out);
>>>> -			break;
>>>> -		}
>>>> -
>>>>  		len = iov_length(&vq->iov[out], in);
>>>>  		iov_iter_init(&iov_iter, READ, &vq->iov[out], in, len);
>>>>
>>>> -		nbytes = copy_to_iter(&pkt->hdr, sizeof(pkt->hdr), &iov_iter);
>>>> -		if (nbytes != sizeof(pkt->hdr)) {
>>>> +		if (likely(mergeable))
>>>> +			pkt->mrg_rxbuf_hdr.num_buffers = cpu_to_le16(headcount);
>>>> +		nbytes = copy_to_iter(&pkt->hdr, vsock_hlen, &iov_iter);
>>>> +		if (nbytes != vsock_hlen) {
>>>>  			virtio_transport_free_pkt(pkt);
>>>>  			vq_err(vq, "Faulted on copying pkt hdr\n");
>>>>  			break;
>>>> @@ -163,7 +228,7 @@ static struct vhost_vsock *vhost_vsock_get(u32 guest_cid)
>>>>  			break;
>>>>  		}
>>>>
>>>> -		vhost_add_used(vq, head, sizeof(pkt->hdr) + pkt->len);
>>>> +		vhost_add_used_n(vq, vq->heads, headcount);
>>>>  		added = true;
>>>>
>>>>  		if (pkt->reply) {
>>>> diff --git a/include/linux/virtio_vsock.h b/include/linux/virtio_vsock.h
>>>> index bf84418..da9e1fe 100644
>>>> --- a/include/linux/virtio_vsock.h
>>>> +++ b/include/linux/virtio_vsock.h
>>>> @@ -50,6 +50,7 @@ struct virtio_vsock_sock {
>>>>
>>>>  struct virtio_vsock_pkt {
>>>>  	struct virtio_vsock_hdr	hdr;
>>>> +	struct virtio_vsock_mrg_rxbuf_hdr mrg_rxbuf_hdr;
>>>>  	struct work_struct work;
>>>>  	struct list_head list;
>>>>  	/* socket refcnt not held, only use for cancellation */
>>>> diff --git a/include/uapi/linux/virtio_vsock.h b/include/uapi/linux/virtio_vsock.h
>>>> index 1d57ed3..2292f30 100644
>>>> --- a/include/uapi/linux/virtio_vsock.h
>>>> +++ b/include/uapi/linux/virtio_vsock.h
>>>> @@ -63,6 +63,11 @@ struct virtio_vsock_hdr {
>>>>  	__le32	fwd_cnt;
>>>>  } __attribute__((packed));
>>>>
>>>> +/* It add mergeable rx buffers feature */
>>>> +struct virtio_vsock_mrg_rxbuf_hdr {
>>>> +	__le16  num_buffers;    /* number of mergeable rx buffers */
>>>> +} __attribute__((packed));
>>>> +
>>>>  enum virtio_vsock_type {
>>>>  	VIRTIO_VSOCK_TYPE_STREAM = 1,
>>>>  };
>>>> -- 
>>>> 1.8.3.1
>>>>
>>>
>>> .
>>>
>>
> 
> .
> 


_______________________________________________
Virtualization mailing list
Virtualization@lists.linux-foundation.org
https://lists.linuxfoundation.org/mailman/listinfo/virtualization
