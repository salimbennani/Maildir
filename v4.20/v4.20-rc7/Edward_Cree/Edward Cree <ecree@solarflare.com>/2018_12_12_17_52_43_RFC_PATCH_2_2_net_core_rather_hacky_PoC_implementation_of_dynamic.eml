Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by i7-8700 with POP3-SSL;
  13 Dec 2018 08:53:53 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga002.jf.intel.com (orsmga002.jf.intel.com [10.7.209.21])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id BC03758079D;
	Wed, 12 Dec 2018 09:52:56 -0800 (PST)
Received: from fmsmga101.fm.intel.com ([10.1.193.65])
  by orsmga002-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 12 Dec 2018 09:52:56 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3APVs+kh/xiXmRJP9uRHKM819IXTAuvvDOBiVQ1KB9?=
 =?us-ascii?q?1usVIJqq85mqBkHD//Il1AaPAd2Lraocw8Pt8InYEVQa5piAtH1QOLdtbDQizf?=
 =?us-ascii?q?ssogo7HcSeAlf6JvO5JwYzHcBFSUM3tyrjaRsdF8nxfUDdrWOv5jAOBBr/KRB1?=
 =?us-ascii?q?JuPoEYLOksi7ze+/94HQbglSmDaxfa55IQmrownWqsQYm5ZpJLwryhvOrHtIeu?=
 =?us-ascii?q?BWyn1tKFmOgRvy5dq+8YB6/ShItP0v68BPUaPhf6QlVrNYFygpM3o05MLwqxbO?=
 =?us-ascii?q?SxaE62YGXWUXlhpIBBXF7A3/U5zsvCb2qvZx1S+HNsDtU7s6RSqt4LtqSB/wiS?=
 =?us-ascii?q?cIKTg58H3MisdtiK5XuQ+tqwBjz4LRZoyeKfhwcb7Hfd4CRWRPQ9hfVyJCDI2y?=
 =?us-ascii?q?YYQAAOgOMvpXoYn8vFsOtRmzCBKwBO7t0DJEmmP60KM43uknDArI3BYgH9ULsH?=
 =?us-ascii?q?nMo9v6KakTXvqvzKbV0D7OcelW1inj54PVdR0uv+2DXahufsXP0kQvDATFjlGK?=
 =?us-ascii?q?poz/MTOV0v4Cs3KF4OZ6Se2vjGsnphh3rzOyxckskpHEipwJxl3A7yl13Yg4Kc?=
 =?us-ascii?q?OiREJmYtOoDIFcuiCYOoduX88uX3tktDs4x7Ecp5K3YDIGxZsnyhLHdvCLbZKE?=
 =?us-ascii?q?7g/gWeuTOzt0mW5pdb2lixqs8EWtzPD3WNOu31ZQtCVFl8HBtnAT2BzX7ciKUu?=
 =?us-ascii?q?V9/ki/1jaVzQzT6f9LIVoylaXFL54t2LkwloAcsUjbHy/2nlv5jLOOe0k65uSl?=
 =?us-ascii?q?7/7rbqjoq5OCLYN4lwLzPrg0lsG+A+k0Kg0OUHKa+eS42r3j50r5QLBSg/0yk6?=
 =?us-ascii?q?nZto3aJMsCqq66HQBVyIAj5Ai7Dzu/19QZk38HI0xfeB+ckYjpNE/BIOriAfe8?=
 =?us-ascii?q?nVusijFryO7CPrH7BZXNNHfDnK/7fblh805c1BYzzddH6pJQC7EBI+z8VlX+td?=
 =?us-ascii?q?zFFRI5Nw20w+D6CNRyzI8eWGSPArOHP6PWq1OH+uUvI+yUbo8PpDn9M+Ql5+Lp?=
 =?us-ascii?q?jXIhmV8SZ6ip3YcNZ3C/BPhmI1iZbmDqgtcOCmoKugs+TOr3iFyNSzJTZnCyX7?=
 =?us-ascii?q?4i6TE/Eo6pEYDDRoW1irybwCi7BoFWZnxBCl2UE3focJuLV+0PaCKVJM9hlDsE?=
 =?us-ascii?q?WKOlS48g0xGuqQD7x6BmLurS5i0Xq5bj2MJp6O3UkBE47SZ0ANiF02GRU2F0mX?=
 =?us-ascii?q?sFSCUt3KB/pkx9yU2P0bJijPxaDtFT4/JJUgEnNZ/T1eB6CtbyWh7fcdeNUlqp?=
 =?us-ascii?q?XtKmATQpRNIr39AOe1p9G8mljh3b3CqlGbkVm6aPBJw16K3c2XfxKt15y3bH0q?=
 =?us-ascii?q?khklYnTtFONW2gmq5w6QzTC5TVnEWekqagbb4c0zLV9Gef0WqOu1lVXxNqXqXb?=
 =?us-ascii?q?Q38TfEvWos7/5kPZUbCuD7MrMg9Cyc6HLqtHcdnpjVRARPf+N9XSeWOxm2GsBR?=
 =?us-ascii?q?mWwrOAdpble2IY3C/FEkgLjxgT/WqaNQg5HiquvnjRDCJwGl71Y0Pj6+9+qGil?=
 =?us-ascii?q?QU8y1AyKa0xh17yo+h8an/CcSvUT3q4atyclsTl7AFG939fOAdqauwVhZLlcYc?=
 =?us-ascii?q?864FpfyWLZtgl9Ppu8L6Bihl8SaRh3s1np1xVtDoVAkM4qrHwxwQp2KKKY1k5B?=
 =?us-ascii?q?djyC0ZDxPL3XNnf9/BS1Z6HK3VHe1c6c+r0T5/Qgt1XjoAapG1I4/HVjzdZU3G?=
 =?us-ascii?q?WT55XQAAUJTJL+T1w49x55p7HdfCkw/IfU1XxqMampvT7OwdMpBO05yhm+e9dT?=
 =?us-ascii?q?Kr+LFAj3E8cCHcihNPQqm0S1bhIDJO1T9LM0M9m6ePec2a+rPPxvnDSpjWlc5I?=
 =?us-ascii?q?B900SM9zdzS+LS3pYFxe2Y0RWDVzvmkFihtcX3k5heZT4OBmq/1TTkBIlJa61o?=
 =?us-ascii?q?fIYEFX2hI9eqydV5nZLtXWBX9ESiB18fxMCmYx6SYEHj0g1K0kQXp2eqmS+5zz?=
 =?us-ascii?q?xyjjEoobCT3C3Iw+T+ahUHPnRHS3VljVfpOYK0lcwVXFC0bwg1kxuo/Ub7x6lB?=
 =?us-ascii?q?qKV/NWXTWlpIfy7tImFmU6uwsKeCYsFV5JMptyVXTPqzYVSARrHhpBsa1jvpH3?=
 =?us-ascii?q?FCyzAjazGqppL5kgR4iG2HNnZzr3nZecZqyRfE/tPcRv1R3jsARCZmjznaHVy8?=
 =?us-ascii?q?P9iv/dWJmJbPqOG+V2S9VpJNdSnn15+PtCy+5WdyGx2wg+izmsH7EQg9ySL618?=
 =?us-ascii?q?NlVSLSoBb+Y4nr0b+3MeZmfkluGV/95NB2GoB4kossmp4Q3WIWiYmS/XoCiW3z?=
 =?us-ascii?q?K8lU2bribHoRQj4G293V7xLk2EF5LnKJ2pj2VnOSwsZ6Y9m6Y2UW2j8y7sxQCa?=
 =?us-ascii?q?eU6qBEkjVxolaisQ3RZv19lC8HyfQy8H4an/0JuA01wyWYA7ASHlNXMTbilhuW?=
 =?us-ascii?q?9NC+sLtYZHy0freuzkp+ksusDLWDogFaRXb4dY0uHS527sVjLl3M1Gf/5Z3jeN?=
 =?us-ascii?q?nVddgTrAGbkw/cj+hJL5I8juYKhS1iOW7nvHwq0eg7jQF13ZGhvYiKMGFt/KO/?=
 =?us-ascii?q?Ah5FOTz5fcIT+jfxjalAmsaaxZygHpJkGj8TRpvnUeqoEC4OtfTgLwuBCyczqn?=
 =?us-ascii?q?CfGbrCBwOf7FpmomnLE5CqMXGXOXYYwc9jRBmbOExQngQUUC8mkZ4+EwCg3Nbh?=
 =?us-ascii?q?f1th5jAN+l74rQNByuByOBn6TGjfvx2kaisuRJicMRpW7RxC50HPPsyF9e9zGy?=
 =?us-ascii?q?BY/pu8rA2CMGCbZgJIDX0XVUyAHVzsIr6u5dzY+eiCGuW+N+fOYamJqeFGV/aI?=
 =?us-ascii?q?wo+v0pJ7/zmWMMWDJGJiD/o92kpMRn15H8XZmzMSSy0YjS7NbsibpAui9S1ztM?=
 =?us-ascii?q?yw7PPrWAf374uVF7RSKclv+wyxgaqbN+6fmid5KTVb1pMN33PIy6If3Fkdiy5w?=
 =?us-ascii?q?bTmtDK8AuDXJTKLRnK9XEhEaZzlyNMtO86IzwA1NNdTHhdPy075yluQ1BEtdVV?=
 =?us-ascii?q?z9hsGpYtQHLHugNFzcHkaEKrSHKSfPw8H2eq68TbxQjONJtxy/ozqbEknjPiid?=
 =?us-ascii?q?mDntTRygLeZMjCSDNhxEpI69agptCXTkTN/+dh27Mdp3gScqzrEumnzKNXATMT?=
 =?us-ascii?q?5nfkNJr72Q6z5Yg/plF2xA6HplMfeLmyKD4+bELZYWtONhAj5omOJC/HQ617xV?=
 =?us-ascii?q?4TlERfNvnivSq99uo1e+neiOyjpoShxOqjlQiYKPvEViP7jZ95ZaVXbF+hIN8X?=
 =?us-ascii?q?ufCxAQq9R5Dd3vvvMY9t+asqv/ISkK2tfV4YNIAcHRKd6vLn8+OADkXjXTCV1B?=
 =?us-ascii?q?BRysLmGXv0tclfuV+2Cb5sw4sp/tnZAJR5dBWVA1H+9cAUNgSo8sOpByCx4ojr?=
 =?us-ascii?q?eBgIYj/3ez5E3cT99Xs7jcUfabG/vrISyUy7JDYk1bkvvDMY0PO9ijiARZYV5g?=
 =?us-ascii?q?kdGPQhKIUA=3D=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0B7AAC/ShFch0O0hNFkHQEBBQEHBQGBU?=
 =?us-ascii?q?wYBCwGDaycKg3KUKoFgCDmDMZQOgXYRGBMBgUuFcyI2Bw0BAwEBAQEBAQIBEwE?=
 =?us-ascii?q?BAQgNCQgpL4I2JAGCYgMDAQIgBFIGCQEBGAwCJgICA0QQBgEMAQUCAQEBgxyCA?=
 =?us-ascii?q?gMCpkJ8M4VAhGiBC4sxF4FAP4ERJwyKZIJXApAMkQAJkVEekUaJKZAWgU0MgXs?=
 =?us-ascii?q?zGggbFYMngicXjh1AATGBBQEBi2GBHwEB?=
X-IPAS-Result: =?us-ascii?q?A0B7AAC/ShFch0O0hNFkHQEBBQEHBQGBUwYBCwGDaycKg3K?=
 =?us-ascii?q?UKoFgCDmDMZQOgXYRGBMBgUuFcyI2Bw0BAwEBAQEBAQIBEwEBAQgNCQgpL4I2J?=
 =?us-ascii?q?AGCYgMDAQIgBFIGCQEBGAwCJgICA0QQBgEMAQUCAQEBgxyCAgMCpkJ8M4VAhGi?=
 =?us-ascii?q?BC4sxF4FAP4ERJwyKZIJXApAMkQAJkVEekUaJKZAWgU0MgXszGggbFYMngicXj?=
 =?us-ascii?q?h1AATGBBQEBi2GBHwEB?=
X-IronPort-AV: E=Sophos;i="5.56,345,1539673200"; 
   d="scan'208";a="66519582"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mga01b.intel.com with ESMTP; 12 Dec 2018 09:52:54 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1727983AbeLLRwv (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Wed, 12 Dec 2018 12:52:51 -0500
Received: from dispatch1-us1.ppe-hosted.com ([67.231.154.164]:47686 "EHLO
        dispatch1-us1.ppe-hosted.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S1726922AbeLLRwu (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Wed, 12 Dec 2018 12:52:50 -0500
X-Virus-Scanned: Proofpoint Essentials engine
Received: from webmail.solarflare.com (webmail.solarflare.com [12.187.104.26])
        (using TLSv1.2 with cipher ECDHE-RSA-AES256-SHA384 (256/256 bits))
        (No client certificate requested)
        by mx1-us1.ppe-hosted.com (Proofpoint Essentials ESMTP Server) with ESMTPS id 4CF62400066;
        Wed, 12 Dec 2018 17:52:49 +0000 (UTC)
Received: from ec-desktop.uk.solarflarecom.com (10.17.20.45) by
 ocex03.SolarFlarecom.com (10.20.40.36) with Microsoft SMTP Server (TLS) id
 15.0.1395.4; Wed, 12 Dec 2018 09:52:45 -0800
From: Edward Cree <ecree@solarflare.com>
Subject: [RFC PATCH 2/2] net: core: rather hacky PoC implementation of dynamic
 calls
To: Nadav Amit <namit@vmware.com>, Josh Poimboeuf <jpoimboe@redhat.com>
CC: <linux-kernel@vger.kernel.org>, <x86@kernel.org>,
        Paolo Abeni <pabeni@redhat.com>
References: <cf6e9449-f3f6-e0fc-8096-eaba6b5a3b97@solarflare.com>
Message-ID: <7ab6063d-92ac-4708-d820-0cf175cf0f92@solarflare.com>
Date: Wed, 12 Dec 2018 17:52:43 +0000
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko/20100101
 Thunderbird/52.5.2
MIME-Version: 1.0
In-Reply-To: <cf6e9449-f3f6-e0fc-8096-eaba6b5a3b97@solarflare.com>
Content-Type: text/plain; charset="utf-8"
Content-Language: en-GB
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.17.20.45]
X-TM-AS-Product-Ver: SMEX-12.5.0.1300-8.5.1010-24280.005
X-TM-AS-Result: No-9.604200-4.000000-10
X-TMASE-MatchedRID: EYA0sup6gNhl+hxxqfcpPsbr3d9/yP1kpPZLm8rbDqJjyv+d0Z0OxRWV
        SFc2ItaOvUbvPAStF2iUTQ2JYQzY21SU1d+VJ4IjnJ5tL+LbGOM4gG1vqBBN+AqiCYa6w8tvoja
        BwpG8gTPfqxfcyiWiyr4tP830vR4Aj1OThq6Zp6BbuDP8ZuCmXkrh/hn4JkBngrAXgr/AjP3NCc
        A6Zdzj71xmTQS70AzY8EVWJSRef2vz02Qfl8DhQMOvQCMFyZ9GyeUl7aCTy8hcKZwALwMGs2pHK
        tkQBynK2toxWPLv+HqE4pusuM4o3hv9h+4KF57mHPYwOJi6PLkhHWssEmb8zhxUkJPe1WBq92tT
        P3Yq8VAHL782mJ86aHLHqPoioImUm0Eb6QA1oW/Y8CtSlWSg/ClayzmQ9QV0gGbipuPzIdHloTS
        mLjO3jRwJa9cCgcwvQlNPb0UTAzIvRbVu13x7nuIfK/Jd5eHmZAQzy8TR+QbqLnOUXH9QdN+HYj
        GHlFYn9l4B8vq7n5v5F1yiVNAj2wjVnlevJ/ToR/j040fRFpKt4laWdJbsDMKQkd8YrGEviMF2e
        oWMKPXi8zVgXoAltkWL4rBlm20v8gATfdkjbTytIWznhjjBtfoLR4+zsDTtqFhHt/jREalnN9Ah
        bYYLy2RWjU8cQ3jQ0ZjgsXBtW+diuoXhHTpajlZca9RSYo/b
X-TM-AS-User-Approved-Sender: No
X-TM-AS-User-Blocked-Sender: No
X-TMASE-Result: 10--9.604200-4.000000
X-TMASE-Version: SMEX-12.5.0.1300-8.5.1010-24280.005
X-MDID: 1544637170-g-HheZNtHp4z
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Uses runtime instrumentation of callees from an indirect call site
 (deliver_skb, and also __netif_receive_skb_one_core()) to populate an
 indirect-call-wrapper branch tree.  Essentially we're doing indirect
 branch prediction in software because the hardware can't be trusted to
 get it right; this is sad.

It's also full of printk()s right now to display what it's doing for
 debugging purposes; obviously those wouldn't be quite the same in a
 finished version.

Signed-off-by: Edward Cree <ecree@solarflare.com>
---
 net/core/dev.c | 222 +++++++++++++++++++++++++++++++++++++++++++++++++++++++--
 1 file changed, 217 insertions(+), 5 deletions(-)

diff --git a/net/core/dev.c b/net/core/dev.c
index 04a6b7100aac..f69c110c34e3 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -145,6 +145,7 @@
 #include <linux/sctp.h>
 #include <net/udp_tunnel.h>
 #include <linux/net_namespace.h>
+#include <linux/static_call.h>
 
 #include "net-sysfs.h"
 
@@ -1935,14 +1936,223 @@ int dev_forward_skb(struct net_device *dev, struct sk_buff *skb)
 }
 EXPORT_SYMBOL_GPL(dev_forward_skb);
 
-static inline int deliver_skb(struct sk_buff *skb,
-			      struct packet_type *pt_prev,
-			      struct net_device *orig_dev)
+static void deliver_skb_update(struct work_struct *unused);
+
+static DECLARE_WORK(deliver_skb_update_work, deliver_skb_update);
+
+typedef int (*deliver_skb_func)(struct sk_buff *, struct net_device *, struct packet_type *, struct net_device *);
+
+struct deliver_skb_candidate {
+	deliver_skb_func func;
+	unsigned long hit_count;
+};
+
+static DEFINE_PER_CPU(struct deliver_skb_candidate[4], deliver_skb_candidates);
+
+static DEFINE_PER_CPU(unsigned long, deliver_skb_miss_count);
+
+/* Used to route around the dynamic version when we're changing it, as well as
+ * as a fallback if none of our static calls match.
+ */
+static int do_deliver_skb(struct sk_buff *skb,
+			  struct packet_type *pt_prev,
+			  struct net_device *orig_dev)
+{
+	struct deliver_skb_candidate *cands = *this_cpu_ptr(&deliver_skb_candidates);
+	deliver_skb_func func = pt_prev->func;
+	unsigned long total_count;
+	int i;
+
+	for (i = 0; i < 4; i++)
+		if (func == cands[i].func) {
+			cands[i].hit_count++;
+			break;
+		}
+	if (i == 4) /* no match */
+		for (i = 0; i < 4; i++)
+			if (!cands[i].func) {
+				cands[i].func = func;
+				cands[i].hit_count = 1;
+				break;
+			}
+	if (i == 4) /* no space */
+		(*this_cpu_ptr(&deliver_skb_miss_count))++;
+
+	total_count = *this_cpu_ptr(&deliver_skb_miss_count);
+	for (i = 0; i < 4; i++)
+		total_count += cands[i].hit_count;
+	if (total_count > 1000) /* Arbitrary threshold */
+		schedule_work(&deliver_skb_update_work);
+	return pt_prev->func(skb, skb->dev, pt_prev, orig_dev);
+}
+
+DEFINE_STATIC_CALL(dispatch_deliver_skb, do_deliver_skb);
+
+static int dummy_deliver_skb(struct sk_buff *skb, struct net_device *dev,
+			     struct packet_type *pt_prev,
+			     struct net_device *orig_dev)
+{
+	WARN_ON_ONCE(1); /* shouldn't ever actually get here */
+	return do_deliver_skb(skb, pt_prev, orig_dev);
+}
+
+DEFINE_STATIC_CALL(dynamic_deliver_skb_1, dummy_deliver_skb);
+DEFINE_STATIC_CALL(dynamic_deliver_skb_2, dummy_deliver_skb);
+
+static DEFINE_PER_CPU(unsigned long, dds1_hit_count);
+static DEFINE_PER_CPU(unsigned long, dds2_hit_count);
+
+static int dynamic_deliver_skb(struct sk_buff *skb,
+			       struct packet_type *pt_prev,
+			       struct net_device *orig_dev)
+{
+	deliver_skb_func func = pt_prev->func;
+
+	if (func == dynamic_deliver_skb_1.func) {
+		(*this_cpu_ptr(&dds1_hit_count))++;
+		return static_call(dynamic_deliver_skb_1, skb, skb->dev,
+				   pt_prev, orig_dev);
+	}
+	if (func == dynamic_deliver_skb_2.func) {
+		(*this_cpu_ptr(&dds2_hit_count))++;
+		return static_call(dynamic_deliver_skb_2, skb, skb->dev,
+				   pt_prev, orig_dev);
+	}
+	return do_deliver_skb(skb, pt_prev, orig_dev);
+}
+
+DEFINE_MUTEX(deliver_skb_update_lock);
+
+static void deliver_skb_add_cand(struct deliver_skb_candidate *top,
+				 size_t ncands,
+				 struct deliver_skb_candidate next)
+{
+	struct deliver_skb_candidate old;
+	int i;
+
+	for (i = 0; i < ncands; i++) {
+		if (next.hit_count > top[i].hit_count) {
+			/* Swap next with top[i], so that the old top[i] can
+			 * shunt along all lower scores
+			 */
+			old = top[i];
+			top[i] = next;
+			next = old;
+		}
+	}
+}
+
+static void deliver_skb_count_hits(struct deliver_skb_candidate *top,
+				   size_t ncands, struct static_call_key *key,
+				   unsigned long __percpu *hit_count)
+{
+	struct deliver_skb_candidate next;
+	int cpu;
+
+	next.func = key->func;
+	next.hit_count = 0;
+	for_each_online_cpu(cpu) {
+		next.hit_count += *per_cpu_ptr(hit_count, cpu);
+		*per_cpu_ptr(hit_count, cpu) = 0;
+	}
+
+	printk(KERN_ERR "hit_count for old %pf: %lu\n", next.func,
+	       next.hit_count);
+
+	deliver_skb_add_cand(top, ncands, next);
+}
+
+static void deliver_skb_update(struct work_struct *unused)
+{
+	struct deliver_skb_candidate top[4], next, *cands, *cands2;
+	int cpu, i, cpu2, j;
+
+	memset(top, 0, sizeof(top));
+
+	printk(KERN_ERR "deliver_skb_update called\n");
+	mutex_lock(&deliver_skb_update_lock);
+	printk(KERN_ERR "deliver_skb_update_lock acquired\n");
+	/* We don't stop the other CPUs adding to their counts while this is
+	 * going on; but it doesn't really matter because this is a heuristic
+	 * anyway so we don't care about perfect accuracy.
+	 */
+	/* First count up the hits on the existing static branches */
+	deliver_skb_count_hits(top, ARRAY_SIZE(top), &dynamic_deliver_skb_1,
+			       &dds1_hit_count);
+	deliver_skb_count_hits(top, ARRAY_SIZE(top), &dynamic_deliver_skb_2,
+			       &dds2_hit_count);
+	/* Next count up the callees seen in the fallback path */
+	for_each_online_cpu(cpu) {
+		cands = *per_cpu_ptr(&deliver_skb_candidates, cpu);
+		printk(KERN_ERR "miss_count for %d: %lu\n", cpu,
+		       *per_cpu_ptr(&deliver_skb_miss_count, cpu));
+		for (i = 0; i < 4; i++) {
+			next = cands[i];
+			if (next.func == NULL)
+				continue;
+			next.hit_count = 0;
+			for_each_online_cpu(cpu2) {
+				cands2 = *per_cpu_ptr(&deliver_skb_candidates,
+						      cpu2);
+				for (j = 0; j < 4; j++) {
+					if (cands2[j].func == next.func) {
+						next.hit_count += cands2[j].hit_count;
+						cands2[j].hit_count = 0;
+						cands2[j].func = NULL;
+						break;
+					}
+				}
+			}
+			printk(KERN_ERR "candidate %d/%d: %pf %lu\n", cpu, i,
+			       next.func, next.hit_count);
+			deliver_skb_add_cand(top, ARRAY_SIZE(top), next);
+		}
+	}
+	/* Record our results (for debugging) */
+	for (i = 0; i < ARRAY_SIZE(top); i++) {
+		if (i < 2) /* 2 == number of static calls in the branch tree */
+			printk(KERN_ERR "selected [%d] %pf, score %lu\n", i,
+			       top[i].func, top[i].hit_count);
+		else
+			printk(KERN_ERR "runnerup [%d] %pf, score %lu\n", i,
+			       top[i].func, top[i].hit_count);
+	}
+	/* It's possible that we could have picked up multiple pushes of the
+	 * workitem, so someone already collected most of the count.  In that
+	 * case, don't make a decision based on only a small number of calls.
+	 */
+	if (top[0].hit_count > 250) {
+		/* Divert callers away from the fast path */
+		static_call_update(dispatch_deliver_skb, do_deliver_skb);
+		printk(KERN_ERR "patched dds to %pf\n", dispatch_deliver_skb.func);
+		/* Wait for existing fast path callers to finish */
+		synchronize_rcu();
+		/* Patch the chosen callees into the fast path */
+		static_call_update(dynamic_deliver_skb_1, *top[0].func);
+		printk(KERN_ERR "patched dds1 to %pf\n", dynamic_deliver_skb_1.func);
+		static_call_update(dynamic_deliver_skb_2, *top[1].func);
+		printk(KERN_ERR "patched dds2 to %pf\n", dynamic_deliver_skb_2.func);
+		/* Ensure the new fast path is seen before we direct anyone
+		 * into it.  This probably isn't necessary (the binary-patching
+		 * framework probably takes care of it) but let's be paranoid.
+		 */
+		wmb();
+		/* Switch callers back onto the fast path */
+		static_call_update(dispatch_deliver_skb, dynamic_deliver_skb);
+		printk(KERN_ERR "patched dds to %pf\n", dispatch_deliver_skb.func);
+	}
+	mutex_unlock(&deliver_skb_update_lock);
+	printk(KERN_ERR "deliver_skb_update finished\n");
+}
+
+static noinline int deliver_skb(struct sk_buff *skb,
+				struct packet_type *pt_prev,
+				struct net_device *orig_dev)
 {
 	if (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))
 		return -ENOMEM;
 	refcount_inc(&skb->users);
-	return pt_prev->func(skb, skb->dev, pt_prev, orig_dev);
+	return static_call(dispatch_deliver_skb, skb, pt_prev, orig_dev);
 }
 
 static inline void deliver_ptype_list_skb(struct sk_buff *skb,
@@ -4951,7 +5161,9 @@ static int __netif_receive_skb_one_core(struct sk_buff *skb, bool pfmemalloc)
 
 	ret = __netif_receive_skb_core(skb, pfmemalloc, &pt_prev);
 	if (pt_prev)
-		ret = pt_prev->func(skb, skb->dev, pt_prev, orig_dev);
+		/* ret = pt_prev->func(skb, skb->dev, pt_prev, orig_dev); */
+		/* but (hopefully) faster */
+		ret = static_call(dispatch_deliver_skb, skb, pt_prev, orig_dev);
 	return ret;
 }
 
