Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by i7-8700 with POP3-SSL;
  12 Dec 2018 19:44:46 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga008.jf.intel.com (orsmga008.jf.intel.com [10.7.209.65])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 4F94558079D;
	Wed, 12 Dec 2018 02:44:29 -0800 (PST)
Received: from fmsmga101.fm.intel.com ([10.1.193.65])
  by orsmga008-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 12 Dec 2018 02:44:28 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3Ad71yVBza090E/7nXCy+O+j09IxM/srCxBDY+r6Qd?=
 =?us-ascii?q?0e4RKvad9pjvdHbS+e9qxAeQG9mDu7Qc06L/iOPJYSQ4+5GPsXQPItRndiQuro?=
 =?us-ascii?q?EopTEmG9OPEkbhLfTnPGQQFcVGU0J5rTngaRAGUMnxaEfPrXKs8DUcBgvwNRZv?=
 =?us-ascii?q?JuTyB4Xek9m72/q99pHPYAhEniaxba9vJxiqsAvdsdUbj5F/Iagr0BvJpXVIe+?=
 =?us-ascii?q?VSxWx2IF+Yggjx6MSt8pN96ipco/0u+dJOXqX8ZKQ4UKdXDC86PGAv5c3krgfM?=
 =?us-ascii?q?QA2S7XYBSGoWkx5IAw/Y7BHmW5r6ryX3uvZh1CScIMb7Vq4/Vyi84Kh3SR/okC?=
 =?us-ascii?q?YHOCA/8GHLkcx7kaZXrAu8qxBj34LYZYeYP+d8cKzAZ9MXXWRPUMZPWSJcAY28?=
 =?us-ascii?q?YYQAAPYcMulaoYb9vEMOoBmlCAmwGO/i0CNEimPs0KEk1ekqDAHI3BYnH9ILqH?=
 =?us-ascii?q?nZsNP1O7oIUe+r1qbD0CnOb+lK1jjn7ojIfQ4uofWNXbltdsfe01MgFxnZgVqK?=
 =?us-ascii?q?r4zlMC2a2/8Xs2eF8uVgVfigi3I9pw5tpTivw94hh4/UjYwW0lDJ7Tt1zJoxKN?=
 =?us-ascii?q?GiVUJ2b8CoHIFNuyyZK4d6WMIvTmNwtCok1rELvYS3cDUExZg53RLTdv+KfoaS?=
 =?us-ascii?q?7h79V+ucJypzimh/d7KlnRmy9FCtyu3iWcmw11ZHtjRFktbSuXAXzRDT6daISu?=
 =?us-ascii?q?F7/ki/3TaDzQfT6vtLIUwslKrbLYAuwqIom5YNrUjOGjX6lFj4gaOIbEko5+ul?=
 =?us-ascii?q?5/j9brjnpJKQL4p0hRv/MqQqlMy/G+M4Mg0WUmif+OS80qDj/ELgTLVJkPI2iK?=
 =?us-ascii?q?/Zv47eJcgCoa64DQlV3Zg56xukETem38oXnWMdIFJGZh2HlY7pNE/KIPziCve/?=
 =?us-ascii?q?mVusnC9xx//aJr3hHonNLn/bnbflfLZ96FBTxBA8zNBC/J9UDrABIPTuWk7+rt?=
 =?us-ascii?q?DYDxk5MxCqzObjEtlyyoQeWWeXCK+DLKzSqUOI5v4oI+SUZI8aojf9K+Q/6P7p?=
 =?us-ascii?q?l3M5mUIdcrOv3ZsYc324GvVmI0OEYXvjmNsBEGEKvhYgQ+zuklGNTTlTZ3OqVa?=
 =?us-ascii?q?Im+j47EJ6mDZvERo21gryB2zm0EodVZmBBDFCMF3Doep6AW/cNbiKSP8BgniYF?=
 =?us-ascii?q?VbinV48uyxWuuBXmxLpgK+re4jcYuo771Nhp++3Tkgk/+iZvD8Sd1GGNTHt4nn?=
 =?us-ascii?q?kSSD80x61/pU19ylGe0al3mfBYFNpT5+9XXQc+L5LT0+t6C9XqUALbYtiJUEqm?=
 =?us-ascii?q?QsmhATwpTdI+2dkOb1x5G9WjlB/D2SWqDqQRl7yKApw0763d02LwJ8Z713bJyq?=
 =?us-ascii?q?0hg0M6TctIMG2snrR/+BTLB47Vj0WZkL6nerkG0y7T6miP12qOs1teUA5rT6rF?=
 =?us-ascii?q?W3cTZk/VrdT84kPPVLuuCbUhMgtcxs+OMKpKatv1jVpYQPfvIsjRY2W0m22oHx?=
 =?us-ascii?q?aH2quMbJb2e2UaxCjSFVILkx4N8nqcNQgxHCGho3nAAzxoDl/vZ0Ls8e9jqHK0?=
 =?us-ascii?q?VEM0zgeKb1F/2Lqx4BIamfucS/YL1LIepCghsyl0HEq639/OF9qApg9hfKJAYd?=
 =?us-ascii?q?M94FZLz37ZuxZ6Ppy6K6Bih1gecwtsskPq1hV3DJhAkMcwoHMrygpyNbyX0Fdb?=
 =?us-ascii?q?ezyE2pDwP6XdKnPu8xC3d67Wxlbe3c6M9acL9vs5pEvsvQGzGkU57nVozsNa03?=
 =?us-ascii?q?2f5pXNEgofSpbxUkcx9xhnqLDWeCg954XI1XJyNam4qCPN29UsBOE90BavY89f?=
 =?us-ascii?q?ML+YFA/1C8AbB9WuJPY2lFSzbxMEPPpd9Kg7P869c/uG2airPPtvnT68jGRH5p?=
 =?us-ascii?q?x93VyI9yZmVuHI2JMFyemC3gSbTzf8kEuhssfvlIBeZDEdAnCwyTL5C45WfKFy?=
 =?us-ascii?q?e5gECX2vI8Gu2tpxnZrtVGNG+165AFMG3tSkeR6Tb1z7wA1R2l4boX2hmSuk0T?=
 =?us-ascii?q?N0lyslobaY3CzL2+7ibgYIOnZXRGl+ilfhOYi1j9EAUEmodQQpjwal5UDhyqhf?=
 =?us-ascii?q?paR/KXTTQEhScyj3KWFiTrW/tr6Yb8FT75MotD1dUP6gblCCVr79vxwa3jvhH2?=
 =?us-ascii?q?RE3jA7dDKqupLjkxxhkm2dL3VzrHvfec5uwxfS/9jcRf9X3jobSyh0kzjXBl6g?=
 =?us-ascii?q?P9a3+dWYjYvMsue7V2i5TJ1cbTHrzZ+ctCu8/WBrAQewn/eplt3lEAg61zT219?=
 =?us-ascii?q?1rVSXOsRb9bZPn16W8MeJ7YEZoAEXw5NZ9GoF7ioEwno0f2WAGhpWJ+noKiWTz?=
 =?us-ascii?q?Ps9a2a7kbHoNWCQEw9ja4AX+3E1jL3SJx5/2V3mHw8thYcW6bX0S2i4n889KD6?=
 =?us-ascii?q?KU5qRenSRpulq4sR7RYf9lkzcYyPsu62QVj/sHuQUz1SWdHq4dHVNDMiPyjRSI?=
 =?us-ascii?q?4MuzrKFWZGapbLix21ByndGnDLGevA5cXGz1dYslHS919s9/Kk7D0GXv6oH4f9?=
 =?us-ascii?q?nddc4TthqRkxvajulaMpQxlucRhSp8OGLwpnkly+89jRxz0pC2pomHK2Nx/K2n?=
 =?us-ascii?q?Bh5ULCH6Z8QW+jv1l6ZRgt6W35yzHpVmAjgER4HnTfWsEDIPr/jnMxuOHSY4qn?=
 =?us-ascii?q?eaH7rfAACe5F1nr3LJD5CkKXWXKGMFwtVlQRmXPFZfjxwMXDUmgp45ERinxM77?=
 =?us-ascii?q?f0d8+D8R/UT0qgdWxuJrKhn/UXrfpAGyZTcvU5WfKBtW7gdf50baK8Ce7+RzHz?=
 =?us-ascii?q?1G8Z2ltgCCNmubZwFQB2ETRkOEH0zjPqWp5dTY8+mYAfe+L+LTYbqUr+xSTeyI?=
 =?us-ascii?q?xZWp0otp5DuMMsSPPn9/D/w0wEZDXHZ5G9jHlDULUSAYiyXNb8uDrhem5iJ3tt?=
 =?us-ascii?q?y/8OjsWA/35YuAFbpSMdZs+xC3m6uDNPSQhCF2KTlGzJMM2GTIxaMb3F4TjSFu?=
 =?us-ascii?q?ajasHa4BtS7LUKLfhKtXAwQHZCN0MctC97g80RVVOc7HltP10aZ1geQoC1heT1?=
 =?us-ascii?q?zuhMGoadYOI2G8L17HAEeLNLKbJTzE2c33YKW8SaFOg+VQrRG/pTGbE0r7NDSZ?=
 =?us-ascii?q?izbpTwyvMf1LjCyDIBxeuYS9fgxxBmT5UN3magO0MNlsgD0ywL00gG7KNGEGPT?=
 =?us-ascii?q?h9dUNNsqOf7SdCjvpjHGxB62JvLfOYlCaB8+nYNpEWvONrAytukeJa5XU6y71N?=
 =?us-ascii?q?4yBFRPx6gi3SrtF1rlGik+mPzCdnURVUpjZKgoKLoVttOaHD+pZcXnbE+UFF0W?=
 =?us-ascii?q?LFJx0UptctMNzivbxSx8OHwLjyJTFe8d/P1cQbAMfQJYSMN393YjTzHzuBJQ0J?=
 =?us-ascii?q?VzOifUjFjEVbn/yW9zXBp5EgoZHql5cmULJXVFUpUPgdDxI2T5Q5PJ5rU2Z8wv?=
 =?us-ascii?q?agh8kS6C/79UGJSQ=3D=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AfAACl5RBch0O0hNFjGQEBAQEBAQEBA?=
 =?us-ascii?q?QEBAQcBAQEBAQGBZYFWghaNGoszgiGJDZA5KwGHOyI4EgEDAQEBAQEBAgETAQE?=
 =?us-ascii?q?BCA0JCCkvgjYkAYJiAwMBAiQTPwYJAQEKEg8lAwwFKQwUGIMcgXUNBaZaM4owj?=
 =?us-ascii?q?DwXgUA/gRGCFH6BQQGGdIImAolGgWiEFX6QRgmGSIsCCxiJaYdbmTmBXYF3TSM?=
 =?us-ascii?q?VO4JtgjKORiEBAYE2AQEcjQgDAQE?=
X-IPAS-Result: =?us-ascii?q?A0AfAACl5RBch0O0hNFjGQEBAQEBAQEBAQEBAQcBAQEBAQG?=
 =?us-ascii?q?BZYFWghaNGoszgiGJDZA5KwGHOyI4EgEDAQEBAQEBAgETAQEBCA0JCCkvgjYkA?=
 =?us-ascii?q?YJiAwMBAiQTPwYJAQEKEg8lAwwFKQwUGIMcgXUNBaZaM4owjDwXgUA/gRGCFH6?=
 =?us-ascii?q?BQQGGdIImAolGgWiEFX6QRgmGSIsCCxiJaYdbmTmBXYF3TSMVO4JtgjKORiEBA?=
 =?us-ascii?q?YE2AQEcjQgDAQE?=
X-IronPort-AV: E=Sophos;i="5.56,344,1539673200"; 
   d="scan'208";a="66454556"
X-Amp-Result: UNKNOWN
X-Amp-Original-Verdict: FILE UNKNOWN
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mga01b.intel.com with ESMTP; 12 Dec 2018 02:44:27 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1727111AbeLLKoZ (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Wed, 12 Dec 2018 05:44:25 -0500
Received: from mx1.redhat.com ([209.132.183.28]:38436 "EHLO mx1.redhat.com"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1726791AbeLLKoY (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Wed, 12 Dec 2018 05:44:24 -0500
Received: from smtp.corp.redhat.com (int-mx06.intmail.prod.int.phx2.redhat.com [10.5.11.16])
        (using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
        (No client certificate requested)
        by mx1.redhat.com (Postfix) with ESMTPS id A5FD588302;
        Wed, 12 Dec 2018 10:44:23 +0000 (UTC)
Received: from sky.random (ovpn-120-222.rdu2.redhat.com [10.10.120.222])
        by smtp.corp.redhat.com (Postfix) with ESMTPS id F41CA4D9E9;
        Wed, 12 Dec 2018 10:44:19 +0000 (UTC)
Date: Wed, 12 Dec 2018 05:44:18 -0500
From: Andrea Arcangeli <aarcange@redhat.com>
To: David Rientjes <rientjes@google.com>
Cc: Linus Torvalds <torvalds@linux-foundation.org>,
        mgorman@techsingularity.net, Vlastimil Babka <vbabka@suse.cz>,
        Michal Hocko <mhocko@kernel.org>, ying.huang@intel.com,
        s.priebe@profihost.ag,
        Linux List Kernel Mailing <linux-kernel@vger.kernel.org>,
        alex.williamson@redhat.com, lkp@01.org, kirill@shutemov.name,
        Andrew Morton <akpm@linux-foundation.org>,
        zi.yan@cs.rutgers.edu
Subject: Re: [LKP] [mm] ac5b2c1891: vm-scalability.throughput -61.3%
 regression
Message-ID: <20181212104418.GE1130@redhat.com>
References: <20181204104558.GV23260@techsingularity.net>
 <20181205204034.GB11899@redhat.com>
 <CAHk-=whi8Ju-cTDL4cYtwuLA7BYgGJYyy6HVMoknZaLHnRc83g@mail.gmail.com>
 <20181205233632.GE11899@redhat.com>
 <CAHk-=wguXjkbK8BUU998s7HD7AXJgBkuc9JmuNxiN7uGQyfSfQ@mail.gmail.com>
 <CAHk-=wjm9V843eg0uesMrxKnCCq7UfWn8VJ+z-cNztb_0fVW6A@mail.gmail.com>
 <alpine.DEB.2.21.1812061505010.162675@chino.kir.corp.google.com>
 <CAHk-=wjVuLjZ1Wr52W=hNqh=_8gbzuKA+YpsVb4NBHCJsE6cyA@mail.gmail.com>
 <alpine.DEB.2.21.1812091538310.215735@chino.kir.corp.google.com>
 <20181210044916.GC24097@redhat.com>
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: <20181210044916.GC24097@redhat.com>
User-Agent: Mutt/1.11.1 (2018-12-01)
X-Scanned-By: MIMEDefang 2.79 on 10.5.11.16
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16 (mx1.redhat.com [10.5.110.28]); Wed, 12 Dec 2018 10:44:24 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Hello,

I now found a two socket EPYC (is this Naples?) to try to confirm the
THP effect of intra-socket THP.

CPU(s):                128
On-line CPU(s) list:   0-127
Thread(s) per core:    2
Core(s) per socket:    32
Socket(s):             2
NUMA node(s):          8
NUMA node0 CPU(s):     0-7,64-71
NUMA node1 CPU(s):     8-15,72-79
NUMA node2 CPU(s):     16-23,80-87
NUMA node3 CPU(s):     24-31,88-95
NUMA node4 CPU(s):     32-39,96-103
NUMA node5 CPU(s):     40-47,104-111
NUMA node6 CPU(s):     48-55,112-119
NUMA node7 CPU(s):     56-63,120-127

# numactl --hardware
available: 8 nodes (0-7)
node 0 cpus: 0 1 2 3 4 5 6 7 64 65 66 67 68 69 70 71
node 0 size: 32658 MB
node 0 free: 31554 MB
node 1 cpus: 8 9 10 11 12 13 14 15 72 73 74 75 76 77 78 79
node 1 size: 32767 MB
node 1 free: 31854 MB
node 2 cpus: 16 17 18 19 20 21 22 23 80 81 82 83 84 85 86 87
node 2 size: 32767 MB
node 2 free: 31535 MB
node 3 cpus: 24 25 26 27 28 29 30 31 88 89 90 91 92 93 94 95
node 3 size: 32767 MB
node 3 free: 31777 MB
node 4 cpus: 32 33 34 35 36 37 38 39 96 97 98 99 100 101 102 103
node 4 size: 32767 MB
node 4 free: 31949 MB
node 5 cpus: 40 41 42 43 44 45 46 47 104 105 106 107 108 109 110 111
node 5 size: 32767 MB
node 5 free: 31957 MB
node 6 cpus: 48 49 50 51 52 53 54 55 112 113 114 115 116 117 118 119
node 6 size: 32767 MB
node 6 free: 31945 MB
node 7 cpus: 56 57 58 59 60 61 62 63 120 121 122 123 124 125 126 127
node 7 size: 32767 MB
node 7 free: 31958 MB
node distances:
node   0   1   2   3   4   5   6   7 
  0:  10  16  16  16  32  32  32  32 
  1:  16  10  16  16  32  32  32  32 
  2:  16  16  10  16  32  32  32  32 
  3:  16  16  16  10  32  32  32  32 
  4:  32  32  32  32  10  16  16  16 
  5:  32  32  32  32  16  10  16  16 
  6:  32  32  32  32  16  16  10  16 
  7:  32  32  32  32  16  16  16  10 
# for i in 0 8 16 24 32 40 48 56; do numactl -m 0 -C $i /tmp/numa-thp-bench; done
random writes MADV_HUGEPAGE 17622885 usec
random writes MADV_NOHUGEPAGE 25316593 usec
random writes MADV_NOHUGEPAGE 25291927 usec
random writes MADV_HUGEPAGE 17672446 usec
random writes MADV_HUGEPAGE 25698555 usec
random writes MADV_NOHUGEPAGE 36413941 usec
random writes MADV_NOHUGEPAGE 36402155 usec
random writes MADV_HUGEPAGE 25689574 usec
random writes MADV_HUGEPAGE 25136558 usec
random writes MADV_NOHUGEPAGE 35562724 usec
random writes MADV_NOHUGEPAGE 35504708 usec
random writes MADV_HUGEPAGE 25123186 usec
random writes MADV_HUGEPAGE 25137002 usec
random writes MADV_NOHUGEPAGE 35577429 usec
random writes MADV_NOHUGEPAGE 35582865 usec
random writes MADV_HUGEPAGE 25116561 usec
random writes MADV_HUGEPAGE 40281721 usec
random writes MADV_NOHUGEPAGE 56891233 usec
random writes MADV_NOHUGEPAGE 56924134 usec
random writes MADV_HUGEPAGE 40286512 usec
random writes MADV_HUGEPAGE 40377662 usec
random writes MADV_NOHUGEPAGE 56731400 usec
random writes MADV_NOHUGEPAGE 56443959 usec
random writes MADV_HUGEPAGE 40379022 usec
random writes MADV_HUGEPAGE 33907588 usec
random writes MADV_NOHUGEPAGE 47609976 usec
random writes MADV_NOHUGEPAGE 47523481 usec
random writes MADV_HUGEPAGE 33881974 usec
random writes MADV_HUGEPAGE 40809719 usec
random writes MADV_NOHUGEPAGE 57148321 usec
random writes MADV_NOHUGEPAGE 57164499 usec
random writes MADV_HUGEPAGE 40802979 usec
# grep EPYC /proc/cpuinfo |head -1
model name      : AMD EPYC 7601 32-Core Processor

I suppose node 0-1-2-3 are socket 0 and node 4-5-6-7 are socket 1.

With the ram kept in nodeid 0, cpuid 0 is NUMA local, cpuid 8,16,24
are NUMA intrasocket remote and cpuid 32 40 48 56 are NUMA
intersocket remote.

local 4k -> local THP: +43.6% improvement

local 4k -> intersocket remote THP: -1.4%
intersocket remote 4k -> intersocket remote THP: +41.6%

local 4k -> intersocket remote 4k: -30.4%
local THP -> intersocket remote THP: -31.4%

local 4k -> intrasocket remote THP: -37.15% (-25% on node 6?)
intrasocket remote 4k -> intrasocket remote THP: +41.23%

local 4k -> intrasocket remote 4k: -55.5% (-46% on node 6?)
local THP -> intrasocket remote THP: -56.25% (-47% on node 6?)

In short intrasocket is a whole lot more expensive (4k -55% THP -56%)
than intersocket (4k -30% THP -31%)... as expected. The benefits of
THP vs 4k remains the same for intrasocket (+41.23%) and intersocket
(+41.6%) and local (+43.6%), also as expected.

The above was measured on bare metal on guests the impact of THP as
usual will be multiplied (I can try to measure that another time).

So while before I couldn't confirm that THP didn't help intersocket, I
think I can confirm it helps just like intrasocket and local now on
this architecture.

Especially intresocket the slowdown from remote THP compared to local
4k is a tiny -1% so in theory __GFP_THISNODE would at least need to
switch to __GFP_THISSOCKET for this architecture.. (I'm not suggesting
that, I'm talking in theory). Intersocket is even more favorable than
a 2 node 1 socket threadripper and a 2 node (2 sockets?) skylake in
fact even on bare metal.

Losing the +41% THP benefit across all distances makes __GFP_THISNODE
again questionable optimization, it only ever pays off in the
intersocket case (-37% is an increase of compute time of +59% which is
pretty bad and we should definitely have some logic that optimizes for
it). However eliminating the possibility of remote THP doesn't look
good, especially because the 4k allocation may end up being remote too
if the node is full of cache (there is no __GFP_THISNODE set on 4k
allocations...).

__GFP_THISNODE made MADV_HUGEPAGE act a bit like a NUMA node reclaim
hack embedded in MADV_HUGEPAGE: if you didn't set MADV_HUGEPAGE you'd
potentially get more intrasocket remote 4k pages instead of local THPs
thanks to the __GFP_THISNODE reclaim.

I think with the current __GFP_THISNODE and HPAGE_PMD_SIZE hardcoding,
you'll get a lot less local node memory when the local node is full of
clean pagecache, because 4k allocations have no __GFP_THISNODE set, so
you'll get a lot more of intrasocket remote 4k even when there's a ton
of +41% faster intrasocket remote THP memory available. This is
because it the new safe __GFP_THISNODE won't shrink the cache without
mbind anymore.

NUMA balancing must be enabled in a system like above and it should be
able to optimize a lack of convergence as long as the workload can fit
in a node (and if the workload doesn't fit in a node __GFP_THISNODE
will backfire anyway).

We need to keep thinking at how to optimize the case of preferring
local 4k to remote THP in a way that won't backfire the moment the
task in the CPU is moved to a different node, but __GFP_THISNODE set
in the only single attempt of allocating THP memory in a THP fault,
isn't the direction because without a __GFP_THISNODE-reclaim, less
local 4k memory will be allocated once the node is full of cache and
the remote THP will be totally ignored when they would perform better
than remote 4k. Ignoring remote THP isn't the purpose of
MADV_HUGEPAGE, quite the contrary.

Thanks,
Andrea
