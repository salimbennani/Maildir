Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by i7-8700 with POP3-SSL;
  14 Dec 2018 20:29:14 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga003.jf.intel.com (orsmga003.jf.intel.com [10.7.209.27])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id EB1C85807A0;
	Fri, 14 Dec 2018 04:16:30 -0800 (PST)
Received: from orsmga101.jf.intel.com ([10.7.208.22])
  by orsmga003-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 14 Dec 2018 04:16:30 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3A76RBJRwL1iL3aUfXCy+O+j09IxM/srCxBDY+r6Qd?=
 =?us-ascii?q?0e8UL/ad9pjvdHbS+e9qxAeQG9mDu7Qc06L/iOPJYSQ4+5GPsXQPItRndiQuro?=
 =?us-ascii?q?EopTEmG9OPEkbhLfTnPGQQFcVGU0J5rTngaRAGUMnxaEfPrXKs8DUcBgvwNRZv?=
 =?us-ascii?q?JuTyB4Xek9m72/q99pHPYAhEniaxba9vJxiqsAvdsdUbj5F/Iagr0BvJpXVIe+?=
 =?us-ascii?q?VSxWx2IF+Yggjx6MSt8pN96ipco/0u+dJOXqX8ZKQ4UKdXDC86PGAv5c3krgfM?=
 =?us-ascii?q?QA2S7XYBSGoWkx5IAw/Y7BHmW5r6ryX3uvZh1CScIMb7Vq4/Vyi84Kh3SR/okC?=
 =?us-ascii?q?YHOCA/8GHLkcx7kaZXrAu8qxBj34LYZYeYP+d8cKzAZ9MXXWpPUNhMWSJPAY2y?=
 =?us-ascii?q?aIkAD+QPMulXs4bzqEAOrQO8CAS3GOPiyTFFimPs0KAg0eksFxzN0gw6H9IJtX?=
 =?us-ascii?q?TZtNT7NL0MXuC60aLGyC/Db/RM1jf98YTGcAouoeuQXbJ1a8XRz1QkGgTKjlWK?=
 =?us-ascii?q?t4PlMDCV1uQWvmif7upgU/+vimEpqwF2vzivwNojhZPVhoIUzVDE8z91wIEvJd?=
 =?us-ascii?q?23UUN2Z8OvHpVXtyGfLYR2Q8UiTnlruCkk0L0Gv4C0fCwQxJQg3R7fZPqKeJWL?=
 =?us-ascii?q?7BL7TOudPyt0iXZ/dL6iiRu+71KsxvD/W8WoylpHryhInsHPu30DzRDe6cmKRu?=
 =?us-ascii?q?F+80qlwzqDyhzf5+NCLEspj6TUMYQhzaQ1lpcLsUTMACv2mELuga+IeUUr5PKo?=
 =?us-ascii?q?5/7kYrr4vJ+cMZF7igXkPqQpgMy/Dvw0MgkIX2eF5eSxzKPv8VH9TblQk/E7nL?=
 =?us-ascii?q?fVvIrHKckYuqK1GQ5Y34Q75xa6FTim0dAYnXcdLFJCfRKKl4zpO1DIIPDlAvaz?=
 =?us-ascii?q?mlesnylxx/DAILLhBozBLn/NkbfnY7l98VVRyBQ8zd9B/ZJYELIBL+zpWk/3qt?=
 =?us-ascii?q?PYCgU1Mwuuw+boENl9zJ8RWXqTAq+FN6PfqVuI5uMsI+aSfoMUtyv9JuMh5/7v?=
 =?us-ascii?q?i385hFAccbOo3ZsRdHC3APBmL1+Fbnrrh9cLCX0KsRYmTOz2lF2CViZeZ3aoUK?=
 =?us-ascii?q?I9+jE0EoWmAZ3DRoCwmrOB2ii7E4ZSZmBHDFCMDHjpe5+FW/cKdCKdPMthniYY?=
 =?us-ascii?q?WrimTo8rzQuuuxPiy7p7MurU/TUVtZDk1Ndr/eHTlhYy9TpyD8SayGyNS2B0nm?=
 =?us-ascii?q?UVRz45xqx/oEp9ykud3qh8mfBXCdtT5/ZRWAcgKZHc1/B6C8z1Wg/ZZNiJU1am?=
 =?us-ascii?q?QtKlAT0rVNI+2d0Obl15G9WjiBDDwiWrD6UUl7yNGJw77Kbc02LtKMZ6znbMzL?=
 =?us-ascii?q?MhgEU+QstTKW2mgbZy9wvJCI7PiUmZk6eqer4a3C7C72qDyWuOvEdFUA9/S6nF?=
 =?us-ascii?q?XHYfZlfIotT9/E/NU7iuCbE/OAtb1cGCMrdKasHujVheWfjsIsrebHyrl2ewHx?=
 =?us-ascii?q?mIwKiMY5Tse2ka2CXdC00EkwQI8HaCNAg+ADqhom3EADxvE1Lvf13j8e1kpHyn?=
 =?us-ascii?q?SU80yhmAb1d92Lqt5h4VmfucRusO0b0epicutS94HFan0NLQENqPuQxhcb5YYd?=
 =?us-ascii?q?M85ldHyG3YuxZ8PpymM6BtmFoefx5rsEPp0hV9Ep9AntQyrHM20ApyLrqV0E9A?=
 =?us-ascii?q?dzOd2pDwJr3XK2no8BCzcaLW3Uve0NKX+qcJ5/Q1sFHjvACvFko//HRrydhV03?=
 =?us-ascii?q?2A5prUCAoeS47+UkEy9xJivbHVfjE955/I1X1rKaS7qDvC1MwmBeQ7yhesZcxQ?=
 =?us-ascii?q?ML6ZGwDoFc0aBM+uKPIxllitbxIEOv1S9aEuM8OncfuGxLCkPOJ6kD26imRH5Z?=
 =?us-ascii?q?h30liQ+CpkVu7Iw5EFzumY3wuaTTvzkE2ts8H3mY9eYzESEXGyySzlBI5Xe61z?=
 =?us-ascii?q?cpwHCWaoI82r2Np+g4ThVGJf9F6mH1kGwtOmeQKOb1zh2g1dzV4XoX27liSi1T?=
 =?us-ascii?q?x7jjYporCZ3CPQ2eTidQMLNXJRS2lmkFjjP5K0j9ccXEWzawgplR2l5Vv1xqRB?=
 =?us-ascii?q?paR/KXXTTllMfyTsM25iVa6wvKKYY8FT8JMorTlXUOOkbFGaS77xuRsb3zn4H2?=
 =?us-ascii?q?tDwjA2bDWqupT/nxxnh2ORNndzrHzFec5uwRfT/sDTRflU3jAeXil3lSHXBkSg?=
 =?us-ascii?q?P9mu5diUlY3Msvq9V2KiUZ1fazLrwpmDtCu45G1qBwOwn/aol93jEAg61zL71t?=
 =?us-ascii?q?ZwWSXJqhb8fpfk16CgPe17eUloAUf269BmFYFmjoswmJYQ1GAah5qP+noLi2Xz?=
 =?us-ascii?q?MdRd2a/laHoNRDgLw8Pa4QT/2U1jKG6JyJz9VnmH3sRhYNy6aHsM2i0h98BKFL?=
 =?us-ascii?q?uU7LtckCt2uFW4rBjdYfhgnjgHz/su52UXg+UIuAor0yWcDaoeHUhePSzwiRuI?=
 =?us-ascii?q?68qyo7lQZGaqabKwzlZxnci9DLGepQFRQHP5epYhHSBq9Ml+PkzD0Gbv6o76Yt?=
 =?us-ascii?q?nfdswctgebkxrbi+hVKZQxlucFhCZ9OGL9u2ElxPA/jRB0wZ66u42HIX13/K2l?=
 =?us-ascii?q?Gh5YKiH1Z8QL9zHxiqZem9ya3oG1EZVnBzULR4DoTfOzHTIWtPTnMRuOETImpn?=
 =?us-ascii?q?eaH7rfARGQ6EN8o33TFJCrMmmdJGMFwtV6WBmdOEtfjRgPUzogmZ45Eh2qyNbl?=
 =?us-ascii?q?cEdk/T0R4l/4qh1RyuNnLRX/U2HfpBu2ZTcwUpSQMB1W7gRa7UfPLcOe9v5zHz?=
 =?us-ascii?q?1f/pC5rgyCMGmbaBpTDW0TRkyIHVPjPqSt5dnd9eiXHPG+IuDKYbWPr+xeSviJ?=
 =?us-ascii?q?yYiu0otg4zaDKMGPMmN+AP0830pJRWp5FNjBmzUTVywXkDrAbs6cpBeh4y13s9?=
 =?us-ascii?q?qw8PLxVAL04ouPDbRSPM5r+xC3h6eDKuGRiDx4KTZeypMD23vIxKID014VjiFk?=
 =?us-ascii?q?byOtHqgYtS7RUKLQnbdaDx4FZCNyN8tI7KM80hNOOc7BjdP12aB3juQoC1deTl?=
 =?us-ascii?q?HhnsCpZcoXI2CyLl/HBUCLNKiYKj3P2c34faS8SbhIhuVOqxKwoSqbE1PkPjmb?=
 =?us-ascii?q?jTbpSgugMftSgyCbJhBevIC9fw1pCWjiStLmdxK6PMV2jT0w3b07mHfKOXQAPj?=
 =?us-ascii?q?h7dkNHtqeQ4j9AgvVjB2xB6WJoLfWFmyac9eXXNowavudrAitqkeJX+3A6y7pT?=
 =?us-ascii?q?7CFZS/15girSrthyo164lumD0CZoUB1LqjxTnoKEoV1iObnF9plHQXvF/AgC7W?=
 =?us-ascii?q?OVCxQLodtqEtzuu6BKxdjJm6L+MzNC89PS/csBCMnYMsOHMHw9MRX3HD7YFhcK?=
 =?us-ascii?q?TTmuNTKXu0sIlPCU623QrZUgrJXosIQBR6UdV1EvEP4eTEN/E48sOpByCxw+nL?=
 =?us-ascii?q?HTr9QP7Hr7jBDLWMBTuNiTWeifDvbvADKYi6RUaRwVx7/xMYUUMMv8wUM0OQoy?=
 =?us-ascii?q?p5jDB0eFBYMFmSZmdAJh5RwVqHU=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0B3AAD3nhNch0O0hNFjHAEBAQQBAQcEA?=
 =?us-ascii?q?QGBVAQBAQsBgmmBAieDfIh4jSYUkkaGcxEYDwQBgyyEGyI3Bg0BAwEBAQEBAQI?=
 =?us-ascii?q?BEwEBAQgNCQgpIwyCNiQBgmIDAwECIA8BDQEBNwEFCQEBHwUCBSECAgMMGS8ZB?=
 =?us-ascii?q?YMcAYIABAEKpE1wgQgngnYBAQWBMAGFeQMFgQuGcoMlgRwXgX+IfoMsgleJRIF?=
 =?us-ascii?q?0lRtHCYcNhi2EFSOBXYUcilksjgWLMIFcgXgzGggoCIMnE4IIDBcSiEyFQD8yA?=
 =?us-ascii?q?QEBAX4DAQEhE4o3gkwBAQ?=
X-IPAS-Result: =?us-ascii?q?A0B3AAD3nhNch0O0hNFjHAEBAQQBAQcEAQGBVAQBAQsBgmm?=
 =?us-ascii?q?BAieDfIh4jSYUkkaGcxEYDwQBgyyEGyI3Bg0BAwEBAQEBAQIBEwEBAQgNCQgpI?=
 =?us-ascii?q?wyCNiQBgmIDAwECIA8BDQEBNwEFCQEBHwUCBSECAgMMGS8ZBYMcAYIABAEKpE1?=
 =?us-ascii?q?wgQgngnYBAQWBMAGFeQMFgQuGcoMlgRwXgX+IfoMsgleJRIF0lRtHCYcNhi2EF?=
 =?us-ascii?q?SOBXYUcilksjgWLMIFcgXgzGggoCIMnE4IIDBcSiEyFQD8yAQEBAX4DAQEhE4o?=
 =?us-ascii?q?3gkwBAQ?=
X-IronPort-AV: E=Sophos;i="5.56,352,1539673200"; 
   d="scan'208";a="44972417"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 14 Dec 2018 04:16:26 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1732653AbeLNMPz (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Fri, 14 Dec 2018 07:15:55 -0500
Received: from mail.kernel.org ([198.145.29.99]:35930 "EHLO mail.kernel.org"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1732640AbeLNMPv (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Fri, 14 Dec 2018 07:15:51 -0500
Received: from localhost (5356596B.cm-6-7b.dynamic.ziggo.nl [83.86.89.107])
        (using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
        (No client certificate requested)
        by mail.kernel.org (Postfix) with ESMTPSA id 25A81214AE;
        Fri, 14 Dec 2018 12:15:50 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=kernel.org;
        s=default; t=1544789750;
        bh=prmj2rE5bCztstZF9F1lLHK03FQpkZRNwrPD3RzpNcQ=;
        h=From:To:Cc:Subject:Date:In-Reply-To:References:From;
        b=TRFq1ZBJaRWyobAwA/5TUHgYF+SSXNhBPJg0amriO4a4RM/b8/lKTuf4jl6DKBaVn
         lT0X6cY2NrU95PBA7IwhwNTakcfeJQPcOBqyytymK9jloMePl4+V5YXkjwV/DYCZ6k
         H2XIctYDskZp91virqasvIgNlmUWw+DdbPGFk9Tc=
From: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
To: linux-kernel@vger.kernel.org
Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>,
        stable@vger.kernel.org,
        Linus Torvalds <torvalds@linux-foundation.org>,
        Andi Kleen <ak@linux.intel.com>,
        Dan Williams <dan.j.williams@intel.com>,
        Thomas Gleixner <tglx@linutronix.de>,
        linux-arch@vger.kernel.org, Kees Cook <keescook@chromium.org>,
        kernel-hardening@lists.openwall.com,
        Al Viro <viro@zeniv.linux.org.uk>, alan@linux.intel.com,
        Ben Hutchings <ben.hutchings@codethink.co.uk>
Subject: [PATCH 4.4 55/88] x86/uaccess: Use __uaccess_begin_nospec() and uaccess_try_nospec
Date: Fri, 14 Dec 2018 13:00:29 +0100
Message-Id: <20181214115707.004243853@linuxfoundation.org>
X-Mailer: git-send-email 2.20.0
In-Reply-To: <20181214115702.151309521@linuxfoundation.org>
References: <20181214115702.151309521@linuxfoundation.org>
User-Agent: quilt/0.65
X-stable: review
X-Patchwork-Hint: ignore
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

4.4-stable review patch.  If anyone has any objections, please let me know.

------------------

From: Dan Williams <dan.j.williams@intel.com>

commit 304ec1b050310548db33063e567123fae8fd0301 upstream.

Quoting Linus:

    I do think that it would be a good idea to very expressly document
    the fact that it's not that the user access itself is unsafe. I do
    agree that things like "get_user()" want to be protected, but not
    because of any direct bugs or problems with get_user() and friends,
    but simply because get_user() is an excellent source of a pointer
    that is obviously controlled from a potentially attacking user
    space. So it's a prime candidate for then finding _subsequent_
    accesses that can then be used to perturb the cache.

__uaccess_begin_nospec() covers __get_user() and copy_from_iter() where the
limit check is far away from the user pointer de-reference. In those cases
a barrier_nospec() prevents speculation with a potential pointer to
privileged memory. uaccess_try_nospec covers get_user_try.

Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
Suggested-by: Andi Kleen <ak@linux.intel.com>
Signed-off-by: Dan Williams <dan.j.williams@intel.com>
Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
Cc: linux-arch@vger.kernel.org
Cc: Kees Cook <keescook@chromium.org>
Cc: kernel-hardening@lists.openwall.com
Cc: gregkh@linuxfoundation.org
Cc: Al Viro <viro@zeniv.linux.org.uk>
Cc: alan@linux.intel.com
Link: https://lkml.kernel.org/r/151727416953.33451.10508284228526170604.stgit@dwillia2-desk3.amr.corp.intel.com
[bwh: Backported to 4.4:
 - Convert several more functions to use __uaccess_begin_nospec(), that
   are just wrappers in mainline
 - Adjust context]
Signed-off-by: Ben Hutchings <ben.hutchings@codethink.co.uk>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 arch/x86/include/asm/uaccess.h    |    6 +++---
 arch/x86/include/asm/uaccess_32.h |   26 +++++++++++++-------------
 arch/x86/include/asm/uaccess_64.h |   20 ++++++++++----------
 arch/x86/lib/usercopy_32.c        |   10 +++++-----
 4 files changed, 31 insertions(+), 31 deletions(-)

--- a/arch/x86/include/asm/uaccess.h
+++ b/arch/x86/include/asm/uaccess.h
@@ -436,7 +436,7 @@ do {									\
 ({									\
 	int __gu_err;							\
 	unsigned long __gu_val;						\
-	__uaccess_begin();						\
+	__uaccess_begin_nospec();					\
 	__get_user_size(__gu_val, (ptr), (size), __gu_err, -EFAULT);	\
 	__uaccess_end();						\
 	(x) = (__force __typeof__(*(ptr)))__gu_val;			\
@@ -546,7 +546,7 @@ struct __large_struct { unsigned long bu
  *	get_user_ex(...);
  * } get_user_catch(err)
  */
-#define get_user_try		uaccess_try
+#define get_user_try		uaccess_try_nospec
 #define get_user_catch(err)	uaccess_catch(err)
 
 #define get_user_ex(x, ptr)	do {					\
@@ -581,7 +581,7 @@ extern void __cmpxchg_wrong_size(void)
 	__typeof__(ptr) __uval = (uval);				\
 	__typeof__(*(ptr)) __old = (old);				\
 	__typeof__(*(ptr)) __new = (new);				\
-	__uaccess_begin();						\
+	__uaccess_begin_nospec();					\
 	switch (size) {							\
 	case 1:								\
 	{								\
--- a/arch/x86/include/asm/uaccess_32.h
+++ b/arch/x86/include/asm/uaccess_32.h
@@ -48,25 +48,25 @@ __copy_to_user_inatomic(void __user *to,
 
 		switch (n) {
 		case 1:
-			__uaccess_begin();
+			__uaccess_begin_nospec();
 			__put_user_size(*(u8 *)from, (u8 __user *)to,
 					1, ret, 1);
 			__uaccess_end();
 			return ret;
 		case 2:
-			__uaccess_begin();
+			__uaccess_begin_nospec();
 			__put_user_size(*(u16 *)from, (u16 __user *)to,
 					2, ret, 2);
 			__uaccess_end();
 			return ret;
 		case 4:
-			__uaccess_begin();
+			__uaccess_begin_nospec();
 			__put_user_size(*(u32 *)from, (u32 __user *)to,
 					4, ret, 4);
 			__uaccess_end();
 			return ret;
 		case 8:
-			__uaccess_begin();
+			__uaccess_begin_nospec();
 			__put_user_size(*(u64 *)from, (u64 __user *)to,
 					8, ret, 8);
 			__uaccess_end();
@@ -111,17 +111,17 @@ __copy_from_user_inatomic(void *to, cons
 
 		switch (n) {
 		case 1:
-			__uaccess_begin();
+			__uaccess_begin_nospec();
 			__get_user_size(*(u8 *)to, from, 1, ret, 1);
 			__uaccess_end();
 			return ret;
 		case 2:
-			__uaccess_begin();
+			__uaccess_begin_nospec();
 			__get_user_size(*(u16 *)to, from, 2, ret, 2);
 			__uaccess_end();
 			return ret;
 		case 4:
-			__uaccess_begin();
+			__uaccess_begin_nospec();
 			__get_user_size(*(u32 *)to, from, 4, ret, 4);
 			__uaccess_end();
 			return ret;
@@ -162,17 +162,17 @@ __copy_from_user(void *to, const void __
 
 		switch (n) {
 		case 1:
-			__uaccess_begin();
+			__uaccess_begin_nospec();
 			__get_user_size(*(u8 *)to, from, 1, ret, 1);
 			__uaccess_end();
 			return ret;
 		case 2:
-			__uaccess_begin();
+			__uaccess_begin_nospec();
 			__get_user_size(*(u16 *)to, from, 2, ret, 2);
 			__uaccess_end();
 			return ret;
 		case 4:
-			__uaccess_begin();
+			__uaccess_begin_nospec();
 			__get_user_size(*(u32 *)to, from, 4, ret, 4);
 			__uaccess_end();
 			return ret;
@@ -190,17 +190,17 @@ static __always_inline unsigned long __c
 
 		switch (n) {
 		case 1:
-			__uaccess_begin();
+			__uaccess_begin_nospec();
 			__get_user_size(*(u8 *)to, from, 1, ret, 1);
 			__uaccess_end();
 			return ret;
 		case 2:
-			__uaccess_begin();
+			__uaccess_begin_nospec();
 			__get_user_size(*(u16 *)to, from, 2, ret, 2);
 			__uaccess_end();
 			return ret;
 		case 4:
-			__uaccess_begin();
+			__uaccess_begin_nospec();
 			__get_user_size(*(u32 *)to, from, 4, ret, 4);
 			__uaccess_end();
 			return ret;
--- a/arch/x86/include/asm/uaccess_64.h
+++ b/arch/x86/include/asm/uaccess_64.h
@@ -57,31 +57,31 @@ int __copy_from_user_nocheck(void *dst,
 		return copy_user_generic(dst, (__force void *)src, size);
 	switch (size) {
 	case 1:
-		__uaccess_begin();
+		__uaccess_begin_nospec();
 		__get_user_asm(*(u8 *)dst, (u8 __user *)src,
 			      ret, "b", "b", "=q", 1);
 		__uaccess_end();
 		return ret;
 	case 2:
-		__uaccess_begin();
+		__uaccess_begin_nospec();
 		__get_user_asm(*(u16 *)dst, (u16 __user *)src,
 			      ret, "w", "w", "=r", 2);
 		__uaccess_end();
 		return ret;
 	case 4:
-		__uaccess_begin();
+		__uaccess_begin_nospec();
 		__get_user_asm(*(u32 *)dst, (u32 __user *)src,
 			      ret, "l", "k", "=r", 4);
 		__uaccess_end();
 		return ret;
 	case 8:
-		__uaccess_begin();
+		__uaccess_begin_nospec();
 		__get_user_asm(*(u64 *)dst, (u64 __user *)src,
 			      ret, "q", "", "=r", 8);
 		__uaccess_end();
 		return ret;
 	case 10:
-		__uaccess_begin();
+		__uaccess_begin_nospec();
 		__get_user_asm(*(u64 *)dst, (u64 __user *)src,
 			       ret, "q", "", "=r", 10);
 		if (likely(!ret))
@@ -91,7 +91,7 @@ int __copy_from_user_nocheck(void *dst,
 		__uaccess_end();
 		return ret;
 	case 16:
-		__uaccess_begin();
+		__uaccess_begin_nospec();
 		__get_user_asm(*(u64 *)dst, (u64 __user *)src,
 			       ret, "q", "", "=r", 16);
 		if (likely(!ret))
@@ -190,7 +190,7 @@ int __copy_in_user(void __user *dst, con
 	switch (size) {
 	case 1: {
 		u8 tmp;
-		__uaccess_begin();
+		__uaccess_begin_nospec();
 		__get_user_asm(tmp, (u8 __user *)src,
 			       ret, "b", "b", "=q", 1);
 		if (likely(!ret))
@@ -201,7 +201,7 @@ int __copy_in_user(void __user *dst, con
 	}
 	case 2: {
 		u16 tmp;
-		__uaccess_begin();
+		__uaccess_begin_nospec();
 		__get_user_asm(tmp, (u16 __user *)src,
 			       ret, "w", "w", "=r", 2);
 		if (likely(!ret))
@@ -213,7 +213,7 @@ int __copy_in_user(void __user *dst, con
 
 	case 4: {
 		u32 tmp;
-		__uaccess_begin();
+		__uaccess_begin_nospec();
 		__get_user_asm(tmp, (u32 __user *)src,
 			       ret, "l", "k", "=r", 4);
 		if (likely(!ret))
@@ -224,7 +224,7 @@ int __copy_in_user(void __user *dst, con
 	}
 	case 8: {
 		u64 tmp;
-		__uaccess_begin();
+		__uaccess_begin_nospec();
 		__get_user_asm(tmp, (u64 __user *)src,
 			       ret, "q", "", "=r", 8);
 		if (likely(!ret))
--- a/arch/x86/lib/usercopy_32.c
+++ b/arch/x86/lib/usercopy_32.c
@@ -570,7 +570,7 @@ do {									\
 unsigned long __copy_to_user_ll(void __user *to, const void *from,
 				unsigned long n)
 {
-	__uaccess_begin();
+	__uaccess_begin_nospec();
 	if (movsl_is_ok(to, from, n))
 		__copy_user(to, from, n);
 	else
@@ -583,7 +583,7 @@ EXPORT_SYMBOL(__copy_to_user_ll);
 unsigned long __copy_from_user_ll(void *to, const void __user *from,
 					unsigned long n)
 {
-	__uaccess_begin();
+	__uaccess_begin_nospec();
 	if (movsl_is_ok(to, from, n))
 		__copy_user_zeroing(to, from, n);
 	else
@@ -596,7 +596,7 @@ EXPORT_SYMBOL(__copy_from_user_ll);
 unsigned long __copy_from_user_ll_nozero(void *to, const void __user *from,
 					 unsigned long n)
 {
-	__uaccess_begin();
+	__uaccess_begin_nospec();
 	if (movsl_is_ok(to, from, n))
 		__copy_user(to, from, n);
 	else
@@ -610,7 +610,7 @@ EXPORT_SYMBOL(__copy_from_user_ll_nozero
 unsigned long __copy_from_user_ll_nocache(void *to, const void __user *from,
 					unsigned long n)
 {
-	__uaccess_begin();
+	__uaccess_begin_nospec();
 #ifdef CONFIG_X86_INTEL_USERCOPY
 	if (n > 64 && cpu_has_xmm2)
 		n = __copy_user_zeroing_intel_nocache(to, from, n);
@@ -627,7 +627,7 @@ EXPORT_SYMBOL(__copy_from_user_ll_nocach
 unsigned long __copy_from_user_ll_nocache_nozero(void *to, const void __user *from,
 					unsigned long n)
 {
-	__uaccess_begin();
+	__uaccess_begin_nospec();
 #ifdef CONFIG_X86_INTEL_USERCOPY
 	if (n > 64 && cpu_has_xmm2)
 		n = __copy_user_intel_nocache(to, from, n);


