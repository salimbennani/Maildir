Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 12 Dec 2018 08:53:25 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga001.jf.intel.com (orsmga001.jf.intel.com [10.7.209.18])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id CB186580380;
	Tue, 11 Dec 2018 09:38:21 -0800 (PST)
Received: from fmsmga105.fm.intel.com ([10.1.193.10])
  by orsmga001-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 11 Dec 2018 09:38:20 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AhjoFIBTKM9vw3IOA8OvbZJTx9dpsv+yvbD5Q0YIu?=
 =?us-ascii?q?jvd0So/mwa64YRyBt8tkgFKBZ4jH8fUM07OQ7/iwHzRYqb+681k6OKRWUBEEjc?=
 =?us-ascii?q?hE1ycBO+WiTXPBEfjxciYhF95DXlI2t1uyMExSBdqsLwaK+i764jEdAAjwOhRo?=
 =?us-ascii?q?LerpBIHSk9631+ev8JHPfglEnjWwba9xIRmssQndqtQdjJd/JKo21hbHuGZDdf?=
 =?us-ascii?q?5MxWNvK1KTnhL86dm18ZV+7SleuO8v+tBZX6nicKs2UbJXDDI9M2Ao/8LrrgXM?=
 =?us-ascii?q?TRGO5nQHTGoblAdDDhXf4xH7WpfxtTb6tvZ41SKHM8D6Uaw4VDK/5KptVRTmij?=
 =?us-ascii?q?oINyQh/W/XlMJ+kaxVrhGmqRN9zY7Ue5mVOfhlc6/BYd8XX3ZNU9xNWyBdBI63?=
 =?us-ascii?q?cosBD/AGPeZdt4Tzo1wOrQCgCgmiGeji1yVHiWP33a05zu8sFgPG3As7H90Qvn?=
 =?us-ascii?q?TZt8n1NKAdUOC00KbI1S/Mb/VL1jjn8oTHbhchofSVUL92bMHfylEvGhvbglmM?=
 =?us-ascii?q?rYHpJS6Z2+oTv2SB8uZtVv6jh3QmpgxzujSj29sgh4vTio4Iyl3I7yZ0zJsvKd?=
 =?us-ascii?q?GmVEJ2YtipG4ZKuS6ALYt5WMYiTnlouCkkzr0Gvoa2fDYFyJs53R7Tcf+HfJaS?=
 =?us-ascii?q?4hLlSumRJS10hHV/eLKwnxqy8E6gxfPgVsSszlpGsi5InsPRun0DyRDf8NWLR/?=
 =?us-ascii?q?hh8ku72DuC1Rjf6uReLkA1karbJYQhwrk1lpcLtUTDHyn2mFj5jaOPdUUr5PKo?=
 =?us-ascii?q?6+L5bbXiv5OcMIF1igfgPaQ0gcG/H+s4PRYUX2SB+uSzyqfj/UvnT7VOlPE2lb?=
 =?us-ascii?q?PZsJ/CKcQBuqG5GxNV0pok6xunCzem0dcYkmcdIFNKZRKKlIzpO1DIIPDlAvaz?=
 =?us-ascii?q?mVWskDF3x//YOr3tGInCLn/GkL35Z7Zy91ZcyBYvzdBY/59UCqsOIPPvWkDrs9?=
 =?us-ascii?q?zUFB85MxGuzObhB9VwzYceWWOJAq+EP6Leq16I5uQzI+aSYI8ZoiryK/8g5/T2?=
 =?us-ascii?q?l382hUcdfbW13ZsQcH24HPNmLFuDbXrvhdcBF2EKvg0lQezuiV2CVyNTZnmoU6?=
 =?us-ascii?q?I94DE7FJypDYPZSo+xh7yB2T+xHodKaWBeFlCMDXDoep2eVPcXaCKSJcxhniYe?=
 =?us-ascii?q?Vbe7SY8szhWutA78y7p6IevY4CwYtZT/1Ndr4+3fjw099TtxD86FyWGCU3l0nn?=
 =?us-ascii?q?8URz8xxK1/olZyylGZ3ah8gvxXD9pT5/xSXwc+NJ7cyfF6Ct/oVgLAeNeJVEip?=
 =?us-ascii?q?QtG8DT4tSdIxxscEY1xhFNW6khDDwy2qDqcImLORGpw77LjQ33jrKMZ70HbJyq?=
 =?us-ascii?q?8hg14iQstMMG2mgrVy9wzSB47PjkWYmLymdaUa3C7R6miDyXCCs11fUA51Sa/F?=
 =?us-ascii?q?R2wQZlPKrdTl4UPPV6KhBq45MgRf08KCKrFFatvyjVpYQvfuI9DeY2O3m2etCh?=
 =?us-ascii?q?eE3LKMbIz2e2oD2CXRElQLkwcW/XyeLwgxGj+ho37CDDxpDV/vf0Ls8ex5qHOn?=
 =?us-ascii?q?Tk81zxuGb1F727qy4B4Vgf2cS/Uc3r8fvCchqjN0HEuy3t7MCtqAoRZhc7tYYd?=
 =?us-ascii?q?8n/FhH0mfZvRRnPpO8N6BimkIecwNvskz00xV4FIpBntYqrX8w1wVyNL+X30lH?=
 =?us-ascii?q?dzOb2pDwJKbaKm3z/BCpdq7X1UvS0NeQ+qcT9vs4r0/vsx2uFkon639nycVa02?=
 =?us-ascii?q?OA5pXWCwofSZLwUlst+xdmvb3bYyk96JnS1X1jKqS0tj7C29Q0BOoq0BqgftFf?=
 =?us-ascii?q?ML+aGw/2CcEVG8+uKOkykVizch0EJPxS9LIzP86+bfuG2bKkM/x6nD27imRL+o?=
 =?us-ascii?q?Z93VyW9yp9U+LHw4wKw/WF0QSZTTf8i1Ghv9vzmYBFYzESA2W+xTLlBI5Xeq19?=
 =?us-ascii?q?Y4ILBX2yLM2ww9V0n4TtVGJA9F6/G1MG39ekeBqTb1Dn3Qxcz18XoWGhmSajyz?=
 =?us-ascii?q?x0kjcprreQ3SDUwuTicgYHNXBPRGV4kVjsJo20hcgAXEe0dwgpiAel5UHiyqhZ?=
 =?us-ascii?q?vqt/NWrTQURPfyTsNGFtSKiwtrmDY85J9pwotz5aUOC9YVCcV778rAEW0yLlH2?=
 =?us-ascii?q?tC2j87cyumtYn+nxx/kGidNmp8rGLFecFswhfS/NzdSuRW3jYcRil4iD/XC0O4?=
 =?us-ascii?q?P9mo+9WUipjCvvq/V2KnSp1cby3rwZmcuyu84G1gGQe/kOyrmt37DQg61jf219?=
 =?us-ascii?q?pwWiXJthr8YpTr2L68MeJoZURoAF7868xnGoBxiIcwhZcQ2WQEiZWR53YIjWDz?=
 =?us-ascii?q?MdBD06LkcHUNXSILw8LS4AX93U1jL3GJyJjjWnST3MRhfMW6bX0M2i0m8c9KEq?=
 =?us-ascii?q?iU4adAnSt0pFq4sA3QbeJ8njcb1fsh9noaj/sVtwoqyyWXGqoSElVAPSzwixSI?=
 =?us-ascii?q?6MizrLlQZGaqa7Sx1VBxnda8DLGZuQFcWW30epMjHS9288V+P0jA0Hz16oH4Zt?=
 =?us-ascii?q?bQacgfuQGTkxfFl+JVMo4+luIWhSp7PmLwpWYqxPQ8jRxqwJG2poyHK3h28aK9?=
 =?us-ascii?q?Ax5YMCD1ZswJ9jHsi6ZegtiZ34S1Epp9HTULWYPiTei0HzIKqfTnKwGOHSU+qn?=
 =?us-ascii?q?iBHLrQAxSf5F16r37VEJCrKnKXJGQfzdp4XxmQP0hfgAESXDUnkZ81DAGqxMr9?=
 =?us-ascii?q?cEhn4jAd/EL3qhxJyuhwLRnwTn/fpBu0ajczUJWeLB1W7hxb50fIK8Oe6Ph/Hz?=
 =?us-ascii?q?pf/p2nowyAMWibZwVODWEUVU2IHVHjPr+y5dbe9+iUHPaxL/zLYb+Ws+xRS++I?=
 =?us-ascii?q?xY6z0oth5zuMNtuAPnhhD/0420pPR3N5G97emzUAVSMXjT/Nb9WAqRe4+y13qN?=
 =?us-ascii?q?2/8fvxVALu44uPF6VdMdF19x+qhqeDMvaahDxlJjZAypMM2XjIxaAF014PkC5u?=
 =?us-ascii?q?bSeiELQauS7JTaLdgatXDx8dayNuO8pE9aM83g9ROcHFjtP5zKJ3jvkwC11dT1?=
 =?us-ascii?q?zuhtmpZdAWI2G6LF7HGEeLNLGcKTLX2c34faO8RqNWjOhPsx2wuDCbE1LsPziZ?=
 =?us-ascii?q?ljnpUQyvPv9IjC2BIBNev4S9eA53CWf/VNLmdgG7MNhvgDw227I0gHbKNW8aMT?=
 =?us-ascii?q?dkckJNrqef7SVXgvhkH2xB73xlLfSLmiqD7unYLIoWvuVvAihui+1a53E6mPNp?=
 =?us-ascii?q?63RHSeZzlS+XstNnpnmomK+EzT8jGBpHrDBGn6qAuF1yMqXWsJ5aVjKM+BML8H?=
 =?us-ascii?q?XVCBkQodZhIsPgtroWydXVkq/3bjBY/Jac4ck0BMXOLs+Ddn07Pl6hFDfKDQct?=
 =?us-ascii?q?QySsMGbFjU1B1vqV8zndp5sir7DomZwTWvpVU0EzGvoGC0NjWtsYL9M/WjIijK?=
 =?us-ascii?q?7egtUE6GSzqDHPS8hA+JPKTPSfBbPoMjndxb1FYQYYhLb9BYcNP4b4nUt4ZR0y?=
 =?us-ascii?q?hoXHH2LKUNxNqzEnZQgx5A1P+XN9ZnYuwE+jYRH+zmUUEKuWmQU7hENUaO8s93?=
 =?us-ascii?q?+44VAsK1TiuCwgkUIpkNPpiHaadzunf/T4Zp1fFyeh7xt5CZj8WQsgNQA=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0ARAABj9Q9ch0O0hNFcCBwBAQEEAQEHB?=
 =?us-ascii?q?AEBgVEHAQELAYFVBYIRJ4wUjBCCIZc+gSQDTw8BARgTAYcuIjQJDQEDAQEBAQE?=
 =?us-ascii?q?BAgETAQEBCA0JCCkjDII2IoJlAwMBAiQZAQE4BQkBAVADMQEFARwHEgWDHIICB?=
 =?us-ascii?q?Zo6PIodgWwzgnYBAQWBBAGGIwgSh2qEPxeBf4ERh24PhU8iiRkHhx6QPwmCJo0?=
 =?us-ascii?q?FghsjYHyICjaHJCyId49pBgIJBw8hgSWCDjMaCBsVgyeCGwwXfwEHgkOKHFUiM?=
 =?us-ascii?q?gGBAQMBASETii8BAQ?=
X-IPAS-Result: =?us-ascii?q?A0ARAABj9Q9ch0O0hNFcCBwBAQEEAQEHBAEBgVEHAQELAYF?=
 =?us-ascii?q?VBYIRJ4wUjBCCIZc+gSQDTw8BARgTAYcuIjQJDQEDAQEBAQEBAgETAQEBCA0JC?=
 =?us-ascii?q?CkjDII2IoJlAwMBAiQZAQE4BQkBAVADMQEFARwHEgWDHIICBZo6PIodgWwzgnY?=
 =?us-ascii?q?BAQWBBAGGIwgSh2qEPxeBf4ERh24PhU8iiRkHhx6QPwmCJo0FghsjYHyICjaHJ?=
 =?us-ascii?q?CyId49pBgIJBw8hgSWCDjMaCBsVgyeCGwwXfwEHgkOKHFUiMgGBAQMBASETii8?=
 =?us-ascii?q?BAQ?=
X-IronPort-AV: E=Sophos;i="5.56,343,1539673200"; 
   d="scan'208";a="141682640"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 11 Dec 2018 09:38:18 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726964AbeLKRiP (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Tue, 11 Dec 2018 12:38:15 -0500
Received: from mail-yw1-f66.google.com ([209.85.161.66]:43124 "EHLO
        mail-yw1-f66.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726620AbeLKRiM (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Tue, 11 Dec 2018 12:38:12 -0500
Received: by mail-yw1-f66.google.com with SMTP id l200so5773692ywe.10
        for <linux-kernel@vger.kernel.org>; Tue, 11 Dec 2018 09:38:11 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=toxicpanda-com.20150623.gappssmtp.com; s=20150623;
        h=from:to:subject:date:message-id:in-reply-to:references;
        bh=qqsfgaIEtlZusOyH8VwHzXg9UNC/o2gxh/Ta/fayrgA=;
        b=02fJu3EwRY7TT8tr0X8A0YaVs6sc3p6RYmOCxZS5jtDz4RZu1sfQIJ8+m3+TfiNhyC
         Q5/DrtNxC5XcINKji/izNTAH2skm3lrnl+QSKt8hRk+iXlXrPosmNqKtvu9/9Aa1TzyU
         p8u2NYDqySq7AThC3WKH1EPera3SUhpCk26FCfGu6JGcEtjgDof++DM+uVS5+Kljyt3U
         7BsZuc0PHrm4/69Ive3pGwurzyq1eEySM7mJVjCOTiMo/+mM6QLkTz2hyflsK1p6gqeO
         1F4tBm4avivK4F0l+U8HDb4suvuHyKD55ApV/r2bXXQ36PluaijMie8tBt4sd6nezLAL
         LlvQ==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:from:to:subject:date:message-id:in-reply-to
         :references;
        bh=qqsfgaIEtlZusOyH8VwHzXg9UNC/o2gxh/Ta/fayrgA=;
        b=e/nmWQPCOym9u4oNxTDm49dNgI7+UvJ3ZYHMMrXFWzLOiRpwye+JYcjtu9+TybzQwo
         Wm+FfIxAhrUHamezjCKaOPxjrHtkb03TNq1z1myL6n2KkwRJcguscex2XMrMRbYOw5Q9
         MYCOPhgzkkZ9nH6MOqEwNxpuQ8JaQ2WySsGl9zUYQt05M9IwSQO4wpWkuc1VeusvFk/V
         ma8nCv3mSYgRdY5mtec+4mQD8bXTsxMm3AsnbeuqW1zZWXYwsOInOyxh6Zx6uikNNdPP
         nHJPdyQo+o6dgrEHA4oE2l+mUVMoK2ckVF1LdUM1j5arnoILMKacbfbFoG6dU/5EeWMq
         JhyQ==
X-Gm-Message-State: AA+aEWZUT9Y+yOULOgr4y8fhhq3US7LVn5+uf0hW/kIN5SFlz5h7wqZI
        8Wzwn3a6XVCEOoPM9jvcZmuvrbvb4ps=
X-Google-Smtp-Source: AFSGD/XOODM7RPX6IBhslmbdg1It8+FvTnuv6mE8nNEfnuaQOF9O9LZWPNt11BUYkh1CQmAAnJVhaQ==
X-Received: by 2002:a81:6c90:: with SMTP id h138mr16997570ywc.379.1544549891199;
        Tue, 11 Dec 2018 09:38:11 -0800 (PST)
Received: from localhost ([107.15.81.208])
        by smtp.gmail.com with ESMTPSA id l83sm4843981ywb.23.2018.12.11.09.38.09
        (version=TLS1_2 cipher=ECDHE-RSA-CHACHA20-POLY1305 bits=256/256);
        Tue, 11 Dec 2018 09:38:09 -0800 (PST)
From: Josef Bacik <josef@toxicpanda.com>
To: kernel-team@fb.com, hannes@cmpxchg.org,
        linux-kernel@vger.kernel.org, tj@kernel.org, david@fromorbit.com,
        akpm@linux-foundation.org, linux-fsdevel@vger.kernel.org,
        linux-mm@kvack.org, riel@redhat.com, jack@suse.cz
Subject: [PATCH 3/3] filemap: drop the mmap_sem for all blocking operations
Date: Tue, 11 Dec 2018 12:38:01 -0500
Message-Id: <20181211173801.29535-4-josef@toxicpanda.com>
X-Mailer: git-send-email 2.14.3
In-Reply-To: <20181211173801.29535-1-josef@toxicpanda.com>
References: <20181211173801.29535-1-josef@toxicpanda.com>
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Currently we only drop the mmap_sem if there is contention on the page
lock.  The idea is that we issue readahead and then go to lock the page
while it is under IO and we want to not hold the mmap_sem during the IO.

The problem with this is the assumption that the readahead does
anything.  In the case that the box is under extreme memory or IO
pressure we may end up not reading anything at all for readahead, which
means we will end up reading in the page under the mmap_sem.

Even if the readahead does something, it could get throttled because of
io pressure on the system and the process is in a lower priority cgroup.

Holding the mmap_sem while doing IO is problematic because it can cause
system-wide priority inversions.  Consider some large company that does
a lot of web traffic.  This large company has load balancing logic in
it's core web server, cause some engineer thought this was a brilliant
plan.  This load balancing logic gets statistics from /proc about the
system, which trip over processes mmap_sem for various reasons.  Now the
web server application is in a protected cgroup, but these other
processes may not be, and if they are being throttled while their
mmap_sem is held we'll stall, and cause this nice death spiral.

Instead rework filemap fault path to drop the mmap sem at any point that
we may do IO or block for an extended period of time.  This includes
while issuing readahead, locking the page, or needing to call ->readpage
because readahead did not occur.  Then once we have a fully uptodate
page we can return with VM_FAULT_RETRY and come back again to find our
nicely in-cache page that was gotten outside of the mmap_sem.

This patch also adds a new helper for locking the page with the mmap_sem
dropped.  This doesn't make sense currently as generally speaking if the
page is already locked it'll have been read in (unless there was an
error) before it was unlocked.  However a forthcoming patchset will
change this with the ability to abort read-ahead bio's if necessary,
making it more likely that we could contend for a page lock and still
have a not uptodate page.  This allows us to deal with this case by
grabbing the lock and issuing the IO without the mmap_sem held, and then
returning VM_FAULT_RETRY to come back around.

Acked-by: Johannes Weiner <hannes@cmpxchg.org>
Signed-off-by: Josef Bacik <josef@toxicpanda.com>
---
 mm/filemap.c | 104 ++++++++++++++++++++++++++++++++++++++++++++++++++++-------
 1 file changed, 92 insertions(+), 12 deletions(-)

diff --git a/mm/filemap.c b/mm/filemap.c
index 8fc45f24b201..10084168eff1 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -2304,28 +2304,76 @@ EXPORT_SYMBOL(generic_file_read_iter);
 
 #ifdef CONFIG_MMU
 #define MMAP_LOTSAMISS  (100)
+static struct file *maybe_unlock_mmap_for_io(struct vm_fault *vmf,
+					     struct file *fpin)
+{
+	int flags = vmf->flags;
+	if (fpin)
+		return fpin;
+	if ((flags & (FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT)) ==
+	    FAULT_FLAG_ALLOW_RETRY) {
+		fpin = get_file(vmf->vma->vm_file);
+		up_read(&vmf->vma->vm_mm->mmap_sem);
+	}
+	return fpin;
+}
+
+/*
+ * Works similar to lock_page_or_retry, except it will pin the file and drop the
+ * mmap_sem if necessary and then lock the page, and return 1 in this case.
+ * This means the caller needs to deal with the fpin appropriately.  0 return is
+ * the same as in lock_page_or_retry.
+ */
+static int lock_page_maybe_drop_mmap(struct vm_fault *vmf, struct page *page,
+				     struct file **fpin)
+{
+	if (trylock_page(page))
+		return 1;
+
+	*fpin = maybe_unlock_mmap_for_io(vmf, *fpin);
+	if (vmf->flags & FAULT_FLAG_RETRY_NOWAIT)
+		return 0;
+	if (vmf->flags & FAULT_FLAG_KILLABLE) {
+		if (__lock_page_killable(page)) {
+			/*
+			 * We didn't have the right flags to drop the mmap_sem,
+			 * but all fault_handlers only check for fatal signals
+			 * if we return VM_FAULT_RETRY, so we need to drop the
+			 * mmap_sem here and return 0 if we don't have a fpin.
+			 */
+			if (*fpin == NULL)
+				up_read(&vmf->vma->vm_mm->mmap_sem);
+			return 0;
+		}
+	} else
+		__lock_page(page);
+	return 1;
+}
+
 
 /*
  * Synchronous readahead happens when we don't even find
  * a page in the page cache at all.
  */
-static void do_sync_mmap_readahead(struct vm_fault *vmf)
+static struct file *do_sync_mmap_readahead(struct vm_fault *vmf)
 {
 	struct file *file = vmf->vma->vm_file;
 	struct file_ra_state *ra = &file->f_ra;
 	struct address_space *mapping = file->f_mapping;
+	struct file *fpin = NULL;
 	pgoff_t offset = vmf->pgoff;
 
 	/* If we don't want any read-ahead, don't bother */
 	if (vmf->vma->vm_flags & VM_RAND_READ)
-		return;
+		return fpin;
 	if (!ra->ra_pages)
-		return;
+		return fpin;
 
 	if (vmf->vma->vm_flags & VM_SEQ_READ) {
+		fpin = maybe_unlock_mmap_for_io(vmf, fpin);
 		page_cache_sync_readahead(mapping, ra, file, offset,
 					  ra->ra_pages);
-		return;
+		return fpin;
 	}
 
 	/* Avoid banging the cache line if not needed */
@@ -2337,37 +2385,43 @@ static void do_sync_mmap_readahead(struct vm_fault *vmf)
 	 * stop bothering with read-ahead. It will only hurt.
 	 */
 	if (ra->mmap_miss > MMAP_LOTSAMISS)
-		return;
+		return fpin;
 
 	/*
 	 * mmap read-around
 	 */
+	fpin = maybe_unlock_mmap_for_io(vmf, fpin);
 	ra->start = max_t(long, 0, offset - ra->ra_pages / 2);
 	ra->size = ra->ra_pages;
 	ra->async_size = ra->ra_pages / 4;
 	ra_submit(ra, mapping, file);
+	return fpin;
 }
 
 /*
  * Asynchronous readahead happens when we find the page and PG_readahead,
  * so we want to possibly extend the readahead further..
  */
-static void do_async_mmap_readahead(struct vm_fault *vmf,
-				    struct page *page)
+static struct file *do_async_mmap_readahead(struct vm_fault *vmf,
+					    struct page *page)
 {
 	struct file *file = vmf->vma->vm_file;
 	struct file_ra_state *ra = &file->f_ra;
 	struct address_space *mapping = file->f_mapping;
+	struct file *fpin = NULL;
 	pgoff_t offset = vmf->pgoff;
 
 	/* If we don't want any read-ahead, don't bother */
 	if (vmf->vma->vm_flags & VM_RAND_READ)
-		return;
+		return fpin;
 	if (ra->mmap_miss > 0)
 		ra->mmap_miss--;
-	if (PageReadahead(page))
+	if (PageReadahead(page)) {
+		fpin = maybe_unlock_mmap_for_io(vmf, fpin);
 		page_cache_async_readahead(mapping, ra, file,
 					   page, offset, ra->ra_pages);
+	}
+	return fpin;
 }
 
 /**
@@ -2397,6 +2451,7 @@ vm_fault_t filemap_fault(struct vm_fault *vmf)
 {
 	int error;
 	struct file *file = vmf->vma->vm_file;
+	struct file *fpin = NULL;
 	struct address_space *mapping = file->f_mapping;
 	struct file_ra_state *ra = &file->f_ra;
 	struct inode *inode = mapping->host;
@@ -2418,10 +2473,10 @@ vm_fault_t filemap_fault(struct vm_fault *vmf)
 		 * We found the page, so try async readahead before
 		 * waiting for the lock.
 		 */
-		do_async_mmap_readahead(vmf, page);
+		fpin = do_async_mmap_readahead(vmf, page);
 	} else if (!page) {
 		/* No page in the page cache at all */
-		do_sync_mmap_readahead(vmf);
+		fpin = do_sync_mmap_readahead(vmf);
 		count_vm_event(PGMAJFAULT);
 		count_memcg_event_mm(vmf->vma->vm_mm, PGMAJFAULT);
 		ret = VM_FAULT_MAJOR;
@@ -2433,7 +2488,7 @@ vm_fault_t filemap_fault(struct vm_fault *vmf)
 			return vmf_error(-ENOMEM);
 	}
 
-	if (!lock_page_or_retry(page, vmf->vma->vm_mm, vmf->flags)) {
+	if (!lock_page_maybe_drop_mmap(vmf, page, &fpin)) {
 		put_page(page);
 		return ret | VM_FAULT_RETRY;
 	}
@@ -2453,6 +2508,16 @@ vm_fault_t filemap_fault(struct vm_fault *vmf)
 	if (unlikely(!PageUptodate(page)))
 		goto page_not_uptodate;
 
+	/*
+	 * We've made it this far and we had to drop our mmap_sem, now is the
+	 * time to return to the upper layer and have it re-find the vma and
+	 * redo the fault.
+	 */
+	if (fpin) {
+		unlock_page(page);
+		goto out_retry;
+	}
+
 	/*
 	 * Found the page and have a reference on it.
 	 * We must recheck i_size under page lock.
@@ -2475,12 +2540,15 @@ vm_fault_t filemap_fault(struct vm_fault *vmf)
 	 * and we need to check for errors.
 	 */
 	ClearPageError(page);
+	fpin = maybe_unlock_mmap_for_io(vmf, fpin);
 	error = mapping->a_ops->readpage(file, page);
 	if (!error) {
 		wait_on_page_locked(page);
 		if (!PageUptodate(page))
 			error = -EIO;
 	}
+	if (fpin)
+		goto out_retry;
 	put_page(page);
 
 	if (!error || error == AOP_TRUNCATED_PAGE)
@@ -2489,6 +2557,18 @@ vm_fault_t filemap_fault(struct vm_fault *vmf)
 	/* Things didn't work out. Return zero to tell the mm layer so. */
 	shrink_readahead_size_eio(file, ra);
 	return VM_FAULT_SIGBUS;
+
+out_retry:
+	/*
+	 * We dropped the mmap_sem, we need to return to the fault handler to
+	 * re-find the vma and come back and find our hopefully still populated
+	 * page.
+	 */
+	if (page)
+		put_page(page);
+	if (fpin)
+		fput(fpin);
+	return ret | VM_FAULT_RETRY;
 }
 EXPORT_SYMBOL(filemap_fault);
 
-- 
2.14.3

