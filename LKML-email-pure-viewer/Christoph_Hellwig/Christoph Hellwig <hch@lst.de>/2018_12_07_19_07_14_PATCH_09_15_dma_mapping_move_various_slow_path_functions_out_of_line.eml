Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 09 Dec 2018 18:52:51 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga005.fm.intel.com (fmsmga005.fm.intel.com [10.253.24.32])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id B0FBA5804F7;
	Fri,  7 Dec 2018 11:11:32 -0800 (PST)
Received: from fmsmga104.fm.intel.com ([10.1.193.100])
  by fmsmga005-1.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 07 Dec 2018 11:11:32 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3APDYi9xFFGrn6UxTIxiHDEZ1GYnF86YWxBRYc798d?=
 =?us-ascii?q?s5kLTJ75os+4bnLW6fgltlLVR4KTs6sC17KG9fi4EUU7or+5+EgYd5JNUxJXwe?=
 =?us-ascii?q?43pCcHRPC/NEvgMfTxZDY7FskRHHVs/nW8LFQHUJ2mPw6arXK99yMdFQviPgRp?=
 =?us-ascii?q?OOv1BpTSj8Oq3Oyu5pHfeQpFiCa+bL9oMBm6sRjau9ULj4dlNqs/0AbCrGFSe+?=
 =?us-ascii?q?RRy2NoJFaTkAj568yt4pNt8Dletuw4+cJYXqr0Y6o3TbpDDDQ7KG81/9HktQPC?=
 =?us-ascii?q?TQSU+HQRVHgdnwdSDAjE6BH6WYrxsjf/u+Fg1iSWIdH6QLYpUjm58axlVAHnhz?=
 =?us-ascii?q?sGNz4h8WHYlMpwjL5AoBm8oxBz2pPYbJ2JOPZ7eK7WYNEUSndbXstJVyJPHJ6y?=
 =?us-ascii?q?YYUMAeQGP+lYoYbyqVQVrRumBwShH//jxzxSi3Pqx6A2z/gtHAfb1wIgBdIOt3?=
 =?us-ascii?q?HUoc3rOacSS+C1zbLIzSnEb/NO2Tf29YrGfQ4lofGIU7NwcMXRwlQoGgPFklqQ?=
 =?us-ascii?q?qZfoPzeO1uQRqWSU8vRvWPuphmU6qA9xuiCiytkwhoTNnI4Z117J+TtjzIooJt?=
 =?us-ascii?q?C0VFR3bN+mHZdIqi2XM497Ttk8T2xouCs20LILtJGhcCQX1pgqxBjSYOGdfYeS?=
 =?us-ascii?q?+BLsTuORLC94hH17fLK/gA6/8VavyuLiTMm4ylVKoTRfktnKqH8N0wbf6s+dSv?=
 =?us-ascii?q?ty5kuh2DCP2B7P6uxcP0w4ia7WJ4Q8zrM+iJYfq1nPEyzqlEnsjaKbdF0o+u2y?=
 =?us-ascii?q?5OTmZrXmqIWcN4hxigzmNqQum8q/Afk3MwQXXGiU5/681Lv98k39WblKifM3kq?=
 =?us-ascii?q?/Hv5DAPskbuKG5DBFP0oo56BawES2m0NIGknkDNl5FfwiHj4fxN1HUPP/4Feu/?=
 =?us-ascii?q?g0irkDpzw/DGP77hDYvXInnMjbfsZrJ9609ayAouwtFT/ZNUCrcdIP3tXk/9rs?=
 =?us-ascii?q?DXDhg8Mwas2eboFM191p8CWWKIGqKZMLndsV6U5u01JOmAfo8VuCvnJPgj6P7j?=
 =?us-ascii?q?lns5mV4bfam00pobcnG4HvJ6I0qHZXrgmMsOEWAPvgAmVuzllEWCUSJPZ3a1R6?=
 =?us-ascii?q?885DY7B5y8AYfAQYCthrqB3CCgE51SZ2BGDE2MEHjye4WFXfcMdDydIst7njMY?=
 =?us-ascii?q?UrihTpcr1Quyuw/i17pnMu3U9zUCupL41Nh14O7TmQso9TNuDcSQyGWNT2Bynm?=
 =?us-ascii?q?MVSD46xqF/oUphylid1ah0meBXFdtW5/lRSAc1KYbcz/BmC9D1Qg/Bfc2GSFC8?=
 =?us-ascii?q?TtWiADE+VNQxw9AVbkZ5GtWiiA3D3iWwD78UkbyLGII78qbG03ftIMZ9zm7M1L?=
 =?us-ascii?q?M9gFk+XstPKWqmi7Zi+AfJGY7GjV+Vl6aweqQaxy7C6mGDwW2KvEFbVQ5wVb7I?=
 =?us-ascii?q?XXQeZkvQsNT46VnOT76oCbQ7LARBzdSOJbdNat3slV9GXuvsOMzCY2KtnGe9HQ?=
 =?us-ascii?q?uHyamSbIX0YWkd3D/SCE4fkw8N+3aLLgw+Biano2LDAz1iD1PvY0Xw8eZgrHO3?=
 =?us-ascii?q?VFM7zwaPb0d5zbq65gYVheCAS/MUxr8EpCYhqzBzHFah39LXC8CMpxZ7cKVbe9?=
 =?us-ascii?q?M95FZH1WTWtwFmOpygLqZihkMRcghtvkPu0Ql3BZtEkcQwsHwqyw9yI7qC0Fxd?=
 =?us-ascii?q?bzOYwYzwOrrPJ2bo+BCgdaHX1U/e0dqM4agP9ek3pE/lvAGqEUoi7W5q091U03?=
 =?us-ascii?q?ua+5XLAxAeUZP3UkYr6Rd6o6vWbTU654PRzXdsK7W7sife29I1A+so0hahf8pF?=
 =?us-ascii?q?PKyYDgPzEs0aCNKoKOwlgFWpahMEPOZP9K87Jc+mdv2G2LK1M+Zkhj6pkWNH4I?=
 =?us-ascii?q?Vl2EKW6yV8UvLI34oCw/yAxAuHVivzg027ss/qnoBIfzcSEXSlySjlHYJeerd9?=
 =?us-ascii?q?fYIWBmiwOc23wdN+ioXpW35Z8l6jGlwH1NWoeRqUc1zywwlQ2V4LrnygnCuy1y?=
 =?us-ascii?q?Z0nC0xrqqDwCzOxPzvdRoGOmJRRGhul0zsIZWyj90BWEiobg4plAaq5Ergxqhb?=
 =?us-ascii?q?orh/IHfXQUtSYyf2KGRiWLOqtrWee85P9I8osSJPXeS+e1+aUL39oxgd0y/5BG?=
 =?us-ascii?q?tR3jM7dzKrupX/gRN6jnmQLHJyrHrfZMFxyg3T5N3aRf5NwDUGQDN0hiXQBli5?=
 =?us-ascii?q?J9Op58mbl4/fsuCiUGKsTp1SfjPszY+atiu75GtqDAa7n/CynN3nDAc73TX619?=
 =?us-ascii?q?lsSSXHshL8bpP32KS9NOJtZlNoC0Pk68pmBoF+lZM9hJIK1ngbnJmV/WcHnn31?=
 =?us-ascii?q?MdVUwq/+aHsNRTgWw9/a+gTl2UtjLm6XyIL9THmS3sxhZ9yiaGMMxi0999xKCL?=
 =?us-ascii?q?uT7LFcmCt1o1m4ohjLbflzgDgd0ucu52AAg+4SpgUt1CqdD6sWHUlZOyzsihuJ?=
 =?us-ascii?q?48q/rKVReGagb7yw2FBiktCmCbGIuhtcV2rhepc+AS9w6d1yME7L0H328I3lec?=
 =?us-ascii?q?PfbdQOth2PiBfAjvNYKJYwlvoMmCpmNnjxvXwjy+4nkxNu2Yu2s5SAK2Vo5Ki5?=
 =?us-ascii?q?GAJXNiXpZ8MP/THglb1RkdyR34CrA5VtADELXIbzQPKsETISs+nnNgmUHD09rH?=
 =?us-ascii?q?ebBaTQHQuF5Eh6qHLPFoihN2uLK3kB0dViWB6dKVRdgA8OWTU1gIU5Ghq2xMD7?=
 =?us-ascii?q?c0d5+zMR5kP+qhRW0eJlLB3/UmbZpAe1ZTY4UpmfLBxK7g5c40fZK9CR7uV2Hy?=
 =?us-ascii?q?tA5J2usBSNKnCHZwRPFWwIWkuEB036Prmz/9bA9fKUBvG5L/vIbrWDsuheV/aO?=
 =?us-ascii?q?xZKy3Ypq5TeMNsOTPnZ8C/03wFZMXXd8G87BgTUAVzQXlz7Rb86cvBq8+jN4rs?=
 =?us-ascii?q?G88PToWQLj/4iPC6FVMdVg5R+2m7qDN/WLiSZ9KDZY0I4MxHDSxLge2l4Slz9h?=
 =?us-ascii?q?dz23Hbscsi7NSbramrVLAB4DdyNzKMxI4rom0QlQIsHbkM36175igv4xCldITl?=
 =?us-ascii?q?jhmsCvZcwXLGCxLlLHBECXNLuYIT3H2d34YaS5SbdIluVbqwWwuSqHE0/kJjmD?=
 =?us-ascii?q?izjpVxW1Pe5Qli2UIBxet5+7cht2DWjjTdTmagC0MdNtjD02x6E0iW3ONWIGLT?=
 =?us-ascii?q?d8dEZNpKWK7SxEmvV/B3BB7n19IOiEgSmZ6ezYKpcQsfRzAyV0l/hV4HI1y7ZO?=
 =?us-ascii?q?6CFERfp1mDbdr9J0olGmlPWPxSRjUBZUtjlLg4eL7g1ePvDY8ZxFRF7A/QkR9i?=
 =?us-ascii?q?OUChIXt55rDcDpt6lMy9/J0qXpJ3MK19PS4NBUO8/ONsuDPGFpZQbuAiDdCAce?=
 =?us-ascii?q?ZTqqM3zPwkJajfee/2GUqZ58rYLjzt5GaLJATFEvXt8TDEtsFcYOaL1tWS4pi/?=
 =?us-ascii?q?bPhtQP+HekhAPcSMVTotbMUffEUtv1LzPMprleZgAUxqv4ZaQeMonk2kMqPllx?=
 =?us-ascii?q?nKzOAFaWUd0b8X4pVRM9vEgYqCs2dWY0wU+wL1r1uHI=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AOAACixApch0O0hNFjHAEBAQQBAQcEA?=
 =?us-ascii?q?QGBUQcBAQsBg2snjBOMEVABAQaBSYkMji+BdikTAYddIjQJDQEDAQEBAQEBAgE?=
 =?us-ascii?q?TAQEBCgsJCCkjDII2JAGCYgMDAQIkCwENAQE3AQUJAQFQA1QGARIFgxyBdQ0Fp?=
 =?us-ascii?q?QKBbDOCdgEBBYcoCIdzhC8XeIEHgRGNbok1BIFwhRCQMAmBFZArI4lzh0WJEJF?=
 =?us-ascii?q?Rgg0zGggcFIMnghsMF4hehWAfMoECAwEBIROKMQEB?=
X-IPAS-Result: =?us-ascii?q?A0AOAACixApch0O0hNFjHAEBAQQBAQcEAQGBUQcBAQsBg2s?=
 =?us-ascii?q?njBOMEVABAQaBSYkMji+BdikTAYddIjQJDQEDAQEBAQEBAgETAQEBCgsJCCkjD?=
 =?us-ascii?q?II2JAGCYgMDAQIkCwENAQE3AQUJAQFQA1QGARIFgxyBdQ0FpQKBbDOCdgEBBYc?=
 =?us-ascii?q?oCIdzhC8XeIEHgRGNbok1BIFwhRCQMAmBFZArI4lzh0WJEJFRgg0zGggcFIMng?=
 =?us-ascii?q?hsMF4hehWAfMoECAwEBIROKMQEB?=
X-IronPort-AV: E=Sophos;i="5.56,327,1539673200"; 
   d="scan'208";a="54304067"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 07 Dec 2018 11:11:30 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726045AbeLGTI3 (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Fri, 7 Dec 2018 14:08:29 -0500
Received: from bombadil.infradead.org ([198.137.202.133]:48612 "EHLO
        bombadil.infradead.org" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726203AbeLGTI2 (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Fri, 7 Dec 2018 14:08:28 -0500
DKIM-Signature: v=1; a=rsa-sha256; q=dns/txt; c=relaxed/relaxed;
        d=infradead.org; s=bombadil.20170209; h=Content-Transfer-Encoding:
        MIME-Version:References:In-Reply-To:Message-Id:Date:Subject:Cc:To:From:Sender
        :Reply-To:Content-Type:Content-ID:Content-Description:Resent-Date:Resent-From
        :Resent-Sender:Resent-To:Resent-Cc:Resent-Message-ID:List-Id:List-Help:
        List-Unsubscribe:List-Subscribe:List-Post:List-Owner:List-Archive;
        bh=SKcfjqcI9VQDKAxDSG/SrkUlhkFBUqjXnXExNajFi+k=; b=DZLFzWt9U+tnYzv04wF2xPtXPe
        dwiRiS/l90420kthszm5BNS44rgL5DMWUqO4t7S6629Wdw8aJ7RPMcrvOxzvEfvqYB2DOno5+fz6L
        vRx/N1ZWTattAMjJosv6YHxewvf5dFp+SOsaR5X5NUT+m0D3UXsgowlJ2QpdDI5FAYTO9UgNRulPX
        tQJfWhke74K7K8f6vsR0Ab+hneFDnk+3tvvzUNQ2QTnpD/GVLrT3ZFwj5FTwf50rHcBIasm6ybRaU
        NpbFFKYLNy8a7Rxyhw2n9oRPZfkeohbPzCcHNXPSOt+hPwWPLJ5ZVEX8KbQ6trBMiw7lsbZRRw25H
        mQ7nuZbA==;
Received: from [199.255.44.128] (helo=localhost)
        by bombadil.infradead.org with esmtpsa (Exim 4.90_1 #2 (Red Hat Linux))
        id 1gVLTy-0006rb-W0; Fri, 07 Dec 2018 19:08:15 +0000
From: Christoph Hellwig <hch@lst.de>
To: iommu@lists.linux-foundation.org,
        Linus Torvalds <torvalds@linux-foundation.org>,
        Jesper Dangaard Brouer <brouer@redhat.com>
Cc: Tariq Toukan <tariqt@mellanox.com>,
        Ilias Apalodimas <ilias.apalodimas@linaro.org>,
        =?UTF-8?q?Toke=20H=C3=B8iland-J=C3=B8rgensen?= <toke@toke.dk>,
        Robin Murphy <robin.murphy@arm.com>,
        Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>,
        Tony Luck <tony.luck@intel.com>,
        Fenghua Yu <fenghua.yu@intel.com>,
        Marek Szyprowski <m.szyprowski@samsung.com>,
        Keith Busch <keith.busch@intel.com>,
        Jonathan Derrick <jonathan.derrick@intel.com>,
        linux-pci@vger.kernel.org, linux-ia64@vger.kernel.org,
        x86@kernel.org, linux-kernel@vger.kernel.org
Subject: [PATCH 09/15] dma-mapping: move various slow path functions out of line
Date: Fri,  7 Dec 2018 11:07:14 -0800
Message-Id: <20181207190720.18517-10-hch@lst.de>
X-Mailer: git-send-email 2.19.1
In-Reply-To: <20181207190720.18517-1-hch@lst.de>
References: <20181207190720.18517-1-hch@lst.de>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
X-SRS-Rewrite: SMTP reverse-path rewritten from <hch@infradead.org> by bombadil.infradead.org. See http://www.infradead.org/rpr.html
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

There is no need to have all setup and coherent allocation / freeing
routines inline.  Move them out of line to keep the implemeation
nicely encapsulated and save some kernel text size.

Signed-off-by: Christoph Hellwig <hch@lst.de>
---
 arch/powerpc/include/asm/dma-mapping.h |   1 -
 include/linux/dma-mapping.h            | 150 +++----------------------
 kernel/dma/mapping.c                   | 140 ++++++++++++++++++++++-
 3 files changed, 151 insertions(+), 140 deletions(-)

diff --git a/arch/powerpc/include/asm/dma-mapping.h b/arch/powerpc/include/asm/dma-mapping.h
index 8fa394520af6..5201f2b7838c 100644
--- a/arch/powerpc/include/asm/dma-mapping.h
+++ b/arch/powerpc/include/asm/dma-mapping.h
@@ -108,7 +108,6 @@ static inline void set_dma_offset(struct device *dev, dma_addr_t off)
 }
 
 #define HAVE_ARCH_DMA_SET_MASK 1
-extern int dma_set_mask(struct device *dev, u64 dma_mask);
 
 extern u64 __dma_get_required_mask(struct device *dev);
 
diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 3b431cc58794..0bbce52606c2 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -440,107 +440,24 @@ bool dma_in_atomic_pool(void *start, size_t size);
 void *dma_alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags);
 bool dma_free_from_pool(void *start, size_t size);
 
-/**
- * dma_mmap_attrs - map a coherent DMA allocation into user space
- * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
- * @vma: vm_area_struct describing requested user mapping
- * @cpu_addr: kernel CPU-view address returned from dma_alloc_attrs
- * @handle: device-view address returned from dma_alloc_attrs
- * @size: size of memory originally requested in dma_alloc_attrs
- * @attrs: attributes of mapping properties requested in dma_alloc_attrs
- *
- * Map a coherent DMA buffer previously allocated by dma_alloc_attrs
- * into user space.  The coherent DMA buffer must not be freed by the
- * driver until the user space mapping has been released.
- */
-static inline int
-dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma, void *cpu_addr,
-	       dma_addr_t dma_addr, size_t size, unsigned long attrs)
-{
-	const struct dma_map_ops *ops = get_dma_ops(dev);
-	BUG_ON(!ops);
-	if (ops->mmap)
-		return ops->mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
-	return dma_common_mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
-}
-
+int dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma,
+		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+		unsigned long attrs);
 #define dma_mmap_coherent(d, v, c, h, s) dma_mmap_attrs(d, v, c, h, s, 0)
 
 int
 dma_common_get_sgtable(struct device *dev, struct sg_table *sgt, void *cpu_addr,
 		dma_addr_t dma_addr, size_t size, unsigned long attrs);
 
-static inline int
-dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt, void *cpu_addr,
-		      dma_addr_t dma_addr, size_t size,
-		      unsigned long attrs)
-{
-	const struct dma_map_ops *ops = get_dma_ops(dev);
-	BUG_ON(!ops);
-	if (ops->get_sgtable)
-		return ops->get_sgtable(dev, sgt, cpu_addr, dma_addr, size,
-					attrs);
-	return dma_common_get_sgtable(dev, sgt, cpu_addr, dma_addr, size,
-			attrs);
-}
-
+int dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt,
+		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+		unsigned long attrs);
 #define dma_get_sgtable(d, t, v, h, s) dma_get_sgtable_attrs(d, t, v, h, s, 0)
 
-#ifndef arch_dma_alloc_attrs
-#define arch_dma_alloc_attrs(dev)	(true)
-#endif
-
-static inline void *dma_alloc_attrs(struct device *dev, size_t size,
-				       dma_addr_t *dma_handle, gfp_t flag,
-				       unsigned long attrs)
-{
-	const struct dma_map_ops *ops = get_dma_ops(dev);
-	void *cpu_addr;
-
-	BUG_ON(!ops);
-	WARN_ON_ONCE(dev && !dev->coherent_dma_mask);
-
-	if (dma_alloc_from_dev_coherent(dev, size, dma_handle, &cpu_addr))
-		return cpu_addr;
-
-	/* let the implementation decide on the zone to allocate from: */
-	flag &= ~(__GFP_DMA | __GFP_DMA32 | __GFP_HIGHMEM);
-
-	if (!arch_dma_alloc_attrs(&dev))
-		return NULL;
-	if (!ops->alloc)
-		return NULL;
-
-	cpu_addr = ops->alloc(dev, size, dma_handle, flag, attrs);
-	debug_dma_alloc_coherent(dev, size, *dma_handle, cpu_addr);
-	return cpu_addr;
-}
-
-static inline void dma_free_attrs(struct device *dev, size_t size,
-				     void *cpu_addr, dma_addr_t dma_handle,
-				     unsigned long attrs)
-{
-	const struct dma_map_ops *ops = get_dma_ops(dev);
-
-	BUG_ON(!ops);
-
-	if (dma_release_from_dev_coherent(dev, get_order(size), cpu_addr))
-		return;
-	/*
-	 * On non-coherent platforms which implement DMA-coherent buffers via
-	 * non-cacheable remaps, ops->free() may call vunmap(). Thus getting
-	 * this far in IRQ context is a) at risk of a BUG_ON() or trying to
-	 * sleep on some machines, and b) an indication that the driver is
-	 * probably misusing the coherent API anyway.
-	 */
-	WARN_ON(irqs_disabled());
-
-	if (!ops->free || !cpu_addr)
-		return;
-
-	debug_dma_free_coherent(dev, size, cpu_addr, dma_handle);
-	ops->free(dev, size, cpu_addr, dma_handle, attrs);
-}
+void *dma_alloc_attrs(struct device *dev, size_t size, dma_addr_t *dma_handle,
+		gfp_t flag, unsigned long attrs);
+void dma_free_attrs(struct device *dev, size_t size, void *cpu_addr,
+		dma_addr_t dma_handle, unsigned long attrs);
 
 static inline void *dma_alloc_coherent(struct device *dev, size_t size,
 		dma_addr_t *dma_handle, gfp_t gfp)
@@ -565,35 +482,9 @@ static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 	return 0;
 }
 
-static inline void dma_check_mask(struct device *dev, u64 mask)
-{
-	if (sme_active() && (mask < (((u64)sme_get_me_mask() << 1) - 1)))
-		dev_warn(dev, "SME is active, device will require DMA bounce buffers\n");
-}
-
-static inline int dma_supported(struct device *dev, u64 mask)
-{
-	const struct dma_map_ops *ops = get_dma_ops(dev);
-
-	if (!ops)
-		return 0;
-	if (!ops->dma_supported)
-		return 1;
-	return ops->dma_supported(dev, mask);
-}
-
-#ifndef HAVE_ARCH_DMA_SET_MASK
-static inline int dma_set_mask(struct device *dev, u64 mask)
-{
-	if (!dev->dma_mask || !dma_supported(dev, mask))
-		return -EIO;
-
-	dma_check_mask(dev, mask);
-
-	*dev->dma_mask = mask;
-	return 0;
-}
-#endif
+int dma_supported(struct device *dev, u64 mask);
+int dma_set_mask(struct device *dev, u64 mask);
+int dma_set_coherent_mask(struct device *dev, u64 mask);
 
 static inline u64 dma_get_mask(struct device *dev)
 {
@@ -602,21 +493,6 @@ static inline u64 dma_get_mask(struct device *dev)
 	return DMA_BIT_MASK(32);
 }
 
-#ifdef CONFIG_ARCH_HAS_DMA_SET_COHERENT_MASK
-int dma_set_coherent_mask(struct device *dev, u64 mask);
-#else
-static inline int dma_set_coherent_mask(struct device *dev, u64 mask)
-{
-	if (!dma_supported(dev, mask))
-		return -EIO;
-
-	dma_check_mask(dev, mask);
-
-	dev->coherent_dma_mask = mask;
-	return 0;
-}
-#endif
-
 /*
  * Set both the DMA mask and the coherent DMA mask to the same thing.
  * Note that we don't check the return value from dma_set_coherent_mask()
diff --git a/kernel/dma/mapping.c b/kernel/dma/mapping.c
index dfe29d18dba1..176ae3e08916 100644
--- a/kernel/dma/mapping.c
+++ b/kernel/dma/mapping.c
@@ -223,7 +223,20 @@ int dma_common_get_sgtable(struct device *dev, struct sg_table *sgt,
 		sg_set_page(sgt->sgl, page, PAGE_ALIGN(size), 0);
 	return ret;
 }
-EXPORT_SYMBOL(dma_common_get_sgtable);
+
+int dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt,
+		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+		unsigned long attrs)
+{
+	const struct dma_map_ops *ops = get_dma_ops(dev);
+	BUG_ON(!ops);
+	if (ops->get_sgtable)
+		return ops->get_sgtable(dev, sgt, cpu_addr, dma_addr, size,
+					attrs);
+	return dma_common_get_sgtable(dev, sgt, cpu_addr, dma_addr, size,
+			attrs);
+}
+EXPORT_SYMBOL(dma_get_sgtable_attrs);
 
 /*
  * Create userspace mapping for the DMA-coherent memory.
@@ -261,7 +274,31 @@ int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,
 	return -ENXIO;
 #endif /* !CONFIG_ARCH_NO_COHERENT_DMA_MMAP */
 }
-EXPORT_SYMBOL(dma_common_mmap);
+
+/**
+ * dma_mmap_attrs - map a coherent DMA allocation into user space
+ * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
+ * @vma: vm_area_struct describing requested user mapping
+ * @cpu_addr: kernel CPU-view address returned from dma_alloc_attrs
+ * @dma_addr: device-view address returned from dma_alloc_attrs
+ * @size: size of memory originally requested in dma_alloc_attrs
+ * @attrs: attributes of mapping properties requested in dma_alloc_attrs
+ *
+ * Map a coherent DMA buffer previously allocated by dma_alloc_attrs into user
+ * space.  The coherent DMA buffer must not be freed by the driver until the
+ * user space mapping has been released.
+ */
+int dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma,
+		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+		unsigned long attrs)
+{
+	const struct dma_map_ops *ops = get_dma_ops(dev);
+	BUG_ON(!ops);
+	if (ops->mmap)
+		return ops->mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
+	return dma_common_mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
+}
+EXPORT_SYMBOL(dma_mmap_attrs);
 
 #ifndef ARCH_HAS_DMA_GET_REQUIRED_MASK
 static u64 dma_default_get_required_mask(struct device *dev)
@@ -294,3 +331,102 @@ u64 dma_get_required_mask(struct device *dev)
 EXPORT_SYMBOL_GPL(dma_get_required_mask);
 #endif
 
+#ifndef arch_dma_alloc_attrs
+#define arch_dma_alloc_attrs(dev)	(true)
+#endif
+
+void *dma_alloc_attrs(struct device *dev, size_t size, dma_addr_t *dma_handle,
+		gfp_t flag, unsigned long attrs)
+{
+	const struct dma_map_ops *ops = get_dma_ops(dev);
+	void *cpu_addr;
+
+	BUG_ON(!ops);
+	WARN_ON_ONCE(dev && !dev->coherent_dma_mask);
+
+	if (dma_alloc_from_dev_coherent(dev, size, dma_handle, &cpu_addr))
+		return cpu_addr;
+
+	/* let the implementation decide on the zone to allocate from: */
+	flag &= ~(__GFP_DMA | __GFP_DMA32 | __GFP_HIGHMEM);
+
+	if (!arch_dma_alloc_attrs(&dev))
+		return NULL;
+	if (!ops->alloc)
+		return NULL;
+
+	cpu_addr = ops->alloc(dev, size, dma_handle, flag, attrs);
+	debug_dma_alloc_coherent(dev, size, *dma_handle, cpu_addr);
+	return cpu_addr;
+}
+EXPORT_SYMBOL(dma_alloc_attrs);
+
+void dma_free_attrs(struct device *dev, size_t size, void *cpu_addr,
+		dma_addr_t dma_handle, unsigned long attrs)
+{
+	const struct dma_map_ops *ops = get_dma_ops(dev);
+
+	BUG_ON(!ops);
+
+	if (dma_release_from_dev_coherent(dev, get_order(size), cpu_addr))
+		return;
+	/*
+	 * On non-coherent platforms which implement DMA-coherent buffers via
+	 * non-cacheable remaps, ops->free() may call vunmap(). Thus getting
+	 * this far in IRQ context is a) at risk of a BUG_ON() or trying to
+	 * sleep on some machines, and b) an indication that the driver is
+	 * probably misusing the coherent API anyway.
+	 */
+	WARN_ON(irqs_disabled());
+
+	if (!ops->free || !cpu_addr)
+		return;
+
+	debug_dma_free_coherent(dev, size, cpu_addr, dma_handle);
+	ops->free(dev, size, cpu_addr, dma_handle, attrs);
+}
+EXPORT_SYMBOL(dma_free_attrs);
+
+static inline void dma_check_mask(struct device *dev, u64 mask)
+{
+	if (sme_active() && (mask < (((u64)sme_get_me_mask() << 1) - 1)))
+		dev_warn(dev, "SME is active, device will require DMA bounce buffers\n");
+}
+
+int dma_supported(struct device *dev, u64 mask)
+{
+	const struct dma_map_ops *ops = get_dma_ops(dev);
+
+	if (!ops)
+		return 0;
+	if (!ops->dma_supported)
+		return 1;
+	return ops->dma_supported(dev, mask);
+}
+EXPORT_SYMBOL(dma_supported);
+
+#ifndef HAVE_ARCH_DMA_SET_MASK
+int dma_set_mask(struct device *dev, u64 mask)
+{
+	if (!dev->dma_mask || !dma_supported(dev, mask))
+		return -EIO;
+
+	dma_check_mask(dev, mask);
+	*dev->dma_mask = mask;
+	return 0;
+}
+EXPORT_SYMBOL(dma_set_mask);
+#endif
+
+#ifndef CONFIG_ARCH_HAS_DMA_SET_COHERENT_MASK
+int dma_set_coherent_mask(struct device *dev, u64 mask)
+{
+	if (!dma_supported(dev, mask))
+		return -EIO;
+
+	dma_check_mask(dev, mask);
+	dev->coherent_dma_mask = mask;
+	return 0;
+}
+EXPORT_SYMBOL(dma_set_coherent_mask);
+#endif
-- 
2.19.1

