Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 30 Nov 2018 08:45:54 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga008.fm.intel.com (fmsmga008.fm.intel.com [10.253.24.58])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 56251580213;
	Thu, 29 Nov 2018 07:01:59 -0800 (PST)
Received: from fmsmga101.fm.intel.com ([10.1.193.65])
  by fmsmga008-1.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 29 Nov 2018 07:01:59 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3ALej0mRHZrOmwclWbsesevZ1GYnF86YWxBRYc798d?=
 =?us-ascii?q?s5kLTJ75osi5bnLW6fgltlLVR4KTs6sC17KG9fi4EUU7or+5+EgYd5JNUxJXwe?=
 =?us-ascii?q?43pCcHRPC/NEvgMfTxZDY7FskRHHVs/nW8LFQHUJ2mPw6arXK99yMdFQviPgRp?=
 =?us-ascii?q?OOv1BpTSj8Oq3Oyu5pHfeQpFiCa+bL9oMBm6sRjau9ULj4dlNqs/0AbCrGFSe+?=
 =?us-ascii?q?RRy2NoJFaTkAj568yt4pNt8Dletuw4+cJYXqr0Y6o3TbpDDDQ7KG81/9HktQPC?=
 =?us-ascii?q?TQSU+HQRVHgdnwdSDAjE6BH6WYrxsjf/u+Fg1iSWIdH6QLYpUjm58axlVAHnhz?=
 =?us-ascii?q?sGNz4h8WHYlMpwjL5AoBm8oxBz2pPYbJ2JOPZ7eK7WYNEUSndbXstJVyJPHJ6y?=
 =?us-ascii?q?YYUMAeQGP+lYoYfyqFQSohWxHgmsHOHixyRUhnL106A2z/4sHBvE0QEmAtkAsG?=
 =?us-ascii?q?7UrNLwNKoKUO611rfHzTreZP1Twzf975LHchA4rf+OR71wa9bRyUw1GAPDk16d?=
 =?us-ascii?q?roPlPymL2eQCsGib7/FtVeaui24htgFwrTavxsAxionPm40a0EzE9SR+wIYzP9?=
 =?us-ascii?q?G4T1R7YdG9HZZWqiqUOYx2QsY4TGFpviY30rkGuZ2+fCgO0pgnwATfa/Off4iL?=
 =?us-ascii?q?/B3jTuCRLil8hH5/f7K/nRmy/E69weP/Tsm5yEhGojZBn9XWq3wA2QLf5tKaRv?=
 =?us-ascii?q?Z+4kutwzeC2gLL5uxHL004j6nWJp87zrMzkpcfq1nPEjP0lUjwkaSYbF8r+vKy?=
 =?us-ascii?q?5OTierjmpoGTN4tzigzmLKQuldKwAf4iPggNQWeb4+K826Pn/UHjR7VKlPI2nr?=
 =?us-ascii?q?HYsJDcO8sbura0DxFJ3osn8RqzEjmr3MoCkXUaL19JZAiLgonrNl3WJfD3F/a/?=
 =?us-ascii?q?g1CikDdxwPDGO6XsApHMLnjFjbfgcq9x601Cxwopy9BQ+ZZUBqgGIPP9XE/+qs?=
 =?us-ascii?q?bYAwQnMwy73ennEs9x1oAAVmKVBK+WLqfSvUWP5uI1LOmAfJUVtyrlK/g5+/7u?=
 =?us-ascii?q?imc0mVscfamqw5Qbcn+5Hul9LkWdYHrshMoBEGgQsgo/SuzqlEONUTpJa3muWK?=
 =?us-ascii?q?I84ykxCJi6AofbWoCtnLuB0T+7HpJMZ2BGFkqDEXDye4WERvcDciSSIsB6nzwA?=
 =?us-ascii?q?VLihTZIh1B60uA/7zbpnMvTb+ikCuZ3/09h14vXZlQsu+jxsE8Sdz2aNQnl2nm?=
 =?us-ascii?q?MPWTA6xqN/oUt7yleF1qh1mPhYFd1V5/NUXQY2L5/cz+pmC9/sXgLNZMuGSFGj?=
 =?us-ascii?q?Qt++GzE+Usoxw8MSY0Z6A9itlAvD3yqtA78WjbCLHoY78qDH0nj1JsZ9zWvG1a?=
 =?us-ascii?q?Y7g1knRMtPKXOphqpl+wfPAI7Jll2Tl7y2eqQEwC7N6GCDwHKNvE5CVg58S6HF?=
 =?us-ascii?q?XXEFaUvQotT0/UfCT76oCbQ6PQpN08+CKq1WatL3iVVKXuvsONPbY2ipgWe/GQ?=
 =?us-ascii?q?6Ixq+QbIrtY2gSwT/SCFYanAwJ/XaJLw4+Bjy/rGLYCzBuEVHvY0bo8eRlrHO7?=
 =?us-ascii?q?T0k0zxyFbkF71rq1/AIViuKYS/8Jwr0EvyIhoS1uHFmhx9LWF8aApw15capBe9?=
 =?us-ascii?q?w9/klI2XjZtwNnOJygNL5thlgFfgRzvkPu0Qh3C4pancgrqnMq0BR9KaaC3Fxd?=
 =?us-ascii?q?cDOY2Ij6OqfLJWnq4BCvd6nW10nC0NaX/6cP7+g4q1XjvQ2zCkou6XJn08NR03?=
 =?us-ascii?q?ud4JXKAxEfUZbwUkYx6hh7qKvWYig754PIy3JsNbO4vSPF29IsHOEl0Aqvf89D?=
 =?us-ascii?q?MKOYEw//C9cVB8ywJ+0lhVeobggIM/tP9KEpJcymcfiG2Km1POt7mDKmjGJH4J?=
 =?us-ascii?q?1y006W9ip8TPLI0IgBw/2CwgSHUDL8hk+7ss/rgYBEeS0SHm2nxCnkGY5dfLF9?=
 =?us-ascii?q?cZwKCGeuOcK3wNp+ioXpW35Z8l6jGlwH1NWoeRqUc1zywwlQ2V4LrnygnCuy1y?=
 =?us-ascii?q?Z0nC0xrqqDwCzOxPzvdAAGOmFXXmZil0rjIY+ug9AcQkeodQkpmAK56kngw6hU?=
 =?us-ascii?q?ubpwL3PUQUdOZCX2K2BiUq2ttruNecJP6ZUosTlJX+S4e1yVVrn9ox4C2SP5A2?=
 =?us-ascii?q?Re3Cw7dy2tupjhnx16iXiRLXZpoHrCZMFwwwzS5NjdRf5XwzoHSzN0iTjRBlig?=
 =?us-ascii?q?Idap+c+YmIvEsuC7T2ihTIFccTH3zYOcsyu2/W9qDgeln/Cwmd3nFhI20TTh2N?=
 =?us-ascii?q?lpViXIrRD8bZft16mhNeJnf09oBELz6sZgG4F+lJcwi48U2XQAmpqV+n8Hm3/p?=
 =?us-ascii?q?MdpHwaL+cGYNRTkTzt/V+gfl31dvLnCIx4L/TXidxsphZ9+nYmIZwC497sZKCL?=
 =?us-ascii?q?uK47xAhyd6vl24rQfJa/hngjgd0ecu6GIdg+wRuwoi1CSdDqoSHUlFJyPskRuI?=
 =?us-ascii?q?4su6rKVWYmavbLex2FB/ndCnELGNvAVcVGzldZclGC969t9/P07U0H3v9oHkf8?=
 =?us-ascii?q?HdbdcSth2XiRjMleZUJ4wqlvoWmyVnI3nysmM/y+40lhFu2ZC6vI6aK2Ri5q65?=
 =?us-ascii?q?AxhYNiHraMMX4D3ikaFensOO1YC1ApphAikLXIfvTf+wDDISsvHnOxyPETImrH?=
 =?us-ascii?q?ebBKHfHRSe6Ep9q3LPEparN2yYJXUDzNViQgWdK1JbgAwOQDo6mZs5HBiwxMP9?=
 =?us-ascii?q?aEd5+iwR5lngpxRX0O1oMB3/UnrFqAesdzc5U5yfLBtQ7gFf6EbZK82e7uRvHy?=
 =?us-ascii?q?5G+p2tthCCKmueZw5QF2EGRlSEB0z/Prmp/dTA8fKXBu+gIPvVe7mOtfZSV/SV?=
 =?us-ascii?q?xZKsz4tm+SyDNt6UM3lmDv073FdDXH9jF8TYnTUPVzIYlybXY8GHoxe8/zV9rt?=
 =?us-ascii?q?qj//TzRALv+YyPBqNSMNVo5hC2mLqDOPSWhCpjMjZY0ZUMxXDTxbgb3V4SjTxu?=
 =?us-ascii?q?dja3HbQBsy7NULzfmqtNAxEHbCNzMdNC77gg0QlVJc7bltT126Z4jvErCldFVl?=
 =?us-ascii?q?/hmsCzacwJOWG9M1zHC1iRNLSbPj3G2MX3YaK6Sb1Ng+RYrRywuTCHE0D9OjSP?=
 =?us-ascii?q?jSXmVxeqMetUliGUIAReuJ2hchZqEWXsVs/mZQahMNBpjTw627k0hnLROG4YMD?=
 =?us-ascii?q?h8dV5Nr7KK4SNZhPV/B3JO7n5/IeaYnCaZ6vHSKowKvvtzHiR0i+Va7Wwmy7tU?=
 =?us-ascii?q?6SFIXuB6lDHOod5uvV6mlPKCyj5mUBpItzZKi5iHvURkOaXF6JZAXWzI8w4K7W?=
 =?us-ascii?q?WVEx4KvcdqCsXzu6BMzdjCjKLyJy1E89LR/MscAdDbKcOHMHU7NxrpFyXZDA8E?=
 =?us-ascii?q?TT6tKGHeiFZRkPCU9n2JsJc6roLgl4YJSr9eBxQJEaYeC0J4DJkBLY1xUzcMj7?=
 =?us-ascii?q?GWlogL6GC4oR2XQ99V7b7dUffHLu/iJ366l75NZ14oyKnkKI0Vftn0wUFtaVlS?=
 =?us-ascii?q?mIXMBlreWs1LriR9bwgy5kJX/y4tHSUIx0v5Z1b1szcoHvmuk0tz01MmbA=3D?=
 =?us-ascii?q?=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AEAACu/v9bh0O0hNFkGwEBAQEDAQEBB?=
 =?us-ascii?q?wMBAQGBUQYBAQELAYJpgQIng3mIGF+LKYIhaJZIFIEQA0wUGAsIAYd0IjQJDQE?=
 =?us-ascii?q?DAQEBAQEBAgETAQEBCA0JCCkjDII2JAGCYgMDAQIgBAsBDQEBNwEFCQEBJAIFI?=
 =?us-ascii?q?QICAwxIGQWDHAGCAQQBCqUXcHwMJ4J2AQEFRGwBhXYDBYELhmCDD4EcF4F/gRG?=
 =?us-ascii?q?CXYM6GQGBSoMcgleJLYFuhA2QN0YJhn6KKSOBWoUQijIsiU+DXYp2gUZsgSEzG?=
 =?us-ascii?q?ggoCIMnEgEMgXyHWIEphGlXPzIBgQEDAQEhE40GAQE?=
X-IPAS-Result: =?us-ascii?q?A0AEAACu/v9bh0O0hNFkGwEBAQEDAQEBBwMBAQGBUQYBAQE?=
 =?us-ascii?q?LAYJpgQIng3mIGF+LKYIhaJZIFIEQA0wUGAsIAYd0IjQJDQEDAQEBAQEBAgETA?=
 =?us-ascii?q?QEBCA0JCCkjDII2JAGCYgMDAQIgBAsBDQEBNwEFCQEBJAIFIQICAwxIGQWDHAG?=
 =?us-ascii?q?CAQQBCqUXcHwMJ4J2AQEFRGwBhXYDBYELhmCDD4EcF4F/gRGCXYM6GQGBSoMcg?=
 =?us-ascii?q?leJLYFuhA2QN0YJhn6KKSOBWoUQijIsiU+DXYp2gUZsgSEzGggoCIMnEgEMgXy?=
 =?us-ascii?q?HWIEphGlXPzIBgQEDAQEhE40GAQE?=
X-IronPort-AV: E=Sophos;i="5.56,295,1539673200"; 
   d="scan'208";a="64564725"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mga01b.intel.com with ESMTP; 29 Nov 2018 07:01:57 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S2389127AbeK3Bg7 (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Thu, 29 Nov 2018 20:36:59 -0500
Received: from mail.kernel.org ([198.145.29.99]:39600 "EHLO mail.kernel.org"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S2388410AbeK3Bg6 (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Thu, 29 Nov 2018 20:36:58 -0500
Received: from localhost (5356596B.cm-6-7b.dynamic.ziggo.nl [83.86.89.107])
        (using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
        (No client certificate requested)
        by mail.kernel.org (Postfix) with ESMTPSA id 499FC213A2;
        Thu, 29 Nov 2018 14:31:24 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=kernel.org;
        s=default; t=1543501884;
        bh=9tAUdfqXACG9zEftg+skvYofyXHrZ2lRv4kJa3RWWOM=;
        h=From:To:Cc:Subject:Date:In-Reply-To:References:From;
        b=UH5+rigi4vCagtTwybBmPFW1z4plR7wSFw5zNo436rhEKb9aUZrDdIgzhM3r4j99E
         vQV6dvR6QQzHLf7QuexTaGRJhmUqRT8J+rkUIT+qr0iPzoMFW9t4y0pl+TthOR4XE4
         WowbImpGytYLiaiI1zB+BUT6umDeauLQcIxWXbS4=
From: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
To: linux-kernel@vger.kernel.org
Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>,
        stable@vger.kernel.org, Aaron Lu <aaron.lu@intel.com>,
        Ye Xiaolong <xiaolong.ye@intel.com>,
        Patrick Bellasi <patrick.bellasi@arm.com>,
        "Peter Zijlstra (Intel)" <peterz@infradead.org>,
        Dietmar Eggemann <dietmar.eggemann@arm.com>,
        Juri Lelli <juri.lelli@redhat.com>,
        Linus Torvalds <torvalds@linux-foundation.org>,
        Morten Rasmussen <morten.rasmussen@arm.com>,
        Quentin Perret <quentin.perret@arm.com>,
        Steve Muckle <smuckle@google.com>,
        Suren Baghdasaryan <surenb@google.com>,
        Thomas Gleixner <tglx@linutronix.de>,
        Todd Kjos <tkjos@google.com>,
        Vincent Guittot <vincent.guittot@linaro.org>,
        Ingo Molnar <mingo@kernel.org>, Sasha Levin <sashal@kernel.org>
Subject: [PATCH 4.19 080/110] sched/fair: Fix cpu_util_wake() for execl type workloads
Date: Thu, 29 Nov 2018 15:12:51 +0100
Message-Id: <20181129135924.486138879@linuxfoundation.org>
X-Mailer: git-send-email 2.19.2
In-Reply-To: <20181129135921.231283053@linuxfoundation.org>
References: <20181129135921.231283053@linuxfoundation.org>
User-Agent: quilt/0.65
X-stable: review
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

4.19-stable review patch.  If anyone has any objections, please let me know.

------------------

[ Upstream commit c469933e772132aad040bd6a2adc8edf9ad6f825 ]

A ~10% regression has been reported for UnixBench's execl throughput
test by Aaron Lu and Ye Xiaolong:

  https://lkml.org/lkml/2018/10/30/765

That test is pretty simple, it does a "recursive" execve() syscall on the
same binary. Starting from the syscall, this sequence is possible:

   do_execve()
     do_execveat_common()
       __do_execve_file()
         sched_exec()
           select_task_rq_fair()          <==| Task already enqueued
             find_idlest_cpu()
               find_idlest_group()
                 capacity_spare_wake()    <==| Functions not called from
		   cpu_util_wake()           | the wakeup path

which means we can end up calling cpu_util_wake() not only from the
"wakeup path", as its name would suggest. Indeed, the task doing an
execve() syscall is already enqueued on the CPU we want to get the
cpu_util_wake() for.

The estimated utilization for a CPU computed in cpu_util_wake() was
written under the assumption that function can be called only from the
wakeup path. If instead the task is already enqueued, we end up with a
utilization which does not remove the current task's contribution from
the estimated utilization of the CPU.
This will wrongly assume a reduced spare capacity on the current CPU and
increase the chances to migrate the task on execve.

The regression is tracked down to:

 commit d519329f72a6 ("sched/fair: Update util_est only on util_avg updates")

because in that patch we turn on by default the UTIL_EST sched feature.
However, the real issue is introduced by:

 commit f9be3e5961c5 ("sched/fair: Use util_est in LB and WU paths")

Let's fix this by ensuring to always discount the task estimated
utilization from the CPU's estimated utilization when the task is also
the current one. The same benchmark of the bug report, executed on a
dual socket 40 CPUs Intel(R) Xeon(R) CPU E5-2690 v2 @ 3.00GHz machine,
reports these "Execl Throughput" figures (higher the better):

   mainline     : 48136.5 lps
   mainline+fix : 55376.5 lps

which correspond to a 15% speedup.

Moreover, since {cpu_util,capacity_spare}_wake() are not really only
used from the wakeup path, let's remove this ambiguity by using a better
matching name: {cpu_util,capacity_spare}_without().

Since we are at that, let's also improve the existing documentation.

Reported-by: Aaron Lu <aaron.lu@intel.com>
Reported-by: Ye Xiaolong <xiaolong.ye@intel.com>
Tested-by: Aaron Lu <aaron.lu@intel.com>
Signed-off-by: Patrick Bellasi <patrick.bellasi@arm.com>
Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
Cc: Dietmar Eggemann <dietmar.eggemann@arm.com>
Cc: Juri Lelli <juri.lelli@redhat.com>
Cc: Linus Torvalds <torvalds@linux-foundation.org>
Cc: Morten Rasmussen <morten.rasmussen@arm.com>
Cc: Peter Zijlstra <peterz@infradead.org>
Cc: Quentin Perret <quentin.perret@arm.com>
Cc: Steve Muckle <smuckle@google.com>
Cc: Suren Baghdasaryan <surenb@google.com>
Cc: Thomas Gleixner <tglx@linutronix.de>
Cc: Todd Kjos <tkjos@google.com>
Cc: Vincent Guittot <vincent.guittot@linaro.org>
Fixes: f9be3e5961c5 (sched/fair: Use util_est in LB and WU paths)
Link: https://lore.kernel.org/lkml/20181025093100.GB13236@e110439-lin/
Signed-off-by: Ingo Molnar <mingo@kernel.org>
Signed-off-by: Sasha Levin <sashal@kernel.org>
---
 kernel/sched/fair.c | 62 +++++++++++++++++++++++++++++++++++----------
 1 file changed, 48 insertions(+), 14 deletions(-)

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 908c9cdae2f0..1162552dc3cc 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -5672,11 +5672,11 @@ static int wake_affine(struct sched_domain *sd, struct task_struct *p,
 	return target;
 }
 
-static unsigned long cpu_util_wake(int cpu, struct task_struct *p);
+static unsigned long cpu_util_without(int cpu, struct task_struct *p);
 
-static unsigned long capacity_spare_wake(int cpu, struct task_struct *p)
+static unsigned long capacity_spare_without(int cpu, struct task_struct *p)
 {
-	return max_t(long, capacity_of(cpu) - cpu_util_wake(cpu, p), 0);
+	return max_t(long, capacity_of(cpu) - cpu_util_without(cpu, p), 0);
 }
 
 /*
@@ -5736,7 +5736,7 @@ find_idlest_group(struct sched_domain *sd, struct task_struct *p,
 
 			avg_load += cfs_rq_load_avg(&cpu_rq(i)->cfs);
 
-			spare_cap = capacity_spare_wake(i, p);
+			spare_cap = capacity_spare_without(i, p);
 
 			if (spare_cap > max_spare_cap)
 				max_spare_cap = spare_cap;
@@ -5887,8 +5887,8 @@ static inline int find_idlest_cpu(struct sched_domain *sd, struct task_struct *p
 		return prev_cpu;
 
 	/*
-	 * We need task's util for capacity_spare_wake, sync it up to prev_cpu's
-	 * last_update_time.
+	 * We need task's util for capacity_spare_without, sync it up to
+	 * prev_cpu's last_update_time.
 	 */
 	if (!(sd_flag & SD_BALANCE_FORK))
 		sync_entity_load_avg(&p->se);
@@ -6214,10 +6214,19 @@ static inline unsigned long cpu_util(int cpu)
 }
 
 /*
- * cpu_util_wake: Compute CPU utilization with any contributions from
- * the waking task p removed.
+ * cpu_util_without: compute cpu utilization without any contributions from *p
+ * @cpu: the CPU which utilization is requested
+ * @p: the task which utilization should be discounted
+ *
+ * The utilization of a CPU is defined by the utilization of tasks currently
+ * enqueued on that CPU as well as tasks which are currently sleeping after an
+ * execution on that CPU.
+ *
+ * This method returns the utilization of the specified CPU by discounting the
+ * utilization of the specified task, whenever the task is currently
+ * contributing to the CPU utilization.
  */
-static unsigned long cpu_util_wake(int cpu, struct task_struct *p)
+static unsigned long cpu_util_without(int cpu, struct task_struct *p)
 {
 	struct cfs_rq *cfs_rq;
 	unsigned int util;
@@ -6229,7 +6238,7 @@ static unsigned long cpu_util_wake(int cpu, struct task_struct *p)
 	cfs_rq = &cpu_rq(cpu)->cfs;
 	util = READ_ONCE(cfs_rq->avg.util_avg);
 
-	/* Discount task's blocked util from CPU's util */
+	/* Discount task's util from CPU's util */
 	util -= min_t(unsigned int, util, task_util(p));
 
 	/*
@@ -6238,14 +6247,14 @@ static unsigned long cpu_util_wake(int cpu, struct task_struct *p)
 	 * a) if *p is the only task sleeping on this CPU, then:
 	 *      cpu_util (== task_util) > util_est (== 0)
 	 *    and thus we return:
-	 *      cpu_util_wake = (cpu_util - task_util) = 0
+	 *      cpu_util_without = (cpu_util - task_util) = 0
 	 *
 	 * b) if other tasks are SLEEPING on this CPU, which is now exiting
 	 *    IDLE, then:
 	 *      cpu_util >= task_util
 	 *      cpu_util > util_est (== 0)
 	 *    and thus we discount *p's blocked utilization to return:
-	 *      cpu_util_wake = (cpu_util - task_util) >= 0
+	 *      cpu_util_without = (cpu_util - task_util) >= 0
 	 *
 	 * c) if other tasks are RUNNABLE on that CPU and
 	 *      util_est > cpu_util
@@ -6258,8 +6267,33 @@ static unsigned long cpu_util_wake(int cpu, struct task_struct *p)
 	 * covered by the following code when estimated utilization is
 	 * enabled.
 	 */
-	if (sched_feat(UTIL_EST))
-		util = max(util, READ_ONCE(cfs_rq->avg.util_est.enqueued));
+	if (sched_feat(UTIL_EST)) {
+		unsigned int estimated =
+			READ_ONCE(cfs_rq->avg.util_est.enqueued);
+
+		/*
+		 * Despite the following checks we still have a small window
+		 * for a possible race, when an execl's select_task_rq_fair()
+		 * races with LB's detach_task():
+		 *
+		 *   detach_task()
+		 *     p->on_rq = TASK_ON_RQ_MIGRATING;
+		 *     ---------------------------------- A
+		 *     deactivate_task()                   \
+		 *       dequeue_task()                     + RaceTime
+		 *         util_est_dequeue()              /
+		 *     ---------------------------------- B
+		 *
+		 * The additional check on "current == p" it's required to
+		 * properly fix the execl regression and it helps in further
+		 * reducing the chances for the above race.
+		 */
+		if (unlikely(task_on_rq_queued(p) || current == p)) {
+			estimated -= min_t(unsigned int, estimated,
+					   (_task_util_est(p) | UTIL_AVG_UNCHANGED));
+		}
+		util = max(util, estimated);
+	}
 
 	/*
 	 * Utilization (estimated) can exceed the CPU capacity, thus let's
-- 
2.17.1



