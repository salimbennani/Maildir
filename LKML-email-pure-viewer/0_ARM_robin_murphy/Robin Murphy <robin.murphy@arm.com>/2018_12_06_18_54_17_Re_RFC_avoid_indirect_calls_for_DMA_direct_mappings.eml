Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 07 Dec 2018 08:52:08 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga008.jf.intel.com (orsmga008.jf.intel.com [10.7.209.65])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id A6AB7580375;
	Thu,  6 Dec 2018 10:54:33 -0800 (PST)
Received: from fmsmga101.fm.intel.com ([10.1.193.65])
  by orsmga008-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 06 Dec 2018 10:54:32 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AloxB0hw54hQOwDvXCy+O+j09IxM/srCxBDY+r6Qd?=
 =?us-ascii?q?0e8WKfad9pjvdHbS+e9qxAeQG9mDu7Qc06L/iOPJYSQ4+5GPsXQPItRndiQuro?=
 =?us-ascii?q?EopTEmG9OPEkbhLfTnPGQQFcVGU0J5rTngaRAGUMnxaEfPrXKs8DUcBgvwNRZv?=
 =?us-ascii?q?JuTyB4Xek9m72/q99pHPYAhEniaxba9vJxiqsAvdsdUbj5F/Iagr0BvJpXVIe+?=
 =?us-ascii?q?VSxWx2IF+Yggjx6MSt8pN96ipco/0u+dJOXqX8ZKQ4UKdXDC86PGAv5c3krgfM?=
 =?us-ascii?q?QA2S7XYBSGoWkx5IAw/Y7BHmW5r6ryX3uvZh1CScIMb7Vq4/Vyi84Kh3SR/okC?=
 =?us-ascii?q?YHOCA/8GHLkcx7kaZXrAu8qxBj34LYZYeYP+d8cKzAZ9MXXXdPUNhfVyJBAY2y?=
 =?us-ascii?q?YYUAAOUDMulEoIf9vEMOoBmlCAmwBu7i0CNEimP40KA41ekqDAHI3BYnH9ILqH?=
 =?us-ascii?q?nasNL1O7wTUeCz0aLGyijDb+lO2Tf96InDbxcsoeuLXb1rcMrRz1MjFwbYjlWK?=
 =?us-ascii?q?sYzlPzOU2/8XvGiB6upsT/6gi2kiqwxopDWk28QiipHRi44L1lzJ8T91zJs7KN?=
 =?us-ascii?q?GmUkJ3fN2pHIdKuyybNYZ6Wt0uT3xqtSog17ELtoK3cDIXxJg72hLTcf6Kf5SO?=
 =?us-ascii?q?7xn+TuieOy14i2hgeL+nhxa970ygyurkW8mq31ZFsDBFnsPPtn8TzRzT7NaISv?=
 =?us-ascii?q?9n8kemwzaP2Bjf6uBCIU8qiarWM4AtzqI0m5YJrEjOEDH6lF/rgKKVakko4Oml?=
 =?us-ascii?q?5ub/brXjvJCcNot0ig/kMqQpn8yyGeA4MgkIX2iG9uWwzb7j8lPjQLVMkPI2lr?=
 =?us-ascii?q?DVsJfUJMQduKG5GRRY0pgs6xmhFTeqytcYkmcdLFJDZh2Hi5LlO1bUIPD3Ffu/?=
 =?us-ascii?q?mUijkC93x/DaOb3sGpHNLnnAkLj/Z7p85FNcxRE3zdBe4ZJUF74ALOjyWk/3qN?=
 =?us-ascii?q?zXEBs5PxaozObgDdV3zpkeVn6XAq+FLKPStkeF5uI1LOmNeI8aojH9J+Il5/7z?=
 =?us-ascii?q?l3A5n1AdcLKt3ZsWbnC4A/tnL1+YYXrqntcOD2MKshAiQ+ztjV2ISSRTaGqqX6?=
 =?us-ascii?q?Ig+jE7D5qrDYXERo+zmrCB3yC7HptQZmBBEV2MFXbod4OZW/YDci6SI8lhkiAa?=
 =?us-ascii?q?WrilUYMuyRautAriwbp9MuXU4jEYtY7k1NVt/eLTjhEy9Tt3D8iHyWGCVWN0k3?=
 =?us-ascii?q?gMRz832qB/vEN8xk2C0ah+n/xXC9hT6+lVXQc9MJ7W1/Z6BMzqWgLdYteJT06r?=
 =?us-ascii?q?Qta8DjE3VN4xx94ObFx7G9WtlR3D2yuqA7kIl72EHpA086Tc32TvKMZ50XrJyK?=
 =?us-ascii?q?4hj1w+SMtVKWKmnrJ/9xTUB4PRjkqWjbiqeroG0C7N7miDy3GOs19eUAJ3VaXF?=
 =?us-ascii?q?XnUfZk/NoNT950PCSaKuCLs9PgtAz86CNrVFatnzgVpaQ/fjPczUY3itlGeoGR?=
 =?us-ascii?q?aI2rSMYZL3dGoHwiXSFlIIkwAJ8naALggxGCGhrnnaDDxvE1Lvfkzt/fN/qHO9?=
 =?us-ascii?q?Uk870QWKY1d92Lqy/x4fneacRO8L3rIYpCchrC15HEq839LTDNqAuwphfaVGbd?=
 =?us-ascii?q?Mh+ltH0njZtwh8PpymIKBvnVoecwVxv0Pz2BR7EIRAkc42rHw0yAp+M76X0FRE?=
 =?us-ascii?q?dzmAx5D/JqXXKnXu/BCoc6PZwFXe38iZ+6gR6PU0sU7svBy0GUU49XVn0N5V02?=
 =?us-ascii?q?WH65XODQoSV4/xU0kt+xh7obHafjcy54fO2XJwNqm0tyfI28g1C+s91hagY9Bf?=
 =?us-ascii?q?PbuEFQ/vCcEVG9KiKe0qm1ezaBIEM/tf9Ko1P8OgavuH17SnPOdmnDK6k2tH5J?=
 =?us-ascii?q?px3V6L9yp5UuTIxYoKw+mE3gubUDfxlE2hssHrlo9efzEdA22/xTLiBIFPfK1y?=
 =?us-ascii?q?fJ8HBnu0LM2z29pxmYTtW3le9FO4A1MG2cmpeQedblDn3A1Q01gXrmKjmSei0z?=
 =?us-ascii?q?N0lDQppLKF3CPS2+TiaAYHOmlTSWhijFfgO4i1g8oBXEi1aQgkjx+l5Uf8x6hG?=
 =?us-ascii?q?q6VzNWjTQUFUfyfoK2FuSLe/tr2HY8RX8pMnrT1XUPigYVCdUrP9oQEV0zngH2?=
 =?us-ascii?q?tdwzA3bSqqtY/6nxx5iWKdKmh8rHzCdMF0xBff4sHcRPFL0joHQil4lSfYBlym?=
 =?us-ascii?q?M9a1+tWUko/JsvqiWGK5Sp1TbS7rwJuAtSSh4m1mGx+/n/G1mtD8FQg60Cn718?=
 =?us-ascii?q?RlVCnSrRb8ZJXr2Lq+Me59YkZoA1r84dJgGo5iioswmI0Q2X8Ci5WW53UHkH3/?=
 =?us-ascii?q?MdVG2a3kanoNSiUGw9rU4AjjxU1iIWiFx4P/VnWB3MRhY8O2bX8R2iI498pKEr?=
 =?us-ascii?q?ub7KRYnStppVq1tQfRYfl+njgH0/cv5mAVg/oVuAUz1CWSGa4dHVNXPSH3kxSI?=
 =?us-ascii?q?7ta+rLhYZWq1cLiw0lZ+ks6lDL2Yvg5cX3P5cI84HSBs9sV/LE7M0Hrr54H4f9?=
 =?us-ascii?q?nQaMgftxyOnBfGkuhVM4kxlvsRiCpjOGL9u2AlyuEhgRxv25G6oJaIK2F38K2l?=
 =?us-ascii?q?BR5YMyX/Z9kP9TH1kaZegsGW0pi0EZp7HTULWIboQeisEDIPrvnnMweOEDshqn?=
 =?us-ascii?q?aUA7bfHAmf6Ft4oHLLCZykK3aXJHwBx9V4WBadPFBfgBwTXDginJ42DAWqy9L6?=
 =?us-ascii?q?cEtj+jAd/F34qgZPyuJ1MRnwSHzfqRysajc1TpifMRVX4htD50fTLcyR8OZzEz?=
 =?us-ascii?q?tE8Z2mqQyHMnabaBhQDWEVRkyEAEjuP7mp5dnd6uiYG/CxL/3UbbWVruxeUfiI?=
 =?us-ascii?q?yImr0otn+TaMK8qOMmNjD/09xkpMQ3R5F97FlDUITiwdjzjNYNKDpBeg5i13qd?=
 =?us-ascii?q?iy8PT1VwLu5ouPCLpSPc9s+xCshqeDOPCfhDxkKTZDzZ4MwX7IyL4C3F8dkS1u?=
 =?us-ascii?q?dj+tEageui7JVq7fhqhXDxsDYSNpKMRI97483hVKOcPDkNz1y6V3juQrBFZFT1?=
 =?us-ascii?q?DhnsCpaNcOI2G8MlPHGUmKOK6HJT3N38H4f6e8RadMg+VTsh26oSybHFP7PjSf?=
 =?us-ascii?q?iznpUAiiMf1NjCGeJhBRpJuxfQptCWf9StLrcRm7MN5xjT0rzrw4nHLKNWgAMT?=
 =?us-ascii?q?did0NBtKGf7SRdgv9nAWxO8mJlLfWYmyae9+TZKowZsf1uAiR1keJV+HU7y7tP?=
 =?us-ascii?q?4yFCS/x4gy/Srt9oo1G7neiD0DtnUBxSqjlVgIKHp1ltOaLc9soIZXGR1xIX7H?=
 =?us-ascii?q?6MDAwK7/toDtnwuq8YntjElIr3MysE/9+CuYM4Bs7JM4q7N2A/OB7lA3aAEAwf?=
 =?us-ascii?q?UzevOXr3g01bjeHX+HqIqJQztpnrntwJULANB3IvEfZPMENvHdpKBYptRjM12e?=
 =?us-ascii?q?qBi8QJ4zy7sQPJRNRyt4rCEPmVBKO8e36ikbBYak5QkvvDJoMJO9i+ghQ6Zw?=
 =?us-ascii?q?=3D=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AJAAC0bwlch0O0hNFkGwEBAQEDAQEBB?=
 =?us-ascii?q?wMBAQGBUgUBAQELAYFaghEng3qUJ4FgLRSXOoFsMxMBh1ciNQgNAQMBAQEBAQE?=
 =?us-ascii?q?CARMBAQEIDQkIKS+CNiQBgmEBAQEBAwECIAQRQQYJAQEKDgMBAgECAQICJgICA?=
 =?us-ascii?q?0YGCAYBDAYCAQEBgxyCAgQBpgV8M4VAhGaBC4sUgVc/gREngW1+iAWCVwKJEYZ?=
 =?us-ascii?q?shW+KcwcCgiAEjxweiXKHRIkOkUUBggszGiODPIInF38BDI0RPwEBMYEFAQGKO?=
 =?us-ascii?q?gEB?=
X-IPAS-Result: =?us-ascii?q?A0AJAAC0bwlch0O0hNFkGwEBAQEDAQEBBwMBAQGBUgUBAQE?=
 =?us-ascii?q?LAYFaghEng3qUJ4FgLRSXOoFsMxMBh1ciNQgNAQMBAQEBAQECARMBAQEIDQkIK?=
 =?us-ascii?q?S+CNiQBgmEBAQEBAwECIAQRQQYJAQEKDgMBAgECAQICJgICA0YGCAYBDAYCAQE?=
 =?us-ascii?q?BgxyCAgQBpgV8M4VAhGaBC4sUgVc/gREngW1+iAWCVwKJEYZshW+KcwcCgiAEj?=
 =?us-ascii?q?xweiXKHRIkOkUUBggszGiODPIInF38BDI0RPwEBMYEFAQGKOgEB?=
X-IronPort-AV: E=Sophos;i="5.56,323,1539673200"; 
   d="scan'208";a="65667073"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mga01b.intel.com with ESMTP; 06 Dec 2018 10:54:29 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726076AbeLFSyZ (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Thu, 6 Dec 2018 13:54:25 -0500
Received: from foss.arm.com ([217.140.101.70]:58772 "EHLO foss.arm.com"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1726011AbeLFSyV (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Thu, 6 Dec 2018 13:54:21 -0500
Received: from usa-sjc-imap-foss1.foss.arm.com (unknown [10.72.51.249])
        by usa-sjc-mx-foss1.foss.arm.com (Postfix) with ESMTP id E27B7A78;
        Thu,  6 Dec 2018 10:54:20 -0800 (PST)
Received: from [10.1.196.75] (e110467-lin.cambridge.arm.com [10.1.196.75])
        by usa-sjc-imap-foss1.foss.arm.com (Postfix) with ESMTPSA id 6691A3F5AF;
        Thu,  6 Dec 2018 10:54:19 -0800 (PST)
Subject: Re: [RFC] avoid indirect calls for DMA direct mappings
To: Christoph Hellwig <hch@lst.de>,
        Linus Torvalds <torvalds@linux-foundation.org>
Cc: iommu@lists.linux-foundation.org, brouer@redhat.com,
        tariqt@mellanox.com, ilias.apalodimas@linaro.org, toke@toke.dk,
        Linux List Kernel Mailing <linux-kernel@vger.kernel.org>
References: <20181206153720.10702-1-hch@lst.de>
 <CAHk-=wgz6PFCm+XMWN31dRHzA3JcNP0x0Y-z5NMMw3dHhrDXAw@mail.gmail.com>
 <20181206184330.GB30039@lst.de>
From: Robin Murphy <robin.murphy@arm.com>
Message-ID: <173bfba7-033d-93c4-6ef1-48c9e39c9efc@arm.com>
Date: Thu, 6 Dec 2018 18:54:17 +0000
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101
 Thunderbird/60.2.1
MIME-Version: 1.0
In-Reply-To: <20181206184330.GB30039@lst.de>
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Language: en-GB
Content-Transfer-Encoding: 7bit
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On 06/12/2018 18:43, Christoph Hellwig wrote:
> On Thu, Dec 06, 2018 at 10:28:22AM -0800, Linus Torvalds wrote:
>> which is counterproductive because it checks for the fast case *after*
>> you've done a load (and a check) that is entirely pointless for the
>> fast case.
>>
>> Put another way, you made the fast case unnecessarily slow.
>>
>> If this fast case matters (and the whole point of the series is that
>> it *does* matter), then you should make sure that
>>
>>   (a) the dma_is_direct() function/macro uses "likely()" for the test
> 
> I'm a little worried about that.  Yes, for the common case it is
> likely.  But if you run a setup where you say always have an iommu
> it is not, in fact it is never called in that case, but we only know
> that at runtime.
> 
>>   (b) the code is organized like this instead:
>>
>> +       if (dma_is_direct(ops))
>> +               dma_direct_unmap_page(dev, addr, size, dir, attrs);
>> +       else if (ops->unmap_page)
>> +               ops->unmap_page(dev, addr, size, dir, attrs);
>>
>> because now you avoid that unnecessary conditional for the common
>> case, and you hopefully never even access the pointer (because you
>> know what's behind it: the direct ops cases that you're
>> short-circuiting).
> 
> Agreed on that, I've fixed it up now (current patch attached below).
> 
>> In fact, as a further micro-optimization it might be a good idea to
>> just specify that the dma_is_direct() ops is a special pointer
>> (perhaps even just say that "NULL means it's direct"), because that
>> then makes the fast-case test much simpler (avoids a whole nasty
>> constant load, and testing for NULL in particular is often much
>> better).
> 
> So I wanted to do the NULL case, and it would be much nicer.  But the
> arm folks went to great length to make sure they don't have a default
> set of dma ops and require it to be explicitly set on every device
> to catch cases where people don't set things up properly and I didn't
> want to piss them off..  But maybe I should just go for it and see who
> screams, as the benefit is pretty obvious.

I'm pretty sure we used to assign dummy_dma_ops explicitly to devices at 
the point we detected the ACPI properties are wrong - that shouldn't be 
too much of a headache to go back to.

Robin.

> 
> ---
>  From 89cc8ab03798177339ad916efabd320e792ee034 Mon Sep 17 00:00:00 2001
> From: Christoph Hellwig <hch@lst.de>
> Date: Mon, 3 Dec 2018 16:49:29 +0100
> Subject: dma-mapping: bypass indirect calls for dma-direct
> 
> Avoid expensive indirect calls in the fast path DMA mapping
> operations by directly calling the dma_direct_* ops if we are using
> the directly mapped DMA operations.
> 
> Signed-off-by: Christoph Hellwig <hch@lst.de>
> ---
>   include/linux/dma-direct.h  |  17 ------
>   include/linux/dma-mapping.h | 110 ++++++++++++++++++++++++++++++++----
>   kernel/dma/direct.c         |  13 +++--
>   3 files changed, 106 insertions(+), 34 deletions(-)
> 
> diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
> index 3b0a3ea3876d..b7338702592a 100644
> --- a/include/linux/dma-direct.h
> +++ b/include/linux/dma-direct.h
> @@ -60,22 +60,5 @@ void dma_direct_free_pages(struct device *dev, size_t size, void *cpu_addr,
>   struct page *__dma_direct_alloc_pages(struct device *dev, size_t size,
>   		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs);
>   void __dma_direct_free_pages(struct device *dev, size_t size, struct page *page);
> -dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
> -		unsigned long offset, size_t size, enum dma_data_direction dir,
> -		unsigned long attrs);
> -void dma_direct_unmap_page(struct device *dev, dma_addr_t addr,
> -		size_t size, enum dma_data_direction dir, unsigned long attrs);
> -int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,
> -		enum dma_data_direction dir, unsigned long attrs);
> -void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,
> -		int nents, enum dma_data_direction dir, unsigned long attrs);
> -void dma_direct_sync_single_for_device(struct device *dev,
> -		dma_addr_t addr, size_t size, enum dma_data_direction dir);
> -void dma_direct_sync_sg_for_device(struct device *dev,
> -		struct scatterlist *sgl, int nents, enum dma_data_direction dir);
> -void dma_direct_sync_single_for_cpu(struct device *dev,
> -		dma_addr_t addr, size_t size, enum dma_data_direction dir);
> -void dma_direct_sync_sg_for_cpu(struct device *dev,
> -		struct scatterlist *sgl, int nents, enum dma_data_direction dir);
>   int dma_direct_supported(struct device *dev, u64 mask);
>   #endif /* _LINUX_DMA_DIRECT_H */
> diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
> index 8916499d2805..98e4dd67c906 100644
> --- a/include/linux/dma-mapping.h
> +++ b/include/linux/dma-mapping.h
> @@ -221,6 +221,69 @@ static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
>   }
>   #endif
>   
> +static inline bool dma_is_direct(const struct dma_map_ops *ops)
> +{
> +	return IS_ENABLED(CONFIG_DMA_DIRECT_OPS) && ops == &dma_direct_ops;
> +}
> +
> +/*
> + * All the dma_direct_* declarations are here just for the indirect call bypass,
> + * and must not be used directly drivers!
> + */
> +dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
> +		unsigned long offset, size_t size, enum dma_data_direction dir,
> +		unsigned long attrs);
> +int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,
> +		enum dma_data_direction dir, unsigned long attrs);
> +
> +#if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \
> +    defined(CONFIG_SWIOTLB)
> +void dma_direct_sync_single_for_device(struct device *dev,
> +		dma_addr_t addr, size_t size, enum dma_data_direction dir);
> +void dma_direct_sync_sg_for_device(struct device *dev,
> +		struct scatterlist *sgl, int nents, enum dma_data_direction dir);
> +#else
> +static inline void dma_direct_sync_single_for_device(struct device *dev,
> +		dma_addr_t addr, size_t size, enum dma_data_direction dir)
> +{
> +}
> +static inline void dma_direct_sync_sg_for_device(struct device *dev,
> +		struct scatterlist *sgl, int nents, enum dma_data_direction dir)
> +{
> +}
> +#endif
> +
> +#if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \
> +    defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU_ALL) || \
> +    defined(CONFIG_SWIOTLB)
> +void dma_direct_unmap_page(struct device *dev, dma_addr_t addr,
> +		size_t size, enum dma_data_direction dir, unsigned long attrs);
> +void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,
> +		int nents, enum dma_data_direction dir, unsigned long attrs);
> +void dma_direct_sync_single_for_cpu(struct device *dev,
> +		dma_addr_t addr, size_t size, enum dma_data_direction dir);
> +void dma_direct_sync_sg_for_cpu(struct device *dev,
> +		struct scatterlist *sgl, int nents, enum dma_data_direction dir);
> +#else
> +static inline void dma_direct_unmap_page(struct device *dev, dma_addr_t addr,
> +		size_t size, enum dma_data_direction dir, unsigned long attrs)
> +{
> +}
> +static inline void dma_direct_unmap_sg(struct device *dev,
> +		struct scatterlist *sgl, int nents, enum dma_data_direction dir,
> +		unsigned long attrs)
> +{
> +}
> +static inline void dma_direct_sync_single_for_cpu(struct device *dev,
> +		dma_addr_t addr, size_t size, enum dma_data_direction dir)
> +{
> +}
> +static inline void dma_direct_sync_sg_for_cpu(struct device *dev,
> +		struct scatterlist *sgl, int nents, enum dma_data_direction dir)
> +{
> +}
> +#endif
> +
>   static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,
>   					      size_t size,
>   					      enum dma_data_direction dir,
> @@ -231,9 +294,12 @@ static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,
>   
>   	BUG_ON(!valid_dma_direction(dir));
>   	debug_dma_map_single(dev, ptr, size);
> -	addr = ops->map_page(dev, virt_to_page(ptr),
> -			     offset_in_page(ptr), size,
> -			     dir, attrs);
> +	if (dma_is_direct(ops))
> +		addr = dma_direct_map_page(dev, virt_to_page(ptr),
> +				offset_in_page(ptr), size, dir, attrs);
> +	else
> +		addr = ops->map_page(dev, virt_to_page(ptr),
> +				offset_in_page(ptr), size, dir, attrs);
>   	debug_dma_map_page(dev, virt_to_page(ptr),
>   			   offset_in_page(ptr), size,
>   			   dir, addr, true);
> @@ -248,7 +314,9 @@ static inline void dma_unmap_single_attrs(struct device *dev, dma_addr_t addr,
>   	const struct dma_map_ops *ops = get_dma_ops(dev);
>   
>   	BUG_ON(!valid_dma_direction(dir));
> -	if (ops->unmap_page)
> +	if (dma_is_direct(ops))
> +		dma_direct_unmap_page(dev, addr, size, dir, attrs);
> +	else if (ops->unmap_page)
>   		ops->unmap_page(dev, addr, size, dir, attrs);
>   	debug_dma_unmap_page(dev, addr, size, dir, true);
>   }
> @@ -265,7 +333,10 @@ static inline int dma_map_sg_attrs(struct device *dev, struct scatterlist *sg,
>   	int ents;
>   
>   	BUG_ON(!valid_dma_direction(dir));
> -	ents = ops->map_sg(dev, sg, nents, dir, attrs);
> +	if (dma_is_direct(ops))
> +		ents = dma_direct_map_sg(dev, sg, nents, dir, attrs);
> +	else
> +		ents = ops->map_sg(dev, sg, nents, dir, attrs);
>   	BUG_ON(ents < 0);
>   	debug_dma_map_sg(dev, sg, nents, ents, dir);
>   
> @@ -280,7 +351,9 @@ static inline void dma_unmap_sg_attrs(struct device *dev, struct scatterlist *sg
>   
>   	BUG_ON(!valid_dma_direction(dir));
>   	debug_dma_unmap_sg(dev, sg, nents, dir);
> -	if (ops->unmap_sg)
> +	if (dma_is_direct(ops))
> +		dma_direct_unmap_sg(dev, sg, nents, dir, attrs);
> +	else if (ops->unmap_sg)
>   		ops->unmap_sg(dev, sg, nents, dir, attrs);
>   }
>   
> @@ -294,7 +367,10 @@ static inline dma_addr_t dma_map_page_attrs(struct device *dev,
>   	dma_addr_t addr;
>   
>   	BUG_ON(!valid_dma_direction(dir));
> -	addr = ops->map_page(dev, page, offset, size, dir, attrs);
> +	if (dma_is_direct(ops))
> +		addr = dma_direct_map_page(dev, page, offset, size, dir, attrs);
> +	else
> +		addr = ops->map_page(dev, page, offset, size, dir, attrs);
>   	debug_dma_map_page(dev, page, offset, size, dir, addr, false);
>   
>   	return addr;
> @@ -308,7 +384,9 @@ static inline void dma_unmap_page_attrs(struct device *dev,
>   	const struct dma_map_ops *ops = get_dma_ops(dev);
>   
>   	BUG_ON(!valid_dma_direction(dir));
> -	if (ops->unmap_page)
> +	if (dma_is_direct(ops))
> +		dma_direct_unmap_page(dev, addr, size, dir, attrs);
> +	else if (ops->unmap_page)
>   		ops->unmap_page(dev, addr, size, dir, attrs);
>   	debug_dma_unmap_page(dev, addr, size, dir, false);
>   }
> @@ -355,7 +433,9 @@ static inline void dma_sync_single_for_cpu(struct device *dev, dma_addr_t addr,
>   	const struct dma_map_ops *ops = get_dma_ops(dev);
>   
>   	BUG_ON(!valid_dma_direction(dir));
> -	if (ops->sync_single_for_cpu)
> +	if (dma_is_direct(ops))
> +		dma_direct_sync_single_for_cpu(dev, addr, size, dir);
> +	else if (ops->sync_single_for_cpu)
>   		ops->sync_single_for_cpu(dev, addr, size, dir);
>   	debug_dma_sync_single_for_cpu(dev, addr, size, dir);
>   }
> @@ -374,7 +454,9 @@ static inline void dma_sync_single_for_device(struct device *dev,
>   	const struct dma_map_ops *ops = get_dma_ops(dev);
>   
>   	BUG_ON(!valid_dma_direction(dir));
> -	if (ops->sync_single_for_device)
> +	if (dma_is_direct(ops))
> +		dma_direct_sync_single_for_device(dev, addr, size, dir);
> +	else if (ops->sync_single_for_device)
>   		ops->sync_single_for_device(dev, addr, size, dir);
>   	debug_dma_sync_single_for_device(dev, addr, size, dir);
>   }
> @@ -393,7 +475,9 @@ dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg,
>   	const struct dma_map_ops *ops = get_dma_ops(dev);
>   
>   	BUG_ON(!valid_dma_direction(dir));
> -	if (ops->sync_sg_for_cpu)
> +	if (dma_is_direct(ops))
> +		dma_direct_sync_sg_for_cpu(dev, sg, nelems, dir);
> +	else if (ops->sync_sg_for_cpu)
>   		ops->sync_sg_for_cpu(dev, sg, nelems, dir);
>   	debug_dma_sync_sg_for_cpu(dev, sg, nelems, dir);
>   }
> @@ -405,7 +489,9 @@ dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
>   	const struct dma_map_ops *ops = get_dma_ops(dev);
>   
>   	BUG_ON(!valid_dma_direction(dir));
> -	if (ops->sync_sg_for_device)
> +	if (dma_is_direct(ops))
> +		dma_direct_sync_sg_for_device(dev, sg, nelems, dir);
> +	else if (ops->sync_sg_for_device)
>   		ops->sync_sg_for_device(dev, sg, nelems, dir);
>   	debug_dma_sync_sg_for_device(dev, sg, nelems, dir);
>   
> diff --git a/kernel/dma/direct.c b/kernel/dma/direct.c
> index 372c1955454a..cd535e7c67d7 100644
> --- a/kernel/dma/direct.c
> +++ b/kernel/dma/direct.c
> @@ -223,6 +223,7 @@ void dma_direct_sync_single_for_device(struct device *dev,
>   	if (!dev_is_dma_coherent(dev))
>   		arch_sync_dma_for_device(dev, paddr, size, dir);
>   }
> +EXPORT_SYMBOL(dma_direct_sync_single_for_device);
>   
>   void dma_direct_sync_sg_for_device(struct device *dev,
>   		struct scatterlist *sgl, int nents, enum dma_data_direction dir)
> @@ -240,6 +241,7 @@ void dma_direct_sync_sg_for_device(struct device *dev,
>   					dir);
>   	}
>   }
> +EXPORT_SYMBOL(dma_direct_sync_sg_for_device);
>   #endif
>   
>   #if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \
> @@ -258,6 +260,7 @@ void dma_direct_sync_single_for_cpu(struct device *dev,
>   	if (is_swiotlb_buffer(paddr))
>   		swiotlb_tbl_sync_single(dev, paddr, size, dir, SYNC_FOR_CPU);
>   }
> +EXPORT_SYMBOL(dma_direct_sync_single_for_cpu);
>   
>   void dma_direct_sync_sg_for_cpu(struct device *dev,
>   		struct scatterlist *sgl, int nents, enum dma_data_direction dir)
> @@ -277,6 +280,7 @@ void dma_direct_sync_sg_for_cpu(struct device *dev,
>   	if (!dev_is_dma_coherent(dev))
>   		arch_sync_dma_for_cpu_all(dev);
>   }
> +EXPORT_SYMBOL(dma_direct_sync_sg_for_cpu);
>   
>   void dma_direct_unmap_page(struct device *dev, dma_addr_t addr,
>   		size_t size, enum dma_data_direction dir, unsigned long attrs)
> @@ -289,6 +293,7 @@ void dma_direct_unmap_page(struct device *dev, dma_addr_t addr,
>   	if (is_swiotlb_buffer(phys))
>   		swiotlb_tbl_unmap_single(dev, phys, size, dir, attrs);
>   }
> +EXPORT_SYMBOL(dma_direct_unmap_page);
>   
>   void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,
>   		int nents, enum dma_data_direction dir, unsigned long attrs)
> @@ -300,11 +305,7 @@ void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,
>   		dma_direct_unmap_page(dev, sg->dma_address, sg_dma_len(sg), dir,
>   			     attrs);
>   }
> -#else
> -void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,
> -		int nents, enum dma_data_direction dir, unsigned long attrs)
> -{
> -}
> +EXPORT_SYMBOL(dma_direct_unmap_sg);
>   #endif
>   
>   static inline bool dma_direct_possible(struct device *dev, dma_addr_t dma_addr,
> @@ -331,6 +332,7 @@ dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
>   		arch_sync_dma_for_device(dev, phys, size, dir);
>   	return dma_addr;
>   }
> +EXPORT_SYMBOL(dma_direct_map_page);
>   
>   int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,
>   		enum dma_data_direction dir, unsigned long attrs)
> @@ -352,6 +354,7 @@ int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,
>   	dma_direct_unmap_sg(dev, sgl, i, dir, attrs | DMA_ATTR_SKIP_CPU_SYNC);
>   	return 0;
>   }
> +EXPORT_SYMBOL(dma_direct_map_sg);
>   
>   /*
>    * Because 32-bit DMA masks are so common we expect every architecture to be
> 
