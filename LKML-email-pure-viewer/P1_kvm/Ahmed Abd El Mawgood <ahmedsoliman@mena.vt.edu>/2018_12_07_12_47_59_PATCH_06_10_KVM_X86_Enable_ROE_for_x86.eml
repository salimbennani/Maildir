Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 07 Dec 2018 21:02:56 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga001.jf.intel.com (orsmga001.jf.intel.com [10.7.209.18])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id E27125804F7;
	Fri,  7 Dec 2018 04:49:27 -0800 (PST)
Received: from fmsmga104.fm.intel.com ([10.1.193.100])
  by orsmga001-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 07 Dec 2018 04:49:26 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3A/bywDxQrph5dBoDc6x++Ej5Ve9psv+yvbD5Q0YIu?=
 =?us-ascii?q?jvd0So/mwa64bRCAt8tkgFKBZ4jH8fUM07OQ7/iwHzRYqb+681k6OKRWUBEEjc?=
 =?us-ascii?q?hE1ycBO+WiTXPBEfjxciYhF95DXlI2t1uyMExSBdqsLwaK+i764jEdAAjwOhRo?=
 =?us-ascii?q?LerpBIHSk9631+ev8JHPfglEnjWwba9xIRmssQndqtQdjJd/JKo21hbHuGZDdf?=
 =?us-ascii?q?5MxWNvK1KTnhL86dm18ZV+7SleuO8v+tBZX6nicKs2UbJXDDI9M2Ao/8LrrgXM?=
 =?us-ascii?q?TRGO5nQHTGoblAdDDhXf4xH7WpfxtTb6tvZ41SKHM8D6Uaw4VDK/5KpwVhTmlD?=
 =?us-ascii?q?kIOCI48GHPi8x/kqRboA66pxdix4LYeZyZOOZicq/Ye94RWGhPUdtLVyFZAo2y?=
 =?us-ascii?q?cZYBAeQCM+hfrYb9qVQBoxSlBQm0Bu7i0SNEi3zs0KEmyektDR3K0Qo9FNwOqn?=
 =?us-ascii?q?TUq9D1Ob8OXOC1yanH0yjMZO5K1Djm9YfDbx8vofWRVrx3a8XQx0YvFwTCjlqN?=
 =?us-ascii?q?tIfoOCma1uQIs2eF8uVgTuWvi2omqwF0uDevwNwhiozXiYIT0F/I7zt5wJovKd?=
 =?us-ascii?q?KmVUF7fMepHZ1NvC+ZL4t7Wt0uT31stSog17ELt4C3cDIXxJkk2xLTcf2KfoqQ?=
 =?us-ascii?q?7h7+VOucIC10iGx4dL+xnRq/9Uutxvf4W8Wo1ltBszBLncPWtn8X0hze8siHRe?=
 =?us-ascii?q?V5/kemwTuPyQ/T5f9eIUwulqrUNYQhwrgumZoXq0jDGTX2mErugK+XcEUr5PSo?=
 =?us-ascii?q?5vz5brn6opKQLZJ4hw/gPqg0h8CyAvg0PhIPUmWZ4ei80afs/Uz9QLVElP02la?=
 =?us-ascii?q?zZvYjeJcQaoK65HgBU3p8g6xmhFTem1soXnX0eIFJCdhOHiZbmO0vVLfDmAvew?=
 =?us-ascii?q?nU6snC1ox//YJL3hBIvCLnzZnLfmZ7Z95FZQyBAvwtBH+5JUFrYBLer3Wk/wt9?=
 =?us-ascii?q?zXEAU1MgOpw+v8DNV914UeWX+AA6ODMaPSt0OI6fwrI+WWeIAVvzP9IeA/5/Hy?=
 =?us-ascii?q?lX85hUMdfa6x0JsXcn+4H+hmLF+eYXb2gtcBDH0FvgwxTOHxjF2CUDhTZ2u9Xq?=
 =?us-ascii?q?4m5zE7Dp6mApnHRoy3nLOB2yK7FIVMZm9aElCMDWvod4KcVvcObyKdPNVtkj8D?=
 =?us-ascii?q?VbinTY8h0gqjtAv7y7phM+rV9TcUtZPl1Nhp+eLTkQs++iBzD8SYy2uNVX17nn?=
 =?us-ascii?q?sURz8q26ByuVZyykyD0ah/gPxUD8ZT6OlLUgohMZ7czup6C839Ww7bf9eJTkqm?=
 =?us-ascii?q?TcuiAT0rUt0xxNoOaV5nG9q+lhDDwzaqA7gNmryIHpM09LjQ33zwJ8lnzXbG27?=
 =?us-ascii?q?Isj10nQstJKG2nibRz9wnVB47VjUqZk7ymergb3C7I7G2D13aBvFlEUA5sVqXI?=
 =?us-ascii?q?RW0QaVHIrdvn/E/CT6WhCbI8MgRfz86OLa9Kat7sjVVCX/rjPNXeY2Ssm2a/Hx?=
 =?us-ascii?q?qIx7WMbJb0dGUZxinSFE8EkwUL93acKQc+Hjuho37ZDDF2CF3geV3s/vdkpHO7?=
 =?us-ascii?q?VEA0yRqKYFNn17eu/h4VhPqcS+4c374euSchrSl0E0i5397MF9WAoA9hdr1GYd?=
 =?us-ascii?q?wh+FdHyX7ZtwtlM5y8LqBig1kecxh3v0LuzRl3Fp9Mkc8wrHMuzQpyL62Y3UhF?=
 =?us-ascii?q?dzOZ25DwJ7LWJnPz/BCpd67ZxFXe3MyK9acI7fQys0/jsx2xFko+73Vn1MFY3G?=
 =?us-ascii?q?GY5prUAwsdT5LwXlws+Bhnur7VeC8954DT1X1yKqS0tj7C29Q0BOoq0BqgftFf?=
 =?us-ascii?q?ML+aGw/2CcEVG8+uKOkykVizch0EJPxS9LIzP86+d/qGxbSnM/p6kDOnjWRI+o?=
 =?us-ascii?q?Z90k2X+ip4S+7I2YsFwv6C0guGUTf8kEmussTtlY9YYjESG3K1yTL4C45Jeq1y?=
 =?us-ascii?q?YYELBH+0I8222tpxnYTtVGNf9FK5AVMJxtWpeRuLY1PhxwJQ0VkYrmK9mSu/yT?=
 =?us-ascii?q?x5iDUprquZ3CzTzOXubhsHOmhXRGZ8iVfgO5S7j9cfXEKwdQgmiAOl5Vrmx6hc?=
 =?us-ascii?q?vKl+L2jTTV1IfiTsNGFiT62wu6GGY85O7pMorCpWXP69YVCcVr7yvR8a3zn/EG?=
 =?us-ascii?q?tZwTAxbyuqtYnhnxxmlGKdK25+rHjDdsFqxhff59vcSeRK3jUcRyl4ijjXBl6i?=
 =?us-ascii?q?MNmv5tiUk5bDsuajV2OuTJFTcC/rzZ+euyu//2FlHRq/n/WrkN39DQc6yTP718?=
 =?us-ascii?q?VtVSjQrBfzeI7r2764MeJmeEllH1v868t8GoFjnYo8npAQ2X4GhpqL+XoLi3v8?=
 =?us-ascii?q?MdJe2ajmdnoCWSYLw8LJ4AjiwEBjLWiGx4PjWnWd38tufd+6YmwN1yI57sBKDr?=
 =?us-ascii?q?qU7bNekSt0pFq4sRzeYfxnkjgBzvsu7WYQg/sVtwo10iWdHrcSEFFdPSztlBSH?=
 =?us-ascii?q?9cqyratJa2a0bbi/ylB+ksu/A7GYrQFRQ3L5epYkHS9t4cRzKlPM0Hvv6o76fN?=
 =?us-ascii?q?ncd84cthqRkx3YlehaNIoxluYWhSpgIW/9oXoly+shgR1vx566upWHK352/KK4?=
 =?us-ascii?q?GRNYMjz1Z8UO+jDil6pen8CW35yxEZVlADkEQJzoTfewGjIIqfvnLxqOECE7qn?=
 =?us-ascii?q?qDArXQBwif6EN7r37VF5CrKmqaJH0YzdVkWRmcK1ZTgAESXDUmgJE5Ehqmy9Dm?=
 =?us-ascii?q?cEd8/joR/EL3qgNQyuJ0MBnySmXfqx2tajgqU5iTNgZW4htB50fIMsye8+RzED?=
 =?us-ascii?q?tD/pC6qAyNK2qbZxlHDG0TW0yEAUzjMaeq5dXa7+eYAe++JePUYbqSsexeS+uI?=
 =?us-ascii?q?xZW334pm+DaMK9yPPmR4AP09wEZDRnd5G8LWmzgUTywXliTNb9OUpRum+y13qN?=
 =?us-ascii?q?y//+rvWA71+YSPDL5SO811+x+qmaeDK/KQhCFhJDZYyJwMxHzIyLse3FIIiCFu?=
 =?us-ascii?q?bT6tEbseui7XUaLQgbRaDxoaayN1KctJ4Lgw3ghLOc7HlNz10qR0geIyC1dATV?=
 =?us-ascii?q?bhgN2mZdQWI2GhM1PKHFyENKmdKj3R3c72YbmwSblRjOhPsx2wuDCbE1LsPziZ?=
 =?us-ascii?q?ljnpUQyvPv9IjC2BIBNev4S9eA53CWf/VNLmdgG7MNhvgD0qwL00g2nGOnIGPT?=
 =?us-ascii?q?dgc0NCsLuQ4DhcgvV+HWxB83VkIfOFmyaf8+nXNJIWveF3DSRzkuJQ+G46xKdN?=
 =?us-ascii?q?7CFYWPx1nzPfr950rFGgl+mPyzxnXwJPqzZLno2Lu0piNL7d9plBX3bE4R0M4X?=
 =?us-ascii?q?+RCxQMu9tqFNnvt7pMxdjIkaKgYAtFpurd+s8bT+LTLtCGNntpZQLjHzjGDQ0E?=
 =?us-ascii?q?ZSSmOWHWmwpWl/TEsjWPrYIxr5H2sIADRr9SSBo+EfZeQmZiGtUeIJ5xFhgji7?=
 =?us-ascii?q?SWkIZc7n2kphTNbMFFuNbBW+7ERb3TKDGchKIMSB8FwKP+IJ5bYpH031xKblh8?=
 =?us-ascii?q?gZjQHEzRTZZBr3slJi4yr0IF1X96SGQpkxbgawWi51cJGPK0lwJwgQx7N6Bl3j?=
 =?us-ascii?q?D24kwzL0SCnCYsk04v0YHvmRiYdD/8Kvf2UYwAT2L0r0EZIILnRBwzZgq32QR0?=
 =?us-ascii?q?ZGnsRL9LibZkM2dxh0uUuoVKMeBTQLcCYxIKw/yTIfIy3hAUriSh2F8C6ezOIY?=
 =?us-ascii?q?VtmRFscpO2qX9EnQV5Y5p9Ia3WOboMzVVKgK+KljGn2/p3wwIEIUsJtmSId2pA?=
 =?us-ascii?q?sUwNO7U9IC6p++VqsRzcxWVrd20FVv5sqfVvsgs6JeWLz3q4+7FGI0G1ceeYKu?=
 =?us-ascii?q?fRuGXak8OMB1M5zE8FkUhD8pBy0Nw/aAyTT00p07KVE1ICMs+GYQdUYMdWsnXf?=
 =?us-ascii?q?Yi+Uvf7lwYp4eY66E6SgZ+iUtb1cpl+nFR0pEppEus4FA5qly0HVM+/8IbsdxA?=
 =?us-ascii?q?8z5Q/3OU7DB/NMLlbDsTgAuYmE0JhvxYAVcjASBn87Kjmw/q7ejgAvifuHGtwx?=
 =?us-ascii?q?ZyFJcJEDMycfWMi3l2ZwtXgIIT3/9+sGxw+F43eoryTVATXnaN5iYf6STRZhDt?=
 =?us-ascii?q?jw5yl5/qSr3w2Euq7CLn33YIwx8uTE7vkX8tPeU6tZ?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AUAABxawpch0O0hNFjGwEBAQEDAQEBB?=
 =?us-ascii?q?wMBAQGBVAMBAQELAYNrJ5gjgiFolmeBEANFDwoBARgTAYdbIjcGDQEDAQEBAQE?=
 =?us-ascii?q?BAgETAQEBCA0JCCkjDII2JAGCYgMDAQIkCwENAQE3AQUJAQEYOAMxAQUBHAYBE?=
 =?us-ascii?q?gWDHIICBZktPIodgWwzgnYBAQWBAAQBAYRYB4FECBKHXoMTgRwXgX+BEYJdhHu?=
 =?us-ascii?q?GFok5lzAJhkaKegsYgVyIBzaHH4kQj2QCBAIEBQIFDyGBO4F3MxolgzqCGwwXg?=
 =?us-ascii?q?0qKHAE4PjIBgQMBAQE7igUBAQ?=
X-IPAS-Result: =?us-ascii?q?A0AUAABxawpch0O0hNFjGwEBAQEDAQEBBwMBAQGBVAMBAQE?=
 =?us-ascii?q?LAYNrJ5gjgiFolmeBEANFDwoBARgTAYdbIjcGDQEDAQEBAQEBAgETAQEBCA0JC?=
 =?us-ascii?q?CkjDII2JAGCYgMDAQIkCwENAQE3AQUJAQEYOAMxAQUBHAYBEgWDHIICBZktPIo?=
 =?us-ascii?q?dgWwzgnYBAQWBAAQBAYRYB4FECBKHXoMTgRwXgX+BEYJdhHuGFok5lzAJhkaKe?=
 =?us-ascii?q?gsYgVyIBzaHH4kQj2QCBAIEBQIFDyGBO4F3MxolgzqCGwwXg0qKHAE4PjIBgQM?=
 =?us-ascii?q?BAQE7igUBAQ?=
X-IronPort-AV: E=Sophos;i="5.56,326,1539673200"; 
   d="scan'208";a="54250546"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 07 Dec 2018 04:49:25 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726245AbeLGMtV (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Fri, 7 Dec 2018 07:49:21 -0500
Received: from mail-wm1-f66.google.com ([209.85.128.66]:55083 "EHLO
        mail-wm1-f66.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726232AbeLGMtT (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Fri, 7 Dec 2018 07:49:19 -0500
Received: by mail-wm1-f66.google.com with SMTP id z18so4280325wmc.4
        for <linux-kernel@vger.kernel.org>; Fri, 07 Dec 2018 04:49:18 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=mena-vt-edu.20150623.gappssmtp.com; s=20150623;
        h=from:to:cc:subject:date:message-id:in-reply-to:references
         :mime-version:content-transfer-encoding;
        bh=/9gDVqCBZ049LKb2pdMtzCpXTWMx0MNMSAP/7qdzAU8=;
        b=wF+nBG3cjnEZpuGz6d/ySLU7qurr0iM9ZSElOZJcdROYTaI9Iuww33oSoSX3nVLZgK
         LI1tobWF3Cl0SMuv1HuDit78xuPV7CkWdp08z8nAhH0X4IAjTRZf3/igJtO3NPzWt46w
         E76b/eBmzw5He4ux0Rw7lum+5Q9Va3jIHuB+u4BhPtgRmKSB3OSdVvjONKSKGPPtvo7m
         Xj/i1oxYXP77qMGSdy6eBI79hnAJjgG343RI8A4Bt7jOgLMJbTtlqrvePZ5zsPFhwTYr
         LtqhVX305jRBe2+n/dU0lXIAZnAKwImyQaBxNRcqsijwVHW/Dj2biYArOswiv1MY0Wpx
         DmHg==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=/9gDVqCBZ049LKb2pdMtzCpXTWMx0MNMSAP/7qdzAU8=;
        b=GZh+Pkpu5DlgfeL5TmRc2kbg8lM1vOCMH7PJ3laVJTkJSCZdrCFtnkmQqlRJEsTaOv
         ZsVCwUP91ok13Hd89ojoXaaSxoau44QDjNHAdI0H8ieIfY6aBAN+c2OZfJWhGSWjWq1W
         vBDjTYkHmqZUFhjzZ7e72s29YCRea3cO/IqjhSogUZ1IYUhQ3+eESTh9PQEIw5O0hxei
         V/Zx/vV+Gt7UdMKGDYJ7damASCwhZ8Da3/Wt52EvqTJmnFx8EciJRDq2cZM8CfyaERq2
         vMWjS6qlQsBg1FMYEZ939+rXT3pOS9CMajF5Rd0J0WZiN49OTXhvPxjMGjskqKMzHPaL
         RqbQ==
X-Gm-Message-State: AA+aEWap14iW5kMY4Ffez0Eiup5gspWevNjdlLZmIYPlERp2QthrXB7r
        FG0XGkNLCSBOYHXa22P8/5RtWw==
X-Google-Smtp-Source: AFSGD/W6HMCVGJvb2LASGsxHG2J4z2voN3mU9kftOEqBLpneCzqhmhbID2nd9AKebT7DrQGXQAxSaQ==
X-Received: by 2002:a1c:c87:: with SMTP id 129mr2001330wmm.116.1544186957122;
        Fri, 07 Dec 2018 04:49:17 -0800 (PST)
Received: from localhost.localdomain ([156.213.98.90])
        by smtp.gmail.com with ESMTPSA id i192sm4362949wmg.7.2018.12.07.04.49.12
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Fri, 07 Dec 2018 04:49:16 -0800 (PST)
From: Ahmed Abd El Mawgood <ahmedsoliman@mena.vt.edu>
To: Paolo Bonzini <pbonzini@redhat.com>, rkrcmar@redhat.com,
        Jonathan Corbet <corbet@lwn.net>,
        Thomas Gleixner <tglx@linutronix.de>,
        Ingo Molnar <mingo@redhat.com>, Borislav Petkov <bp@alien8.de>,
        hpa@zytor.com, x86@kernel.org, kvm@vger.kernel.org,
        linux-doc@vger.kernel.org, linux-kernel@vger.kernel.org,
        ahmedsoliman0x666@gmail.com, ovich00@gmail.com,
        kernel-hardening@lists.openwall.com, nigel.edwards@hpe.com,
        Boris Lukashev <blukashev@sempervictus.com>,
        Igor Stoppa <igor.stoppa@gmail.com>
Cc: Ahmed Abd El Mawgood <ahmedsoliman@mena.vt.edu>
Subject: [PATCH 06/10] KVM: X86: Enable ROE for x86
Date: Fri,  7 Dec 2018 14:47:59 +0200
Message-Id: <20181207124803.10828-7-ahmedsoliman@mena.vt.edu>
X-Mailer: git-send-email 2.19.2
In-Reply-To: <20181207124803.10828-1-ahmedsoliman@mena.vt.edu>
References: <20181207124803.10828-1-ahmedsoliman@mena.vt.edu>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

This patch implements kvm_roe_arch_commit_protection and
kvm_roe_arch_is_userspace for x86, and invoke kvm_roe via the
appropriate vmcall.

Signed-off-by: Ahmed Abd El Mawgood <ahmedsoliman@mena.vt.edu>
---
 arch/x86/include/asm/kvm_host.h |   2 +-
 arch/x86/kvm/Makefile           |   4 +-
 arch/x86/kvm/mmu.c              |  71 +++++-----------------
 arch/x86/kvm/mmu.h              |  30 +++++++++-
 arch/x86/kvm/roe.c              | 101 ++++++++++++++++++++++++++++++++
 arch/x86/kvm/roe_arch.h         |  28 +++++++++
 arch/x86/kvm/x86.c              |  11 ++--
 7 files changed, 183 insertions(+), 64 deletions(-)
 create mode 100644 arch/x86/kvm/roe.c
 create mode 100644 arch/x86/kvm/roe_arch.h

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index fbda5a917c..e56903e6ff 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1230,7 +1230,7 @@ void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,
 		u64 acc_track_mask, u64 me_mask);
 
 void kvm_mmu_reset_context(struct kvm_vcpu *vcpu);
-void kvm_mmu_slot_remove_write_access(struct kvm *kvm,
+void kvm_mmu_slot_apply_write_access(struct kvm *kvm,
 				      struct kvm_memory_slot *memslot);
 void kvm_mmu_zap_collapsible_sptes(struct kvm *kvm,
 				   const struct kvm_memory_slot *memslot);
diff --git a/arch/x86/kvm/Makefile b/arch/x86/kvm/Makefile
index dc4f2fdf5e..a8c915a326 100644
--- a/arch/x86/kvm/Makefile
+++ b/arch/x86/kvm/Makefile
@@ -9,7 +9,9 @@ CFLAGS_vmx.o := -I.
 KVM := ../../../virt/kvm
 
 kvm-y			+= $(KVM)/kvm_main.o $(KVM)/coalesced_mmio.o \
-				$(KVM)/eventfd.o $(KVM)/irqchip.o $(KVM)/vfio.o
+			   $(KVM)/eventfd.o $(KVM)/irqchip.o $(KVM)/vfio.o \
+			   $(KVM)/roe.o roe.o
+
 kvm-$(CONFIG_KVM_ASYNC_PF)	+= $(KVM)/async_pf.o
 
 kvm-y			+= x86.o mmu.o emulate.o i8259.o irq.o lapic.o \
diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c
index a300e4acb8..fde565c8a1 100644
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@ -23,7 +23,7 @@
 #include "x86.h"
 #include "kvm_cache_regs.h"
 #include "cpuid.h"
-
+#include "roe_arch.h"
 #include <linux/kvm_host.h>
 #include <linux/types.h>
 #include <linux/string.h>
@@ -1314,8 +1314,8 @@ static void pte_list_remove(struct kvm_rmap_head *rmap_head, u64 *sptep)
 	__pte_list_remove(sptep, rmap_head);
 }
 
-static struct kvm_rmap_head *__gfn_to_rmap(gfn_t gfn, int level,
-					   struct kvm_memory_slot *slot)
+struct kvm_rmap_head *__gfn_to_rmap(gfn_t gfn, int level,
+		struct kvm_memory_slot *slot)
 {
 	unsigned long idx;
 
@@ -1365,16 +1365,6 @@ static void rmap_remove(struct kvm *kvm, u64 *spte)
 	__pte_list_remove(spte, rmap_head);
 }
 
-/*
- * Used by the following functions to iterate through the sptes linked by a
- * rmap.  All fields are private and not assumed to be used outside.
- */
-struct rmap_iterator {
-	/* private fields */
-	struct pte_list_desc *desc;	/* holds the sptep if not NULL */
-	int pos;			/* index of the sptep */
-};
-
 /*
  * Iteration must be started by this function.  This should also be used after
  * removing/dropping sptes from the rmap link because in such cases the
@@ -1382,8 +1372,7 @@ struct rmap_iterator {
  *
  * Returns sptep if found, NULL otherwise.
  */
-static u64 *rmap_get_first(struct kvm_rmap_head *rmap_head,
-			   struct rmap_iterator *iter)
+u64 *rmap_get_first(struct kvm_rmap_head *rmap_head, struct rmap_iterator *iter)
 {
 	u64 *sptep;
 
@@ -1409,7 +1398,7 @@ static u64 *rmap_get_first(struct kvm_rmap_head *rmap_head,
  *
  * Returns sptep if found, NULL otherwise.
  */
-static u64 *rmap_get_next(struct rmap_iterator *iter)
+u64 *rmap_get_next(struct rmap_iterator *iter)
 {
 	u64 *sptep;
 
@@ -1480,7 +1469,7 @@ static void drop_large_spte(struct kvm_vcpu *vcpu, u64 *sptep)
  *
  * Return true if tlb need be flushed.
  */
-static bool spte_write_protect(u64 *sptep, bool pt_protect)
+bool spte_write_protect(u64 *sptep, bool pt_protect)
 {
 	u64 spte = *sptep;
 
@@ -1498,8 +1487,7 @@ static bool spte_write_protect(u64 *sptep, bool pt_protect)
 }
 
 static bool __rmap_write_protect(struct kvm *kvm,
-				 struct kvm_rmap_head *rmap_head,
-				 bool pt_protect, void *data)
+		struct kvm_rmap_head *rmap_head, bool pt_protect)
 {
 	u64 *sptep;
 	struct rmap_iterator iter;
@@ -1598,7 +1586,7 @@ static void kvm_mmu_write_protect_pt_masked(struct kvm *kvm,
 	while (mask) {
 		rmap_head = __gfn_to_rmap(slot->base_gfn + gfn_offset + __ffs(mask),
 					  PT_PAGE_TABLE_LEVEL, slot);
-		__rmap_write_protect(kvm, rmap_head, false, NULL);
+		__rmap_write_protect(kvm, rmap_head, false);
 
 		/* clear the first set bit */
 		mask &= mask - 1;
@@ -1668,22 +1656,6 @@ int kvm_arch_write_log_dirty(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
-bool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,
-				    struct kvm_memory_slot *slot, u64 gfn)
-{
-	struct kvm_rmap_head *rmap_head;
-	int i;
-	bool write_protected = false;
-
-	for (i = PT_PAGE_TABLE_LEVEL; i <= PT_MAX_HUGEPAGE_LEVEL; ++i) {
-		rmap_head = __gfn_to_rmap(gfn, i, slot);
-		write_protected |= __rmap_write_protect(kvm, rmap_head, true,
-				NULL);
-	}
-
-	return write_protected;
-}
-
 static bool rmap_write_protect(struct kvm_vcpu *vcpu, u64 gfn)
 {
 	struct kvm_memory_slot *slot;
@@ -5524,10 +5496,6 @@ void kvm_mmu_uninit_vm(struct kvm *kvm)
 	kvm_page_track_unregister_notifier(kvm, node);
 }
 
-/* The return value indicates if tlb flush on all vcpus is needed. */
-typedef bool (*slot_level_handler) (struct kvm *kvm,
-		struct kvm_rmap_head *rmap_head, void *data);
-
 /* The caller should hold mmu-lock before calling this function. */
 static __always_inline bool
 slot_handle_level_range(struct kvm *kvm, struct kvm_memory_slot *memslot,
@@ -5571,9 +5539,8 @@ slot_handle_level(struct kvm *kvm, struct kvm_memory_slot *memslot,
 			lock_flush_tlb, data);
 }
 
-static __always_inline bool
-slot_handle_all_level(struct kvm *kvm, struct kvm_memory_slot *memslot,
-		      slot_level_handler fn, bool lock_flush_tlb, void *data)
+bool slot_handle_all_level(struct kvm *kvm, struct kvm_memory_slot *memslot,
+		slot_level_handler fn, bool lock_flush_tlb, void *data)
 {
 	return slot_handle_level(kvm, memslot, fn, PT_PAGE_TABLE_LEVEL,
 				 PT_MAX_HUGEPAGE_LEVEL, lock_flush_tlb, data);
@@ -5625,21 +5592,15 @@ static bool slot_rmap_write_protect(struct kvm *kvm,
 				    struct kvm_rmap_head *rmap_head,
 				    void *data)
 {
-	return __rmap_write_protect(kvm, rmap_head, false, data);
+	return __rmap_write_protect(kvm, rmap_head, false);
 }
 
-void kvm_mmu_slot_remove_write_access(struct kvm *kvm,
-				      struct kvm_memory_slot *memslot)
+void kvm_mmu_slot_apply_write_access(struct kvm *kvm,
+		struct kvm_memory_slot *memslot)
 {
-	bool flush;
-
-	spin_lock(&kvm->mmu_lock);
-	flush = slot_handle_all_level(kvm, memslot, slot_rmap_write_protect,
-				      false, NULL);
-	spin_unlock(&kvm->mmu_lock);
-
+	bool flush = protect_all_levels(kvm, memslot);
 	/*
-	 * kvm_mmu_slot_remove_write_access() and kvm_vm_ioctl_get_dirty_log()
+	 * kvm_mmu_slot_apply_write_access() and kvm_vm_ioctl_get_dirty_log()
 	 * which do tlb flush out of mmu-lock should be serialized by
 	 * kvm->slots_lock otherwise tlb flush would be missed.
 	 */
@@ -5736,7 +5697,7 @@ void kvm_mmu_slot_largepage_remove_write_access(struct kvm *kvm,
 					false, NULL);
 	spin_unlock(&kvm->mmu_lock);
 
-	/* see kvm_mmu_slot_remove_write_access */
+	/* see kvm_mmu_slot_apply_write_access*/
 	lockdep_assert_held(&kvm->slots_lock);
 
 	if (flush)
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h
index 49d7f2f002..35b46a6a0a 100644
--- a/arch/x86/kvm/mmu.h
+++ b/arch/x86/kvm/mmu.h
@@ -4,7 +4,7 @@
 
 #include <linux/kvm_host.h>
 #include "kvm_cache_regs.h"
-
+#include "roe_arch.h"
 #define PT64_PT_BITS 9
 #define PT64_ENT_PER_PAGE (1 << PT64_PT_BITS)
 #define PT32_PT_BITS 10
@@ -43,6 +43,24 @@
 #define PT32_ROOT_LEVEL 2
 #define PT32E_ROOT_LEVEL 3
 
+#define for_each_rmap_spte(_rmap_head_, _iter_, _spte_)			\
+	for (_spte_ = rmap_get_first(_rmap_head_, _iter_);		\
+			_spte_; _spte_ = rmap_get_next(_iter_))
+
+/*
+ * Used by the following functions to iterate through the sptes linked by a
+ * rmap.  All fields are private and not assumed to be used outside.
+ */
+struct rmap_iterator {
+	/* private fields */
+	struct pte_list_desc *desc;     /* holds the sptep if not NULL */
+	int pos;                        /* index of the sptep */
+};
+
+u64 *rmap_get_first(struct kvm_rmap_head *rmap_head,
+		struct rmap_iterator *iter);
+u64 *rmap_get_next(struct rmap_iterator *iter);
+bool spte_write_protect(u64 *sptep, bool pt_protect);
 static inline u64 rsvd_bits(int s, int e)
 {
 	if (e < s)
@@ -203,13 +221,19 @@ static inline u8 permission_fault(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,
 	return -(u32)fault & errcode;
 }
 
+/* The return value indicates if tlb flush on all vcpus is needed. */
+typedef bool (*slot_level_handler) (struct kvm *kvm,
+		struct kvm_rmap_head *rmap_head, void *data);
+
 void kvm_mmu_invalidate_zap_all_pages(struct kvm *kvm);
 void kvm_zap_gfn_range(struct kvm *kvm, gfn_t gfn_start, gfn_t gfn_end);
 
 void kvm_mmu_gfn_disallow_lpage(struct kvm_memory_slot *slot, gfn_t gfn);
 void kvm_mmu_gfn_allow_lpage(struct kvm_memory_slot *slot, gfn_t gfn);
-bool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,
-				    struct kvm_memory_slot *slot, u64 gfn);
 int kvm_arch_write_log_dirty(struct kvm_vcpu *vcpu);
 gfn_t spte_to_gfn(u64 *sptep);
+bool slot_handle_all_level(struct kvm *kvm, struct kvm_memory_slot *memslot,
+		slot_level_handler fn, bool lock_flush_tlb, void *data);
+struct kvm_rmap_head *__gfn_to_rmap(gfn_t gfn, int level,
+		struct kvm_memory_slot *slot);
 #endif
diff --git a/arch/x86/kvm/roe.c b/arch/x86/kvm/roe.c
new file mode 100644
index 0000000000..f787106be8
--- /dev/null
+++ b/arch/x86/kvm/roe.c
@@ -0,0 +1,101 @@
+// SPDX-License-Identifier: GPL-2.0
+
+/*
+ * KVM Read Only Enforcement
+ * Copyright (c) 2018 Ahmed Abd El Mawgood
+ *
+ * Author: Ahmed Abd El Mawgood <ahmedsoliman@mena.vt.edu>
+ *
+ */
+#include <linux/types.h>
+#include <linux/kvm_host.h>
+#include <kvm/roe.h>
+
+
+#include <asm/kvm_host.h>
+#include "kvm_cache_regs.h"
+#include "mmu.h"
+#include "roe_arch.h"
+
+static bool __rmap_write_protect_roe(struct kvm *kvm,
+		struct kvm_rmap_head *rmap_head, bool pt_protect,
+		struct kvm_memory_slot *memslot)
+{
+	u64 *sptep;
+	struct rmap_iterator iter;
+	bool prot;
+	bool flush = false;
+
+	for_each_rmap_spte(rmap_head, &iter, sptep) {
+		int idx = spte_to_gfn(sptep) - memslot->base_gfn;
+
+		prot = !test_bit(idx, memslot->roe_bitmap) && pt_protect;
+		flush |= spte_write_protect(sptep, prot);
+	}
+	return flush;
+}
+
+bool kvm_mmu_slot_gfn_write_protect_roe(struct kvm *kvm,
+		struct kvm_memory_slot *slot, u64 gfn)
+{
+	struct kvm_rmap_head *rmap_head;
+	int i;
+	bool write_protected = false;
+
+	for (i = PT_PAGE_TABLE_LEVEL; i <= PT_MAX_HUGEPAGE_LEVEL; ++i) {
+		rmap_head = __gfn_to_rmap(gfn, i, slot);
+		write_protected |= __rmap_write_protect_roe(kvm, rmap_head,
+				true, slot);
+	}
+	return write_protected;
+}
+
+static bool slot_rmap_apply_protection(struct kvm *kvm,
+		struct kvm_rmap_head *rmap_head, void *data)
+{
+	struct kvm_memory_slot *memslot = (struct kvm_memory_slot *) data;
+	bool prot_mask = !(memslot->flags & KVM_MEM_READONLY);
+
+	return __rmap_write_protect_roe(kvm, rmap_head, prot_mask, memslot);
+}
+
+bool roe_protect_all_levels(struct kvm *kvm, struct kvm_memory_slot *memslot)
+{
+	bool flush;
+
+	spin_lock(&kvm->mmu_lock);
+	flush = slot_handle_all_level(kvm, memslot, slot_rmap_apply_protection,
+			false, memslot);
+	spin_unlock(&kvm->mmu_lock);
+	return flush;
+}
+
+void kvm_roe_arch_commit_protection(struct kvm *kvm,
+		struct kvm_memory_slot *slot)
+{
+	kvm_mmu_slot_apply_write_access(kvm, slot);
+	kvm_arch_flush_shadow_memslot(kvm, slot);
+}
+EXPORT_SYMBOL_GPL(kvm_roe_arch_commit_protection);
+
+bool kvm_roe_arch_is_userspace(struct kvm_vcpu *vcpu)
+{
+	u64 rflags;
+	u64 cr0 = kvm_read_cr0(vcpu);
+	u64 iopl;
+
+	// first checking we are not in protected mode
+	if ((cr0 & 1) == 0)
+		return false;
+	/*
+	 * we don't need to worry about comments in __get_regs
+	 * because we are sure that this function will only be
+	 * triggered at the end of a hypercall instruction.
+	 */
+	rflags = kvm_get_rflags(vcpu);
+	iopl = (rflags >> 12) & 3;
+	if (iopl != 3)
+		return false;
+	return true;
+}
+EXPORT_SYMBOL_GPL(kvm_roe_arch_is_userspace);
diff --git a/arch/x86/kvm/roe_arch.h b/arch/x86/kvm/roe_arch.h
new file mode 100644
index 0000000000..17a8b79d36
--- /dev/null
+++ b/arch/x86/kvm/roe_arch.h
@@ -0,0 +1,28 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+
+#ifndef __KVM_ROE_HARCH_H__
+#define __KVM_ROE_HARCH_H__
+/*
+ * KVM Read Only Enforcement
+ * Copyright (c) 2018 Ahmed Abd El Mawgood
+ *
+ * Author: Ahmed Abd El Mawgood <ahmedsoliman@mena.vt.edu>
+ *
+ */
+#include "mmu.h"
+
+bool roe_protect_all_levels(struct kvm *kvm, struct kvm_memory_slot *memslot);
+
+static inline bool protect_all_levels(struct kvm *kvm,
+		struct kvm_memory_slot *memslot)
+{
+	return roe_protect_all_levels(kvm, memslot);
+}
+bool kvm_mmu_slot_gfn_write_protect_roe(struct kvm *kvm,
+		struct kvm_memory_slot *slot, u64 gfn);
+static inline bool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,
+		struct kvm_memory_slot *slot, u64 gfn)
+{
+	return kvm_mmu_slot_gfn_write_protect_roe(kvm, slot, gfn);
+}
+#endif
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index d02937760c..28475c83f9 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -20,6 +20,7 @@
  */
 
 #include <linux/kvm_host.h>
+#include <kvm/roe.h>
 #include "irq.h"
 #include "mmu.h"
 #include "i8254.h"
@@ -4409,7 +4410,7 @@ int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm, struct kvm_dirty_log *log)
 
 	/*
 	 * All the TLBs can be flushed out of mmu lock, see the comments in
-	 * kvm_mmu_slot_remove_write_access().
+	 * kvm_mmu_slot_apply_write_access().
 	 */
 	lockdep_assert_held(&kvm->slots_lock);
 	if (is_dirty)
@@ -6928,7 +6929,6 @@ static int kvm_pv_clock_pairing(struct kvm_vcpu *vcpu, gpa_t paddr,
 	return ret;
 }
 #endif
-
 /*
  * kvm_pv_kick_cpu_op:  Kick a vcpu.
  *
@@ -7000,6 +7000,9 @@ int kvm_emulate_hypercall(struct kvm_vcpu *vcpu)
 		ret = kvm_pv_send_ipi(vcpu->kvm, a0, a1, a2, a3, op_64_bit);
 		break;
 #endif
+	case KVM_HC_ROE:
+		ret = kvm_roe(vcpu, a0, a1, a2, a3);
+		break;
 	default:
 		ret = -KVM_ENOSYS;
 		break;
@@ -9263,8 +9266,8 @@ static void kvm_mmu_slot_apply_flags(struct kvm *kvm,
 				     struct kvm_memory_slot *new)
 {
 	/* Still write protect RO slot */
+	kvm_mmu_slot_apply_write_access(kvm, new);
 	if (new->flags & KVM_MEM_READONLY) {
-		kvm_mmu_slot_remove_write_access(kvm, new);
 		return;
 	}
 
@@ -9302,7 +9305,7 @@ static void kvm_mmu_slot_apply_flags(struct kvm *kvm,
 		if (kvm_x86_ops->slot_enable_log_dirty)
 			kvm_x86_ops->slot_enable_log_dirty(kvm, new);
 		else
-			kvm_mmu_slot_remove_write_access(kvm, new);
+			kvm_mmu_slot_apply_write_access(kvm, new);
 	} else {
 		if (kvm_x86_ops->slot_disable_log_dirty)
 			kvm_x86_ops->slot_disable_log_dirty(kvm, new);
-- 
2.19.2

