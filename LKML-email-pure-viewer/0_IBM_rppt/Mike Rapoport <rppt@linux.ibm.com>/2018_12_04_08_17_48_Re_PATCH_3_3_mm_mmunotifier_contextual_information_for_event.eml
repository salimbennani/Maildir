Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 04 Dec 2018 18:30:21 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga002.jf.intel.com (orsmga002.jf.intel.com [10.7.209.21])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id B009F580375;
	Tue,  4 Dec 2018 00:18:09 -0800 (PST)
Received: from fmsmga103.fm.intel.com ([10.1.193.90])
  by orsmga002-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 04 Dec 2018 00:18:08 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3APrbHqBI2NWAj1fNtW9mcpTZWNBhigK39O0sv0rFi?=
 =?us-ascii?q?tYgULvj7rarrMEGX3/hxlliBBdydt6oUzbKO+4nbGkU4qa6bt34DdJEeHzQksu?=
 =?us-ascii?q?4x2zIaPcieFEfgJ+TrZSFpVO5LVVti4m3peRMNQJW2aFLduGC94iAPERvjKwV1?=
 =?us-ascii?q?Ov71GonPhMiryuy+4ZLebxlLiTanfb9+MAi9oBnMuMURnYZsMLs6xAHTontPde?=
 =?us-ascii?q?RWxGdoKkyWkh3h+Mq+/4Nt/jpJtf45+MFOTav1f6IjTbxFFzsmKHw65NfqtRbY?=
 =?us-ascii?q?UwSC4GYXX3gMnRpJBwjF6wz6Xov0vyDnuOdxxDWWMMvrRr0vRz+s87lkRwPpiC?=
 =?us-ascii?q?cfNj427mfXitBrjKlGpB6tvgFzz5LIbI2QMvd1Y6HTcs4ARWdZUMhfVzJPDJ6y?=
 =?us-ascii?q?b4QAE+UOIOlWoIvzqFUNthu+HQuhCfjzyjJLnHL6wbE23v4jHAzAwQcuH8gOsH?=
 =?us-ascii?q?PRrNjtM6kSUOO1w7fVxjvfdfxWwTD96JDPchA7vfGHQLV9cdDJyUk3CwPIlVGQ?=
 =?us-ascii?q?qY3jPzOI2eUBqWmb4PFlVe61l2EnrARxryGpy8wxiYfJnpoYxk7Y+Sh92oo5ON?=
 =?us-ascii?q?O1RFBhbdK5E5ZcqzuWOop0T886R2xkojs2x7IFtJKhYiQG1JUqywTCZ/GDcoWF?=
 =?us-ascii?q?5A/oWvyLLjdinn1lfaqyhxas/kikze3xTte00FlUoSpfiNXMtW4C1wbV6seZTv?=
 =?us-ascii?q?tx5ECh2SyA1wzL6+FEJ147lbbDJpI/3rI9koAfvVnNEyPogkn6kaybelk+9uWp?=
 =?us-ascii?q?6enrerDmqYWdN49whAH+KKMumsmnDOQ8MwgOWXWb+Oul2L3g40L5WrNKgeMykq?=
 =?us-ascii?q?XAt5DbK8IbqbCjAwJOzIYj5AiwDy283NQbg3YHNlRFdwyDj4TzPFHOOv/4Ae+l?=
 =?us-ascii?q?g1uwiDdr2+zGPrr5D5XJL3jDk6nucaxy6k5B0wczydFf55RJCrAOOv7zW0nxtM?=
 =?us-ascii?q?DGAR89KQC73+HnCNBl3IMERW2PGrOZML/VsVKQ5uIgOeiMZJMPtzb6MfQl5+Pu?=
 =?us-ascii?q?gmU/mV8SZqSp2ZoXaHalHvVpOUmZYHzsgssfHmcOpAYxUOvqiFiaWz5Je3myR7?=
 =?us-ascii?q?485i08CI++D4fDQZ6ijKaC3Ce8GJ1WYGdGB0uIEXfpcYWERvgNZDiTIs9njjwL?=
 =?us-ascii?q?S7yhR5U92hGpsQ/w06BnIfbM+i0EqZLj08B46PfIlREy8jx0DN6R03uXQGF2hW?=
 =?us-ascii?q?4IQz423KZioU1y0FuD0K54g+BGGtxX/f9GTgA6NZvExexgF9/yQh7BfsuOSFu+?=
 =?us-ascii?q?QdWpGzcxQsg1w98PeUl9HdqigwvH3yqrBb8VirOKCIY18qLaw3j+OcJ9x2za26?=
 =?us-ascii?q?kmilksWtFPOnG+hq5j6wjTAJbEnFiDmKa0a6sQxi7N+32FzWqVok5YVgl8UaHG?=
 =?us-ascii?q?XXAaYkvbttD55kLET7+zBrUrKApBycieKqRUbt3ll0lJRPDmON7GeWK+h3+wBQ?=
 =?us-ascii?q?qUxrOLdIflZn8S3DvDB0QekwAc53CGNRMgCSenuG/eCD1uFVTyY0Lj6+V+qXW7?=
 =?us-ascii?q?TlMqwAGOdUFuy721+hsNj/yGV/wTxq4EuDsmqzhsAFa93tfWC92cpwphfKRQe8?=
 =?us-ascii?q?897E1A1WLDswx9P5qgL695i14acgR3uV7u1hptBoVBl8gqsG0lzA5oJa2E11NB?=
 =?us-ascii?q?ciuS3YrsNb3PNmny4BevZrbM1VHaztmX9bkA6fQip1r4oQGmC1At83Nk09lSzX?=
 =?us-ascii?q?uR/ZHKDAsUUZLsXUc77Rl6p7fGYiYj44PYz2FjMa6xsjXawdImGPMlygq8f9dY?=
 =?us-ascii?q?KK6FFBLyH9cGCMS0Ke0qm0KmbhQLPO1J8K40PsWmd+aJ2aKxPeZgmi6mgnpD4I?=
 =?us-ascii?q?xnzk2M8C98QPbS35kZ2/GYwheHVzDkgVe7s8D4hZpLZSsPEWq40yTkApBeZrdz?=
 =?us-ascii?q?fYoSFWihOcm3ychgiJ73XH5Y8kWuB1cH2M+vZBqTYEbx3QxW1UQLv3OnnTG0wC?=
 =?us-ascii?q?BzkzEstqCfxjDBw/z+dBobPW5GXGljjVb2LYm0ldwaR1Wobw4ymRuh5Eb6wbVb?=
 =?us-ascii?q?patlI2nSR0dIYzb5L2V4XqSssbqCZtZF6Ik0viVPTOS8fVeaR6b/oxQAySPjHG?=
 =?us-ascii?q?hexDchezGxoJr5nB96iGObLHltqnrUYsVwxRbZ5NzBSv9dxDsGRC9kiTbJAliw?=
 =?us-ascii?q?JcWm/dKRl53bqOCxS3qhVoFPcSns1Y6BtDG05WpwDR29nvCznMbqEQw70S/9yt?=
 =?us-ascii?q?lrWj/ErBf6Yon3yau6NfhrcVVvBF/588B6AJ1xkpMshJEM3ngXnpaV8mAGkWvp?=
 =?us-ascii?q?MdVbxLjxbHwCRTMQx97V4Q7l2FBsL36TxoL5UGmdzdVlZ9WgfmwW3Sc95dhQCK?=
 =?us-ascii?q?iI9LxEgTd1ol2goALUYPh9nS0SxeEg6XEEmOEJpBQizj+GDb8MB0lYMjfhlxCJ?=
 =?us-ascii?q?79C4saVWa3yjcbm21Epig9+hCKuOrR1bWHb8Yp0iBzN/7t1jMFLQ133+8p3keN?=
 =?us-ascii?q?7Vbd4JtxybiRHAj/VOKJIql/oHni5nOWP7vX04xO83lx1u3ZemvIeZL2Vh5r62?=
 =?us-ascii?q?AhldNjftfcMc5intjbpCnsaRx42gBY9uGjIPXJv1V/6oFC8SuO/jNwaPFj08t3?=
 =?us-ascii?q?iaFaDeHQ+Z9EdpsXbPH4q3OHGQIXkT1c9iSwWFJExDnAAUWy03npwjGQCv2sPh?=
 =?us-ascii?q?a1115iwL6l77txtMyf9oNx/lXWfEvwqobjY0SJ6CLBtZ9A1C5kHVMdCA4eJ3BS?=
 =?us-ascii?q?1X4pqhrAmVIGyBewtIFX0JWlCDB137Priu4sTA8/GFBuWkKfvCe66OqetYV/eH?=
 =?us-ascii?q?3p+v1opm/zCRNsSAJHViDvs72lZdUnB9AcjWhzIPSykPnSLXc8GbvAu8+jFwrs?=
 =?us-ascii?q?2n8PTkRgPv5YiMC7tTK9lv+Aq2gaCMN+GOniZ5NC1Y24gIxX/JzrgfwVEThztv?=
 =?us-ascii?q?dzmrDbQPqyrNQLjMla9QCh4RczlzO9dQ76IgwglNPtbWhcno1rFjlP46FVdEWU?=
 =?us-ascii?q?blmsGofsEKJ2C9NFXaBEeELriGJDvLw93pbqO4U7FfkOJUtxioszaBD0DjJiiD?=
 =?us-ascii?q?lyXuVx23LeFMjSSbMAZCt42nbhltCXbsTNT9Zx2/Md93iyA2wLIuinPLM24cLS?=
 =?us-ascii?q?Zzc0dXorKM6iNYh+11G3Zd4Xp9MemEhyGZ4vHYK5kMt/trByV0l+RC73U7y7ta?=
 =?us-ascii?q?9idERPNulSvWr95upUymk+aVxjpmVhpOtihEhIaRsUp+PqXZ88oIZXGR/h0J7H?=
 =?us-ascii?q?6LBjwQqtdlA8GpsKdVmfbVk6emDTZB/pru9NMbH8/ONIrTKmI5LTLtGTjJHE0E?=
 =?us-ascii?q?RzvtPmba0R8O2MqO/2GY+8Bp4qPnn4ADH/oCDAQ4?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0ADAAAcNwZch0O0hNFaChkBAQEBAQEBA?=
 =?us-ascii?q?QEBAQEHAQEBAQEBgVMCAQEBAQELAYFagQ+BAieDOj+IeIsvUAEBBoE1FIkLhHu?=
 =?us-ascii?q?JL4FsGQEBGAMQAYgOIjYHDQEDAQEBAQEBAgETAQEBCgsJCCkjDII2JAGCYQEBA?=
 =?us-ascii?q?QEDAQIJBhEECwFABgYJAQEIAhUDAgIFIQICAwwFFEgFHYI0SwGBdA0DAQGIWZE?=
 =?us-ascii?q?HiVgBAQFufDOKMoELixEXeIEHg241hFaDL4JXAosglGBGCYcChh+EEyMKAoFPT?=
 =?us-ascii?q?YREgyWHFSyJW45EAgQGBQITAYFNA4IDTSMVgycJgh4XEodbhjJwAYEEAQEhimI?=
 =?us-ascii?q?BAQ?=
X-IPAS-Result: =?us-ascii?q?A0ADAAAcNwZch0O0hNFaChkBAQEBAQEBAQEBAQEHAQEBAQE?=
 =?us-ascii?q?BgVMCAQEBAQELAYFagQ+BAieDOj+IeIsvUAEBBoE1FIkLhHuJL4FsGQEBGAMQA?=
 =?us-ascii?q?YgOIjYHDQEDAQEBAQEBAgETAQEBCgsJCCkjDII2JAGCYQEBAQEDAQIJBhEECwF?=
 =?us-ascii?q?ABgYJAQEIAhUDAgIFIQICAwwFFEgFHYI0SwGBdA0DAQGIWZEHiVgBAQFufDOKM?=
 =?us-ascii?q?oELixEXeIEHg241hFaDL4JXAosglGBGCYcChh+EEyMKAoFPTYREgyWHFSyJW45?=
 =?us-ascii?q?EAgQGBQITAYFNA4IDTSMVgycJgh4XEodbhjJwAYEEAQEhimIBAQ?=
X-IronPort-AV: E=Sophos;i="5.56,313,1539673200"; 
   d="scan'208";a="54408403"
X-Amp-Result: UNKNOWN
X-Amp-Original-Verdict: FILE UNKNOWN
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 04 Dec 2018 00:18:06 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726059AbeLDISD (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Tue, 4 Dec 2018 03:18:03 -0500
Received: from mx0a-001b2d01.pphosted.com ([148.163.156.1]:36718 "EHLO
        mx0a-001b2d01.pphosted.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S1725996AbeLDISC (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Tue, 4 Dec 2018 03:18:02 -0500
Received: from pps.filterd (m0098393.ppops.net [127.0.0.1])
        by mx0a-001b2d01.pphosted.com (8.16.0.22/8.16.0.22) with SMTP id wB48DQDq018877
        for <linux-kernel@vger.kernel.org>; Tue, 4 Dec 2018 03:18:00 -0500
Received: from e06smtp01.uk.ibm.com (e06smtp01.uk.ibm.com [195.75.94.97])
        by mx0a-001b2d01.pphosted.com with ESMTP id 2p5masm89q-1
        (version=TLSv1.2 cipher=AES256-GCM-SHA384 bits=256 verify=NOT)
        for <linux-kernel@vger.kernel.org>; Tue, 04 Dec 2018 03:18:00 -0500
Received: from localhost
        by e06smtp01.uk.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
        for <linux-kernel@vger.kernel.org> from <rppt@linux.ibm.com>;
        Tue, 4 Dec 2018 08:17:57 -0000
Received: from b06cxnps4074.portsmouth.uk.ibm.com (9.149.109.196)
        by e06smtp01.uk.ibm.com (192.168.101.131) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
        (version=TLSv1/SSLv3 cipher=AES256-GCM-SHA384 bits=256/256)
        Tue, 4 Dec 2018 08:17:52 -0000
Received: from b06wcsmtp001.portsmouth.uk.ibm.com (b06wcsmtp001.portsmouth.uk.ibm.com [9.149.105.160])
        by b06cxnps4074.portsmouth.uk.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id wB48HpEI36372502
        (version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256 verify=FAIL);
        Tue, 4 Dec 2018 08:17:51 GMT
Received: from b06wcsmtp001.portsmouth.uk.ibm.com (unknown [127.0.0.1])
        by IMSVA (Postfix) with ESMTP id B6B11A405B;
        Tue,  4 Dec 2018 08:17:51 +0000 (GMT)
Received: from b06wcsmtp001.portsmouth.uk.ibm.com (unknown [127.0.0.1])
        by IMSVA (Postfix) with ESMTP id 0076AA405C;
        Tue,  4 Dec 2018 08:17:50 +0000 (GMT)
Received: from rapoport-lnx (unknown [9.148.206.196])
        by b06wcsmtp001.portsmouth.uk.ibm.com (Postfix) with ESMTPS;
        Tue,  4 Dec 2018 08:17:49 +0000 (GMT)
Date: Tue, 4 Dec 2018 10:17:48 +0200
From: Mike Rapoport <rppt@linux.ibm.com>
To: jglisse@redhat.com
Cc: linux-mm@kvack.org, Andrew Morton <akpm@linux-foundation.org>,
        linux-kernel@vger.kernel.org,
        Matthew Wilcox <mawilcox@microsoft.com>,
        Ross Zwisler <zwisler@kernel.org>, Jan Kara <jack@suse.cz>,
        Dan Williams <dan.j.williams@intel.com>,
        Paolo Bonzini <pbonzini@redhat.com>,
        Radim =?utf-8?B?S3LEjW3DocWZ?= <rkrcmar@redhat.com>,
        Michal Hocko <mhocko@kernel.org>,
        Christian Koenig <christian.koenig@amd.com>,
        Felix Kuehling <felix.kuehling@amd.com>,
        Ralph Campbell <rcampbell@nvidia.com>,
        John Hubbard <jhubbard@nvidia.com>, kvm@vger.kernel.org,
        linux-rdma@vger.kernel.org, linux-fsdevel@vger.kernel.org,
        dri-devel@lists.freedesktop.org
Subject: Re: [PATCH 3/3] mm/mmu_notifier: contextual information for event
 triggering invalidation
References: <20181203201817.10759-1-jglisse@redhat.com>
 <20181203201817.10759-4-jglisse@redhat.com>
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Disposition: inline
Content-Transfer-Encoding: 8bit
In-Reply-To: <20181203201817.10759-4-jglisse@redhat.com>
User-Agent: Mutt/1.5.24 (2015-08-30)
X-TM-AS-GCONF: 00
x-cbid: 18120408-4275-0000-0000-000002EBB36D
X-IBM-AV-DETECTION: SAVI=unused REMOTE=unused XFE=unused
x-cbparentid: 18120408-4276-0000-0000-000037F8B9D3
Message-Id: <20181204081746.GJ26700@rapoport-lnx>
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10434:,, definitions=2018-12-04_04:,,
 signatures=0
X-Proofpoint-Spam-Details: rule=outbound_notspam policy=outbound score=0 priorityscore=1501
 malwarescore=0 suspectscore=4 phishscore=0 bulkscore=0 spamscore=0
 clxscore=1015 lowpriorityscore=0 mlxscore=0 impostorscore=0
 mlxlogscore=999 adultscore=0 classifier=spam adjust=0 reason=mlx
 scancount=1 engine=8.0.1-1810050000 definitions=main-1812040075
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On Mon, Dec 03, 2018 at 03:18:17PM -0500, jglisse@redhat.com wrote:
> From: Jérôme Glisse <jglisse@redhat.com>
> 
> CPU page table update can happens for many reasons, not only as a result
> of a syscall (munmap(), mprotect(), mremap(), madvise(), ...) but also
> as a result of kernel activities (memory compression, reclaim, migration,
> ...).
> 
> Users of mmu notifier API track changes to the CPU page table and take
> specific action for them. While current API only provide range of virtual
> address affected by the change, not why the changes is happening.
> 
> This patchset adds event information so that users of mmu notifier can
> differentiate among broad category:
>     - UNMAP: munmap() or mremap()
>     - CLEAR: page table is cleared (migration, compaction, reclaim, ...)
>     - PROTECTION_VMA: change in access protections for the range
>     - PROTECTION_PAGE: change in access protections for page in the range
>     - SOFT_DIRTY: soft dirtyness tracking
> 
> Being able to identify munmap() and mremap() from other reasons why the
> page table is cleared is important to allow user of mmu notifier to
> update their own internal tracking structure accordingly (on munmap or
> mremap it is not longer needed to track range of virtual address as it
> becomes invalid).
> 
> Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
> Cc: Andrew Morton <akpm@linux-foundation.org>
> Cc: Matthew Wilcox <mawilcox@microsoft.com>
> Cc: Ross Zwisler <zwisler@kernel.org>
> Cc: Jan Kara <jack@suse.cz>
> Cc: Dan Williams <dan.j.williams@intel.com>
> Cc: Paolo Bonzini <pbonzini@redhat.com>
> Cc: Radim Krčmář <rkrcmar@redhat.com>
> Cc: Michal Hocko <mhocko@kernel.org>
> Cc: Christian Koenig <christian.koenig@amd.com>
> Cc: Felix Kuehling <felix.kuehling@amd.com>
> Cc: Ralph Campbell <rcampbell@nvidia.com>
> Cc: John Hubbard <jhubbard@nvidia.com>
> Cc: kvm@vger.kernel.org
> Cc: linux-rdma@vger.kernel.org
> Cc: linux-fsdevel@vger.kernel.org
> Cc: dri-devel@lists.freedesktop.org
> ---
>  fs/dax.c                     |  1 +
>  fs/proc/task_mmu.c           |  1 +
>  include/linux/mmu_notifier.h | 33 +++++++++++++++++++++++++++++++++
>  kernel/events/uprobes.c      |  1 +
>  mm/huge_memory.c             |  4 ++++
>  mm/hugetlb.c                 |  4 ++++
>  mm/khugepaged.c              |  1 +
>  mm/ksm.c                     |  2 ++
>  mm/madvise.c                 |  1 +
>  mm/memory.c                  |  5 +++++
>  mm/migrate.c                 |  2 ++
>  mm/mprotect.c                |  1 +
>  mm/mremap.c                  |  1 +
>  mm/oom_kill.c                |  1 +
>  mm/rmap.c                    |  2 ++
>  15 files changed, 60 insertions(+)
> 
> diff --git a/fs/dax.c b/fs/dax.c
> index e22508ee19ec..83092c5ac5f0 100644
> --- a/fs/dax.c
> +++ b/fs/dax.c
> @@ -761,6 +761,7 @@ static void dax_entry_mkclean(struct address_space *mapping, pgoff_t index,
>  		struct mmu_notifier_range range;
>  		unsigned long address;
> 
> +		range.event = MMU_NOTIFY_PROTECTION_PAGE;
>  		range.mm = vma->vm_mm;
> 
>  		cond_resched();
> diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c
> index 53d625925669..4abb1668eeb3 100644
> --- a/fs/proc/task_mmu.c
> +++ b/fs/proc/task_mmu.c
> @@ -1144,6 +1144,7 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,
>  			range.start = 0;
>  			range.end = -1UL;
>  			range.mm = mm;
> +			range.event = MMU_NOTIFY_SOFT_DIRTY;
>  			mmu_notifier_invalidate_range_start(&range);
>  		}
>  		walk_page_range(0, mm->highest_vm_end, &clear_refs_walk);
> diff --git a/include/linux/mmu_notifier.h b/include/linux/mmu_notifier.h
> index cbeece8e47d4..3077d487be8b 100644
> --- a/include/linux/mmu_notifier.h
> +++ b/include/linux/mmu_notifier.h
> @@ -25,10 +25,43 @@ struct mmu_notifier_mm {
>  	spinlock_t lock;
>  };
> 
> +/*
> + * What event is triggering the invalidation:

Can you please make it kernel-doc comment?

> + *
> + * MMU_NOTIFY_UNMAP
> + *    either munmap() that unmap the range or a mremap() that move the range
> + *
> + * MMU_NOTIFY_CLEAR
> + *    clear page table entry (many reasons for this like madvise() or replacing
> + *    a page by another one, ...).
> + *
> + * MMU_NOTIFY_PROTECTION_VMA
> + *    update is due to protection change for the range ie using the vma access
> + *    permission (vm_page_prot) to update the whole range is enough no need to
> + *    inspect changes to the CPU page table (mprotect() syscall)
> + *
> + * MMU_NOTIFY_PROTECTION_PAGE
> + *    update is due to change in read/write flag for pages in the range so to
> + *    mirror those changes the user must inspect the CPU page table (from the
> + *    end callback).
> + *
> + *
> + * MMU_NOTIFY_SOFT_DIRTY
> + *    soft dirty accounting (still same page and same access flags)
> + */
> +enum mmu_notifier_event {
> +	MMU_NOTIFY_UNMAP = 0,
> +	MMU_NOTIFY_CLEAR,
> +	MMU_NOTIFY_PROTECTION_VMA,
> +	MMU_NOTIFY_PROTECTION_PAGE,
> +	MMU_NOTIFY_SOFT_DIRTY,
> +};
> +
>  struct mmu_notifier_range {
>  	struct mm_struct *mm;
>  	unsigned long start;
>  	unsigned long end;
> +	enum mmu_notifier_event event;
>  	bool blockable;
>  };
> 
> diff --git a/kernel/events/uprobes.c b/kernel/events/uprobes.c
> index aa7996ca361e..b6ef3be1c24e 100644
> --- a/kernel/events/uprobes.c
> +++ b/kernel/events/uprobes.c
> @@ -174,6 +174,7 @@ static int __replace_page(struct vm_area_struct *vma, unsigned long addr,
>  	struct mmu_notifier_range range;
>  	struct mem_cgroup *memcg;
> 
> +	range.event = MMU_NOTIFY_CLEAR;
>  	range.start = addr;
>  	range.end = addr + PAGE_SIZE;
>  	range.mm = mm;
> diff --git a/mm/huge_memory.c b/mm/huge_memory.c
> index 1a7a059dbf7d..4919be71ffd0 100644
> --- a/mm/huge_memory.c
> +++ b/mm/huge_memory.c
> @@ -1182,6 +1182,7 @@ static vm_fault_t do_huge_pmd_wp_page_fallback(struct vm_fault *vmf,
>  		cond_resched();
>  	}
> 
> +	range.event = MMU_NOTIFY_CLEAR;
>  	range.start = haddr;
>  	range.end = range.start + HPAGE_PMD_SIZE;
>  	range.mm = vma->vm_mm;
> @@ -1347,6 +1348,7 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd)
>  				    vma, HPAGE_PMD_NR);
>  	__SetPageUptodate(new_page);
> 
> +	range.event = MMU_NOTIFY_CLEAR;
>  	range.start = haddr;
>  	range.end = range.start + HPAGE_PMD_SIZE;
>  	range.mm = vma->vm_mm;
> @@ -2029,6 +2031,7 @@ void __split_huge_pud(struct vm_area_struct *vma, pud_t *pud,
>  	struct mm_struct *mm = vma->vm_mm;
>  	struct mmu_notifier_range range;
> 
> +	range.event = MMU_NOTIFY_CLEAR;
>  	range.start = address & HPAGE_PUD_MASK;
>  	range.end = range.start + HPAGE_PUD_SIZE;
>  	range.mm = mm;
> @@ -2248,6 +2251,7 @@ void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,
>  	struct mm_struct *mm = vma->vm_mm;
>  	struct mmu_notifier_range range;
> 
> +	range.event = MMU_NOTIFY_CLEAR;
>  	range.start = address & HPAGE_PMD_MASK;
>  	range.end = range.start + HPAGE_PMD_SIZE;
>  	range.mm = mm;
> diff --git a/mm/hugetlb.c b/mm/hugetlb.c
> index 4bfbdab44d51..9ffe34173834 100644
> --- a/mm/hugetlb.c
> +++ b/mm/hugetlb.c
> @@ -3244,6 +3244,7 @@ int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,
> 
>  	cow = (vma->vm_flags & (VM_SHARED | VM_MAYWRITE)) == VM_MAYWRITE;
> 
> +	range.event = MMU_NOTIFY_CLEAR;
>  	range.start = vma->vm_start;
>  	range.end = vma->vm_end;
>  	range.mm = src;
> @@ -3344,6 +3345,7 @@ void __unmap_hugepage_range(struct mmu_gather *tlb, struct vm_area_struct *vma,
>  	unsigned long sz = huge_page_size(h);
>  	struct mmu_notifier_range range;
> 
> +	range.event = MMU_NOTIFY_CLEAR;
>  	range.start = start;
>  	range.end = end;
>  	range.mm = mm;
> @@ -3629,6 +3631,7 @@ static vm_fault_t hugetlb_cow(struct mm_struct *mm, struct vm_area_struct *vma,
>  	__SetPageUptodate(new_page);
>  	set_page_huge_active(new_page);
> 
> +	range.event = MMU_NOTIFY_CLEAR;
>  	range.start = haddr;
>  	range.end = range.start + huge_page_size(h);
>  	range.mm = mm;
> @@ -4346,6 +4349,7 @@ unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
>  	bool shared_pmd = false;
>  	struct mmu_notifier_range range;
> 
> +	range.event = MMU_NOTIFY_PROTECTION_VMA;
>  	range.start = start;
>  	range.end = end;
>  	range.mm = mm;
> diff --git a/mm/khugepaged.c b/mm/khugepaged.c
> index e9fe0c9a9f56..c5c78ba30b38 100644
> --- a/mm/khugepaged.c
> +++ b/mm/khugepaged.c
> @@ -1016,6 +1016,7 @@ static void collapse_huge_page(struct mm_struct *mm,
>  	pte = pte_offset_map(pmd, address);
>  	pte_ptl = pte_lockptr(mm, pmd);
> 
> +	range.event = MMU_NOTIFY_CLEAR;
>  	range.start = address;
>  	range.end = range.start + HPAGE_PMD_SIZE;
>  	range.mm = mm;
> diff --git a/mm/ksm.c b/mm/ksm.c
> index 262694d0cd4c..f8fbb92ca1bd 100644
> --- a/mm/ksm.c
> +++ b/mm/ksm.c
> @@ -1050,6 +1050,7 @@ static int write_protect_page(struct vm_area_struct *vma, struct page *page,
> 
>  	BUG_ON(PageTransCompound(page));
> 
> +	range.event = MMU_NOTIFY_CLEAR;
>  	range.start = pvmw.address;
>  	range.end = range.start + PAGE_SIZE;
>  	range.mm = mm;
> @@ -1139,6 +1140,7 @@ static int replace_page(struct vm_area_struct *vma, struct page *page,
>  	if (!pmd)
>  		goto out;
> 
> +	range.event = MMU_NOTIFY_CLEAR;
>  	range.start = addr;
>  	range.end = addr + PAGE_SIZE;
>  	range.mm = mm;
> diff --git a/mm/madvise.c b/mm/madvise.c
> index f20dd80ca21b..c415985d6a04 100644
> --- a/mm/madvise.c
> +++ b/mm/madvise.c
> @@ -466,6 +466,7 @@ static int madvise_free_single_vma(struct vm_area_struct *vma,
>  	if (!vma_is_anonymous(vma))
>  		return -EINVAL;
> 
> +	range.event = MMU_NOTIFY_CLEAR;
>  	range.start = max(vma->vm_start, start_addr);
>  	if (range.start >= vma->vm_end)
>  		return -EINVAL;
> diff --git a/mm/memory.c b/mm/memory.c
> index 36e0b83949fc..4ad63002d770 100644
> --- a/mm/memory.c
> +++ b/mm/memory.c
> @@ -1007,6 +1007,7 @@ int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
>  	 * is_cow_mapping() returns true.
>  	 */
>  	is_cow = is_cow_mapping(vma->vm_flags);
> +	range.event = MMU_NOTIFY_PROTECTION_PAGE;
>  	range.start = addr;
>  	range.end = end;
>  	range.mm = src_mm;
> @@ -1334,6 +1335,7 @@ void unmap_vmas(struct mmu_gather *tlb,
>  {
>  	struct mmu_notifier_range range;
> 
> +	range.event = MMU_NOTIFY_UNMAP;
>  	range.start = start_addr;
>  	range.end = end_addr;
>  	range.mm = vma->vm_mm;
> @@ -1358,6 +1360,7 @@ void zap_page_range(struct vm_area_struct *vma, unsigned long start,
>  	struct mmu_notifier_range range;
>  	struct mmu_gather tlb;
> 
> +	range.event = MMU_NOTIFY_CLEAR;
>  	range.start = start;
>  	range.end = range.start + size;
>  	range.mm = vma->vm_mm;
> @@ -1387,6 +1390,7 @@ static void zap_page_range_single(struct vm_area_struct *vma, unsigned long addr
>  	struct mmu_notifier_range range;
>  	struct mmu_gather tlb;
> 
> +	range.event = MMU_NOTIFY_CLEAR;
>  	range.start = address;
>  	range.end = range.start + size;
>  	range.mm = vma->vm_mm;
> @@ -2260,6 +2264,7 @@ static vm_fault_t wp_page_copy(struct vm_fault *vmf)
>  	struct mem_cgroup *memcg;
>  	struct mmu_notifier_range range;
> 
> +	range.event = MMU_NOTIFY_CLEAR;
>  	range.start = vmf->address & PAGE_MASK;
>  	range.end = range.start + PAGE_SIZE;
>  	range.mm = mm;
> diff --git a/mm/migrate.c b/mm/migrate.c
> index 4896dd9d8b28..a2caaabfc5a1 100644
> --- a/mm/migrate.c
> +++ b/mm/migrate.c
> @@ -2306,6 +2306,7 @@ static void migrate_vma_collect(struct migrate_vma *migrate)
>  	struct mmu_notifier_range range;
>  	struct mm_walk mm_walk;
> 
> +	range.event = MMU_NOTIFY_CLEAR;
>  	range.start = migrate->start;
>  	range.end = migrate->end;
>  	range.mm = mm_walk.mm;
> @@ -2726,6 +2727,7 @@ static void migrate_vma_pages(struct migrate_vma *migrate)
>  			if (!notified) {
>  				notified = true;
> 
> +				range.event = MMU_NOTIFY_CLEAR;
>  				range.start = addr;
>  				range.end = migrate->end;
>  				range.mm = mm;
> diff --git a/mm/mprotect.c b/mm/mprotect.c
> index f466adf31e12..6d41321b2f3e 100644
> --- a/mm/mprotect.c
> +++ b/mm/mprotect.c
> @@ -186,6 +186,7 @@ static inline unsigned long change_pmd_range(struct vm_area_struct *vma,
> 
>  		/* invoke the mmu notifier if the pmd is populated */
>  		if (!range.start) {
> +			range.event = MMU_NOTIFY_PROTECTION_VMA;
>  			range.start = addr;
>  			range.end = end;
>  			range.mm = mm;
> diff --git a/mm/mremap.c b/mm/mremap.c
> index db060acb4a8c..856a5e6bb226 100644
> --- a/mm/mremap.c
> +++ b/mm/mremap.c
> @@ -203,6 +203,7 @@ unsigned long move_page_tables(struct vm_area_struct *vma,
>  	old_end = old_addr + len;
>  	flush_cache_range(vma, old_addr, old_end);
> 
> +	range.event = MMU_NOTIFY_UNMAP;
>  	range.start = old_addr;
>  	range.end = old_end;
>  	range.mm = vma->vm_mm;
> diff --git a/mm/oom_kill.c b/mm/oom_kill.c
> index b29ab2624e95..f4bde1c34714 100644
> --- a/mm/oom_kill.c
> +++ b/mm/oom_kill.c
> @@ -519,6 +519,7 @@ bool __oom_reap_task_mm(struct mm_struct *mm)
>  			struct mmu_notifier_range range;
>  			struct mmu_gather tlb;
> 
> +			range.event = MMU_NOTIFY_CLEAR;
>  			range.start = vma->vm_start;
>  			range.end = vma->vm_end;
>  			range.mm = mm;
> diff --git a/mm/rmap.c b/mm/rmap.c
> index 09c5d9e5c766..b1afbbcc236a 100644
> --- a/mm/rmap.c
> +++ b/mm/rmap.c
> @@ -896,6 +896,7 @@ static bool page_mkclean_one(struct page *page, struct vm_area_struct *vma,
>  	 * We have to assume the worse case ie pmd for invalidation. Note that
>  	 * the page can not be free from this function.
>  	 */
> +	range.event = MMU_NOTIFY_PROTECTION_PAGE;
>  	range.mm = vma->vm_mm;
>  	range.start = address;
>  	range.end = min(vma->vm_end, range.start +
> @@ -1372,6 +1373,7 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,
>  	 * Note that the page can not be free in this function as call of
>  	 * try_to_unmap() must hold a reference on the page.
>  	 */
> +	range.event = MMU_NOTIFY_CLEAR;
>  	range.mm = vma->vm_mm;
>  	range.start = vma->vm_start;
>  	range.end = min(vma->vm_end, range.start +
> -- 
> 2.17.2
> 

-- 
Sincerely yours,
Mike.

