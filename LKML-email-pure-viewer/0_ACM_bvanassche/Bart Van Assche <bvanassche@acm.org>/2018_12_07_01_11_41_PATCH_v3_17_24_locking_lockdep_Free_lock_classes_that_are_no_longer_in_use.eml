Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 07 Dec 2018 16:17:05 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga007.jf.intel.com (orsmga007.jf.intel.com [10.7.209.58])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id A1CE55804F9;
	Thu,  6 Dec 2018 17:14:28 -0800 (PST)
Received: from fmsmga101.fm.intel.com ([10.1.193.65])
  by orsmga007-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 06 Dec 2018 17:14:27 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3A+ssFwBC+6BJHmMeD/ZWGUyQJP3N1i/DPJgcQr6Af?=
 =?us-ascii?q?oPdwSP7/oMywAkXT6L1XgUPTWs2DsrQY07qQ6/iocFdDyK7JiGoFfp1IWk1Nou?=
 =?us-ascii?q?QttCtkPvS4D1bmJuXhdS0wEZcKflZk+3amLRodQ56mNBXdrXKo8DEdBAj0OxZr?=
 =?us-ascii?q?KeTpAI7SiNm82/yv95HJbAhEmDmwbaluIBmqsA7cqtQYjYx+J6gr1xDHuGFIe+?=
 =?us-ascii?q?NYxWNpIVKcgRPx7dqu8ZBg7ipdpesv+9ZPXqvmcas4S6dYDCk9PGAu+MLrrxjD?=
 =?us-ascii?q?QhCR6XYaT24bjwBHAwnB7BH9Q5fxri73vfdz1SWGIcH7S60/VC+85Kl3VhDnlC?=
 =?us-ascii?q?YHNyY48G7JjMxwkLlbqw+lqxBm3oLYfJ2ZOP94c6jAf90VWHBBU95RWSJfH42y?=
 =?us-ascii?q?YYgBAe0DMuZWoIbzqFoOrQCmBQSuH+7j1jxFi2Xq0aAgz+gtDQfL1xEgEdIUt3?=
 =?us-ascii?q?TUqc34OboIXuCv0KnD0DrMYOlQ2Tzg9IXIaQshru2MXb1ubMHczlIgFx3fgVWW?=
 =?us-ascii?q?s4PlJCiV2fgNs2eF9OdvT/6gi2s9pwF2uDivyd4hh4/UjYwW0lDJ7Tt1zJoxKN?=
 =?us-ascii?q?GiVUJ2b8CoHIFNuyyZK4d6WMIvTmNwtConyLALuoS3cDYWxJkk3RLSZfiKf5KV?=
 =?us-ascii?q?7h/jVOudOSp0iG5qdb6lmhq//0atxvf/W8Wu01tHqixImcTWuH8XzRzc8M2HR+?=
 =?us-ascii?q?N9/ki/3TaP0Bje6v9LIU8qj6rXMZ0hzaAqlpoVr0vDGjX6mELsjK+Zbkkk++6o?=
 =?us-ascii?q?5Pr7Yrj+uJOQK4t5hhvjPqkghMCzG/k0PwsSU2SB+Omx1qXv/UjjT7VLiv02nL?=
 =?us-ascii?q?PZsJffJckDoq65AglV0pss6hqmDDepzs4YnX8ZI1JBYR6HiIboO1fQL/DiFvq/?=
 =?us-ascii?q?nVusnylxx/zcPb3uHI/NLn7dn7flZ7p97FRcyAUrwdBF+51UEq0BIO70WkLpsN?=
 =?us-ascii?q?zYDxw5PBKuz+foFdVwzYceWWOJAq+EP6Leq16I5uQzI+aSYI8ZoiryK/8g5/T2?=
 =?us-ascii?q?l382hUcdfbW13ZsQcH24Hu5pI0SFYXX2hdcNC2cKvhckQ+zsh12PSjpTZ3e0X6?=
 =?us-ascii?q?Ih6TA3EoOmDYHfRo+zhLyNxju0HppTZmpeEFCDDW/od5mYW/cLcC+SIMhhnSIe?=
 =?us-ascii?q?WbiiTI8h0xeutAjhxrpjL+rU/DAYtJ352Nh04e3TiQ899ThuA8uB1GGNSnl+nn?=
 =?us-ascii?q?kUSD8uwKB/vUt9x0+Z3qhjg/xYEt9T6+lTUgggN57R1Oh6C9H0WgLccdaFUlem?=
 =?us-ascii?q?QtO6AT4vStI92cMBY0F4G9+6lBDMwzKqA6MJl7yMHJE08bjT337rK8Z5ynbJzq?=
 =?us-ascii?q?8hj1Y9T8tLNG2mgLN/9gfJC47IlUWZi7ildaAG0CHR82eDyHKEvFtEXw5oTaXF?=
 =?us-ascii?q?QXcfa1PLotvj+EPNUaWiCbQ9PQtH0s6NNK1KZtrtjVVFQffjPM/TY2awm2e2GB?=
 =?us-ascii?q?aJyamAbIvse2UBwirdDFIInBwU/XaDLQI+HDuuo3rCDDxyElLie17j/vNgqHyl?=
 =?us-ascii?q?VEM0zxuFb0t617Wr/B4YnvicS/IV3rIZtyYtsTR0HFCh393ID9qMvRZufKJZYd?=
 =?us-ascii?q?kl+ldIyXrZtxBhPpynN61iml8ecwFwv0Py1xV2Cp9MkdQwoHMt1gpyLaOY0FVO?=
 =?us-ascii?q?dz6D2ZDwO7vXKnT9/Ry1aq7W3E3e38iS+qsV9Ps4rFDjthmzFkU+63Vnz8VV03?=
 =?us-ascii?q?yE65rQCAodT53wXVg39hRgvL7afzQy6JnS1X1vNqm0rCTP29YoBOsj1xahcM1T?=
 =?us-ascii?q?MKKCFA/uDcIaA9KiJ/Atm1isdhgEJvxd9LYoP8O6cPuLwK6qPPt6kD26l2hG4Y?=
 =?us-ascii?q?B93ViK9ypzUePI25cFw/eF3gqITTv8jVGhstzploBAfz0dAm2/yS38Do5LeqJy?=
 =?us-ascii?q?ZZoLCXupI8Cv3NV+hoLiVGRC9FG+AFMKwsmpdgSIb1z8xAFfyV4YoXi6liSm1T?=
 =?us-ascii?q?x0lDcpo7Gb3CzPxeTiaRUGNnRKRGlkkVfjP4y0g8oGU0ivaggjjAGl6lrix6hH?=
 =?us-ascii?q?uKR/KHHeQEdJfyTsL2BuSLCwtqeEY8NU7JMosCNXUPmzYFyAS779pQca3D3nH2?=
 =?us-ascii?q?dE2D87cDSqsI3jnxNmkGKdMGpzrH3BdMFy3xjf/t/cRf1W3jYcXyl3kzrXBlu9?=
 =?us-ascii?q?P9mv49qUkYzOsuS/V2KnS51SfjPnzYKGtCun+2JqBQezkOy0mt3iCQI6yzP018?=
 =?us-ascii?q?F2VSXUqxbxeonq2L69Me59fEloBVn85tF+GoF/lIswmZ4R1WIbhpWT4XoIj2Pz?=
 =?us-ascii?q?Pc9H1qL5aXoHXSQLzMLN4Aj5xE1jKWqEx4HjWXWc2MdhZ8S6bXkQ2iIy9M1KDK?=
 =?us-ascii?q?aU7LpZnSp6uFa4rATRYeRjkTcZ0/ch9Hkag+QRsgo30iqdGqwSHVVfPSH0khSI?=
 =?us-ascii?q?7tO+o79NaGehb7ewz1Z+ks67DL6ZvA5cQnn5d4wmHS9x6MV/LV3N3Gfy6oHiZN?=
 =?us-ascii?q?nfc9YTugeInBfHiuhfMIgxmeYShSp7JWL9umUoy/I8jRxrx527vZKIK2Nw/KKi?=
 =?us-ascii?q?GR5YNyb4Z8cS+jHrkKZfkdya34GpHpV9BDoLWIHkQu6vEDIXrf7nLRqBECUgqn?=
 =?us-ascii?q?eHHrrSBRSQ511hr3LLDpCnLXWXJGQCwNVmRRmdIlFfgQ8OUDU7mJ45Ch6lxMj7?=
 =?us-ascii?q?fEhl4TAR40byqgFQxeJwKxn/TmDfqR+qajguTZiQMgFW4hte50vPMsye8+FzHz?=
 =?us-ascii?q?xe/pK7twyAMWibZwVODWEUVU2IHVHjPr+y5dbe9+iUHPaxL/zLYb+Ws+xRS++I?=
 =?us-ascii?q?xY6z0oth5zuNNsKPPmR7D/0mwEVDW2p1G9/emzUJRCwajCbNb8+dpBeh9Sx7tM?=
 =?us-ascii?q?G/8PL3WA3x4YuDEaddMdJq+xqum6eMK/aQhDplKTZfzp4MxWHHyLkc3F4RiiFi?=
 =?us-ascii?q?bTqtEa4HtS7CUq3QgLJXDwUAZiN3NctI6b883wZXNc7ajNP1yqB3jvovB1hZUl?=
 =?us-ascii?q?zhn9mjZdYWLGGlKFPHGEGLOaycKj3W2M73e72zSL1KgOVQth29ojKbE07lPjSe?=
 =?us-ascii?q?mDjlTRGvMedQjC6FOBxSop2ychFoCWL7VtLpdgW7MMNrjT0x2bA0mnLKNWsGPT?=
 =?us-ascii?q?dgaU9Csr2Q4j1egvhkH2xB73xlLfSLmiqD7unYLIoWvuVvAihui+1a53E6waNP?=
 =?us-ascii?q?7C5YXPx1hDfSrtl2rlCmiOaPzD9nUBtIqjlThYOLvV9tOaPW9pRbXXbE/RQN7X?=
 =?us-ascii?q?ifChgQpttlDMHvtL5UytTViK3zLzJCoJro+p4ZBs7bL+qdPXYhOAavEznRXyUf?=
 =?us-ascii?q?Sjv+H2jFmwRjmffa32aOpZg34szmhpMIQ6RAX1w4PvoRDwJiBtNUc8Q/ZS8tjb?=
 =?us-ascii?q?PO1J1A3nG5thSEAZwC5p0=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AYAAANyQlch0O0hNFbCB4BBgcGgVEJC?=
 =?us-ascii?q?wGCAIFrJwqYGYIhlzqBbBsYEwGHVyI0CQ0BAwEBAQEBAQIBEwEBAQgNCQgpIwy?=
 =?us-ascii?q?CNiQBgmIDAwECJAsBDQEBNwEFCQEBUAMyBR0ZBYMcggIEAZpaPIodgWwzgnYBA?=
 =?us-ascii?q?QWHKggWh1pFg2oXgX+BEYJkhGkhhgCJExqGDHxQj1wJijuHACOBXIgGCC6HHgi?=
 =?us-ascii?q?ZBIFGgg0zGjBDgmyCGwsBF4IsjBJRgQUBASGHUA0XB4EBAYEeAQE?=
X-IPAS-Result: =?us-ascii?q?A0AYAAANyQlch0O0hNFbCB4BBgcGgVEJCwGCAIFrJwqYGYI?=
 =?us-ascii?q?hlzqBbBsYEwGHVyI0CQ0BAwEBAQEBAQIBEwEBAQgNCQgpIwyCNiQBgmIDAwECJ?=
 =?us-ascii?q?AsBDQEBNwEFCQEBUAMyBR0ZBYMcggIEAZpaPIodgWwzgnYBAQWHKggWh1pFg2o?=
 =?us-ascii?q?XgX+BEYJkhGkhhgCJExqGDHxQj1wJijuHACOBXIgGCC6HHgiZBIFGgg0zGjBDg?=
 =?us-ascii?q?myCGwsBF4IsjBJRgQUBASGHUA0XB4EBAYEeAQE?=
X-IronPort-AV: E=Sophos;i="5.56,324,1539673200"; 
   d="scan'208";a="65726216"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mga01b.intel.com with ESMTP; 06 Dec 2018 17:14:25 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726264AbeLGBOW (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Thu, 6 Dec 2018 20:14:22 -0500
Received: from out002.mailprotect.be ([83.217.72.86]:42239 "EHLO
        out002.mailprotect.be" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726112AbeLGBN2 (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Thu, 6 Dec 2018 20:13:28 -0500
DKIM-Signature: v=1; a=rsa-sha256; q=dns/txt; c=relaxed/relaxed;
        d=mailprotect.be; s=mail; h=Content-Transfer-Encoding:MIME-Version:References
        :In-Reply-To:Message-Id:Date:Subject:Cc:To:From:reply-to:sender:bcc:
        content-type; bh=xhhwBbmgibRmWK19SzaQXeguUzbtQSFMMt/3y9U6Occ=; b=B7rm44Nt35i2
        mZ1m6wOj5M2p0PB4OYuJpZfCTooHhJSVSu6aadAHPlOua76vv1LDzRuQhuFUlhVqaHPiFCoHtp5qK
        TdmKxWbXn99+HIAr/Wptl07n3pgu9h/QciDi2fxMPTnd364yUlGZRMKY6cJdbTnl4zXFI41gpsEX7
        z0e8O9YHVnASgWJ/9EbUR7mualPksRaV6W0AlNqlU0J9Qx2X/9QV0Fbtk51Z2kgNtwuDeidDNpN9K
        E5dA2H2jJJ6U/jyakgrhXTSdQyY94IB9235opPj8oGkb4Y4yxyFfKQ2ZbY1ICTPiGOx+OSHETUbkR
        dVAdXn51dA4L3anUXJZUDw==;
Received: from smtp-auth.mailprotect.be ([178.208.39.155])
        by com-mpt-out002.mailprotect.be with esmtp (Exim 4.89)
        (envelope-from <bvanassche@acm.org>)
        id 1gV4hb-000C89-1s; Fri, 07 Dec 2018 02:13:11 +0100
Received: from desktop-bart.svl.corp.google.com (unknown [104.133.8.89])
        (using TLSv1 with cipher DHE-RSA-AES256-SHA (256/256 bits))
        (No client certificate requested)
        by smtp-auth.mailprotect.be (Postfix) with ESMTPSA id 33718C0D81;
        Fri,  7 Dec 2018 02:13:08 +0100 (CET)
From: Bart Van Assche <bvanassche@acm.org>
To: mingo@redhat.com
Cc: peterz@infradead.org, tj@kernel.org, longman@redhat.com,
        johannes.berg@intel.com, linux-kernel@vger.kernel.org,
        Bart Van Assche <bvanassche@acm.org>,
        Johannes Berg <johannes@sipsolutions.net>
Subject: [PATCH v3 17/24] locking/lockdep: Free lock classes that are no longer in use
Date: Thu,  6 Dec 2018 17:11:41 -0800
Message-Id: <20181207011148.251812-18-bvanassche@acm.org>
X-Mailer: git-send-email 2.20.0.rc2.403.gdbc3b29805-goog
In-Reply-To: <20181207011148.251812-1-bvanassche@acm.org>
References: <20181207011148.251812-1-bvanassche@acm.org>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
X-Originating-IP: 178.208.39.155
X-SpamExperts-Domain: mailprotect.be
X-SpamExperts-Username: 178.208.39.128/27
Authentication-Results: mailprotect.be; auth=pass smtp.auth=178.208.39.128/27@mailprotect.be
X-SpamExperts-Outgoing-Class: ham
X-SpamExperts-Outgoing-Evidence: SB/global_tokens (0.00315275240301)
X-Recommended-Action: accept
X-Filter-ID: EX5BVjFpneJeBchSMxfU5sW3rKDaOoRsJ3SHsH5pEIV602E9L7XzfQH6nu9C/Fh9KJzpNe6xgvOx
 q3u0UDjvO1tLifGj39bI0bcPyaJsYTb/+zriNZuqQk0xRpGwjn+MTR8dtByWYYhgj25jR+mEA3YO
 AtfhCcV13BpIh8lqRXSSiFVwqwU9VgKUrYQ0lqWyFDaym8oavoPYsm7m1mVEGf0/5ST3nkt5XYDK
 YAuRpmF7E2F3eqKXiTd2ECUghryWXlqwG31yUGf3t09vBVYS71V1Pk0aHudBU1wyWVTA5OYS1waI
 XXCBiQFDV0yen8nGcSeERs4TOTnIH1kc1IWc5eLazqbUYkjl7F1LyK0RORrzhZOlSiptyzsethBi
 xxxfSNgvEAWLFiDAlQR6Wu0I/iWDegq4fQ11AxLVlrI7DVH6GzPz+r852EXXeYX4ImMyzDg6HYuT
 CyYgL61SIkBTYXPEvhymCNb22qmKERGbL1SqvGGff0htQqhQf0/aDrhpLM4xnCPpmyyS7jgD9Nen
 VVoB/PLX+rmdznc91veCJAwpCI+5VHpzDqRcT2+GtIE8SRAgmLDKAcp9i+Q+BEiSUj07SNjVkl0P
 Pj+OUYZX3/3Z86X+I1ED1LLv7p/g4HOOdbYb9IXfYGRpVS/0hA4Mwp9kB6JZdl+SJy+nWImyKEy/
 zJE11ZjKQFiKV8EwnkodMjMm/D2rEPnNAuCCnt9B91Ax/uigYcgpQGDKFCrOdd8gr/U0flMcy2Vi
 /IcBgY4aVBKUAPQbBk8nfWT8Lcjyyn9ZSA9YgSwQmLSJwsFnO3lUxpF8dVvwCXY/4mE1q/c/E53d
 gi3vLmcjfrrMypRHn63D6wJ0G3dAGqTMy1gFVzccEHuX93FS4h2/zDJNFeX5X1KrFIDnIhhlHDLh
 vRwgYpikjrjEsQHYqzPksYepQjQSeP/rn+UB/EmrduGPxHj/dKsQ+WcgzL7pTOYRE2yEvlBrMyfp
 YuyanAZpMXssfrVwLrD8rqLg0SqQCDr4y/2a4ErnbLaYB3hAzdROEMst5sLEb9McgnPq1sY5GeOX
 A3LYhBKQrXFod+WvUUP1Uys1EcG7yIy1/E3Wa9/NTnLEueuO5TcDeKjrEmYPn2IVWRtfdRHps2br
 TOkutpvyrdmtvbYO9ASW9x0dzW2b+IEXgSV5FVT0nNNOs1IDBtijAxf8XG910biZEcomWehz4u8s
 zDg6HYuTCyYgL61SIkBTYSiCBqrB4vMmHpjmx6wTDBzk2EPy497nssgnoMkIb3rMdj8luXC7km26
 QtXP9s02LA==
X-Report-Abuse-To: spam@com-mpt-mgt001.mailprotect.be
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Instead of leaving lock classes that are no longer in use in the
lock_classes array, reuse entries from that array that are no longer
in use. Maintain a linked list of free lock classes with list head
'free_lock_class'. Initialize that list from inside register_lock_class()
instead of from inside lockdep_init() because register_lock_class() can
be called before lockdep_init() has been called. Only add freed lock
classes to the free_lock_classes list after a grace period to avoid that
a lock_classes[] element would be reused while an RCU reader is
accessing it.

Cc: Peter Zijlstra <peterz@infradead.org>
Cc: Waiman Long <longman@redhat.com>
Cc: Johannes Berg <johannes@sipsolutions.net>
Signed-off-by: Bart Van Assche <bvanassche@acm.org>
---
 include/linux/lockdep.h  |   9 +-
 kernel/locking/lockdep.c | 242 ++++++++++++++++++++++++++++++++-------
 2 files changed, 208 insertions(+), 43 deletions(-)

diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index 9421f028c26c..72b4d5bb409f 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -63,7 +63,8 @@ extern struct lock_class_key __lockdep_no_validate__;
 #define LOCKSTAT_POINTS		4
 
 /*
- * The lock-class itself:
+ * The lock-class itself. The order of the structure members matters.
+ * reinit_class() zeroes the key member and all subsequent members.
  */
 struct lock_class {
 	/*
@@ -72,7 +73,9 @@ struct lock_class {
 	struct hlist_node		hash_entry;
 
 	/*
-	 * global list of all lock-classes:
+	 * Entry in all_lock_classes when in use. Entry in free_lock_classes
+	 * when not in use. Instances that are being freed are on the
+	 * zapped_classes list.
 	 */
 	struct list_head		lock_entry;
 
@@ -106,7 +109,7 @@ struct lock_class {
 	unsigned long			contention_point[LOCKSTAT_POINTS];
 	unsigned long			contending_point[LOCKSTAT_POINTS];
 #endif
-};
+} __no_randomize_layout;
 
 #ifdef CONFIG_LOCK_STAT
 struct lock_time {
diff --git a/kernel/locking/lockdep.c b/kernel/locking/lockdep.c
index 92bdb187987f..ce8abd268727 100644
--- a/kernel/locking/lockdep.c
+++ b/kernel/locking/lockdep.c
@@ -134,8 +134,8 @@ static struct lock_list list_entries[MAX_LOCKDEP_ENTRIES];
 /*
  * All data structures here are protected by the global debug_lock.
  *
- * Mutex key structs only get allocated, once during bootup, and never
- * get freed - this significantly simplifies the debugging code.
+ * nr_lock_classes is the number of elements of lock_classes[] that is
+ * in use.
  */
 unsigned long nr_lock_classes;
 #ifndef CONFIG_DEBUG_LOCKDEP
@@ -277,11 +277,18 @@ static inline void lock_release_holdtime(struct held_lock *hlock)
 #endif
 
 /*
- * We keep a global list of all lock classes. The list only grows,
- * never shrinks. The list is only accessed with the lockdep
- * spinlock lock held.
+ * We keep a global list of all lock classes. The list is only accessed with
+ * the lockdep spinlock lock held. The zapped_classes list contains lock
+ * classes that are about to be freed but that may still be accessed by an RCU
+ * reader. free_lock_classes is a list with free elements. These elements are
+ * linked together by the lock_entry member in struct lock_class.
  */
 LIST_HEAD(all_lock_classes);
+static LIST_HEAD(zapped_classes);
+static LIST_HEAD(free_lock_classes);
+static bool initialization_happened;
+static bool rcu_callback_scheduled;
+static struct rcu_head free_zapped_classes_rcu_head;
 
 /*
  * The lockdep classes are in a hash-table as well, for fast lookup:
@@ -735,6 +742,21 @@ static bool assign_lock_key(struct lockdep_map *lock)
 	return true;
 }
 
+/*
+ * Initialize the lock_classes[] array elements and also the free_lock_classes
+ * list.
+ */
+static void init_data_structures(void)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(lock_classes); i++) {
+		list_add_tail(&lock_classes[i].lock_entry, &free_lock_classes);
+		INIT_LIST_HEAD(&lock_classes[i].locks_after);
+		INIT_LIST_HEAD(&lock_classes[i].locks_before);
+	}
+}
+
 /*
  * Register a lock's class in the hash-table, if the class is not present
  * yet. Otherwise we look it up. We cache the result in the lock object
@@ -775,11 +797,14 @@ register_lock_class(struct lockdep_map *lock, unsigned int subclass, int force)
 			goto out_unlock_set;
 	}
 
-	/*
-	 * Allocate a new key from the static array, and add it to
-	 * the hash:
-	 */
-	if (nr_lock_classes >= MAX_LOCKDEP_KEYS) {
+	/* Allocate a new lock class and add it to the hash. */
+	if (unlikely(!initialization_happened)) {
+		initialization_happened = true;
+		init_data_structures();
+	}
+	class = list_first_entry_or_null(&free_lock_classes, typeof(*class),
+					 lock_entry);
+	if (!class) {
 		if (!debug_locks_off_graph_unlock()) {
 			return NULL;
 		}
@@ -788,13 +813,13 @@ register_lock_class(struct lockdep_map *lock, unsigned int subclass, int force)
 		dump_stack();
 		return NULL;
 	}
-	class = lock_classes + nr_lock_classes++;
+	nr_lock_classes++;
 	debug_atomic_inc(nr_unused_locks);
 	class->key = key;
 	class->name = lock->name;
 	class->subclass = subclass;
-	INIT_LIST_HEAD(&class->locks_before);
-	INIT_LIST_HEAD(&class->locks_after);
+	WARN_ON_ONCE(!list_empty(&class->locks_before));
+	WARN_ON_ONCE(!list_empty(&class->locks_after));
 	class->name_version = count_matching_names(class);
 	/*
 	 * We use RCU's safe list-add method to make
@@ -804,7 +829,7 @@ register_lock_class(struct lockdep_map *lock, unsigned int subclass, int force)
 	/*
 	 * Add it to the global list of classes:
 	 */
-	list_add_tail(&class->lock_entry, &all_lock_classes);
+	list_move_tail(&class->lock_entry, &all_lock_classes);
 
 	if (verbose(class)) {
 		graph_unlock();
@@ -1845,6 +1870,13 @@ check_prev_add(struct task_struct *curr, struct held_lock *prev,
 	struct lock_list this;
 	int ret;
 
+	if (WARN_ON_ONCE(!hlock_class(prev)->hash_entry.pprev) ||
+	    WARN_ONCE(!hlock_class(next)->hash_entry.pprev,
+		      KERN_INFO "Detected use-after-free of lock class %s\n",
+		      hlock_class(next)->name)) {
+		return 2;
+	}
+
 	/*
 	 * Prove that the new <prev> -> <next> dependency would not
 	 * create a circular dependency in the graph. (We do this by
@@ -2236,17 +2268,14 @@ static inline int add_chain_cache(struct task_struct *curr,
 }
 
 /*
- * Look up a dependency chain.
+ * Look up a dependency chain. Must be called with either the graph lock or
+ * the RCU read lock held.
  */
 static inline struct lock_chain *lookup_chain_cache(u64 chain_key)
 {
 	struct hlist_head *hash_head = chainhashentry(chain_key);
 	struct lock_chain *chain;
 
-	/*
-	 * We can walk it lock-free, because entries only get added
-	 * to the hash:
-	 */
 	hlist_for_each_entry_rcu(chain, hash_head, entry) {
 		if (chain->chain_key == chain_key) {
 			debug_atomic_inc(chain_lookup_hits);
@@ -3338,6 +3367,9 @@ static int __lock_acquire(struct lockdep_map *lock, unsigned int subclass,
 	if (nest_lock && !__lock_is_held(nest_lock, -1))
 		return print_lock_nested_lock_not_held(curr, hlock, ip);
 
+	WARN_ON_ONCE(depth && !hlock_class(hlock - 1)->hash_entry.pprev);
+	WARN_ON_ONCE(!hlock_class(hlock)->hash_entry.pprev);
+
 	if (!validate_chain(curr, lock, hlock, chain_head, chain_key))
 		return 0;
 
@@ -4126,11 +4158,93 @@ void lockdep_reset(void)
 	raw_local_irq_restore(flags);
 }
 
+/* Must be called with the graph lock held. */
+static void remove_class_from_lock_chain(struct lock_chain *chain,
+					 struct lock_class *class)
+{
+	u64 chain_key;
+	int i;
+
+#ifdef CONFIG_PROVE_LOCKING
+	for (i = chain->base; i < chain->base + chain->depth; i++) {
+		if (chain_hlocks[i] != class - lock_classes)
+			continue;
+		if (--chain->depth > 0)
+			memmove(&chain_hlocks[i], &chain_hlocks[i + 1],
+				(chain->base + chain->depth - i) *
+				sizeof(chain_hlocks[0]));
+		/*
+		 * Each lock class occurs at most once in a
+		 * lock chain so once we found a match we can
+		 * break out of this loop.
+		 */
+		goto recalc;
+	}
+	/* Since the chain has not been modified, return. */
+	return;
+
+recalc:
+	/*
+	 * Note: calling hlist_del_rcu() from inside a
+	 * hlist_for_each_entry_rcu() loop is safe.
+	 */
+	if (chain->depth == 0) {
+		/* To do: decrease chain count. See also inc_chains(). */
+		hlist_del_rcu(&chain->entry);
+		return;
+	}
+	chain_key = 0;
+	for (i = chain->base; i < chain->base + chain->depth; i++)
+		chain_key = iterate_chain_key(chain_key, chain_hlocks[i] + 1);
+	if (chain->chain_key == chain_key)
+		return;
+	hlist_del_rcu(&chain->entry);
+	chain->chain_key = chain_key;
+	hlist_add_head_rcu(&chain->entry, chainhashentry(chain_key));
+#endif
+}
+
+/* Must be called with the graph lock held. */
+static void remove_class_from_lock_chains(struct lock_class *class)
+{
+	struct lock_chain *chain;
+	struct hlist_head *head;
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(chainhash_table); i++) {
+		head = chainhash_table + i;
+		hlist_for_each_entry_rcu(chain, head, entry) {
+			remove_class_from_lock_chain(chain, class);
+		}
+	}
+}
+
+/*
+ * Move a class to the zapped_classes list if it is no longer in use. Must be
+ * called with the graph lock held.
+ */
+static void check_free_class(struct lock_class *class)
+{
+	/*
+	 * If the list_del_rcu(&entry->entry) call made both lock order lists
+	 * empty, remove @class from the all_lock_classes list and add it to
+	 * the zapped_classes list.
+	 */
+	if (class->hash_entry.pprev &&
+	    list_empty(&class->locks_after) &&
+	    list_empty(&class->locks_before)) {
+		list_move_tail(&class->lock_entry, &zapped_classes);
+		hlist_del_rcu(&class->hash_entry);
+		class->hash_entry.pprev = NULL;
+	}
+}
+
 /*
  * Remove all references to a lock class. The caller must hold the graph lock.
  */
 static void zap_class(struct lock_class *class)
 {
+	struct lock_class *links_to;
 	struct lock_list *entry;
 	int i;
 
@@ -4141,14 +4255,30 @@ static void zap_class(struct lock_class *class)
 	for (i = 0, entry = list_entries; i < nr_list_entries; i++, entry++) {
 		if (entry->class != class && entry->links_to != class)
 			continue;
+		links_to = entry->links_to;
+		WARN_ON_ONCE(entry->class == links_to);
 		list_del_rcu(&entry->entry);
 	}
-	/*
-	 * Unhash the class and remove it from the all_lock_classes list:
-	 */
-	hlist_del_rcu(&class->hash_entry);
-	class->hash_entry.pprev = NULL;
-	list_del(&class->lock_entry);
+	check_free_class(class);
+	WARN_ONCE(class->hash_entry.pprev,
+		  KERN_INFO "%s() failed for class %s\n", __func__,
+		  class->name);
+
+	remove_class_from_lock_chains(class);
+}
+
+static void reinit_class(struct lock_class *class)
+{
+	void *const p = class;
+	const unsigned int offset = offsetof(struct lock_class, key);
+
+	WARN_ON_ONCE(!class->lock_entry.next);
+	WARN_ON_ONCE(!list_empty(&class->locks_after));
+	WARN_ON_ONCE(!list_empty(&class->locks_before));
+	memset(p + offset, 0, sizeof(*class) - offset);
+	WARN_ON_ONCE(!class->lock_entry.next);
+	WARN_ON_ONCE(!list_empty(&class->locks_after));
+	WARN_ON_ONCE(!list_empty(&class->locks_before));
 }
 
 static inline int within(const void *addr, void *start, unsigned long size)
@@ -4156,6 +4286,38 @@ static inline int within(const void *addr, void *start, unsigned long size)
 	return addr >= start && addr < start + size;
 }
 
+/*
+ * Free all lock classes that are on the zapped_classes list. Called as an
+ * RCU callback function.
+ */
+static void free_zapped_classes(struct callback_head *ch)
+{
+	struct lock_class *class;
+	unsigned long flags;
+	int locked;
+
+	raw_local_irq_save(flags);
+	locked = graph_lock();
+	rcu_callback_scheduled = false;
+	list_for_each_entry(class, &zapped_classes, lock_entry) {
+		reinit_class(class);
+		nr_lock_classes--;
+	}
+	list_splice_init(&zapped_classes, &free_lock_classes);
+	if (locked)
+		graph_unlock();
+	raw_local_irq_restore(flags);
+}
+
+/* Must be called with the graph lock held. */
+static void schedule_free_zapped_classes(void)
+{
+	if (rcu_callback_scheduled)
+		return;
+	rcu_callback_scheduled = true;
+	call_rcu(&free_zapped_classes_rcu_head, free_zapped_classes);
+}
+
 /*
  * Used in module.c to remove lock classes from memory that is going to be
  * freed; and possibly re-used by other modules.
@@ -4181,29 +4343,26 @@ void lockdep_free_key_range(void *start, unsigned long size)
 	for (i = 0; i < CLASSHASH_SIZE; i++) {
 		head = classhash_table + i;
 		hlist_for_each_entry_rcu(class, head, hash_entry) {
-			if (within(class->key, start, size))
-				zap_class(class);
-			else if (within(class->name, start, size))
-				zap_class(class);
+			if (!class->hash_entry.pprev ||
+			    (!within(class->key, start, size) &&
+			     !within(class->name, start, size)))
+				continue;
+			zap_class(class);
 		}
 	}
 
+	schedule_free_zapped_classes();
+
 	if (locked)
 		graph_unlock();
 	raw_local_irq_restore(flags);
 
 	/*
-	 * Wait for any possible iterators from look_up_lock_class() to pass
-	 * before continuing to free the memory they refer to.
-	 *
-	 * sync_sched() is sufficient because the read-side is IRQ disable.
-	 */
-	synchronize_sched();
-
-	/*
-	 * XXX at this point we could return the resources to the pool;
-	 * instead we leak them. We would need to change to bitmap allocators
-	 * instead of the linear allocators we have now.
+	 * Do not wait for concurrent look_up_lock_class() calls. If any such
+	 * concurrent call would return a pointer to one of the lock classes
+	 * freed by this function then that means that there is a race in the
+	 * code that calls look_up_lock_class(), namely concurrently accessing
+	 * and freeing a synchronization object.
 	 */
 }
 
@@ -4249,6 +4408,9 @@ void lockdep_reset_lock(struct lockdep_map *lock)
 		if (class)
 			zap_class(class);
 	}
+
+	schedule_free_zapped_classes();
+
 	/*
 	 * Debug check: in the end all mapped classes should
 	 * be gone.
-- 
2.20.0.rc2.403.gdbc3b29805-goog

