Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from pop3.zju.edu.cn (61.164.42.155:110) by
  likexu-MOBL1.ccr.corp.intel.com with POP3; 21 Nov 2018 10:59:03 -0000
Received: from icoremail.net (unknown [209.85.214.170])
	by mail-app2 (Coremail) with SMTP id by_KCgDXv0L33vRbk7vEAQ--.56384S3;
	Wed, 21 Nov 2018 12:28:40 +0800 (CST)
Received: from mail-pl1-f170.google.com (unknown [209.85.214.170])
	by mx2.icoremail.net (Coremail) with SMTP id AQAAfwCHbEj23vRba19hAA--.6671S3;
	Wed, 21 Nov 2018 12:28:39 +0800 (CST)
Received: by mail-pl1-f170.google.com with SMTP id b5-v6so3543035pla.6
        for <xuliker@zju.edu.cn>; Tue, 20 Nov 2018 20:28:39 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-original-authentication-results:x-gm-message-state:delivered-to
         :subject:to:cc:references:from:message-id:date:user-agent
         :mime-version:in-reply-to:content-language:content-transfer-encoding
         :sender:precedence:list-id;
        bh=qlXNnk0fz2zW30JawgPi0MLm0NV117hTjPsSlAiELmg=;
        b=JeNiXtwipTiRfYpc5IYLEPaLchLUERN4WhortHhBSTs6LwMIeGnys/9K59pIdphhVg
         KuQr6kdo6aJga27hZV+HlnTPaEMCNfuYVJMatP89rsovjt9rBOV4rEIpqu9uqAmHfVRm
         trLCmovWiOUVdOSvFjO/3br9MIFWcLd8/vqMeW+9b6OQ9MVaF+HX7QATIscqh/RPzSxB
         ZSbYZIWuHrAV+p5Hyma9iHJ176SKz50eI7cVsaIFDXUOAZImziwfZSzT+zaz8g5XxW5Y
         BgmIRnzXzmLVIp8CifTsUGxiw0ouGh30K5Nirz1qWoyBi+yNO0EBlZxrMVJNFCxfQb2v
         faog==
X-Original-Authentication-Results: mx.google.com;       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org
X-Gm-Message-State: AA+aEWZO6VBQyJ14jB0PmoJecAADx73zw7t94GfGdpvNx5o1CPMvhBJy
	8CJPFZ79G/RcFvuJggquB24GD8Y6ZNBqa+VynzHQnj/mJ6cbXQo=
X-Received: by 2002:a17:902:bd46:: with SMTP id b6mr5121123plx.231.1542774518585;
        Tue, 20 Nov 2018 20:28:38 -0800 (PST)
X-Forwarded-To: xuliker@zju.edu.cn
X-Forwarded-For: liker.xu@gmail.com xuliker@zju.edu.cn
Delivered-To: liker.xu@gmail.com
Received: by 2002:a17:90a:d106:0:0:0:0 with SMTP id l6-v6csp1472006pju;
        Tue, 20 Nov 2018 20:28:37 -0800 (PST)
X-Google-Smtp-Source: AJdET5dHCr/PVSCHghmVyHO/+jCGAMeFo5kKIC5iE5glYiazpw1dxsDo8mKGmIA0lbnBRMicTwLg
X-Received: by 2002:a62:13c3:: with SMTP id 64mr5156567pft.93.1542774517142;
        Tue, 20 Nov 2018 20:28:37 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1542774517; cv=none;
        d=google.com; s=arc-20160816;
        b=BIgaWQzBdu4/3aSr1WUipmbWYoal0vN3vzVH7LsLQYRMtnagcU3UrIBDfBufdxdiT8
         BKsmnqjzG2Dpq4pWo2MQvN2S/o/QJL2I2+cZ2wj3WW2cYGFiAyQFOmmy3LTwqKtlkLPO
         6drnXl0rEgCi457oU0WSfh9wL0mHGiTiVP3m/zKlD/8k3dTiJLN18N33djqTXQ4RLySX
         kvzx0ls3IJ8PGtREinaBKqua4VuQl774nR5jj94q9EghweQETjgTve2dZjyXHqKLYM5+
         OaB7TA+5NAtA2N6f6HG1HN6li9SjBN7ribM5Rr6ySJImUZB/p2JIWDo1hnmp/PS0Vmr2
         s3Vg==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=list-id:precedence:sender:content-transfer-encoding
         :content-language:in-reply-to:mime-version:user-agent:date
         :message-id:from:references:cc:to:subject;
        bh=qlXNnk0fz2zW30JawgPi0MLm0NV117hTjPsSlAiELmg=;
        b=EIhcO9M/R73RfDqRpjsSu03RhDBtGcOxPIi883q4pXL7o4Y/gRGGB325RBTKi1Yven
         jJdBfpNatj7yBWpQnYkhiGTaW5Jjkv3HUEn7R+LI/94V35kOWiD4RWA0zkTPR3J9H3qv
         iAkP7EPTuaIrY1qJD/jVxnPInRBOYPSwBSfl79Hc4KbrA4ot4yKT8bmicWq2Q01h7wTI
         ei9cq8f1XdAkUKLKd99b9+Gdol58QzCRorZchRN6148gbua0h9t9NEOP9oMWXXR5Qxs5
         5yyFnEBuUk8BNowKl+qbGaY6b18vCieM+Zpm9I+SCAchqfrGGfvg+HSI3XdMNeOZkVT8
         F8dg==
ARC-Authentication-Results: i=1; mx.google.com;
       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org
Received: from vger.kernel.org (vger.kernel.org. [209.132.180.67])
        by mx.google.com with ESMTP id cd16si18022021plb.47.2018.11.20.20.28.09;
        Tue, 20 Nov 2018 20:28:37 -0800 (PST)
Received-SPF: pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) client-ip=209.132.180.67;
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726894AbeKUO6f (ORCPT <rfc822;nullbytes00@gmail.com>
        + 99 others); Wed, 21 Nov 2018 09:58:35 -0500
Received: from mail-pl1-f194.google.com ([209.85.214.194]:34975 "EHLO
        mail-pl1-f194.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1725939AbeKUO6e (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Wed, 21 Nov 2018 09:58:34 -0500
Received: by mail-pl1-f194.google.com with SMTP id v1-v6so3546054plo.2;
        Tue, 20 Nov 2018 20:25:50 -0800 (PST)
X-Received: by 2002:a17:902:aa84:: with SMTP id d4-v6mr5236385plr.25.1542774349548;
        Tue, 20 Nov 2018 20:25:49 -0800 (PST)
Received: from ?IPv6:2601:647:4800:973f:8a0:7611:3223:f4db? ([2601:647:4800:973f:8a0:7611:3223:f4db])
        by smtp.gmail.com with ESMTPSA id t13sm86916556pgr.42.2018.11.20.20.25.47
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Tue, 20 Nov 2018 20:25:48 -0800 (PST)
Subject: Re: [PATCH V10 09/19] block: introduce bio_bvecs()
To: Ming Lei <ming.lei@redhat.com>
Cc: Christoph Hellwig <hch@lst.de>, Jens Axboe <axboe@kernel.dk>,
        linux-block@vger.kernel.org, linux-kernel@vger.kernel.org,
        linux-mm@kvack.org, Dave Chinner <dchinner@redhat.com>,
        Kent Overstreet <kent.overstreet@gmail.com>,
        Mike Snitzer <snitzer@redhat.com>, dm-devel@redhat.com,
        Alexander Viro <viro@zeniv.linux.org.uk>,
        linux-fsdevel@vger.kernel.org, Shaohua Li <shli@kernel.org>,
        linux-raid@vger.kernel.org, linux-erofs@lists.ozlabs.org,
        David Sterba <dsterba@suse.com>, linux-btrfs@vger.kernel.org,
        "Darrick J . Wong" <darrick.wong@oracle.com>,
        linux-xfs@vger.kernel.org, Gao Xiang <gaoxiang25@huawei.com>,
        Theodore Ts'o <tytso@mit.edu>, linux-ext4@vger.kernel.org,
        Coly Li <colyli@suse.de>, linux-bcache@vger.kernel.org,
        Boaz Harrosh <ooo@electrozaur.com>,
        Bob Peterson <rpeterso@redhat.com>, cluster-devel@redhat.com
References: <20181115085306.9910-1-ming.lei@redhat.com>
 <20181115085306.9910-10-ming.lei@redhat.com> <20181116134541.GH3165@lst.de>
 <002fe56b-25e4-573e-c09b-bb12c3e8d25a@grimberg.me>
 <20181120161651.GB2629@lst.de>
 <53526aae-fb9b-ee38-0a01-e5899e2d4e4d@grimberg.me>
 <20181121005902.GA31748@ming.t460p>
 <2d9bee7a-f010-dcf4-1184-094101058584@grimberg.me>
 <20181121034415.GA8408@ming.t460p>
From: Sagi Grimberg <sagi@grimberg.me>
Message-ID: <2a47d336-c19b-6bf4-c247-d7382871eeea@grimberg.me>
Date: Tue, 20 Nov 2018 20:25:46 -0800
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101
 Thunderbird/60.2.1
MIME-Version: 1.0
In-Reply-To: <20181121034415.GA8408@ming.t460p>
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Language: en-US
Content-Transfer-Encoding: 7bit
Sender: liker.xu+caf_=xuliker=zju.edu.cn@gmail.com
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-CM-TRANSID: AQAAfwCHbEj23vRba19hAA--.6671S3
Authentication-Results: mail-app2; spf=pass smtp.mail=liker.xu+caf_=xu
	liker=zju.edu.cn@gmail.com;
X-Coremail-Antispam: 1UD129KBjvJXoWxur15CF4UWr1xZF43XrWDJwb_yoWrGr13pF
	WYkFyqkrn7Jw4rKw4xX3W7W3sa93yrAFyUJw4rKw1rAwn5GF92gF48tF1a9Fs7ZF4v9r12
	vayqyayxWw1DZ3DanT9S1TB71UUUUUUqnTZGkaVYY2UrUUUU0bIjqfuFe4nvWSU8nxnvy2
	9KBjDU0xBIdaVrnRJUUUmqb7Iv0xC_Kw4lb4IE77IF4wAFF20E14v26r1j6r4UM7CIcVAF
	z4kK6r1j6r18M28lY4IEw2IIxxk0rwA2F7IY1VAKz4vEj48ve4kI8wA2z4x0Y4vE2Ix0cI
	8IcVAFwI0_Xr0_Ar1l84ACjcxK6xIIjxv20xvEc7CjxVAFwI0_Gr0_Cr1l84ACjcxK6I8E
	87Iv67AKxVW0oVCq3wA2z4x0Y4vEx4A2jsIEc7CjxVAFwI0_GcCE3s1le2I262IYc4CY6c
	8Ij28IcVAaY2xG8wAqx4xG64xvF2IEw4CE5I8CrVC2j2WlYx0E2Ix0cI8IcVAFwI0_Jr0_
	Jr4lYx0Ex4A2jsIE14v26r4j6F4UMcvjeVCFs4IE7xkEbVWUJVW8JwACjcxG0xvEwIxGrw
	CjxxvEa2IrMxk0xIA0c2IEe2xFo4CEbIxvr21lc7CjxVAKzI0EY4vE52x082I5MxkFs20E
	Y4vE44CYbxCE4x80FwCY02Avz4vEIxC_Grylc2IjII80xcxEwVAKI48JMxvI42IY6xIIjx
	v20xvE14v26r1I6r4UMxvI42IY6xIIjxv20xvEc7CjxVAFwI0_Jr0_Gr1lcIIF0xvEx4A2
	jsIE14v26F4UJVW0owCYIxAIcVC2z280aVCY1x0267AKxVWxJr0_GcWl42xK82IYc2Ij64
	vIr41l42xK82IY64kExVAvwVAq07x20xyl4x8a6x804xWl4I8I3I0E4IkC6x0Yz7v_Jr0_
	Gr1lx2IqxVAqx4xG67AKxVWUJVWUGwC20s026x8GjcxK67AKxVWUGVWUWwC2zVAF1VAY17
	CE14v26r4a6rW5MIIYrxkI7VAKI48JMIIF0xvE42xK8VAvwI8IcIk0rVWrJr0_WFyUJbIY
	CTnIWIevJa73UjIFyTuYvjxUHZN3UUUUU


>> I would like to avoid growing bvec tables and keep everything
>> preallocated. Plus, a bvec_iter operates on a bvec which means
>> we'll need a table there as well... Not liking it so far...
> 
> In case of bios in one request, we can't know how many bvecs there
> are except for calling rq_bvecs(), so it may not be suitable to
> preallocate the table. If you have to send the IO request in one send(),
> runtime allocation may be inevitable.

I don't want to do that, I want to work on a single bvec at a time like
the current implementation does.

> If you don't require to send the IO request in one send(), you may send
> one bio in one time, and just uses the bio's bvec table directly,
> such as the single bio case in lo_rw_aio().

we'd need some indication that we need to reinit my iter with the
new bvec, today we do:

static inline void nvme_tcp_advance_req(struct nvme_tcp_request *req,
                 int len)
{
         req->snd.data_sent += len;
         req->pdu_sent += len;
         iov_iter_advance(&req->snd.iter, len);
         if (!iov_iter_count(&req->snd.iter) &&
             req->snd.data_sent < req->data_len) {
                 req->snd.curr_bio = req->snd.curr_bio->bi_next;
                 nvme_tcp_init_send_iter(req);
         }
}

and initialize the send iter. I imagine that now I will need to
switch to the next bvec and only if I'm on the last I need to
use the next bio...

Do you offer an API for that?


>>> can this way avoid your blocking issue? You may see this
>>> example in branch 'rq->bio != rq->biotail' of lo_rw_aio().
>>
>> This is exactly an example of not ignoring the bios...
> 
> Yeah, that is the most common example, given merge is enabled
> in most of cases. If the driver or device doesn't care merge,
> you can disable it and always get single bio request, then the
> bio's bvec table can be reused for send().

Does bvec_iter span bvecs with your patches? I didn't see that change?

>> I'm not sure how this helps me either. Unless we can set a bvec_iter to
>> span bvecs or have an abstract bio crossing when we re-initialize the
>> bvec_iter I don't see how I can ignore bios completely...
> 
> rq_for_each_bvec() will iterate over all bvecs from all bios, so you
> needn't to see any bio in this req.

But I don't need this iteration, I need a transparent API like;
bvec2 = rq_bvec_next(rq, bvec)

This way I can simply always reinit my iter without thinking about how
the request/bios/bvecs are constructed...

> rq_bvecs() will return how many bvecs there are in this request(cover
> all bios in this req)

Still not very useful given that I don't want to use a table...

>>> So looks nvme-tcp host driver might be the 2nd driver which benefits
>>> from multi-page bvec directly.
>>>
>>> The multi-page bvec V11 has passed my tests and addressed almost
>>> all the comments during review on V10. I removed bio_vecs() in V11,
>>> but it won't be big deal, we can introduce them anytime when there
>>> is the requirement.
>>
>> multipage-bvecs and nvme-tcp are going to conflict, so it would be good
>> to coordinate on this. I think that nvme-tcp host needs some adjustments
>> as setting a bvec_iter. I'm under the impression that the change is rather
>> small and self-contained, but I'm not sure I have the full
>> picture here.
> 
> I guess I may not get your exact requirement on block io iterator from nvme-tcp
> too, :-(

They are pretty much listed above. Today nvme-tcp sets an iterator with:

vec = __bvec_iter_bvec(bio->bi_io_vec, bio->bi_iter);
nsegs = bio_segments(bio);
size = bio->bi_iter.bi_size;
offset = bio->bi_iter.bi_bvec_done;
iov_iter_bvec(&req->snd.iter, WRITE, vec, nsegs, size);

and when done, iterate to the next bio and do the same.

With multipage bvec it would be great if we can simply have
something like rq_bvec_next() that would pretty much satisfy
the requirements from the nvme-tcp side...
