Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 28 Nov 2018 08:47:31 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga006.jf.intel.com (orsmga006.jf.intel.com [10.7.209.51])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 1386C58041B;
	Tue, 27 Nov 2018 08:59:01 -0800 (PST)
Received: from orsmga106.jf.intel.com ([10.7.208.65])
  by orsmga006-1.jf.intel.com with ESMTP; 27 Nov 2018 08:59:00 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3Ar9EfTBRLg6BVfeOtzq5jjjYWmdpsv+yvbD5Q0YIu?=
 =?us-ascii?q?jvd0So/mwa65YRePt8tkgFKBZ4jH8fUM07OQ7/iwHzRYqb+681k6OKRWUBEEjc?=
 =?us-ascii?q?hE1ycBO+WiTXPBEfjxciYhF95DXlI2t1uyMExSBdqsLwaK+i764jEdAAjwOhRo?=
 =?us-ascii?q?LerpBIHSk9631+ev8JHPfglEnjWwba9xIRmssQndqtQdjJd/JKo21hbHuGZDdf?=
 =?us-ascii?q?5MxWNvK1KTnhL86dm18ZV+7SleuO8v+tBZX6nicKs2UbJXDDI9M2Ao/8LrrgXM?=
 =?us-ascii?q?TRGO5nQHTGoblAdDDhXf4xH7WpfxtTb6tvZ41SKHM8D6Uaw4VDK/5KpwVhTmlD?=
 =?us-ascii?q?kIOCI48GHPi8x/kqRboA66pxdix4LYeZyZOOZicq/Ye94RWGhPUdtLVyFZAo2y?=
 =?us-ascii?q?cZYBAeQCM+hfrYb9qVQBoxSlBQm0Bu7i0SNEi3zs0KEmyektDR3K0Qo9FNwOqn?=
 =?us-ascii?q?TUq9D1Ob8OXOCz0abI1yvMbvNL0jn474jIdBchoe+WUrJ0dsrR11QkGgTfgVWW?=
 =?us-ascii?q?tIPlJS2a1+QOs2id8epgUfmii2EgqwF2rTivwtkjhpPViYISz1DJ7CN0y5s2K9?=
 =?us-ascii?q?2gUEN3f8KoHZ9Kuy2HOYZ6XNkuT3xrtSom0LELuJy2cDAUxJs92xLTd/mKfoqM?=
 =?us-ascii?q?7x39SOqcJCp0iXJgdb6imxq/9EatxvD/W8Wo1VtHoTdJktfPu30I2RHf9tWLRu?=
 =?us-ascii?q?d480ev1zaC1h3f5+dZKk4uj6XbMYQuwrsom5oTr0vDGij2lV3ojK+ZaEok4PKk?=
 =?us-ascii?q?6+f5bbX8oJ+TKYt0hhv5MqQ0lcyzGeU4Mg4QUGiH4emwyqHv8EnjTLlXgPA6jL?=
 =?us-ascii?q?PVvI3ZKMgHvKK0Ag1Y3p4m6xmlDjem1NoYnWMALFJAYB+Hi4npO1fTIPH3FPu/?=
 =?us-ascii?q?gEqjkC1tx//YOr3tG5LNL3bFkLj/Z7Zw8FBcyAUtwtBF/Z5UCa8OIOj1WkDvsN?=
 =?us-ascii?q?zUFBg5Mxa7w+r/EtVyypseWX6TAq+eKK7StV6I5uExLOWWa44VpS3wK/wk5/7o?=
 =?us-ascii?q?kH84lkURfaiv3ZsLdn+4Gu5qLFmeYXrpmt0BC3sFvhIiTOz2j12PST5TaGyzX6?=
 =?us-ascii?q?Ig/D47D5iqDYfeRo+3hryB0zy2HplXZmBAF1CNHm3kd4SCW/cQdi2SJtVtnSAD?=
 =?us-ascii?q?VbikU4Uhzw2htBfmy7p7KerZ4i8YtZX929Rv5O3Tkhcy9TpzD8mG12GNTmd0nn?=
 =?us-ascii?q?4HRjMs3aB/p1B9xUmH0aRin/NYEtlT7etTUggmLZ7c0/B6C9fqVw3bZdeJSFGm?=
 =?us-ascii?q?Qta8DTErVN0xwcQDY0J8G9WkkxDC0DCmA74Tl7yXGpM09rjQ0GT2J8Z403zGzr?=
 =?us-ascii?q?Uuj0E6QstTMm2rnq1/+BLVB4LTl0WZlryldaIT3CPW8GeDzGyOvFxXUQJqUKXF?=
 =?us-ascii?q?W2wfaVXSrdjj+kzCSLquA6w9MgRd0c6CNrdKatrxgFpbXvjjJsrRb3ixm2iqAx?=
 =?us-ascii?q?aI3a2DbIztd2UZ3yXdDUwEnhsX/XaHMwg+Gyigr3jfDDxoCVLgfUfs/fNip3O8?=
 =?us-ascii?q?S08+1xuKYFF517qp5h4VguSRRO4J0bIapigtsTV0E0y739LNFdWAoRFsfKFdYd?=
 =?us-ascii?q?M7/VdG2njVtw17Ppy8Ma9igkQSfBhwv0Przx93EJlPkdA2rHM2ywp/Mb6Y0FJE?=
 =?us-ascii?q?dzOfwZDwOr3WJnPu/By1bK7WwFXe0NeQ+qcA8/k4r1TjvAe0Fkst6Xln0t9V02?=
 =?us-ascii?q?eC6ZXOFgYdTZXxUkMv/Rhgu77aejU955/T1XB0K6a0tiHN2t03C+ol0BqvZMpf?=
 =?us-ascii?q?P76eGw/0EM0aANauJfcum1ioaBIEIe9T+LQ1P8Oga/uJxqqrMPx8kzKhiGRN+J?=
 =?us-ascii?q?p93V6U9ypgVu7I2I4IwvGF3gedSzjwllagssDtloBCajEfBW6/ySniBI5Maax+?=
 =?us-ascii?q?Z4cLCWGyI8KpwtVynYLiW3ld9FS7HVMJxNepeQaOb1z6xQBQz0UXrmC9liui0j?=
 =?us-ascii?q?N0lCslrqyB0yzUwuTubQYINXRPRGZ/k1jsO4+0j9YBUUisbggpkgal5Engy6ha?=
 =?us-ascii?q?oqR/M3fcQUNScyfqKGFiV7O6tqCebM5X9JMorSJXXfy8YFCbULL8owEW0zj+H2?=
 =?us-ascii?q?tY3z07czCqupPknx11km6dLXBzrGbHdsF03xvQ+NvcRftJ1DocWCZ4kSXXBkS7?=
 =?us-ascii?q?P9Sx/9WbjY3DsuO9V2KmTJFTajPkzYCDtCu6421lHxu/kuu3mt3mFwg6zCD628?=
 =?us-ascii?q?NrVSXOsBbzfI3r276mPuJge0liHEX85NZiGoFijoswg4kd2HgAiZWS53YHk2bz?=
 =?us-ascii?q?MdNA1KL6bXoNQyMLwtHP7Ajk3k1jMmyGx4bjWnqBxcthYsGwYnkK1SIl88BKFK?=
 =?us-ascii?q?CU4aRHnSRvo1q0twLRYeVnkTcbxvsj83oag+APuAowwSSRGLESHU9EPSPykxSE?=
 =?us-ascii?q?9cyxrKJSZGy3a7i/yFJ+ncy9DLGFugxcXXf5epQ4EiNq4MRwLknM0GHt5YH+Y9?=
 =?us-ascii?q?bfc8gTthKPnhfEjuhVLo8xl/UQiSpmP2L9oWMqy+ohgRNy2pG6uZCNK39x86Kh?=
 =?us-ascii?q?Hh5YKjr1atsP+jHqiKZShMeX0JqpHpV8ATULR5roQOmsEDITs/TnKgmPHCc9qn?=
 =?us-ascii?q?edBbrQAwuf5F16oHLIFpChL2uXK2UBzdV+WBmdI1RSjxsOUzU9mp45CxqmxMj8?=
 =?us-ascii?q?cEpi4jAR51j4qgZDy+5yNhn/VHvfqxmsajsuVJefKx9W5BlY50jJKcye8v5zHy?=
 =?us-ascii?q?ZA852jtgONL3KUZx9SAWEVQECEBE3jPrqz5dnG6eeYHfGzL//PYbWItOxfWO2E?=
 =?us-ascii?q?xZOp0ot64TmMMt+DMWVlD/0+wkBDR2x2G9zFmzUTTCwajyLMb8mGpBel5y14tM?=
 =?us-ascii?q?a//O7wWALo44uPBKBfMdFu+xCwnKeCOPSchCd/KTZEyJwMwWXEx6QY3F4Xkytu?=
 =?us-ascii?q?bSWiEawctS7RS6Ldgq9XAAAaayNwN8tI7rgw3gpXOc7chdP6yKR4juMuC1peUV?=
 =?us-ascii?q?zhm8epZdEFIm2nNVPHAlqLO6qCJTHR3873Zqa8Q6VKjOpIrx2wpSqbE0j7MzuZ?=
 =?us-ascii?q?ijnmTAqvPv9MjS2BOBxeuZqwchJsCWjlUdLnZQe3MN5xjT0q37I0gmnGOnIbMT?=
 =?us-ascii?q?h5a0lNtKGf7TtEgvVjHGxM9mZlLeiBmyqD8+bUMIoZsft1DSRyjO9a5HU6y71I?=
 =?us-ascii?q?7CBLXvB1mS3SrsJwrFGiiOWA1j1nUB9WoDZRmI2LpVliObne9pRYWXfL5hQN7W?=
 =?us-ascii?q?CRCxQMv9RkC93vtLpWytjAj6/zLDZC89TJ/coTHcTUKcSHMGY/PhrtAjLbEAwF?=
 =?us-ascii?q?TTv4fV3Y0mVbnOuf5zWvv5Ewq57tntJaQ6VKXVY0EPIcIlpkBsAfJ4VlWXUvlr?=
 =?us-ascii?q?vNyIYh4XeupR/dSd8ShpfbX+qfB/boYGKWibhIagEgzrT/IoAecIb83homInV/?=
 =?us-ascii?q?nZjQCgLuTM1OpTMpOgMqsVhO92ZWQWo01EboLAiq5SlXXdW9ghcyiw02RO039T?=
 =?us-ascii?q?Hl+B9jLVPWrS8xlg80kM/khzmKWDfrKeG7WoQAT2LLtkw1er/2Sg1ya0Xmgk1g?=
 =?us-ascii?q?MHHISrtaiL1IfHpuzgTbvM0LUdVcV6xffBgIxbmzbvAlzVlG4nGswWdD5O3YGd?=
 =?us-ascii?q?1pkw58NdaFqX5a1h0rSdkvOavWbP5AzUBVrqaPuDK4k+423QkSLloM92XUfzQH?=
 =?us-ascii?q?7ghAE7A8JGKS4/Bo4AqO029GcXIPEeA3vvZj8E8VMv6FiSnn1ugHYmawPO/XA6?=
 =?us-ascii?q?6CsnXEk8+FWEh4gkUEmlJM4Lxw0Mo5aGKbUEcgyLbXHBMMY46KDAhPZIJg5X7c?=
 =?us-ascii?q?fCuU+bHIyIh+eZesEO/pQPWmvbwRxEmjGVBtV6gF8MlJO5C2zEDeZZPrJaAIjx?=
 =?us-ascii?q?Ak/hjqIn2MFvEPcxWOxnNPituzhLV20JJdKywDDGE1ZSyt6vDdpwIxgPeeQtoy?=
 =?us-ascii?q?SnMbWIIeMTQxQsLs32YTkm5BB3GImqRR4gmP4zLmoy3WRnGoYtBiafuZfxZEGN?=
 =?us-ascii?q?S34i4+9K7wglnSpNGWBmf1MpxHvdvJ4PlS85OGDfxTZbpwqUHRn89fXXP8Fyb3?=
 =?us-ascii?q?GMOxb7z5dJMha9f5QiKiW0G7jzsqVcr3N9uFIa+PjgXlA41Ttd/flB8uONWhXg?=
 =?us-ascii?q?wPBwt9veEE6bxxY0VXbJc2ewLAswU4Krz6JVnI0ZOpWWn7bXN/Vf9Thc+3fbNS?=
 =?us-ascii?q?xiwqJruz0n0sZpggyeW9t0IAEtVCxDLf3/enY8F3GW67IXxAZwjKojRz3zxiN/?=
 =?us-ascii?q?wzxuN52xPVrVQZNC6jcO1vdXwCvtcgCFfUKnJzXC5waEWRgsLg+Aml3/Uy9jFB?=
 =?us-ascii?q?mt9SmblKqnXxvZ/3ZDOqRbylrojTvyM8bN8g5aprPtqnauiPqJ6WpSDSU5DKrk?=
 =?us-ascii?q?XRUzWmGuVTgMR4JCNeXelGnnwjNcUauI1Hr00rWZF6b5lOC6RknLmybiJjFzJa?=
 =?us-ascii?q?mTUBT5+o2D0En/f6xb3XilGRepFkOxsB5tEKgdIGWigzbi4AjKCkUIrSmmCeTX?=
 =?us-ascii?q?UTO0EY6gEIrAYBkJJgO+zi66LWQ5JWjT1bufR5Vm3MDJYsv1/6THyGxFv1UvOs?=
 =?us-ascii?q?l8S30g9IivHhyN8WXFh4E0cZj+JXkFY4bbhtLuwTs5TMvzugcULhoHmrye2oPl?=
 =?us-ascii?q?BdxMTYMVriA86NsWv6Tz1Z4mUdX5RC4G/QGI5UkAdjbqsv4lJWL8Tufkf4+ixh?=
 =?us-ascii?q?woFBHKexXsPtwEwq6T4CRiG3A59CAfxntF7/RjJoedaopY/jNpEURXVfq7OHrF?=
 =?us-ascii?q?IMuUNhKSOogblbLcZT6zoLQnAbqDORptajS8td3ed5CJgDJto5sHD4TvAXcKON?=
 =?us-ascii?q?qmE77+S8gkTS/Cox5RLjnG2+?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AcAADtdv1bh0O0hNFkHAEBAQQBAQcEA?=
 =?us-ascii?q?QGBUQcBAQsBgTCCOyeMEYwIgWg5kjWEd4EkA1AOAQEYEwGBKoIChU4iNAkNAQM?=
 =?us-ascii?q?BAQEBAQECARMBAQEIDQkIKSMMgjYkAYJiAwMBAhcNCwENAQEeGQEFCQEBGDgDM?=
 =?us-ascii?q?QEFARwGARIFgxyCAgEEmXo8ih2BbDOCdgEBBYEEAQGGHggSh1CDD4EcF4F/gUS?=
 =?us-ascii?q?FfQKHPIkFIQOBbpR3CYIggxKLfhiBWYd5NocDLJdkAgQCBAUCBQ8hgSVsgSFNI?=
 =?us-ascii?q?4M8ghsMF4NKihwBNz8ygQUBAY0PAQE?=
X-IPAS-Result: =?us-ascii?q?A0AcAADtdv1bh0O0hNFkHAEBAQQBAQcEAQGBUQcBAQsBgTC?=
 =?us-ascii?q?COyeMEYwIgWg5kjWEd4EkA1AOAQEYEwGBKoIChU4iNAkNAQMBAQEBAQECARMBA?=
 =?us-ascii?q?QEIDQkIKSMMgjYkAYJiAwMBAhcNCwENAQEeGQEFCQEBGDgDMQEFARwGARIFgxy?=
 =?us-ascii?q?CAgEEmXo8ih2BbDOCdgEBBYEEAQGGHggSh1CDD4EcF4F/gUSFfQKHPIkFIQOBb?=
 =?us-ascii?q?pR3CYIggxKLfhiBWYd5NocDLJdkAgQCBAUCBQ8hgSVsgSFNI4M8ghsMF4NKihw?=
 =?us-ascii?q?BNz8ygQUBAY0PAQE?=
X-IronPort-AV: E=Sophos;i="5.56,287,1539673200"; 
   d="scan'208";a="41757989"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 27 Nov 2018 08:58:58 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1731655AbeK1Dyh (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Tue, 27 Nov 2018 22:54:37 -0500
Received: from mail-wr1-f66.google.com ([209.85.221.66]:43412 "EHLO
        mail-wr1-f66.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1731626AbeK1Dye (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Tue, 27 Nov 2018 22:54:34 -0500
Received: by mail-wr1-f66.google.com with SMTP id r10so23423057wrs.10
        for <linux-kernel@vger.kernel.org>; Tue, 27 Nov 2018 08:56:02 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=google.com; s=20161025;
        h=from:to:cc:subject:date:message-id:in-reply-to:references
         :mime-version:content-transfer-encoding;
        bh=+9PASwcFfpBLwSrOwdc0AKJIhUGrXXEqSi5SjKKHdKY=;
        b=u/tHG7L/dvzXO2Z5xqv5L5gom3lFGmLsX7vVGU3qPNV5x4MLKRQwycUOifAbttsa4m
         ZvfEPPzmxM1/tDWJaUAwQn2EcDG39xu0t2aT03r/UYRdiWnGaMZX8++ZYWlyrA9jt1At
         ix01aJC3UMH4eLOM4EFhwYq7YJzBbHXOeomWC0QTp57nZbj9udbNjfw7V4nZGXU99r6U
         kT5QlOOuiFDBSHhjWdndolUwx9T62YImsfthNi2pNijFd4DtQcU0bho3OTqxyi5T6Wfz
         WdlAyunoD/gA7nXEyHIqPv53u8+3cFoxiFTdfJTtHBs9xyIKWMhsiX+PVm9yCyF4N5cX
         owuA==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
         :references:mime-version:content-transfer-encoding;
        bh=+9PASwcFfpBLwSrOwdc0AKJIhUGrXXEqSi5SjKKHdKY=;
        b=rtUxRj21s1pa5+fjK3knwN/ku3CMBICE1HMYPttPiBPBSgSeYoLJRjRIHIrz0RyWY3
         6/oxi7OHhvpR2NX7UYHSKvrp0W2V4bzXghudQGRWwEUVSvM9RLmPsvtaL8RZoLi4X4Gc
         s/ff9J3bnbpz3mexKtcQJnpHUTcEmZlnurVzb+muAyV7ql7M01A8ygq3cFp8dPsJYoF/
         v/YqNceJ/gGh/A4loM5/NHRut64D46Z/mbgjjZECJszWDJzrGFFz1DwG49BSM+YUpStU
         GzuqLvsivwe+Aur2Sboc0rzTB2WQAQ0Z17zfGadfOlR77jrwy5/Ak/Hk3PS3FQecQFE1
         uupQ==
X-Gm-Message-State: AA+aEWap9DBqS42GDOdhYIyn4q/QhQaLOIsvxH+fGYIhcUTvy2xl6DQV
        VMtRfZHEy336O0u8kP43cYbRLw==
X-Google-Smtp-Source: AFSGD/WyX7Oc54FZ4FIdPTTCIIIhOaHS2bfDZhgW5z/SZTk1Hy43v05HyH+kROj1PpvLUIhysHUBoA==
X-Received: by 2002:adf:9dd2:: with SMTP id q18mr28381177wre.12.1543337761151;
        Tue, 27 Nov 2018 08:56:01 -0800 (PST)
Received: from andreyknvl0.muc.corp.google.com ([2a00:79e0:15:10:3180:41f8:3010:ff61])
        by smtp.gmail.com with ESMTPSA id k73sm6383099wmd.36.2018.11.27.08.55.59
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Tue, 27 Nov 2018 08:56:00 -0800 (PST)
From: Andrey Konovalov <andreyknvl@google.com>
To: Andrey Ryabinin <aryabinin@virtuozzo.com>,
        Alexander Potapenko <glider@google.com>,
        Dmitry Vyukov <dvyukov@google.com>,
        Catalin Marinas <catalin.marinas@arm.com>,
        Will Deacon <will.deacon@arm.com>,
        Christoph Lameter <cl@linux.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Mark Rutland <mark.rutland@arm.com>,
        Nick Desaulniers <ndesaulniers@google.com>,
        Marc Zyngier <marc.zyngier@arm.com>,
        Dave Martin <dave.martin@arm.com>,
        Ard Biesheuvel <ard.biesheuvel@linaro.org>,
        "Eric W . Biederman" <ebiederm@xmission.com>,
        Ingo Molnar <mingo@kernel.org>,
        Paul Lawrence <paullawrence@google.com>,
        Geert Uytterhoeven <geert@linux-m68k.org>,
        Arnd Bergmann <arnd@arndb.de>,
        "Kirill A . Shutemov" <kirill.shutemov@linux.intel.com>,
        Greg Kroah-Hartman <gregkh@linuxfoundation.org>,
        Kate Stewart <kstewart@linuxfoundation.org>,
        Mike Rapoport <rppt@linux.vnet.ibm.com>,
        kasan-dev@googlegroups.com, linux-doc@vger.kernel.org,
        linux-kernel@vger.kernel.org, linux-arm-kernel@lists.infradead.org,
        linux-sparse@vger.kernel.org, linux-mm@kvack.org,
        linux-kbuild@vger.kernel.org
Cc: Kostya Serebryany <kcc@google.com>,
        Evgeniy Stepanov <eugenis@google.com>,
        Lee Smith <Lee.Smith@arm.com>,
        Ramana Radhakrishnan <Ramana.Radhakrishnan@arm.com>,
        Jacob Bramley <Jacob.Bramley@arm.com>,
        Ruben Ayrapetyan <Ruben.Ayrapetyan@arm.com>,
        Jann Horn <jannh@google.com>,
        Mark Brand <markbrand@google.com>,
        Chintan Pandya <cpandya@codeaurora.org>,
        Vishwath Mohan <vishwath@google.com>,
        Andrey Konovalov <andreyknvl@google.com>
Subject: [PATCH v12 07/25] kasan: rename kasan_zero_page to kasan_early_shadow_page
Date: Tue, 27 Nov 2018 17:55:25 +0100
Message-Id: <1aa5a8e562380e0bfb1d58c8ec3130436cff8b47.1543337629.git.andreyknvl@google.com>
X-Mailer: git-send-email 2.20.0.rc0.387.gc7a69e6b6c-goog
In-Reply-To: <cover.1543337629.git.andreyknvl@google.com>
References: <cover.1543337629.git.andreyknvl@google.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

With tag based KASAN mode the early shadow value is 0xff and not 0x00,
so this patch renames kasan_zero_(page|pte|pmd|pud|p4d) to
kasan_early_shadow_(page|pte|pmd|pud|p4d) to avoid confusion.

Suggested-by: Mark Rutland <mark.rutland@arm.com>
Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
---
 arch/arm64/mm/kasan_init.c     | 43 ++++++++++++---------
 arch/s390/mm/dump_pagetables.c | 17 +++++----
 arch/s390/mm/kasan_init.c      | 33 +++++++++-------
 arch/x86/mm/dump_pagetables.c  | 11 +++---
 arch/x86/mm/kasan_init_64.c    | 55 +++++++++++++-------------
 arch/xtensa/mm/kasan_init.c    | 18 +++++----
 include/linux/kasan.h          | 12 +++---
 mm/kasan/init.c                | 70 +++++++++++++++++++---------------
 8 files changed, 145 insertions(+), 114 deletions(-)

diff --git a/arch/arm64/mm/kasan_init.c b/arch/arm64/mm/kasan_init.c
index 63527e585aac..4ebc19422931 100644
--- a/arch/arm64/mm/kasan_init.c
+++ b/arch/arm64/mm/kasan_init.c
@@ -47,8 +47,9 @@ static pte_t *__init kasan_pte_offset(pmd_t *pmdp, unsigned long addr, int node,
 				      bool early)
 {
 	if (pmd_none(READ_ONCE(*pmdp))) {
-		phys_addr_t pte_phys = early ? __pa_symbol(kasan_zero_pte)
-					     : kasan_alloc_zeroed_page(node);
+		phys_addr_t pte_phys = early ?
+				__pa_symbol(kasan_early_shadow_pte)
+					: kasan_alloc_zeroed_page(node);
 		__pmd_populate(pmdp, pte_phys, PMD_TYPE_TABLE);
 	}
 
@@ -60,8 +61,9 @@ static pmd_t *__init kasan_pmd_offset(pud_t *pudp, unsigned long addr, int node,
 				      bool early)
 {
 	if (pud_none(READ_ONCE(*pudp))) {
-		phys_addr_t pmd_phys = early ? __pa_symbol(kasan_zero_pmd)
-					     : kasan_alloc_zeroed_page(node);
+		phys_addr_t pmd_phys = early ?
+				__pa_symbol(kasan_early_shadow_pmd)
+					: kasan_alloc_zeroed_page(node);
 		__pud_populate(pudp, pmd_phys, PMD_TYPE_TABLE);
 	}
 
@@ -72,8 +74,9 @@ static pud_t *__init kasan_pud_offset(pgd_t *pgdp, unsigned long addr, int node,
 				      bool early)
 {
 	if (pgd_none(READ_ONCE(*pgdp))) {
-		phys_addr_t pud_phys = early ? __pa_symbol(kasan_zero_pud)
-					     : kasan_alloc_zeroed_page(node);
+		phys_addr_t pud_phys = early ?
+				__pa_symbol(kasan_early_shadow_pud)
+					: kasan_alloc_zeroed_page(node);
 		__pgd_populate(pgdp, pud_phys, PMD_TYPE_TABLE);
 	}
 
@@ -87,8 +90,9 @@ static void __init kasan_pte_populate(pmd_t *pmdp, unsigned long addr,
 	pte_t *ptep = kasan_pte_offset(pmdp, addr, node, early);
 
 	do {
-		phys_addr_t page_phys = early ? __pa_symbol(kasan_zero_page)
-					      : kasan_alloc_zeroed_page(node);
+		phys_addr_t page_phys = early ?
+				__pa_symbol(kasan_early_shadow_page)
+					: kasan_alloc_zeroed_page(node);
 		next = addr + PAGE_SIZE;
 		set_pte(ptep, pfn_pte(__phys_to_pfn(page_phys), PAGE_KERNEL));
 	} while (ptep++, addr = next, addr != end && pte_none(READ_ONCE(*ptep)));
@@ -205,14 +209,14 @@ void __init kasan_init(void)
 	kasan_map_populate(kimg_shadow_start, kimg_shadow_end,
 			   early_pfn_to_nid(virt_to_pfn(lm_alias(_text))));
 
-	kasan_populate_zero_shadow((void *)KASAN_SHADOW_START,
-				   (void *)mod_shadow_start);
-	kasan_populate_zero_shadow((void *)kimg_shadow_end,
-				   kasan_mem_to_shadow((void *)PAGE_OFFSET));
+	kasan_populate_early_shadow((void *)KASAN_SHADOW_START,
+				    (void *)mod_shadow_start);
+	kasan_populate_early_shadow((void *)kimg_shadow_end,
+				    kasan_mem_to_shadow((void *)PAGE_OFFSET));
 
 	if (kimg_shadow_start > mod_shadow_end)
-		kasan_populate_zero_shadow((void *)mod_shadow_end,
-					   (void *)kimg_shadow_start);
+		kasan_populate_early_shadow((void *)mod_shadow_end,
+					    (void *)kimg_shadow_start);
 
 	for_each_memblock(memory, reg) {
 		void *start = (void *)__phys_to_virt(reg->base);
@@ -227,14 +231,15 @@ void __init kasan_init(void)
 	}
 
 	/*
-	 * KAsan may reuse the contents of kasan_zero_pte directly, so we
-	 * should make sure that it maps the zero page read-only.
+	 * KAsan may reuse the contents of kasan_early_shadow_pte directly,
+	 * so we should make sure that it maps the zero page read-only.
 	 */
 	for (i = 0; i < PTRS_PER_PTE; i++)
-		set_pte(&kasan_zero_pte[i],
-			pfn_pte(sym_to_pfn(kasan_zero_page), PAGE_KERNEL_RO));
+		set_pte(&kasan_early_shadow_pte[i],
+			pfn_pte(sym_to_pfn(kasan_early_shadow_page),
+				PAGE_KERNEL_RO));
 
-	memset(kasan_zero_page, 0, PAGE_SIZE);
+	memset(kasan_early_shadow_page, 0, PAGE_SIZE);
 	cpu_replace_ttbr1(lm_alias(swapper_pg_dir));
 
 	/* At this point kasan is fully initialized. Enable error messages */
diff --git a/arch/s390/mm/dump_pagetables.c b/arch/s390/mm/dump_pagetables.c
index 363f6470d742..3b93ba0b5d8d 100644
--- a/arch/s390/mm/dump_pagetables.c
+++ b/arch/s390/mm/dump_pagetables.c
@@ -111,11 +111,12 @@ static void note_page(struct seq_file *m, struct pg_state *st,
 }
 
 #ifdef CONFIG_KASAN
-static void note_kasan_zero_page(struct seq_file *m, struct pg_state *st)
+static void note_kasan_early_shadow_page(struct seq_file *m,
+						struct pg_state *st)
 {
 	unsigned int prot;
 
-	prot = pte_val(*kasan_zero_pte) &
+	prot = pte_val(*kasan_early_shadow_pte) &
 		(_PAGE_PROTECT | _PAGE_INVALID | _PAGE_NOEXEC);
 	note_page(m, st, prot, 4);
 }
@@ -154,8 +155,8 @@ static void walk_pmd_level(struct seq_file *m, struct pg_state *st,
 	int i;
 
 #ifdef CONFIG_KASAN
-	if ((pud_val(*pud) & PAGE_MASK) == __pa(kasan_zero_pmd)) {
-		note_kasan_zero_page(m, st);
+	if ((pud_val(*pud) & PAGE_MASK) == __pa(kasan_early_shadow_pmd)) {
+		note_kasan_early_shadow_page(m, st);
 		return;
 	}
 #endif
@@ -185,8 +186,8 @@ static void walk_pud_level(struct seq_file *m, struct pg_state *st,
 	int i;
 
 #ifdef CONFIG_KASAN
-	if ((p4d_val(*p4d) & PAGE_MASK) == __pa(kasan_zero_pud)) {
-		note_kasan_zero_page(m, st);
+	if ((p4d_val(*p4d) & PAGE_MASK) == __pa(kasan_early_shadow_pud)) {
+		note_kasan_early_shadow_page(m, st);
 		return;
 	}
 #endif
@@ -215,8 +216,8 @@ static void walk_p4d_level(struct seq_file *m, struct pg_state *st,
 	int i;
 
 #ifdef CONFIG_KASAN
-	if ((pgd_val(*pgd) & PAGE_MASK) == __pa(kasan_zero_p4d)) {
-		note_kasan_zero_page(m, st);
+	if ((pgd_val(*pgd) & PAGE_MASK) == __pa(kasan_early_shadow_p4d)) {
+		note_kasan_early_shadow_page(m, st);
 		return;
 	}
 #endif
diff --git a/arch/s390/mm/kasan_init.c b/arch/s390/mm/kasan_init.c
index acb9645b762b..bac5c27d11fc 100644
--- a/arch/s390/mm/kasan_init.c
+++ b/arch/s390/mm/kasan_init.c
@@ -107,7 +107,8 @@ static void __init kasan_early_vmemmap_populate(unsigned long address,
 			if (mode == POPULATE_ZERO_SHADOW &&
 			    IS_ALIGNED(address, PGDIR_SIZE) &&
 			    end - address >= PGDIR_SIZE) {
-				pgd_populate(&init_mm, pg_dir, kasan_zero_p4d);
+				pgd_populate(&init_mm, pg_dir,
+						kasan_early_shadow_p4d);
 				address = (address + PGDIR_SIZE) & PGDIR_MASK;
 				continue;
 			}
@@ -120,7 +121,8 @@ static void __init kasan_early_vmemmap_populate(unsigned long address,
 			if (mode == POPULATE_ZERO_SHADOW &&
 			    IS_ALIGNED(address, P4D_SIZE) &&
 			    end - address >= P4D_SIZE) {
-				p4d_populate(&init_mm, p4_dir, kasan_zero_pud);
+				p4d_populate(&init_mm, p4_dir,
+						kasan_early_shadow_pud);
 				address = (address + P4D_SIZE) & P4D_MASK;
 				continue;
 			}
@@ -133,7 +135,8 @@ static void __init kasan_early_vmemmap_populate(unsigned long address,
 			if (mode == POPULATE_ZERO_SHADOW &&
 			    IS_ALIGNED(address, PUD_SIZE) &&
 			    end - address >= PUD_SIZE) {
-				pud_populate(&init_mm, pu_dir, kasan_zero_pmd);
+				pud_populate(&init_mm, pu_dir,
+						kasan_early_shadow_pmd);
 				address = (address + PUD_SIZE) & PUD_MASK;
 				continue;
 			}
@@ -146,7 +149,8 @@ static void __init kasan_early_vmemmap_populate(unsigned long address,
 			if (mode == POPULATE_ZERO_SHADOW &&
 			    IS_ALIGNED(address, PMD_SIZE) &&
 			    end - address >= PMD_SIZE) {
-				pmd_populate(&init_mm, pm_dir, kasan_zero_pte);
+				pmd_populate(&init_mm, pm_dir,
+						kasan_early_shadow_pte);
 				address = (address + PMD_SIZE) & PMD_MASK;
 				continue;
 			}
@@ -188,7 +192,7 @@ static void __init kasan_early_vmemmap_populate(unsigned long address,
 				pte_val(*pt_dir) = __pa(page) | pgt_prot;
 				break;
 			case POPULATE_ZERO_SHADOW:
-				page = kasan_zero_page;
+				page = kasan_early_shadow_page;
 				pte_val(*pt_dir) = __pa(page) | pgt_prot_zero;
 				break;
 			}
@@ -256,14 +260,14 @@ void __init kasan_early_init(void)
 	unsigned long vmax;
 	unsigned long pgt_prot = pgprot_val(PAGE_KERNEL_RO);
 	pte_t pte_z;
-	pmd_t pmd_z = __pmd(__pa(kasan_zero_pte) | _SEGMENT_ENTRY);
-	pud_t pud_z = __pud(__pa(kasan_zero_pmd) | _REGION3_ENTRY);
-	p4d_t p4d_z = __p4d(__pa(kasan_zero_pud) | _REGION2_ENTRY);
+	pmd_t pmd_z = __pmd(__pa(kasan_early_shadow_pte) | _SEGMENT_ENTRY);
+	pud_t pud_z = __pud(__pa(kasan_early_shadow_pmd) | _REGION3_ENTRY);
+	p4d_t p4d_z = __p4d(__pa(kasan_early_shadow_pud) | _REGION2_ENTRY);
 
 	kasan_early_detect_facilities();
 	if (!has_nx)
 		pgt_prot &= ~_PAGE_NOEXEC;
-	pte_z = __pte(__pa(kasan_zero_page) | pgt_prot);
+	pte_z = __pte(__pa(kasan_early_shadow_page) | pgt_prot);
 
 	memsize = get_mem_detect_end();
 	if (!memsize)
@@ -292,10 +296,13 @@ void __init kasan_early_init(void)
 	}
 
 	/* init kasan zero shadow */
-	crst_table_init((unsigned long *)kasan_zero_p4d, p4d_val(p4d_z));
-	crst_table_init((unsigned long *)kasan_zero_pud, pud_val(pud_z));
-	crst_table_init((unsigned long *)kasan_zero_pmd, pmd_val(pmd_z));
-	memset64((u64 *)kasan_zero_pte, pte_val(pte_z), PTRS_PER_PTE);
+	crst_table_init((unsigned long *)kasan_early_shadow_p4d,
+				p4d_val(p4d_z));
+	crst_table_init((unsigned long *)kasan_early_shadow_pud,
+				pud_val(pud_z));
+	crst_table_init((unsigned long *)kasan_early_shadow_pmd,
+				pmd_val(pmd_z));
+	memset64((u64 *)kasan_early_shadow_pte, pte_val(pte_z), PTRS_PER_PTE);
 
 	shadow_alloc_size = memsize >> KASAN_SHADOW_SCALE_SHIFT;
 	pgalloc_low = round_up((unsigned long)_end, _SEGMENT_SIZE);
diff --git a/arch/x86/mm/dump_pagetables.c b/arch/x86/mm/dump_pagetables.c
index fc37bbd23eb8..c4696ab9a72b 100644
--- a/arch/x86/mm/dump_pagetables.c
+++ b/arch/x86/mm/dump_pagetables.c
@@ -380,7 +380,7 @@ static void walk_pte_level(struct seq_file *m, struct pg_state *st, pmd_t addr,
 
 /*
  * This is an optimization for KASAN=y case. Since all kasan page tables
- * eventually point to the kasan_zero_page we could call note_page()
+ * eventually point to the kasan_early_shadow_page we could call note_page()
  * right away without walking through lower level page tables. This saves
  * us dozens of seconds (minutes for 5-level config) while checking for
  * W+X mapping or reading kernel_page_tables debugfs file.
@@ -388,10 +388,11 @@ static void walk_pte_level(struct seq_file *m, struct pg_state *st, pmd_t addr,
 static inline bool kasan_page_table(struct seq_file *m, struct pg_state *st,
 				void *pt)
 {
-	if (__pa(pt) == __pa(kasan_zero_pmd) ||
-	    (pgtable_l5_enabled() && __pa(pt) == __pa(kasan_zero_p4d)) ||
-	    __pa(pt) == __pa(kasan_zero_pud)) {
-		pgprotval_t prot = pte_flags(kasan_zero_pte[0]);
+	if (__pa(pt) == __pa(kasan_early_shadow_pmd) ||
+	    (pgtable_l5_enabled() &&
+			__pa(pt) == __pa(kasan_early_shadow_p4d)) ||
+	    __pa(pt) == __pa(kasan_early_shadow_pud)) {
+		pgprotval_t prot = pte_flags(kasan_early_shadow_pte[0]);
 		note_page(m, st, __pgprot(prot), 0, 5);
 		return true;
 	}
diff --git a/arch/x86/mm/kasan_init_64.c b/arch/x86/mm/kasan_init_64.c
index 04a9cf6b034f..462fde83b515 100644
--- a/arch/x86/mm/kasan_init_64.c
+++ b/arch/x86/mm/kasan_init_64.c
@@ -211,7 +211,8 @@ static void __init kasan_early_p4d_populate(pgd_t *pgd,
 	unsigned long next;
 
 	if (pgd_none(*pgd)) {
-		pgd_entry = __pgd(_KERNPG_TABLE | __pa_nodebug(kasan_zero_p4d));
+		pgd_entry = __pgd(_KERNPG_TABLE |
+					__pa_nodebug(kasan_early_shadow_p4d));
 		set_pgd(pgd, pgd_entry);
 	}
 
@@ -222,7 +223,8 @@ static void __init kasan_early_p4d_populate(pgd_t *pgd,
 		if (!p4d_none(*p4d))
 			continue;
 
-		p4d_entry = __p4d(_KERNPG_TABLE | __pa_nodebug(kasan_zero_pud));
+		p4d_entry = __p4d(_KERNPG_TABLE |
+					__pa_nodebug(kasan_early_shadow_pud));
 		set_p4d(p4d, p4d_entry);
 	} while (p4d++, addr = next, addr != end && p4d_none(*p4d));
 }
@@ -261,10 +263,11 @@ static struct notifier_block kasan_die_notifier = {
 void __init kasan_early_init(void)
 {
 	int i;
-	pteval_t pte_val = __pa_nodebug(kasan_zero_page) | __PAGE_KERNEL | _PAGE_ENC;
-	pmdval_t pmd_val = __pa_nodebug(kasan_zero_pte) | _KERNPG_TABLE;
-	pudval_t pud_val = __pa_nodebug(kasan_zero_pmd) | _KERNPG_TABLE;
-	p4dval_t p4d_val = __pa_nodebug(kasan_zero_pud) | _KERNPG_TABLE;
+	pteval_t pte_val = __pa_nodebug(kasan_early_shadow_page) |
+				__PAGE_KERNEL | _PAGE_ENC;
+	pmdval_t pmd_val = __pa_nodebug(kasan_early_shadow_pte) | _KERNPG_TABLE;
+	pudval_t pud_val = __pa_nodebug(kasan_early_shadow_pmd) | _KERNPG_TABLE;
+	p4dval_t p4d_val = __pa_nodebug(kasan_early_shadow_pud) | _KERNPG_TABLE;
 
 	/* Mask out unsupported __PAGE_KERNEL bits: */
 	pte_val &= __default_kernel_pte_mask;
@@ -273,16 +276,16 @@ void __init kasan_early_init(void)
 	p4d_val &= __default_kernel_pte_mask;
 
 	for (i = 0; i < PTRS_PER_PTE; i++)
-		kasan_zero_pte[i] = __pte(pte_val);
+		kasan_early_shadow_pte[i] = __pte(pte_val);
 
 	for (i = 0; i < PTRS_PER_PMD; i++)
-		kasan_zero_pmd[i] = __pmd(pmd_val);
+		kasan_early_shadow_pmd[i] = __pmd(pmd_val);
 
 	for (i = 0; i < PTRS_PER_PUD; i++)
-		kasan_zero_pud[i] = __pud(pud_val);
+		kasan_early_shadow_pud[i] = __pud(pud_val);
 
 	for (i = 0; pgtable_l5_enabled() && i < PTRS_PER_P4D; i++)
-		kasan_zero_p4d[i] = __p4d(p4d_val);
+		kasan_early_shadow_p4d[i] = __p4d(p4d_val);
 
 	kasan_map_early_shadow(early_top_pgt);
 	kasan_map_early_shadow(init_top_pgt);
@@ -326,7 +329,7 @@ void __init kasan_init(void)
 
 	clear_pgds(KASAN_SHADOW_START & PGDIR_MASK, KASAN_SHADOW_END);
 
-	kasan_populate_zero_shadow((void *)(KASAN_SHADOW_START & PGDIR_MASK),
+	kasan_populate_early_shadow((void *)(KASAN_SHADOW_START & PGDIR_MASK),
 			kasan_mem_to_shadow((void *)PAGE_OFFSET));
 
 	for (i = 0; i < E820_MAX_ENTRIES; i++) {
@@ -338,41 +341,41 @@ void __init kasan_init(void)
 
 	shadow_cpu_entry_begin = (void *)CPU_ENTRY_AREA_BASE;
 	shadow_cpu_entry_begin = kasan_mem_to_shadow(shadow_cpu_entry_begin);
-	shadow_cpu_entry_begin = (void *)round_down((unsigned long)shadow_cpu_entry_begin,
-						PAGE_SIZE);
+	shadow_cpu_entry_begin = (void *)round_down(
+			(unsigned long)shadow_cpu_entry_begin, PAGE_SIZE);
 
 	shadow_cpu_entry_end = (void *)(CPU_ENTRY_AREA_BASE +
 					CPU_ENTRY_AREA_MAP_SIZE);
 	shadow_cpu_entry_end = kasan_mem_to_shadow(shadow_cpu_entry_end);
-	shadow_cpu_entry_end = (void *)round_up((unsigned long)shadow_cpu_entry_end,
-					PAGE_SIZE);
+	shadow_cpu_entry_end = (void *)round_up(
+			(unsigned long)shadow_cpu_entry_end, PAGE_SIZE);
 
-	kasan_populate_zero_shadow(
+	kasan_populate_early_shadow(
 		kasan_mem_to_shadow((void *)PAGE_OFFSET + MAXMEM),
 		shadow_cpu_entry_begin);
 
 	kasan_populate_shadow((unsigned long)shadow_cpu_entry_begin,
 			      (unsigned long)shadow_cpu_entry_end, 0);
 
-	kasan_populate_zero_shadow(shadow_cpu_entry_end,
-				kasan_mem_to_shadow((void *)__START_KERNEL_map));
+	kasan_populate_early_shadow(shadow_cpu_entry_end,
+			kasan_mem_to_shadow((void *)__START_KERNEL_map));
 
 	kasan_populate_shadow((unsigned long)kasan_mem_to_shadow(_stext),
 			      (unsigned long)kasan_mem_to_shadow(_end),
 			      early_pfn_to_nid(__pa(_stext)));
 
-	kasan_populate_zero_shadow(kasan_mem_to_shadow((void *)MODULES_END),
-				(void *)KASAN_SHADOW_END);
+	kasan_populate_early_shadow(kasan_mem_to_shadow((void *)MODULES_END),
+					(void *)KASAN_SHADOW_END);
 
 	load_cr3(init_top_pgt);
 	__flush_tlb_all();
 
 	/*
-	 * kasan_zero_page has been used as early shadow memory, thus it may
-	 * contain some garbage. Now we can clear and write protect it, since
-	 * after the TLB flush no one should write to it.
+	 * kasan_early_shadow_page has been used as early shadow memory, thus
+	 * it may contain some garbage. Now we can clear and write protect it,
+	 * since after the TLB flush no one should write to it.
 	 */
-	memset(kasan_zero_page, 0, PAGE_SIZE);
+	memset(kasan_early_shadow_page, 0, PAGE_SIZE);
 	for (i = 0; i < PTRS_PER_PTE; i++) {
 		pte_t pte;
 		pgprot_t prot;
@@ -380,8 +383,8 @@ void __init kasan_init(void)
 		prot = __pgprot(__PAGE_KERNEL_RO | _PAGE_ENC);
 		pgprot_val(prot) &= __default_kernel_pte_mask;
 
-		pte = __pte(__pa(kasan_zero_page) | pgprot_val(prot));
-		set_pte(&kasan_zero_pte[i], pte);
+		pte = __pte(__pa(kasan_early_shadow_page) | pgprot_val(prot));
+		set_pte(&kasan_early_shadow_pte[i], pte);
 	}
 	/* Flush TLBs again to be sure that write protection applied. */
 	__flush_tlb_all();
diff --git a/arch/xtensa/mm/kasan_init.c b/arch/xtensa/mm/kasan_init.c
index 6b95ca43aec0..1734cda6bc4a 100644
--- a/arch/xtensa/mm/kasan_init.c
+++ b/arch/xtensa/mm/kasan_init.c
@@ -24,12 +24,13 @@ void __init kasan_early_init(void)
 	int i;
 
 	for (i = 0; i < PTRS_PER_PTE; ++i)
-		set_pte(kasan_zero_pte + i,
-			mk_pte(virt_to_page(kasan_zero_page), PAGE_KERNEL));
+		set_pte(kasan_early_shadow_pte + i,
+			mk_pte(virt_to_page(kasan_early_shadow_page),
+				PAGE_KERNEL));
 
 	for (vaddr = 0; vaddr < KASAN_SHADOW_SIZE; vaddr += PMD_SIZE, ++pmd) {
 		BUG_ON(!pmd_none(*pmd));
-		set_pmd(pmd, __pmd((unsigned long)kasan_zero_pte));
+		set_pmd(pmd, __pmd((unsigned long)kasan_early_shadow_pte));
 	}
 	early_trap_init();
 }
@@ -80,13 +81,16 @@ void __init kasan_init(void)
 	populate(kasan_mem_to_shadow((void *)VMALLOC_START),
 		 kasan_mem_to_shadow((void *)XCHAL_KSEG_BYPASS_VADDR));
 
-	/* Write protect kasan_zero_page and zero-initialize it again. */
+	/*
+	 * Write protect kasan_early_shadow_page and zero-initialize it again.
+	 */
 	for (i = 0; i < PTRS_PER_PTE; ++i)
-		set_pte(kasan_zero_pte + i,
-			mk_pte(virt_to_page(kasan_zero_page), PAGE_KERNEL_RO));
+		set_pte(kasan_early_shadow_pte + i,
+			mk_pte(virt_to_page(kasan_early_shadow_page),
+				PAGE_KERNEL_RO));
 
 	local_flush_tlb_all();
-	memset(kasan_zero_page, 0, PAGE_SIZE);
+	memset(kasan_early_shadow_page, 0, PAGE_SIZE);
 
 	/* At this point kasan is fully initialized. Enable error messages. */
 	current->kasan_depth = 0;
diff --git a/include/linux/kasan.h b/include/linux/kasan.h
index b66fdf5ea7ab..ec22d548d0d7 100644
--- a/include/linux/kasan.h
+++ b/include/linux/kasan.h
@@ -14,13 +14,13 @@ struct task_struct;
 #include <asm/kasan.h>
 #include <asm/pgtable.h>
 
-extern unsigned char kasan_zero_page[PAGE_SIZE];
-extern pte_t kasan_zero_pte[PTRS_PER_PTE];
-extern pmd_t kasan_zero_pmd[PTRS_PER_PMD];
-extern pud_t kasan_zero_pud[PTRS_PER_PUD];
-extern p4d_t kasan_zero_p4d[MAX_PTRS_PER_P4D];
+extern unsigned char kasan_early_shadow_page[PAGE_SIZE];
+extern pte_t kasan_early_shadow_pte[PTRS_PER_PTE];
+extern pmd_t kasan_early_shadow_pmd[PTRS_PER_PMD];
+extern pud_t kasan_early_shadow_pud[PTRS_PER_PUD];
+extern p4d_t kasan_early_shadow_p4d[MAX_PTRS_PER_P4D];
 
-int kasan_populate_zero_shadow(const void *shadow_start,
+int kasan_populate_early_shadow(const void *shadow_start,
 				const void *shadow_end);
 
 static inline void *kasan_mem_to_shadow(const void *addr)
diff --git a/mm/kasan/init.c b/mm/kasan/init.c
index c7550eb65922..2b21d3717d62 100644
--- a/mm/kasan/init.c
+++ b/mm/kasan/init.c
@@ -30,13 +30,13 @@
  *   - Latter it reused it as zero shadow to cover large ranges of memory
  *     that allowed to access, but not handled by kasan (vmalloc/vmemmap ...).
  */
-unsigned char kasan_zero_page[PAGE_SIZE] __page_aligned_bss;
+unsigned char kasan_early_shadow_page[PAGE_SIZE] __page_aligned_bss;
 
 #if CONFIG_PGTABLE_LEVELS > 4
-p4d_t kasan_zero_p4d[MAX_PTRS_PER_P4D] __page_aligned_bss;
+p4d_t kasan_early_shadow_p4d[MAX_PTRS_PER_P4D] __page_aligned_bss;
 static inline bool kasan_p4d_table(pgd_t pgd)
 {
-	return pgd_page(pgd) == virt_to_page(lm_alias(kasan_zero_p4d));
+	return pgd_page(pgd) == virt_to_page(lm_alias(kasan_early_shadow_p4d));
 }
 #else
 static inline bool kasan_p4d_table(pgd_t pgd)
@@ -45,10 +45,10 @@ static inline bool kasan_p4d_table(pgd_t pgd)
 }
 #endif
 #if CONFIG_PGTABLE_LEVELS > 3
-pud_t kasan_zero_pud[PTRS_PER_PUD] __page_aligned_bss;
+pud_t kasan_early_shadow_pud[PTRS_PER_PUD] __page_aligned_bss;
 static inline bool kasan_pud_table(p4d_t p4d)
 {
-	return p4d_page(p4d) == virt_to_page(lm_alias(kasan_zero_pud));
+	return p4d_page(p4d) == virt_to_page(lm_alias(kasan_early_shadow_pud));
 }
 #else
 static inline bool kasan_pud_table(p4d_t p4d)
@@ -57,10 +57,10 @@ static inline bool kasan_pud_table(p4d_t p4d)
 }
 #endif
 #if CONFIG_PGTABLE_LEVELS > 2
-pmd_t kasan_zero_pmd[PTRS_PER_PMD] __page_aligned_bss;
+pmd_t kasan_early_shadow_pmd[PTRS_PER_PMD] __page_aligned_bss;
 static inline bool kasan_pmd_table(pud_t pud)
 {
-	return pud_page(pud) == virt_to_page(lm_alias(kasan_zero_pmd));
+	return pud_page(pud) == virt_to_page(lm_alias(kasan_early_shadow_pmd));
 }
 #else
 static inline bool kasan_pmd_table(pud_t pud)
@@ -68,16 +68,16 @@ static inline bool kasan_pmd_table(pud_t pud)
 	return 0;
 }
 #endif
-pte_t kasan_zero_pte[PTRS_PER_PTE] __page_aligned_bss;
+pte_t kasan_early_shadow_pte[PTRS_PER_PTE] __page_aligned_bss;
 
 static inline bool kasan_pte_table(pmd_t pmd)
 {
-	return pmd_page(pmd) == virt_to_page(lm_alias(kasan_zero_pte));
+	return pmd_page(pmd) == virt_to_page(lm_alias(kasan_early_shadow_pte));
 }
 
-static inline bool kasan_zero_page_entry(pte_t pte)
+static inline bool kasan_early_shadow_page_entry(pte_t pte)
 {
-	return pte_page(pte) == virt_to_page(lm_alias(kasan_zero_page));
+	return pte_page(pte) == virt_to_page(lm_alias(kasan_early_shadow_page));
 }
 
 static __init void *early_alloc(size_t size, int node)
@@ -92,7 +92,8 @@ static void __ref zero_pte_populate(pmd_t *pmd, unsigned long addr,
 	pte_t *pte = pte_offset_kernel(pmd, addr);
 	pte_t zero_pte;
 
-	zero_pte = pfn_pte(PFN_DOWN(__pa_symbol(kasan_zero_page)), PAGE_KERNEL);
+	zero_pte = pfn_pte(PFN_DOWN(__pa_symbol(kasan_early_shadow_page)),
+				PAGE_KERNEL);
 	zero_pte = pte_wrprotect(zero_pte);
 
 	while (addr + PAGE_SIZE <= end) {
@@ -112,7 +113,8 @@ static int __ref zero_pmd_populate(pud_t *pud, unsigned long addr,
 		next = pmd_addr_end(addr, end);
 
 		if (IS_ALIGNED(addr, PMD_SIZE) && end - addr >= PMD_SIZE) {
-			pmd_populate_kernel(&init_mm, pmd, lm_alias(kasan_zero_pte));
+			pmd_populate_kernel(&init_mm, pmd,
+					lm_alias(kasan_early_shadow_pte));
 			continue;
 		}
 
@@ -145,9 +147,11 @@ static int __ref zero_pud_populate(p4d_t *p4d, unsigned long addr,
 		if (IS_ALIGNED(addr, PUD_SIZE) && end - addr >= PUD_SIZE) {
 			pmd_t *pmd;
 
-			pud_populate(&init_mm, pud, lm_alias(kasan_zero_pmd));
+			pud_populate(&init_mm, pud,
+					lm_alias(kasan_early_shadow_pmd));
 			pmd = pmd_offset(pud, addr);
-			pmd_populate_kernel(&init_mm, pmd, lm_alias(kasan_zero_pte));
+			pmd_populate_kernel(&init_mm, pmd,
+					lm_alias(kasan_early_shadow_pte));
 			continue;
 		}
 
@@ -181,12 +185,14 @@ static int __ref zero_p4d_populate(pgd_t *pgd, unsigned long addr,
 			pud_t *pud;
 			pmd_t *pmd;
 
-			p4d_populate(&init_mm, p4d, lm_alias(kasan_zero_pud));
+			p4d_populate(&init_mm, p4d,
+					lm_alias(kasan_early_shadow_pud));
 			pud = pud_offset(p4d, addr);
-			pud_populate(&init_mm, pud, lm_alias(kasan_zero_pmd));
+			pud_populate(&init_mm, pud,
+					lm_alias(kasan_early_shadow_pmd));
 			pmd = pmd_offset(pud, addr);
 			pmd_populate_kernel(&init_mm, pmd,
-						lm_alias(kasan_zero_pte));
+					lm_alias(kasan_early_shadow_pte));
 			continue;
 		}
 
@@ -209,13 +215,13 @@ static int __ref zero_p4d_populate(pgd_t *pgd, unsigned long addr,
 }
 
 /**
- * kasan_populate_zero_shadow - populate shadow memory region with
- *                               kasan_zero_page
+ * kasan_populate_early_shadow - populate shadow memory region with
+ *                               kasan_early_shadow_page
  * @shadow_start - start of the memory range to populate
  * @shadow_end   - end of the memory range to populate
  */
-int __ref kasan_populate_zero_shadow(const void *shadow_start,
-				const void *shadow_end)
+int __ref kasan_populate_early_shadow(const void *shadow_start,
+					const void *shadow_end)
 {
 	unsigned long addr = (unsigned long)shadow_start;
 	unsigned long end = (unsigned long)shadow_end;
@@ -231,7 +237,7 @@ int __ref kasan_populate_zero_shadow(const void *shadow_start,
 			pmd_t *pmd;
 
 			/*
-			 * kasan_zero_pud should be populated with pmds
+			 * kasan_early_shadow_pud should be populated with pmds
 			 * at this moment.
 			 * [pud,pmd]_populate*() below needed only for
 			 * 3,2 - level page tables where we don't have
@@ -241,21 +247,25 @@ int __ref kasan_populate_zero_shadow(const void *shadow_start,
 			 * The ifndef is required to avoid build breakage.
 			 *
 			 * With 5level-fixup.h, pgd_populate() is not nop and
-			 * we reference kasan_zero_p4d. It's not defined
+			 * we reference kasan_early_shadow_p4d. It's not defined
 			 * unless 5-level paging enabled.
 			 *
 			 * The ifndef can be dropped once all KASAN-enabled
 			 * architectures will switch to pgtable-nop4d.h.
 			 */
 #ifndef __ARCH_HAS_5LEVEL_HACK
-			pgd_populate(&init_mm, pgd, lm_alias(kasan_zero_p4d));
+			pgd_populate(&init_mm, pgd,
+					lm_alias(kasan_early_shadow_p4d));
 #endif
 			p4d = p4d_offset(pgd, addr);
-			p4d_populate(&init_mm, p4d, lm_alias(kasan_zero_pud));
+			p4d_populate(&init_mm, p4d,
+					lm_alias(kasan_early_shadow_pud));
 			pud = pud_offset(p4d, addr);
-			pud_populate(&init_mm, pud, lm_alias(kasan_zero_pmd));
+			pud_populate(&init_mm, pud,
+					lm_alias(kasan_early_shadow_pmd));
 			pmd = pmd_offset(pud, addr);
-			pmd_populate_kernel(&init_mm, pmd, lm_alias(kasan_zero_pte));
+			pmd_populate_kernel(&init_mm, pmd,
+					lm_alias(kasan_early_shadow_pte));
 			continue;
 		}
 
@@ -350,7 +360,7 @@ static void kasan_remove_pte_table(pte_t *pte, unsigned long addr,
 		if (!pte_present(*pte))
 			continue;
 
-		if (WARN_ON(!kasan_zero_page_entry(*pte)))
+		if (WARN_ON(!kasan_early_shadow_page_entry(*pte)))
 			continue;
 		pte_clear(&init_mm, addr, pte);
 	}
@@ -480,7 +490,7 @@ int kasan_add_zero_shadow(void *start, unsigned long size)
 	    WARN_ON(size % (KASAN_SHADOW_SCALE_SIZE * PAGE_SIZE)))
 		return -EINVAL;
 
-	ret = kasan_populate_zero_shadow(shadow_start, shadow_end);
+	ret = kasan_populate_early_shadow(shadow_start, shadow_end);
 	if (ret)
 		kasan_remove_zero_shadow(shadow_start,
 					size >> KASAN_SHADOW_SCALE_SHIFT);
-- 
2.20.0.rc0.387.gc7a69e6b6c-goog

