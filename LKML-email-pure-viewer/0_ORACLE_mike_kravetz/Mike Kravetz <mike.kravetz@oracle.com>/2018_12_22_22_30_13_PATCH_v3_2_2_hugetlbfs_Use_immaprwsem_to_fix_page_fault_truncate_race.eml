Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by i7-8700 with POP3-SSL;
  24 Dec 2018 15:59:12 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga006.fm.intel.com (fmsmga006.fm.intel.com [10.253.24.20])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 4B1815805CF;
	Sat, 22 Dec 2018 14:34:56 -0800 (PST)
Received: from fmsmga102.fm.intel.com ([10.1.193.69])
  by fmsmga006-1.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 22 Dec 2018 14:34:55 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3Aux60/BOG5XSya6A2AbMl6mtUPXoX/o7sNwtQ0KIM?=
 =?us-ascii?q?zox0KPj6p8bcNUDSrc9gkEXOFd2Cra4c26yO6+jJYi8p2d65qncMcZhBBVcuqP?=
 =?us-ascii?q?49uEgeOvODElDxN/XwbiY3T4xoXV5h+GynYwAOQJ6tL1LdrWev4jEMBx7xKRR6?=
 =?us-ascii?q?JvjvGo7Vks+7y/2+94fcbglUhzexe69+IAmrpgjNq8cahpdvJLwswRXTuHtIfO?=
 =?us-ascii?q?pWxWJsJV2Nmhv3+9m98p1+/SlOovwt78FPX7n0cKQ+VrxYES8pM3sp683xtBnM?=
 =?us-ascii?q?VhWA630BWWgLiBVIAgzF7BbnXpfttybxq+Rw1DWGMcDwULs5Qiqp4bt1RxD0iS?=
 =?us-ascii?q?cHLz85/3/Risxsl6JQvRatqwViz4LIfI2ZMfxzdb7fc9wHX2pMRsZfWTJcDI2y?=
 =?us-ascii?q?bIUBCOgOPehDr4nlpVYDtgeyCRW2Ce/z0DJEmmP60Ksn2OohCwHG2wkgEsoJvn?=
 =?us-ascii?q?TVr9X6KroZX+WvzKbS0zXMce9W2Dbg44XPdxAhoPKMUqxqfcrS00kgDR3FgUuM?=
 =?us-ascii?q?qY3kJD6V0f4NvHKH4OpkS+2jkXIoqwZ0ojW2wMonl4rHhpoNx1za6Sl0xJw5Kc?=
 =?us-ascii?q?C2RUJle9KoDZhduz2AO4Z3QM4uW3xktSknxrEcpJK2cykHxI46yxLCavGLaZWE?=
 =?us-ascii?q?7xzlWe2MOzl3nmhld6i6hxuq8Uiv1On8Vs6s3VZUoSpKjMPMumoO1xPN8MiHTO?=
 =?us-ascii?q?Vy/kO71TaIzQDT5flIIUEylaXFN54s2qA8moYXvEjZAyP7llv6gLWLekgn5uSk?=
 =?us-ascii?q?8eXqb7f+qp+ZLYB0iwX+Mqo0msy4BOQ1KgwOX2md+eSh27zv5E75T6tQjv0wjK?=
 =?us-ascii?q?bZtInWJcMVp66/HQBVyJ0u6wiwDzi4ytQUh3oHI0xfeBKBkYfpP0vCIPfiDfew?=
 =?us-ascii?q?m1isiitkx+jaPr39BZXANnzDkLbifblj8UJdxxczwMtb55JVDLEBPf3yVlXwtN?=
 =?us-ascii?q?zeEh82LQi0z/z7B9V604MUQXiPDbOBMKPOrV+I4foiI+mWa48UpDbyMf8l6+Tu?=
 =?us-ascii?q?jX8kg1Ade6ap0IATaHC5GPRmPkqYbWDtgtcHDWcFoA4+QPb2h12FVD5Zf2yyUL?=
 =?us-ascii?q?4k5jEnFIKmCp/OSZq2gLyB2ye7HYdaZnpcBVCOCnroc4SEW/ERaCOdOMNhkzoE?=
 =?us-ascii?q?VaS/RI8lzx2hqAj6y79/JOrO5iIYrY7j1MRy5+DLlhE97zp0A96Z02GMVW50mG?=
 =?us-ascii?q?wISiQy3KB+p0x911iC3bJ5g/xeCdxc+fdJXh0mOp7byuxwE8ryVR7ZfteVVFam?=
 =?us-ascii?q?Rc2rDiwrQdIv3dACeUZ9FM+kjhDYwSWqBbgZl7iIBJwx9qLRxHzxJ8d7y3bb26?=
 =?us-ascii?q?gtlVgmQs1TNWK4gq5z7RTcB4nMk0+Bjaalabwc3DLR9GeE1WeBoVtXUBBuXqnf?=
 =?us-ascii?q?XXAQfE3Wrc/n6UPEVLKuDbUnMg1cyc+NMKdKa9vpjUlYS/fnItjRf2Wxm2KoDx?=
 =?us-ascii?q?aS2ryMdJbqe3ka3CjFFEgEkgUT/XGaNQg+BiatuX7eACZpFVL1Z0Ps8O9+qHyg?=
 =?us-ascii?q?Q08wzgGKaVBh1rWv9h4Ug/ycV+0c3rYetCg9rDV0GU6338jKBNqYuwphYKJcbM?=
 =?us-ascii?q?sn71dDy23YuBJyP5y6I695m14Rbh54v0Xt1xVwFIVNiswqrHIszApvJqOUylJB?=
 =?us-ascii?q?dzWE3Z/uPr3bMHX9/Beqa6TOwFHRzM6W+rsT6PQ/s1jivBypGlA+/Hl90thV02?=
 =?us-ascii?q?GT5pPFDAcJVZLxU0A39wV1pr3AYyk94Z/U2mNoMaWurjDC3NcpDvM/yhm8Z9df?=
 =?us-ascii?q?LL+EFAjqHs0ABsihNPYlm0K0YRIDJuxS8rA7P9mnd/efxKGkJuJgky+4gmRd54?=
 =?us-ascii?q?B91FmB9y59Su7OwpYEzOuU3gqBVzfgklihttr7lpxDZTEXBmC/0zTrBJZNZq1u?=
 =?us-ascii?q?eoYGEWevI8ytytR+nZLtQGNY9Fi4ClMC2c+pfweSblPn0Q1R00QXvWKomS+iwz?=
 =?us-ascii?q?NolDEpq7KV3DbSzOT6aBoHJmlLSXFgjVj2O4i0jNMaXE+yYwgyjhSl5kX6x6ld?=
 =?us-ascii?q?pKthKWneWkNIfynwL2F/Xaq8rLuCY8hT6Jw2tSVbSvizYVefSrTluRsVzzvjH3?=
 =?us-ascii?q?dCxDA8bzyruo/2nxtghGKfLXZzqmHUecVxxRfZ+dzdSuRd3jsARClklzbXAkKw?=
 =?us-ascii?q?MMWu/dWRj53DqPyxV3q9Vp1Pdinm1YCAuzG65WFwAR2/guq8mtvoEQUh1S/71t?=
 =?us-ascii?q?9qVTjHrRrmY4nr0bi6PvxjfkVyGFD87M96EJlkkoQsnJEQxWQahpKN8HoCkGfz?=
 =?us-ascii?q?Ms9U1bjwbXUTXj4L38Da4BL+10J9NHKJyJn0VnGcwstnetm7bXka2iM778BWFq?=
 =?us-ascii?q?iU6KZIkjdyolq9tQjRe+Ryni8Byfsy734Xm/0JuA0ozimHA7EdB09YPTH3lxST?=
 =?us-ascii?q?7tCzt6FXZGepcbit20tyh9GhDLeeog5CXHb1YIstHSh17s9nKlLDzGXz6p34eN?=
 =?us-ascii?q?nXdd8crQebkxDEj+hIMp4xkucKiDFjOWL8u30l1uE6gQZv3ZG8oIiINWFt8Lil?=
 =?us-ascii?q?DR5fMz3/f9kT9S31jaZCgsaW2JiiHpd7FTUOQpToVuikEC4IufTkKgaOEyA8qn?=
 =?us-ascii?q?GBFrrEBgKf7EZmr3TSE5GkLX2XJX8Zzcl8SxmZPkBQnAcUXDAilJ4jCg+q3NDh?=
 =?us-ascii?q?cFt+5j0J5l/4rQZMyvt1NxblVGfTvhyoajAySJiQNxdW6gBC50HIMc2R9O5zHi?=
 =?us-ascii?q?dY/oG/owyJMGCUewNIDWQRUEyeG1/jJqWu5cXH8+WABuqxMfrOYbaNqexfTfuI?=
 =?us-ascii?q?xoiv0oxp/zmSLMWPI2JiAuY/2kpCW3B5BsvYly8OSywRiyLCccqbqA2g9S1wq8?=
 =?us-ascii?q?C16O7rVx735YuTF7tSNs1i9AqrgaiYLe6fmiZ4JSxc1pMN3nLIzLkf3FgPiyBh?=
 =?us-ascii?q?bTWtELIAtTLTQ6LUgKNYExkbayZrPstS8608xhVNOdLcit7tyr54j/s1B01ZWl?=
 =?us-ascii?q?D7hs6pZdIFI2emOVPDBUaLMqmGJDLRz8H2Z6O8VaNfjOFOuxKsvjabFlfpPi6f?=
 =?us-ascii?q?mDnxSxCvLeZMgTmbPRNEvoG9dxdtCW77Q9Pnah20Ktl3jTItzL0wh3PKM3McMD?=
 =?us-ascii?q?dmf0NMqL2Q8T1XgvFlF2Nd6XplKLrMpyHM6+jeN4ZTtP5DATp9nOEc5241j/NR?=
 =?us-ascii?q?7SdZVLl2lTHUo9pGvV6riK+MxyBhXR4IrSxE16yRukA3HKTa+9FjQ3/a8Q4fpT?=
 =?us-ascii?q?GVChcH4dh/A8Pip7J4zcLGn6b+bjxF9oSHroMnG8HIJZffYzIaOh3zFWuRVVNd?=
 =?us-ascii?q?QA=3D=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0ABAABiux5ch0O0hNFjGQEBAQEBAQEBA?=
 =?us-ascii?q?QEBAQcBAQEBAQGBUgMBAQEBAQsBg2snjHaNPZdPFIFiEAEBGBMBhzAiNQgNAQM?=
 =?us-ascii?q?BAQEBAQECARMBAQEKCwkIKSMMgjopAYJnAwMBAiQZAQE3AQUJAQE3GQNUBxIFg?=
 =?us-ascii?q?x2CAgQBpTuBbDOCdgEBBYcdCId+hEGBVz+IXy1XhSCJSwSBeIUekGgHAoIlBI8?=
 =?us-ascii?q?0DBiBYIhShzQtiSyQKQIEAgQFAhMBgUgDggkzGgQfgzyCGwsBF4NKinRRgQUBA?=
 =?us-ascii?q?Y1rAQE?=
X-IPAS-Result: =?us-ascii?q?A0ABAABiux5ch0O0hNFjGQEBAQEBAQEBAQEBAQcBAQEBAQG?=
 =?us-ascii?q?BUgMBAQEBAQsBg2snjHaNPZdPFIFiEAEBGBMBhzAiNQgNAQMBAQEBAQECARMBA?=
 =?us-ascii?q?QEKCwkIKSMMgjopAYJnAwMBAiQZAQE3AQUJAQE3GQNUBxIFgx2CAgQBpTuBbDO?=
 =?us-ascii?q?CdgEBBYcdCId+hEGBVz+IXy1XhSCJSwSBeIUekGgHAoIlBI80DBiBYIhShzQti?=
 =?us-ascii?q?SyQKQIEAgQFAhMBgUgDggkzGgQfgzyCGwsBF4NKinRRgQUBAY1rAQE?=
X-IronPort-AV: E=Sophos;i="5.56,385,1539673200"; 
   d="scan'208";a="57991544"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 22 Dec 2018 14:34:54 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S2404965AbeLVWbv (ORCPT <rfc822;like.xu@linux.intel.com>
        + 22 others); Sat, 22 Dec 2018 17:31:51 -0500
Received: from aserp2130.oracle.com ([141.146.126.79]:45788 "EHLO
        aserp2130.oracle.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S2404309AbeLVWbu (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Sat, 22 Dec 2018 17:31:50 -0500
Received: from pps.filterd (aserp2130.oracle.com [127.0.0.1])
        by aserp2130.oracle.com (8.16.0.22/8.16.0.22) with SMTP id wBMMTqYX066326;
        Sat, 22 Dec 2018 22:30:37 GMT
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=oracle.com; h=from : to : cc :
 subject : date : message-id : in-reply-to : references; s=corp-2018-07-02;
 bh=VKohsUrMKKgNGnQ0gYAjxcXhQ+gP8dLYA/uers/DyHM=;
 b=cawiLHyvjX8YZrNblKRgGF2vBdj3B//f2HVRxrhsu+XtAz5UM2lbR8HXrLiY9/BaZb2a
 2n9L02+IHeEE+rezbMDJE/xNfPrXGNdmeFArjT7OCDph0ZoPBLjyKIuU4mTwqjaiJof6
 xIs3kVf4ZEUaedyCPoe2hQJYSLEuCvVKOCxcz5jINxTt8oEHXfwdpD2hk/jNM4qTxsDE
 pg1TJUb4qffz9hfjapPG7E/aLoIkiJR5XLnESiH9aRHxpS+FJWPIkEjnllkU72IrfgU+
 Ri7n4LDOcVk6jNhAOs49zCgTR80spaf2n4msAzmxM+0lQwhcUIei1zk+U7gbXjfQKg/W lA== 
Received: from aserv0022.oracle.com (aserv0022.oracle.com [141.146.126.234])
        by aserp2130.oracle.com with ESMTP id 2phasdhh0p-1
        (version=TLSv1.2 cipher=ECDHE-RSA-AES256-GCM-SHA384 bits=256 verify=OK);
        Sat, 22 Dec 2018 22:30:37 +0000
Received: from userv0122.oracle.com (userv0122.oracle.com [156.151.31.75])
        by aserv0022.oracle.com (8.14.4/8.14.4) with ESMTP id wBMMUadv012434
        (version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256 verify=OK);
        Sat, 22 Dec 2018 22:30:36 GMT
Received: from abhmp0007.oracle.com (abhmp0007.oracle.com [141.146.116.13])
        by userv0122.oracle.com (8.14.4/8.14.4) with ESMTP id wBMMUZZw013491;
        Sat, 22 Dec 2018 22:30:35 GMT
Received: from monkey.oracle.com (/50.38.38.67)
        by default (Oracle Beehive Gateway v4.0)
        with ESMTP ; Sat, 22 Dec 2018 14:30:35 -0800
From: Mike Kravetz <mike.kravetz@oracle.com>
To: linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc: Michal Hocko <mhocko@kernel.org>, Hugh Dickins <hughd@google.com>,
        Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>,
        "Aneesh Kumar K . V" <aneesh.kumar@linux.vnet.ibm.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        "Kirill A . Shutemov" <kirill.shutemov@linux.intel.com>,
        Davidlohr Bueso <dave@stgolabs.net>,
        Prakash Sangappa <prakash.sangappa@oracle.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Mike Kravetz <mike.kravetz@oracle.com>, stable@vger.kernel.org
Subject: [PATCH v3 2/2] hugetlbfs: Use i_mmap_rwsem to fix page fault/truncate race
Date: Sat, 22 Dec 2018 14:30:13 -0800
Message-Id: <20181222223013.22193-3-mike.kravetz@oracle.com>
X-Mailer: git-send-email 2.17.2
In-Reply-To: <20181222223013.22193-1-mike.kravetz@oracle.com>
References: <20181222223013.22193-1-mike.kravetz@oracle.com>
X-Proofpoint-Virus-Version: vendor=nai engine=5900 definitions=9115 signatures=668680
X-Proofpoint-Spam-Details: rule=notspam policy=default score=0 suspectscore=0 malwarescore=0
 phishscore=0 bulkscore=0 spamscore=0 mlxscore=0 mlxlogscore=972
 adultscore=0 classifier=spam adjust=0 reason=mlx scancount=1
 engine=8.0.1-1810050000 definitions=main-1812220204
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

hugetlbfs page faults can race with truncate and hole punch operations.
Current code in the page fault path attempts to handle this by 'backing
out' operations if we encounter the race.  One obvious omission in the
current code is removing a page newly added to the page cache.  This is
pretty straight forward to address, but there is a more subtle and
difficult issue of backing out hugetlb reservations.  To handle this
correctly, the 'reservation state' before page allocation needs to be
noted so that it can be properly backed out.  There are four distinct
possibilities for reservation state: shared/reserved, shared/no-resv,
private/reserved and private/no-resv.  Backing out a reservation may
require memory allocation which could fail so that needs to be taken
into account as well.

Instead of writing the required complicated code for this rare
occurrence, just eliminate the race.  i_mmap_rwsem is now held in read
mode for the duration of page fault processing.  Hold i_mmap_rwsem
longer in truncation and hold punch code to cover the call to
remove_inode_hugepages.

With this modification, code in remove_inode_hugepages checking for
races becomes 'dead' as it can not longer happen.  Remove the dead code
and expand comments to explain reasoning.  Similarly, checks for races
with truncation in the page fault path can be simplified and removed.

Cc: <stable@vger.kernel.org>
Fixes: ebed4bfc8da8 ("hugetlb: fix absurd HugePages_Rsvd")
Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
---
 fs/hugetlbfs/inode.c | 61 ++++++++++++++++++++------------------------
 mm/hugetlb.c         | 21 ++++++++-------
 2 files changed, 38 insertions(+), 44 deletions(-)

diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c
index 32920a10100e..a2fcea5f8225 100644
--- a/fs/hugetlbfs/inode.c
+++ b/fs/hugetlbfs/inode.c
@@ -383,17 +383,16 @@ hugetlb_vmdelete_list(struct rb_root_cached *root, pgoff_t start, pgoff_t end)
  * truncation is indicated by end of range being LLONG_MAX
  *	In this case, we first scan the range and release found pages.
  *	After releasing pages, hugetlb_unreserve_pages cleans up region/reserv
- *	maps and global counts.  Page faults can not race with truncation
- *	in this routine.  hugetlb_no_page() prevents page faults in the
- *	truncated range.  It checks i_size before allocation, and again after
- *	with the page table lock for the page held.  The same lock must be
- *	acquired to unmap a page.
+ *	maps and global counts.
  * hole punch is indicated if end is not LLONG_MAX
  *	In the hole punch case we scan the range and release found pages.
  *	Only when releasing a page is the associated region/reserv map
  *	deleted.  The region/reserv map for ranges without associated
- *	pages are not modified.  Page faults can race with hole punch.
- *	This is indicated if we find a mapped page.
+ *	pages are not modified.
+ *
+ * Callers of this routine must hold the i_mmap_rwsem in write mode to prevent
+ * races with page faults.
+ *
  * Note: If the passed end of range value is beyond the end of file, but
  * not LLONG_MAX this routine still performs a hole punch operation.
  */
@@ -423,32 +422,14 @@ static void remove_inode_hugepages(struct inode *inode, loff_t lstart,
 
 		for (i = 0; i < pagevec_count(&pvec); ++i) {
 			struct page *page = pvec.pages[i];
-			u32 hash;
 
 			index = page->index;
-			hash = hugetlb_fault_mutex_hash(h, current->mm,
-							&pseudo_vma,
-							mapping, index, 0);
-			mutex_lock(&hugetlb_fault_mutex_table[hash]);
-
 			/*
-			 * If page is mapped, it was faulted in after being
-			 * unmapped in caller.  Unmap (again) now after taking
-			 * the fault mutex.  The mutex will prevent faults
-			 * until we finish removing the page.
-			 *
-			 * This race can only happen in the hole punch case.
-			 * Getting here in a truncate operation is a bug.
+			 * A mapped page is impossible as callers should unmap
+			 * all references before calling.  And, i_mmap_rwsem
+			 * prevents the creation of additional mappings.
 			 */
-			if (unlikely(page_mapped(page))) {
-				BUG_ON(truncate_op);
-
-				i_mmap_lock_write(mapping);
-				hugetlb_vmdelete_list(&mapping->i_mmap,
-					index * pages_per_huge_page(h),
-					(index + 1) * pages_per_huge_page(h));
-				i_mmap_unlock_write(mapping);
-			}
+			VM_BUG_ON(page_mapped(page));
 
 			lock_page(page);
 			/*
@@ -470,7 +451,6 @@ static void remove_inode_hugepages(struct inode *inode, loff_t lstart,
 			}
 
 			unlock_page(page);
-			mutex_unlock(&hugetlb_fault_mutex_table[hash]);
 		}
 		huge_pagevec_release(&pvec);
 		cond_resched();
@@ -482,9 +462,20 @@ static void remove_inode_hugepages(struct inode *inode, loff_t lstart,
 
 static void hugetlbfs_evict_inode(struct inode *inode)
 {
+	struct address_space *mapping = inode->i_mapping;
 	struct resv_map *resv_map;
 
+	/*
+	 * The vfs layer guarantees that there are no other users of this
+	 * inode.  Therefore, it would be safe to call remove_inode_hugepages
+	 * without holding i_mmap_rwsem.  We acquire and hold here to be
+	 * consistent with other callers.  Since there will be no contention
+	 * on the semaphore, overhead is negligible.
+	 */
+	i_mmap_lock_write(mapping);
 	remove_inode_hugepages(inode, 0, LLONG_MAX);
+	i_mmap_unlock_write(mapping);
+
 	resv_map = (struct resv_map *)inode->i_mapping->private_data;
 	/* root inode doesn't have the resv_map, so we should check it */
 	if (resv_map)
@@ -505,8 +496,8 @@ static int hugetlb_vmtruncate(struct inode *inode, loff_t offset)
 	i_mmap_lock_write(mapping);
 	if (!RB_EMPTY_ROOT(&mapping->i_mmap.rb_root))
 		hugetlb_vmdelete_list(&mapping->i_mmap, pgoff, 0);
-	i_mmap_unlock_write(mapping);
 	remove_inode_hugepages(inode, offset, LLONG_MAX);
+	i_mmap_unlock_write(mapping);
 	return 0;
 }
 
@@ -540,8 +531,8 @@ static long hugetlbfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
 			hugetlb_vmdelete_list(&mapping->i_mmap,
 						hole_start >> PAGE_SHIFT,
 						hole_end  >> PAGE_SHIFT);
-		i_mmap_unlock_write(mapping);
 		remove_inode_hugepages(inode, hole_start, hole_end);
+		i_mmap_unlock_write(mapping);
 		inode_unlock(inode);
 	}
 
@@ -624,7 +615,11 @@ static long hugetlbfs_fallocate(struct file *file, int mode, loff_t offset,
 		/* addr is the offset within the file (zero based) */
 		addr = index * hpage_size;
 
-		/* mutex taken here, fault path and hole punch */
+		/*
+		 * fault mutex taken here, protects against fault path
+		 * and hole punch.  inode_lock previously taken protects
+		 * against truncation.
+		 */
 		hash = hugetlb_fault_mutex_hash(h, mm, &pseudo_vma, mapping,
 						index, addr);
 		mutex_lock(&hugetlb_fault_mutex_table[hash]);
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 2a3162030167..cfd9790b01e3 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -3757,16 +3757,16 @@ static vm_fault_t hugetlb_no_page(struct mm_struct *mm,
 	}
 
 	/*
-	 * Use page lock to guard against racing truncation
-	 * before we get page_table_lock.
+	 * We can not race with truncation due to holding i_mmap_rwsem.
+	 * Check once here for faults beyond end of file.
 	 */
+	size = i_size_read(mapping->host) >> huge_page_shift(h);
+	if (idx >= size)
+		goto out;
+
 retry:
 	page = find_lock_page(mapping, idx);
 	if (!page) {
-		size = i_size_read(mapping->host) >> huge_page_shift(h);
-		if (idx >= size)
-			goto out;
-
 		/*
 		 * Check for page in userfault range
 		 */
@@ -3856,9 +3856,6 @@ static vm_fault_t hugetlb_no_page(struct mm_struct *mm,
 	}
 
 	ptl = huge_pte_lock(h, mm, ptep);
-	size = i_size_read(mapping->host) >> huge_page_shift(h);
-	if (idx >= size)
-		goto backout;
 
 	ret = 0;
 	if (!huge_pte_none(huge_ptep_get(ptep)))
@@ -3961,8 +3958,10 @@ vm_fault_t hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 
 	/*
 	 * Acquire i_mmap_rwsem before calling huge_pte_alloc and hold
-	 * until finished with ptep.  This prevents huge_pmd_unshare from
-	 * being called elsewhere and making the ptep no longer valid.
+	 * until finished with ptep.  This serves two purposes:
+	 * 1) It prevents huge_pmd_unshare from being called elsewhere
+	 *    and making the ptep no longer valid.
+	 * 2) It synchronizes us with file truncation.
 	 *
 	 * ptep could have already be assigned via huge_pte_offset.  That
 	 * is OK, as huge_pte_alloc will return the same value unless
-- 
2.17.2

