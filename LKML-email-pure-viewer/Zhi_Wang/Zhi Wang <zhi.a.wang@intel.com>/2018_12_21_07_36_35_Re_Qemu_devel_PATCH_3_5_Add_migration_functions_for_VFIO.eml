Return-Path: <qemu-devel-bounces+like.xu=linux.intel.com@nongnu.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by i7-8700 with POP3-SSL;
  24 Dec 2018 15:46:47 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga004.jf.intel.com (orsmga004.jf.intel.com [10.7.209.38])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 68FDC580522
	for <like.xu@linux.intel.com>; Thu, 20 Dec 2018 23:46:53 -0800 (PST)
Received: from orsmga101.jf.intel.com ([10.7.208.22])
  by orsmga004-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 20 Dec 2018 23:46:53 -0800
IronPort-PHdr: =?us-ascii?q?9a23=3AxNIl5B2bUQ8/iY/osmDT+DRfVm0co7zxezQtwd8Z?=
 =?us-ascii?q?sesWKv3xwZ3uMQTl6Ol3ixeRBMOHs6IC07KempujcFRI2YyGvnEGfc4EfD4+ou?=
 =?us-ascii?q?JSoTYdBtWYA1bwNv/gYn9yNs1DUFh44yPzahANS47xaFLIv3K98yMZFAnhOgpp?=
 =?us-ascii?q?POT1HZPZg9iq2+yo9JDffwZFiCChbb9uMR67sRjfus4KjIV4N60/0AHJonxGe+?=
 =?us-ascii?q?RXwWNnO1eelAvi68mz4ZBu7T1et+ou+MBcX6r6eb84TaFDAzQ9L281/szrugLd?=
 =?us-ascii?q?QgaJ+3ART38ZkhtMAwjC8RH6QpL8uTb0u+ZhxCWXO9D9QKsqUjq+8ahkVB7oiD?=
 =?us-ascii?q?8GNzEn9mHXltdwh79frB64uhBz35LYbISTOfFjfK3SYMkaSHJfUMZfVyJPAY2y?=
 =?us-ascii?q?YIUAAOUDIelWoJTzp0MMoBW8CgSgGe3ixiNWiX/txqA6z+YsHBva0AA8Ed8Dsn?=
 =?us-ascii?q?LZp8j1OqcIVuC1ybHFwjbDb/NXxDzy6JLHchYuofqRWr9xcMrRyUg1GwzflFmR?=
 =?us-ascii?q?p5bqPzWa1ukWsmib6fZgWvyri2I9tw5xpT2vy94qh4LUhYwV0kjJ+TtlzIs2P9?=
 =?us-ascii?q?G0VVN3bN2+HJdOuSyXN5F6Tt4gTm1wpSo3zqMKtYS7cSUK0pgqxwDTZ+aaf4WI?=
 =?us-ascii?q?/x7uUvuaLy1ii3J/Yr2/gg6/8Ui+xe34Ucm5yFJKritektnQrXABzRPT6s6aSv?=
 =?us-ascii?q?dn+UehwzmP2xjS6uFCP080ibLWJ4A9zrMzjJYfrFnPEyzslEnogqKbdl8o9vWq?=
 =?us-ascii?q?5uj/Z7XpvJ6cN4t6igHkNaQun9SyAf0mPQgLQmiX4Pmz26P9/ULnRLVGl+Y5kq?=
 =?us-ascii?q?7EsJDcOcsUuLW5DwhR0oYi6BawES2q0dsFnXQfKFJFeRSHj5XmOl3UIfD4C+u/?=
 =?us-ascii?q?jEqokDtx2//GObjhD47LLnjElrfhcrB961NGxAo019Bf6IpYCqsdL/LrRk/xqN?=
 =?us-ascii?q?vYAwc9Mwy1wOboFs9x14wDWW+UBq+ZMaXSsUKH5+41IumMYpMVtyj5K/Q/+/Hu?=
 =?us-ascii?q?ino5yhcge7K0184XdGygBaYhZEGYemb3xNEGF2gMo0w5VuOtjVSDVTtaYTG1R7?=
 =?us-ascii?q?494TcgT5urCJqGSo2zjbjS4SGgA5cDY2lHDkyLQ27lcpjBV/oSZSbXOMJ4jzEf?=
 =?us-ascii?q?SZCnTIku0wzosxX1nKF6JOjZ8TFNqJT4ydJu7PfSnxxhyTshPcSU1Cm8Rmc8yn?=
 =?us-ascii?q?IBQXknmrJ/qEtVy1Gf3Kw+iPtdQ5gbrdxOVAg+OJ+Uj8VgBtGzEEqVd82ESRCp?=
 =?us-ascii?q?RdOpBTwwZtIsysADYgB2HND03T7Z2C//Mb8U34OCApN8prDc3j74Ydl0zXnu1a?=
 =?us-ascii?q?89gl1gScxKYz71zpVj/hTeUtaa236SkLynIOFFhHbA?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AFAAAEmhxchxHrdtBkGQEBAQEBAQEBA?=
 =?us-ascii?q?QEBAQcBAQEBAQGBUQQBAQEBAQsBgTCCKSYTg32IGV+LHSmBNy2XX4EiURUYFIR?=
 =?us-ascii?q?AAoJsIjQJDQEDAQEBAQEBAgETAQEBCgsJCBsOIwyCOgUCAx8BBoJgAQEBAQMBA?=
 =?us-ascii?q?iAVDgopAwMBAgMDAQEKEQQBAQECAiIBAwICAwFCCQgGAQwGAgEBAYMdggIBBKZ?=
 =?us-ascii?q?UgS+FQIR2gQuLNHqBHIERJ4Jrgx4EgSiDP4JXAoknEg41gUaEYzeFSYo/WgcCg?=
 =?us-ascii?q?imPNwYYgWCIUYcuiVCQQ4FGgg5NI1CCbIIkAxeLd4JEIzEBgQaLS1aBdwEB?=
X-IPAS-Result: =?us-ascii?q?A0AFAAAEmhxchxHrdtBkGQEBAQEBAQEBAQEBAQcBAQEBAQG?=
 =?us-ascii?q?BUQQBAQEBAQsBgTCCKSYTg32IGV+LHSmBNy2XX4EiURUYFIRAAoJsIjQJDQEDA?=
 =?us-ascii?q?QEBAQEBAgETAQEBCgsJCBsOIwyCOgUCAx8BBoJgAQEBAQMBAiAVDgopAwMBAgM?=
 =?us-ascii?q?DAQEKEQQBAQECAiIBAwICAwFCCQgGAQwGAgEBAYMdggIBBKZUgS+FQIR2gQuLN?=
 =?us-ascii?q?HqBHIERJ4Jrgx4EgSiDP4JXAoknEg41gUaEYzeFSYo/WgcCgimPNwYYgWCIUYc?=
 =?us-ascii?q?uiVCQQ4FGgg5NI1CCbIIkAxeLd4JEIzEBgQaLS1aBdwEB?=
X-IronPort-AV: E=Sophos;i="5.56,380,1539673200"; 
   d="scan'208";a="45924850"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from lists.gnu.org ([208.118.235.17])
  by mtab.intel.com with ESMTP/TLS/AES256-SHA; 20 Dec 2018 23:46:52 -0800
Received: from localhost ([::1]:43243 helo=lists.gnu.org)
	by lists.gnu.org with esmtp (Exim 4.71)
	(envelope-from <qemu-devel-bounces+like.xu=linux.intel.com@nongnu.org>)
	id 1gaFWF-0001ot-R9
	for like.xu@linux.intel.com; Fri, 21 Dec 2018 02:46:51 -0500
Received: from eggs.gnu.org ([2001:4830:134:3::10]:35357)
	by lists.gnu.org with esmtp (Exim 4.71)
	(envelope-from <zhi.a.wang@intel.com>) id 1gaFNa-0000b1-9T
	for qemu-devel@nongnu.org; Fri, 21 Dec 2018 02:37:56 -0500
Received: from Debian-exim by eggs.gnu.org with spam-scanned (Exim 4.71)
	(envelope-from <zhi.a.wang@intel.com>) id 1gaFNW-0006Uz-JB
	for qemu-devel@nongnu.org; Fri, 21 Dec 2018 02:37:54 -0500
Received: from mga17.intel.com ([192.55.52.151]:64382)
	by eggs.gnu.org with esmtps (TLS1.0:DHE_RSA_AES_256_CBC_SHA1:32)
	(Exim 4.71) (envelope-from <zhi.a.wang@intel.com>)
	id 1gaFNW-0006Ij-8Q
	for qemu-devel@nongnu.org; Fri, 21 Dec 2018 02:37:50 -0500
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from orsmga007.jf.intel.com ([10.7.209.58])
	by fmsmga107.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384;
	20 Dec 2018 23:37:40 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.56,380,1539673200"; d="scan'208";a="100438363"
Received: from jsanz-mobl.ger.corp.intel.com (HELO [10.252.11.224])
	([10.252.11.224])
	by orsmga007.jf.intel.com with ESMTP; 20 Dec 2018 23:37:32 -0800
To: Zhao Yan <yan.y.zhao@intel.com>, "Gonglei (Arei)" <arei.gonglei@huawei.com>
References: <1542746383-18288-1-git-send-email-kwankhede@nvidia.com>
	<1542746383-18288-4-git-send-email-kwankhede@nvidia.com>
	<33183CC9F5247A488A2544077AF19020DB1D1315@dggeml531-mbs.china.huawei.com>
	<20181219021218.GA8139@joy-OptiPlex-7040>
From: Zhi Wang <zhi.a.wang@intel.com>
Message-ID: <1dd14bcc-11d6-3d68-62d9-3d7292b93bd7@intel.com>
Date: Fri, 21 Dec 2018 02:36:35 -0500
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101
	Thunderbird/60.2.1
MIME-Version: 1.0
In-Reply-To: <20181219021218.GA8139@joy-OptiPlex-7040>
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Language: en-US
Content-Transfer-Encoding: 7bit
X-detected-operating-system: by eggs.gnu.org: Genre and OS details not
	recognized.
X-Received-From: 192.55.52.151
X-Mailman-Approved-At: Fri, 21 Dec 2018 02:46:33 -0500
Subject: Re: [Qemu-devel] [PATCH 3/5] Add migration functions for VFIO
 devices
X-BeenThere: qemu-devel@nongnu.org
X-Mailman-Version: 2.1.21
Precedence: list
List-Id: <qemu-devel.nongnu.org>
List-Unsubscribe: <https://lists.nongnu.org/mailman/options/qemu-devel>,
	<mailto:qemu-devel-request@nongnu.org?subject=unsubscribe>
List-Archive: <http://lists.nongnu.org/archive/html/qemu-devel/>
List-Post: <mailto:qemu-devel@nongnu.org>
List-Help: <mailto:qemu-devel-request@nongnu.org?subject=help>
List-Subscribe: <https://lists.nongnu.org/mailman/listinfo/qemu-devel>,
	<mailto:qemu-devel-request@nongnu.org?subject=subscribe>
Cc: "Liujinsong \(Paul\)" <liu.jinsong@huawei.com>,
	"cjia@nvidia.com" <cjia@nvidia.com>, "aik@ozlabs.ru" <aik@ozlabs.ru>,
	"Zhengxiao.zx@Alibaba-inc.com" <Zhengxiao.zx@Alibaba-inc.com>,
	"shuangtai.tst@alibaba-inc.com" <shuangtai.tst@alibaba-inc.com>,
	"qemu-devel@nongnu.org" <qemu-devel@nongnu.org>,
	Kirti Wankhede <kwankhede@nvidia.com>,
	"eauger@redhat.com" <eauger@redhat.com>,
	"yi.l.liu@intel.com" <yi.l.liu@intel.com>,
	"eskultet@redhat.com" <eskultet@redhat.com>,
	"ziye.yang@intel.com" <ziye.yang@intel.com>,
	"mlevitsk@redhat.com" <mlevitsk@redhat.com>,
	"pasic@linux.ibm.com" <pasic@linux.ibm.com>,
	"felipe@nutanix.com" <felipe@nutanix.com>,
	"Ken.Xue@amd.com" <Ken.Xue@amd.com>,
	"kevin.tian@intel.com" <kevin.tian@intel.com>,
	"dgilbert@redhat.com" <dgilbert@redhat.com>,
	"alex.williamson@redhat.com" <alex.williamson@redhat.com>,
	"changpeng.liu@intel.com" <changpeng.liu@intel.com>,
	"cohuck@redhat.com" <cohuck@redhat.com>,
	Huangzhichao <huangzhichao@huawei.com>,
	"jonathan.davies@nutanix.com" <jonathan.davies@nutanix.com>, "Hu,
	Robert" <robert.hu@intel.com>
Errors-To: qemu-devel-bounces+like.xu=linux.intel.com@nongnu.org
Sender: "Qemu-devel" <qemu-devel-bounces+like.xu=linux.intel.com@nongnu.org>

It's nice to see cloud vendors are also quite interested in VFIO 
migration interfaces and functions. From what Yan said and Huawei's 
requirements, there should be more devices which don't have private 
memory, maybe GPU is almost the only one which has the private memory.

As VFIO is a generic user-space device controlling interfaces nowadays 
in the kernel and perhaps becomes into an standard in future, I guess we 
also need to think more about a generic framework and how to let the 
non-GPU devices to step into VFIO easily.

 From perspective of the vendors of the devices and the cloud vendors 
who want to build their migration support on top of VFIO, it would be 
nice to have a simple and friendly path for them.

Thanks,
Zhi.

On 12/18/18 9:12 PM, Zhao Yan wrote:
> right, a capabilities field in struct vfio_device_migration_info can avoid
> populating iteration APIs and migration states into every vendor drivers
> who actually may not requires those APIs and simply do nothing or return
> value 0 in response to those APIs.
> 
> struct vfio_device_migration_info {
>          __u32 device_state;         /* VFIO device state */
> +     __u32 capabilities;    /* VFIO device capabilities */
>          struct {
>              __u64 precopy_only;
>              __u64 compatible;
>              __u64 postcopy_only;
>              __u64 threshold_size;
>          } pending;	
>       ...
> };
>   
> So, only for devices who need iteration APIs, like GPU with standalone
> video memory, can set flag VFIO_MIGRATION_HAS_ITERTATION to this
> capabilities field. Then callbacks like save_live_iterate(),
> is_active_iterate(), save_live_pending() will check the flag
> VFIO_MIGRATION_HAS_ITERTATION in capabilities field and send requests
> into vendor driver.
> 
> But, for simple devices who only use system memory, like IGD and NIC,
> will not set the flag VFIO_MIGRATION_HAS_ITERTATION, and as a result, no
> need to handle requests like "Get buffer", "Set buffer", "Get pending
> bytes" triggered by QEMU iteration callbacks. And therefore, detailed
> migration states are not cared for vendor drivers for these devices.
> 
> Thanks to Gonglei for providing this idea and details.
> Free free to give your comments to the above description.
> 
> 
> On Mon, Dec 17, 2018 at 11:19:49AM +0000, Gonglei (Arei) wrote:
>> Hi,
>>
>> It's great to see this patch series, which is a very important step, although
>> currently only consider GPU mdev devices to support hot migration.
>>
>> However, this is based on the VFIO framework after all, so we expect
>> that we can make this live migration framework more general.
>>
>> For example, the vfio_save_pending() callback is used to obtain device
>> memory (such as GPU memory), but if the device (such as network card)
>> has no special proprietary memory, but only system memory?
>> It is too much to perform a null operation for this kind of device by writing
>> memory to the vendor driver of kernel space.
>>
>> I think we can acquire the capability from the vendor driver before using this.
>> If there is device memory that needs iterative copying, the vendor driver return
>> ture, otherwise return false. Then QEMU implement the specific logic,
>> otherwise return directly. Just like getting the capability list of KVM
>> module, can we?
>>
>>
>> Regards,
>> -Gonglei
>>
>>
>>> -----Original Message-----
>>> From: Qemu-devel
>>> [mailto:qemu-devel-bounces+arei.gonglei=huawei.com@nongnu.org] On
>>> Behalf Of Kirti Wankhede
>>> Sent: Wednesday, November 21, 2018 4:40 AM
>>> To: alex.williamson@redhat.com; cjia@nvidia.com
>>> Cc: Zhengxiao.zx@Alibaba-inc.com; kevin.tian@intel.com; yi.l.liu@intel.com;
>>> eskultet@redhat.com; ziye.yang@intel.com; qemu-devel@nongnu.org;
>>> cohuck@redhat.com; shuangtai.tst@alibaba-inc.com; dgilbert@redhat.com;
>>> zhi.a.wang@intel.com; mlevitsk@redhat.com; pasic@linux.ibm.com;
>>> aik@ozlabs.ru; Kirti Wankhede <kwankhede@nvidia.com>;
>>> eauger@redhat.com; felipe@nutanix.com; jonathan.davies@nutanix.com;
>>> changpeng.liu@intel.com; Ken.Xue@amd.com
>>> Subject: [Qemu-devel] [PATCH 3/5] Add migration functions for VFIO devices
>>>
>>> - Migration function are implemented for VFIO_DEVICE_TYPE_PCI device.
>>> - Added SaveVMHandlers and implemented all basic functions required for live
>>>    migration.
>>> - Added VM state change handler to know running or stopped state of VM.
>>> - Added migration state change notifier to get notification on migration state
>>>    change. This state is translated to VFIO device state and conveyed to vendor
>>>    driver.
>>> - VFIO device supportd migration or not is decided based of migration region
>>>    query. If migration region query is successful then migration is supported
>>>    else migration is blocked.
>>> - Structure vfio_device_migration_info is mapped at 0th offset of migration
>>>    region and should always trapped by VFIO device's driver. Added both type of
>>>    access support, trapped or mmapped, for data section of the region.
>>> - To save device state, read data offset and size using structure
>>>    vfio_device_migration_info.data, accordingly copy data from the region.
>>> - To restore device state, write data offset and size in the structure and write
>>>    data in the region.
>>> - To get dirty page bitmap, write start address and pfn count then read count of
>>>    pfns copied and accordingly read those from the rest of the region or
>>> mmaped
>>>    part of the region. This copy is iterated till page bitmap for all requested
>>>    pfns are copied.
>>>
>>> Signed-off-by: Kirti Wankhede <kwankhede@nvidia.com>
>>> Reviewed-by: Neo Jia <cjia@nvidia.com>
>>> ---
>>>   hw/vfio/Makefile.objs         |   2 +-
>>>   hw/vfio/migration.c           | 729
>>> ++++++++++++++++++++++++++++++++++++++++++
>>>   include/hw/vfio/vfio-common.h |  23 ++
>>>   3 files changed, 753 insertions(+), 1 deletion(-)
>>>   create mode 100644 hw/vfio/migration.c
>>>
>> [skip]
>>
>>> +
>>> +static SaveVMHandlers savevm_vfio_handlers = {
>>> +    .save_setup = vfio_save_setup,
>>> +    .save_live_iterate = vfio_save_iterate,
>>> +    .save_live_complete_precopy = vfio_save_complete_precopy,
>>> +    .save_live_pending = vfio_save_pending,
>>> +    .save_cleanup = vfio_save_cleanup,
>>> +    .load_state = vfio_load_state,
>>> +    .load_setup = vfio_load_setup,
>>> +    .load_cleanup = vfio_load_cleanup,
>>> +    .is_active_iterate = vfio_is_active_iterate,
>>> +};
>>> +
>>
>>   

