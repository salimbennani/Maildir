Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 07 Dec 2018 16:18:07 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga007.jf.intel.com (orsmga007.jf.intel.com [10.7.209.58])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 5A864580375;
	Thu,  6 Dec 2018 21:43:10 -0800 (PST)
Received: from orsmga102-1.jf.intel.com (HELO mga09.intel.com) ([10.7.208.27])
  by orsmga007-1.jf.intel.com with ESMTP; 06 Dec 2018 21:43:10 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AfsRJXxeUhAosiW7Lsqp7hFeVlGMj4u6mDksu8pMi?=
 =?us-ascii?q?zoh2WeGdxc6+YRSN2/xhgRfzUJnB7Loc0qyK6/CmATRIyK3CmUhKSIZLWR4BhJ?=
 =?us-ascii?q?detC0bK+nBN3fGKuX3ZTcxBsVIWQwt1Xi6NU9IBJS2PAWK8TW94jEIBxrwKxd+?=
 =?us-ascii?q?KPjrFY7OlcS30P2594HObwlSizexfbB/IA+qoQnNq8IbnZZsJqEtxxXTv3BGYf?=
 =?us-ascii?q?5WxWRmJVKSmxbz+MK994N9/ipTpvws6ddOXb31cKokQ7NYCi8mM30u683wqRbD?=
 =?us-ascii?q?VwqP6WACXWgQjxFFHhLK7BD+Xpf2ryv6qu9w0zSUMMHqUbw5Xymp4rx1QxH0li?=
 =?us-ascii?q?gIKz858HnWisNuiqJbvAmhrAF7z4LNfY2ZKOZycqbbcNgHR2ROQ9xRWjRBDI2i?=
 =?us-ascii?q?coUBAekPM+FaoInzvFsOtRmzCBKwCO/z0DJEmmX70bEm3+knDArI3BYgH9ULsH?=
 =?us-ascii?q?nMrtv1Kb0dUea6zKLVzzrDbvVW2Tjg44XPchEhoPeMXb1qfcrR1EkgDQXFjlqL?=
 =?us-ascii?q?pIzkOTOVyvoCs2yB4+V8UuKvjncqpgdsqTas3schkpfFip4Rx1ze6Cl0zpg5Kc?=
 =?us-ascii?q?elREN4fdKoCppduiOCO4drRs4uXXtktSY6x7EcuZO3YjIGxZckyhPZdveJaZKH?=
 =?us-ascii?q?4gj5W+aUOTp4hGxqeLa4hxuq70igxfPzVtOu3FZJsCVFiN/MuW4J1xDJ7ciHUP?=
 =?us-ascii?q?R98l+g2TaJyQ/T9vlJLV4omaffMZIt37A9moQJvUjeHSL6hF/6ga6Ue0k8/+in?=
 =?us-ascii?q?8eXnYrHopp+GMI90jxnzMqAvmsy5HOQ5PRECX2uF9uSm0r3s40n5TKxNjvw4lK?=
 =?us-ascii?q?nWroraKN8Fpq62HQBVyJwv6xWhADe81tQXg30HIEhCeBKdgIjlI0vOL+zgDfej?=
 =?us-ascii?q?n1Ssly9mx/THPr3iHJrBNHfCkKr6cLZ56k5czhczzN9F65JVDLEBPOz8WkvruN?=
 =?us-ascii?q?PECR85NhS+w/z7B9VlyoMeRWWPD7eZMKzIsF+I+vggI+6WaI8VpTbyMf4l5/H1?=
 =?us-ascii?q?gH89mF8de7Sp3JQNZHC5GPRmP1uWYX72jtgdFmcKuxI0TPb2h12aTT5Te3GyUr?=
 =?us-ascii?q?o+5jE8Fo2qF4TDRoergLyH2ye2BZlWZmFAClCRHnbkbYSEW/EQaC2MJs9tiCAL?=
 =?us-ascii?q?Vb+kS4U5zxGhqBf6y6Z7LurT4iAXr4nj1Nhy5+3Qjx0y7yZ7D8aG3mGJTmF0mH?=
 =?us-ascii?q?4IRjAs0KB+p0x91kmM0axij/NEEtxT4utDUh0mOp7E0+x6F9fyVxrCftiTTlaq?=
 =?us-ascii?q?WNGmATArQdI3zN8DeEJ9G9SkjhDe0CumGb4Vl7qXBJMq9qLQxWT+J8F4y3zezq?=
 =?us-ascii?q?kuk0EmQtdTNW2hnqNw6hLcB5DXnEmDl6alb6Ic3DXT+2eFymaOuEJYUAt0Uaje?=
 =?us-ascii?q?WXAfZ03Wrcn250/YTr+uD6gnPRVFycKYNqRKbdjph01cRPj/INTef36xm2CoCB?=
 =?us-ascii?q?mV3LyMcpTld38d3CrHDkgEiB4c/XCdOAg6ByehpX/eDTN0GVLuZUPs7fdxqHeh?=
 =?us-ascii?q?QkAoyAGKalVr16Cp9R4NmfycV/QT06oYuCcgrjV0G0q939LWCtaauwptZqJcYc?=
 =?us-ascii?q?k54FdG02LZuBdwPpihL6Bkm14ffB57v0Lo1xVrFIpAldImo28tzAp3MaiYyk9O?=
 =?us-ascii?q?dyuE3ZDsPb3aMnP9/BSxZK/ZxF7f0Mya9bwS6PslsVrjugKpFk0883h819lV0n?=
 =?us-ascii?q?2c5ojFDQYIUJLxVFo3+AZ+p73AfiY94IbU32V2Maaoqj/Cx84pBOw9xxegYtdT?=
 =?us-ascii?q?Kr+LGBXzEs0aHceuLuMqlkOtbhIFOuBS6aE1M9mnd/uAxK6kIuJgkCi6gmRA5Y?=
 =?us-ascii?q?B3yliM+DZkSu7Uw5YFxOmV3hGGVzjgllihqN34mYdeaTEUAGW/0ynkCJdNaaJo?=
 =?us-ascii?q?eYYEFHmhI9ewxtV4nJPtX39Y9Fi+B1IJwsOpeBySb0Dj0g1Uz0gYvXunmS6gxT?=
 =?us-ascii?q?xujz4ptraf3DDJw+n6dBsHO3RHRWl4gVf3PIi0icsXXEypbwgviRuk6lz2x6ld?=
 =?us-ascii?q?pKRjMWbTRV1EcDTxL2FnSqGwrKaNY9ZT6JM0tiVaSOa8bkqASr7+oBsa1DnvH3?=
 =?us-ascii?q?BEyzA4dDGqu5P5kAJ8iG+GKHZzrXzZedx/xBvF5dzcQ+JR0SQCRCVilTbXAV28?=
 =?us-ascii?q?NcGz/dqIj5fDrvy+V2W5W5xTaybrypmMtCm65W1sGhC/m/Gzmtv6EQk1yyP71t?=
 =?us-ascii?q?9qVTnWoxb4eIXky6O6Med/dElyGFD889Z6Gp15koYoh5Efw3kaio+V/Xoai2jz?=
 =?us-ascii?q?N9pb1Ln6bHoMQz4L3tHU7BLk2E1lMnKG2Yb5Wm+BzctmYtmwenkW1T4l78BWFK?=
 =?us-ascii?q?eU66RJnS50olq7tw3de/Z8kSkGyfsy9nEamfoGtxQ3ziWSGb0SGUhYPSrxlxWH?=
 =?us-ascii?q?9dy+raNXZHqxfri0zkZxgdehDLSaqAFGRHn5YosiHTN37shnLFLM13jz5pvled?=
 =?us-ascii?q?bKa9ITqwaUkwzBj+VOLJIxl/wKhTdoOG7nvH0lzfI7ggJq3Z2goIeHLGBt9rqj?=
 =?us-ascii?q?AhFELj31e98T+jb1gKlFhMmW2ISvHoh7FjUPQZvlVvaoEDMUtfT6OAeCCjw8qn?=
 =?us-ascii?q?GHGbXBGQ+T8ltpr3XKE5qzLXGYOGEZzcl+RBmaPEFfghobXDAkkZ4iDAyq2Nbt?=
 =?us-ascii?q?cERk5jAS+174rAFDyvlzOhn7U2ffohqoazguRJieKhpW8h9N50PPPcOC6eJzGj?=
 =?us-ascii?q?lS/oe9owyVNmybewNIAHkJW0yDGlDjJ6Su6sPG8+SCHeq+KP3OYbqVpOxaVveI?=
 =?us-ascii?q?w4+v04R88zaNMMWPImdtD/kh1kVfWnB5HtzTmy8TRCwPiyLNc8mbqQ+8+iJpr8?=
 =?us-ascii?q?C/8/frWAP16YuMEbtSNtpv9A6sgaeeLO6dnyJ5KTde1pMRyn7E0rkf3FgOiy5w?=
 =?us-ascii?q?czmhC6gPtSnITKjIgK9YEwYbaz9vNMtP968zxBRNNtXBitzr1r90lPo1C1ZeWF?=
 =?us-ascii?q?zlm8GpY9EKIm6nOFPGAkaLKKqJJTnRz87rZqO8TKVajP9IuB2opTabD0jjMyyf?=
 =?us-ascii?q?lznoUhCjK/1DgDuHMxxepo69dA1gCWziTNLgdx26P8V7jTwwwb0omHzKMXQQPi?=
 =?us-ascii?q?R7c0NItreQ9z9Xgu1jG2xd6XpoNemFlDyf7+ndK5YWt+NkAyV0l+1A5nQ6xKBY?=
 =?us-ascii?q?7CVFRPxzhSvTocRio1CgkumT1DVnVABCpSpMhIKO7g1ePvDw8JVaVGmM2RUX8W?=
 =?us-ascii?q?KWQ0ADpMNgItnuvb1Aj9bIiaT/IStD9NSS+tETUZv6MsWCZVo8OBzmUA/VCgRN?=
 =?us-ascii?q?GS+qM26Zh1FUlv6693uJo5x8oZ/pzsldAoRHXUA4Q6tJQn9uG8YPddIuBmsp?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0ADAAB9Bwpch0O0hNFjGwEBAQEDAQEBB?=
 =?us-ascii?q?wMBAQGBUQYBAQELAYNrJ4wTjjFollOBJANMFBgTAYdXIjQJDQEDAQEBAQEBAgE?=
 =?us-ascii?q?TAQEBCA0JCCkvgjYkAYJiAwMBAiRSBgkBARg4A1QGEwWDHIICBaZ2M4owh3CEL?=
 =?us-ascii?q?4IWgRGCXQeLCgKJLIF5hBR7UY9dBwKRPwsYX32IPIcdAZkagUZsgSEzGiNQgmy?=
 =?us-ascii?q?CJxeOKjIBATEBgQQBAYo/AQE?=
X-IPAS-Result: =?us-ascii?q?A0ADAAB9Bwpch0O0hNFjGwEBAQEDAQEBBwMBAQGBUQYBAQE?=
 =?us-ascii?q?LAYNrJ4wTjjFollOBJANMFBgTAYdXIjQJDQEDAQEBAQEBAgETAQEBCA0JCCkvg?=
 =?us-ascii?q?jYkAYJiAwMBAiRSBgkBARg4A1QGEwWDHIICBaZ2M4owh3CEL4IWgRGCXQeLCgK?=
 =?us-ascii?q?JLIF5hBR7UY9dBwKRPwsYX32IPIcdAZkagUZsgSEzGiNQgmyCJxeOKjIBATEBg?=
 =?us-ascii?q?QQBAYo/AQE?=
X-IronPort-AV: E=Sophos;i="5.56,324,1539673200"; 
   d="scan'208";a="56486382"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 06 Dec 2018 21:42:08 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726211AbeLGFmE (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Fri, 7 Dec 2018 00:42:04 -0500
Received: from mga01.intel.com ([192.55.52.88]:55177 "EHLO mga01.intel.com"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1726197AbeLGFmB (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Fri, 7 Dec 2018 00:42:01 -0500
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from fmsmga007.fm.intel.com ([10.253.24.52])
  by fmsmga101.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 06 Dec 2018 21:42:01 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.56,324,1539673200"; 
   d="scan'208";a="105567281"
Received: from yhuang-mobile.sh.intel.com ([10.239.196.133])
  by fmsmga007.fm.intel.com with ESMTP; 06 Dec 2018 21:41:58 -0800
From: Huang Ying <ying.huang@intel.com>
To: Andrew Morton <akpm@linux-foundation.org>
Cc: linux-mm@kvack.org, linux-kernel@vger.kernel.org,
        Huang Ying <ying.huang@intel.com>,
        "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        Michal Hocko <mhocko@kernel.org>,
        Johannes Weiner <hannes@cmpxchg.org>,
        Shaohua Li <shli@kernel.org>, Hugh Dickins <hughd@google.com>,
        Minchan Kim <minchan@kernel.org>,
        Rik van Riel <riel@redhat.com>,
        Dave Hansen <dave.hansen@linux.intel.com>,
        Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>,
        Zi Yan <zi.yan@cs.rutgers.edu>,
        Daniel Jordan <daniel.m.jordan@oracle.com>
Subject: [PATCH -V8 12/21] swap: Support PMD swap mapping in swapoff
Date: Fri,  7 Dec 2018 13:41:12 +0800
Message-Id: <20181207054122.27822-13-ying.huang@intel.com>
X-Mailer: git-send-email 2.18.1
In-Reply-To: <20181207054122.27822-1-ying.huang@intel.com>
References: <20181207054122.27822-1-ying.huang@intel.com>
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

During swapoff, for a huge swap cluster, we need to allocate a THP,
read its contents into the THP and unuse the PMD and PTE swap mappings
to it.  If failed to allocate a THP, the huge swap cluster will be
split.

During unuse, if it is found that the swap cluster mapped by a PMD
swap mapping is split already, we will split the PMD swap mapping and
unuse the PTEs.

Signed-off-by: "Huang, Ying" <ying.huang@intel.com>
Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
Cc: Andrea Arcangeli <aarcange@redhat.com>
Cc: Michal Hocko <mhocko@kernel.org>
Cc: Johannes Weiner <hannes@cmpxchg.org>
Cc: Shaohua Li <shli@kernel.org>
Cc: Hugh Dickins <hughd@google.com>
Cc: Minchan Kim <minchan@kernel.org>
Cc: Rik van Riel <riel@redhat.com>
Cc: Dave Hansen <dave.hansen@linux.intel.com>
Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Cc: Zi Yan <zi.yan@cs.rutgers.edu>
Cc: Daniel Jordan <daniel.m.jordan@oracle.com>
---
 include/asm-generic/pgtable.h | 14 +-----
 include/linux/huge_mm.h       |  8 ++++
 mm/huge_memory.c              |  4 +-
 mm/swapfile.c                 | 86 ++++++++++++++++++++++++++++++++++-
 4 files changed, 97 insertions(+), 15 deletions(-)

diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h
index 20aab7bfd487..5216124ba13c 100644
--- a/include/asm-generic/pgtable.h
+++ b/include/asm-generic/pgtable.h
@@ -931,22 +931,12 @@ static inline int pmd_none_or_trans_huge_or_clear_bad(pmd_t *pmd)
 	barrier();
 #endif
 	/*
-	 * !pmd_present() checks for pmd migration entries
-	 *
-	 * The complete check uses is_pmd_migration_entry() in linux/swapops.h
-	 * But using that requires moving current function and pmd_trans_unstable()
-	 * to linux/swapops.h to resovle dependency, which is too much code move.
-	 *
-	 * !pmd_present() is equivalent to is_pmd_migration_entry() currently,
-	 * because !pmd_present() pages can only be under migration not swapped
-	 * out.
-	 *
-	 * pmd_none() is preseved for future condition checks on pmd migration
+	 * pmd_none() is preseved for future condition checks on pmd swap
 	 * entries and not confusing with this function name, although it is
 	 * redundant with !pmd_present().
 	 */
 	if (pmd_none(pmdval) || pmd_trans_huge(pmdval) ||
-		(IS_ENABLED(CONFIG_ARCH_ENABLE_THP_MIGRATION) && !pmd_present(pmdval)))
+	    (IS_ENABLED(CONFIG_HAVE_PMD_SWAP_ENTRY) && !pmd_present(pmdval)))
 		return 1;
 	if (unlikely(pmd_bad(pmdval))) {
 		pmd_clear_bad(pmd);
diff --git a/include/linux/huge_mm.h b/include/linux/huge_mm.h
index ea4999a4b6cd..6236f8b1d04b 100644
--- a/include/linux/huge_mm.h
+++ b/include/linux/huge_mm.h
@@ -376,6 +376,8 @@ static inline gfp_t alloc_hugepage_direct_gfpmask(struct vm_area_struct *vma,
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 
 #ifdef CONFIG_THP_SWAP
+extern int split_huge_swap_pmd(struct vm_area_struct *vma, pmd_t *pmd,
+			       unsigned long address, pmd_t orig_pmd);
 extern int do_huge_pmd_swap_page(struct vm_fault *vmf, pmd_t orig_pmd);
 
 static inline bool transparent_hugepage_swapin_enabled(
@@ -401,6 +403,12 @@ static inline bool transparent_hugepage_swapin_enabled(
 	return false;
 }
 #else /* CONFIG_THP_SWAP */
+static inline int split_huge_swap_pmd(struct vm_area_struct *vma, pmd_t *pmd,
+				      unsigned long address, pmd_t orig_pmd)
+{
+	return 0;
+}
+
 static inline int do_huge_pmd_swap_page(struct vm_fault *vmf, pmd_t orig_pmd)
 {
 	return 0;
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 0ae7f824dbeb..f3c0a9e8fb9a 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -1721,8 +1721,8 @@ static void __split_huge_swap_pmd(struct vm_area_struct *vma,
 }
 
 #ifdef CONFIG_THP_SWAP
-static int split_huge_swap_pmd(struct vm_area_struct *vma, pmd_t *pmd,
-			       unsigned long address, pmd_t orig_pmd)
+int split_huge_swap_pmd(struct vm_area_struct *vma, pmd_t *pmd,
+			unsigned long address, pmd_t orig_pmd)
 {
 	struct mm_struct *mm = vma->vm_mm;
 	spinlock_t *ptl;
diff --git a/mm/swapfile.c b/mm/swapfile.c
index c22c11b4a879..b85ec810d941 100644
--- a/mm/swapfile.c
+++ b/mm/swapfile.c
@@ -1931,6 +1931,11 @@ static inline int pte_same_as_swp(pte_t pte, pte_t swp_pte)
 	return pte_same(pte_swp_clear_soft_dirty(pte), swp_pte);
 }
 
+static inline int pmd_same_as_swp(pmd_t pmd, pmd_t swp_pmd)
+{
+	return pmd_same(pmd_swp_clear_soft_dirty(pmd), swp_pmd);
+}
+
 /*
  * No need to decide whether this PTE shares the swap entry with others,
  * just let do_wp_page work it out if a write is requested later - to
@@ -1992,6 +1997,53 @@ static int unuse_pte(struct vm_area_struct *vma, pmd_t *pmd,
 	return ret;
 }
 
+#ifdef CONFIG_THP_SWAP
+static int unuse_pmd(struct vm_area_struct *vma, pmd_t *pmd,
+		     unsigned long addr, swp_entry_t entry, struct page *page)
+{
+	struct mem_cgroup *memcg;
+	spinlock_t *ptl;
+	int ret = 1;
+
+	if (mem_cgroup_try_charge(page, vma->vm_mm, GFP_KERNEL,
+				  &memcg, true)) {
+		ret = -ENOMEM;
+		goto out_nolock;
+	}
+
+	ptl = pmd_lock(vma->vm_mm, pmd);
+	if (unlikely(!pmd_same_as_swp(*pmd, swp_entry_to_pmd(entry)))) {
+		mem_cgroup_cancel_charge(page, memcg, true);
+		ret = 0;
+		goto out;
+	}
+
+	add_mm_counter(vma->vm_mm, MM_SWAPENTS, -HPAGE_PMD_NR);
+	add_mm_counter(vma->vm_mm, MM_ANONPAGES, HPAGE_PMD_NR);
+	get_page(page);
+	set_pmd_at(vma->vm_mm, addr, pmd,
+		   pmd_mkold(mk_huge_pmd(page, vma->vm_page_prot)));
+	page_add_anon_rmap(page, vma, addr, true);
+	mem_cgroup_commit_charge(page, memcg, true, true);
+	swap_free(entry, HPAGE_PMD_NR);
+	/*
+	 * Move the page to the active list so it is not
+	 * immediately swapped out again after swapon.
+	 */
+	activate_page(page);
+out:
+	spin_unlock(ptl);
+out_nolock:
+	return ret;
+}
+#else
+static int unuse_pmd(struct vm_area_struct *vma, pmd_t *pmd,
+		     unsigned long addr, swp_entry_t entry, struct page *page)
+{
+	return 0;
+}
+#endif
+
 static int unuse_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 				unsigned long addr, unsigned long end,
 				swp_entry_t entry, struct page *page)
@@ -2032,7 +2084,7 @@ static inline int unuse_pmd_range(struct vm_area_struct *vma, pud_t *pud,
 				unsigned long addr, unsigned long end,
 				swp_entry_t entry, struct page *page)
 {
-	pmd_t *pmd;
+	pmd_t swp_pmd = swp_entry_to_pmd(entry), *pmd, orig_pmd;
 	unsigned long next;
 	int ret;
 
@@ -2040,6 +2092,27 @@ static inline int unuse_pmd_range(struct vm_area_struct *vma, pud_t *pud,
 	do {
 		cond_resched();
 		next = pmd_addr_end(addr, end);
+		orig_pmd = *pmd;
+		if (IS_ENABLED(CONFIG_THP_SWAP) && is_swap_pmd(orig_pmd)) {
+			if (likely(!pmd_same_as_swp(orig_pmd, swp_pmd)))
+				continue;
+			/*
+			 * Huge cluster has been split already, split
+			 * PMD swap mapping and fallback to unuse PTE
+			 */
+			if (!PageTransCompound(page)) {
+				ret = split_huge_swap_pmd(vma, pmd,
+							  addr, orig_pmd);
+				if (ret)
+					return ret;
+				ret = unuse_pte_range(vma, pmd, addr,
+						      next, entry, page);
+			} else
+				ret = unuse_pmd(vma, pmd, addr, entry, page);
+			if (ret)
+				return ret;
+			continue;
+		}
 		if (pmd_none_or_trans_huge_or_clear_bad(pmd))
 			continue;
 		ret = unuse_pte_range(vma, pmd, addr, next, entry, page);
@@ -2233,6 +2306,7 @@ int try_to_unuse(unsigned int type, bool frontswap,
 	 * there are races when an instance of an entry might be missed.
 	 */
 	while ((i = find_next_to_unuse(si, i, frontswap)) != 0) {
+retry:
 		if (signal_pending(current)) {
 			retval = -EINTR;
 			break;
@@ -2248,6 +2322,8 @@ int try_to_unuse(unsigned int type, bool frontswap,
 		page = read_swap_cache_async(entry,
 					GFP_HIGHUSER_MOVABLE, NULL, 0, false);
 		if (!page) {
+			struct swap_cluster_info *ci = NULL;
+
 			/*
 			 * Either swap_duplicate() failed because entry
 			 * has been freed independently, and will not be
@@ -2264,6 +2340,14 @@ int try_to_unuse(unsigned int type, bool frontswap,
 			 */
 			if (!swcount || swcount == SWAP_MAP_BAD)
 				continue;
+			if (si->cluster_info)
+				ci = si->cluster_info + i / SWAPFILE_CLUSTER;
+			/* Split huge cluster if failed to allocate huge page */
+			if (cluster_is_huge(ci)) {
+				retval = split_swap_cluster(entry, 0);
+				if (!retval || retval == -EEXIST)
+					goto retry;
+			}
 			retval = -ENOMEM;
 			break;
 		}
-- 
2.18.1

