Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 09 Dec 2018 18:50:40 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga004.jf.intel.com (orsmga004.jf.intel.com [10.7.209.38])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id DEECB5803E4;
	Fri,  7 Dec 2018 08:08:49 -0800 (PST)
Received: from orsmga106.jf.intel.com ([10.7.208.65])
  by orsmga004-1.jf.intel.com with ESMTP; 07 Dec 2018 08:08:49 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AafrSDBWBK4X9bEDvYFtN1yA+sqnV8LGtZVwlr6E/?=
 =?us-ascii?q?grcLSJyIuqrYZhGFuadThVPEFb/W9+hDw7KP9fy4CSpYud6oizMrSNR0TRgLiM?=
 =?us-ascii?q?EbzUQLIfWuLgnFFsPsdDEwB89YVVVorDmROElRH9viNRWJ+iXhpTEdFQ/iOgVr?=
 =?us-ascii?q?O+/7BpDdj9it1+C15pbffxhEiCCybL9uLxi6txndutULioZ+N6g9zQfErGFVcO?=
 =?us-ascii?q?pM32NoIlyTnxf45siu+ZNo7jpdtfE8+cNeSKv2Z6s3Q6BWAzQgKGA1+dbktQLf?=
 =?us-ascii?q?QguV53sTSXsZnxxVCAXY9h76X5Pxsizntuph3SSRIMP7QawoVTmk8qxmUwHjhj?=
 =?us-ascii?q?sZODEl8WHXks1wg7xdoBK9vBx03orYbJiIOPZiYq/ReNUXTndDUMlMTSxMGoOy?=
 =?us-ascii?q?YZUSAeQPPuhWqIvyp1UBohSiBAmsH/vvxz1Ti3/qwaE3yfgtHBva0AA8Gd8FrX?=
 =?us-ascii?q?TarM/yNKcXSe27zbPHzTLeZPxX2Dfy8o7Ich88qvyLUrJ/a83RyEYuFwzfiFWQ?=
 =?us-ascii?q?ppLpMjOO2+QNrmiU9eRhWvyui2I9tw5xpT2vy94qh4LUhYwV0kjJ+TtlzIsxP9?=
 =?us-ascii?q?G0VUB2bcC+HJdNtCyWK5F6T8IgTm1wpSo21qcKtYO5cSUK0pgr2gDTZvOdf4SW?=
 =?us-ascii?q?4R/vTOmcLDdiiH57d7+ygwy+/Va9xuHiTMW4zVJHojdDn9LRrH4CzQbT5dKCSv?=
 =?us-ascii?q?Zl/keuxzKP1wfL5+FaLkA7i7DbJ4QiwrEujJoTt1rMHivslEXxlq+WeV0o+umu?=
 =?us-ascii?q?6+v5frXrvoGQO5Nwhw3kL6gjlNKzDf4lPgUNQ2SX4+Wx2b/78U38WrpKj/k2kq?=
 =?us-ascii?q?fDsJDdIMQWvqq5Aw5T0oY+5BezFjSm38oCnXkBMl1FfAuLj4/nOlHIPv/4F+yy?=
 =?us-ascii?q?g0qjkDh13fDKJL7hDYvXLnjFjrjheaxx60lGyAo81dxf/Y5bCqkdIPLvXU/8rN?=
 =?us-ascii?q?jYDh46MwOq2ermB8h925gaWWKOBK+ZLazTvUWJ5uIpP+mDeosVtCzhJPgi4v7k?=
 =?us-ascii?q?lWU5lkMFfam1wZsXb2i1HvR8LEWYfXrjmNABHX0KvgojVuPqjlKCXCVXZ3azWa?=
 =?us-ascii?q?I8+z46BJinDYfFWoCinriB0D2nEZ1RY2BMEkqMHmvwd4WYR/cMbzqfIs1mkjMa?=
 =?us-ascii?q?T7ShSIgh1ReotA/90LdnKuvU+isFtZPsztR15uvTlQ0s+jxwFciSz2aNT2RslG?=
 =?us-ascii?q?MSWzA2xLx/oVB6ylqby6d4mPxYFdtQ5/9TSAc1L5zcwvd+C9DzXALBY9iIREyn?=
 =?us-ascii?q?QtWgHTE+UNYxz8USbEZ6HtWolgrD0DayA78Ji7yLA4Q58qHG0Hj3Pcp9z3fG27?=
 =?us-ascii?q?Mnj1kpWcZPMWymhqhi9wncHYLJkkOZl7q0eqQYxiLC6GCDzW+WtkFCTAFwSbnF?=
 =?us-ascii?q?XWwYZkbOsNv2/F3CQKG0BbQnKARBz9WCJbVMatHuiVVGR/LjONDFbmK1mmewAw?=
 =?us-ascii?q?uIx7yWYIrrfWUdwDvSCEwenw8P+naGMBA0Bj29rGLGEDxuCVXvblvx/uZktnO0?=
 =?us-ascii?q?UFU4zwGQYE1nzLq65BgViOeYS/MS2LIEpSggpy91HFa7w9LZFd6AqxB9c6VbZN?=
 =?us-ascii?q?M3+E1H2n7BtwxhIpygKLhvhkIfcwRyuEPuyxV3C4Vancg2tn8qyxByKaaZ0FNH?=
 =?us-ascii?q?eDOVxpTwOrzRKmnv8xGjca/W2lfC0NmI/qcD8ug3q1LmvAuxDEot721n08VJ03?=
 =?us-ascii?q?ub/pjFFg4SXoz+U0kp9xl2varVYjQg6IzO031sMq60siHZ1tItBeslzAugfthF?=
 =?us-ascii?q?PKOFEg/yD9MVB8y0JOM2nFipawoOPPpO+64sI8Oma/yG1barPeZ9hj6pkX5L4Y?=
 =?us-ascii?q?F90k2W8Sp8RfXF35IEw/GewwuGWC3wjFanssDrh49EYSseEXa4ySjhHIRRfLF9?=
 =?us-ascii?q?fZ4XCWeyJM263s9+h5/xVHFC7l6sGlQG1NWveRWMdVz93BRc2lgNrnyjhCS30S?=
 =?us-ascii?q?Z0kzYvrqeE2CzOwuLidAcIO2JRRWlii0vsLpawj9wAQEeoaA0pngO/5Unm36hb?=
 =?us-ascii?q?uLh/L27LTEZIfij6NWFjXrGrtrqfZc5C85cosSRRUOShblGWUL/9ox0G0yz9G2?=
 =?us-ascii?q?tS3iw0dzavup/hhRx1lHqdLGpvrHreYcxwxw3Q5MbfRf5S2ToKXjJ4hiPUBli/?=
 =?us-ascii?q?Ptmp+8ubl5HYv+C6VmKhUIBTcCbxwYOBsiu7+XNlARmlk/+vnd3nFBAw0TXn2N?=
 =?us-ascii?q?lySSXIsBH8b5Hr1qugNuJrZEtoBF7668dhHoF+k40whIwf2HQAh5WV+2YHnnn3?=
 =?us-ascii?q?MdlBxa3+a38NTyYRw9HJ+AjlxFFjLnWRyoL6THqdw9FtZ9mnYmwM3CI94NtHCK?=
 =?us-ascii?q?OV7LxCgCt0rUC0rQPXYfhhgDgdzeEi52Idg+EMoAAt1DmSAqgOHUlEOizhjxGI?=
 =?us-ascii?q?79G9rKpNfmquf6a/1FFind+/FrGNuR9TWGz2epo5GS9w78N/ME/D0XHp64Hkfs?=
 =?us-ascii?q?XQYswXthGOjxjAiO1VIoorlvUWnSpnJX79vXo9xu4nihxu2Iu2vYmdJGVr4aK5?=
 =?us-ascii?q?GQVYNjzuasMX+zHti7ten8mM04CuGJVhBisEXJ/yQf20FzISsOztNxySHz0ktn?=
 =?us-ascii?q?ebBb3fEBeE50h8qHLPF42rO2uTJHkE1thiQBidJEpCgAEbRjk6n5g5Fhy0y8zl?=
 =?us-ascii?q?akt2+jcR5lvgoBtW1u1oLwX/UnvYpAqwaDc7Up2fIwRN7g1Y+0fZK8+e7v9wHy?=
 =?us-ascii?q?FF+J2utheNJ3efZwRJC2EJR0OFC0riPrmo+dnP7eyYCvCiIPvJZLWEsfZeWOuQ?=
 =?us-ascii?q?xZKzzotm+C6BN8WIPnl/Fv07x1BMXXZjF8TCnDUPTSMXmjnJb86aohe85yJ2ot?=
 =?us-ascii?q?q+8PTtRALg+4+PB6FOPtVo/hC8mb2DOPKIhCZlNTZY0YsBxX/Wx7ge314SiCBu?=
 =?us-ascii?q?eyOuEbQasi7NQ7zfmrVKDx4AcC5zMMpI76Qh3ghCI8Lbi9X11qJmgf4xEVtKSV?=
 =?us-ascii?q?vhmsSxb8wQP269LE/HBFqMNLmePzLL3tr4YbmiRrxQlulUsxywtC2fE0/iODSD?=
 =?us-ascii?q?ijboWwquMeFKkCGUIhhetJuhfRZqDGjpVMjmZQGjMN9rkT02xqU5hm/LNW4ZKz?=
 =?us-ascii?q?Rzb1lBoaGQ7SxGhPVyAGhB7ntjLemZlCeV9ejYKpAKsfR1BiR4jf5V4HM/y7FN?=
 =?us-ascii?q?9iFLWOR1mDfOrt5pu1ynkvOAyjxiUBpPrDZEnIOKvUV4NqXf+ZlNQnLE/BML7W?=
 =?us-ascii?q?WNBBUGvdplCtvzu69OztjDjr78KDBH84GcwcxJKNLZJ4qkKn0oMV/JESTICQID?=
 =?us-ascii?q?BWqkL2zQhEx1lPCU622bqYU8ppHwmZ0IDLhBWwpxXqcWB1poGPQOOpZpUykonK?=
 =?us-ascii?q?Ddi9UV4mG66h7LS5MJkIrAU6e9CPKnGDeQi7YMMxQBxqP5JIIcbNDT21FrdVR7?=
 =?us-ascii?q?2o/NHhyDDph2viR9Y1ps8w12+39kQzh2ghq9Zw=3D=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0ANAACymQpch0O0hNFbCBwBAQEEAQEHB?=
 =?us-ascii?q?AEBgVEHAQELAYNrJwqMCYwRgiFollMUgWIRGBMBh10iNAkNAQMBAQEBAQECARM?=
 =?us-ascii?q?BAQEIDQkIKS+CNiQBgmIDAwECJFIGCQEBUANUBgENBQWDHIICBaZRM4osh3OEL?=
 =?us-ascii?q?4IWgRGCXYURDYVzAoEqAQGIC4FwhBR7kDABBgIBgVpLhCGDeYcBCxiJY4dViRC?=
 =?us-ascii?q?QC4FGgg1NI4FugU6CJxeOKjIBATGBBAEBAQ6JOIEfAQE?=
X-IPAS-Result: =?us-ascii?q?A0ANAACymQpch0O0hNFbCBwBAQEEAQEHBAEBgVEHAQELAYN?=
 =?us-ascii?q?rJwqMCYwRgiFollMUgWIRGBMBh10iNAkNAQMBAQEBAQECARMBAQEIDQkIKS+CN?=
 =?us-ascii?q?iQBgmIDAwECJFIGCQEBUANUBgENBQWDHIICBaZRM4osh3OEL4IWgRGCXYURDYV?=
 =?us-ascii?q?zAoEqAQGIC4FwhBR7kDABBgIBgVpLhCGDeYcBCxiJY4dViRCQC4FGgg1NI4Fug?=
 =?us-ascii?q?U6CJxeOKjIBATGBBAEBAQ6JOIEfAQE?=
X-IronPort-AV: E=Sophos;i="5.56,326,1539673200"; 
   d="scan'208";a="43177048"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 07 Dec 2018 08:08:48 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726266AbeLGQFf (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Fri, 7 Dec 2018 11:05:35 -0500
Received: from szxga05-in.huawei.com ([45.249.212.191]:15657 "EHLO huawei.com"
        rhost-flags-OK-OK-OK-FAIL) by vger.kernel.org with ESMTP
        id S1726233AbeLGQFd (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Fri, 7 Dec 2018 11:05:33 -0500
Received: from DGGEMS403-HUB.china.huawei.com (unknown [172.30.72.60])
        by Forcepoint Email with ESMTP id 9DCE25445B45D;
        Sat,  8 Dec 2018 00:05:26 +0800 (CST)
Received: from localhost.localdomain (10.175.124.28) by smtp.huawei.com
 (10.3.19.203) with Microsoft SMTP Server (TLS) id 14.3.408.0; Sat, 8 Dec 2018
 00:05:18 +0800
From: Gao Xiang <gaoxiang25@huawei.com>
To: Greg Kroah-Hartman <gregkh@linuxfoundation.org>,
        <devel@driverdev.osuosl.org>
CC: LKML <linux-kernel@vger.kernel.org>,
        <linux-erofs@lists.ozlabs.org>, "Chao Yu" <yuchao0@huawei.com>,
        Miao Xie <miaoxie@huawei.com>, <weidu.du@huawei.com>,
        Gao Xiang <gaoxiang25@huawei.com>
Subject: [PATCH 5/7] staging: erofs: refine compressed pages preload flow
Date: Sat, 8 Dec 2018 00:19:16 +0800
Message-ID: <20181207161918.20747-6-gaoxiang25@huawei.com>
X-Mailer: git-send-email 2.14.4
In-Reply-To: <20181207161918.20747-1-gaoxiang25@huawei.com>
References: <20181207161918.20747-1-gaoxiang25@huawei.com>
MIME-Version: 1.0
Content-Type: text/plain
X-Originating-IP: [10.175.124.28]
X-CFilter-Loop: Reflected
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Currently, there are two kinds of compressed pages in erofs:
  1) file pages for the in-place decompression and
  2) managed pages for cached decompression.
Both are all stored in grp->compressed_pages[].

For managed pages, they could already exist or could be preloaded
in this round, including the following cases in detail:
  1) Already valid (loaded in some previous round);
  2) PAGE_UNALLOCATED, should be allocated at the time of submission;
  3) Just found in the managed cache, and with an extra page ref.
Currently, 1) and 3) can be distinguishable by lock_page and
checking its PG_private, which is guaranteed by the reclaim path,
but it's better to do a double check by using an extra tag.

This patch reworks the preload flow by introducing such the tag
by using tagged pointer, too many #ifdefs are removed as well.

Reviewed-by: Chao Yu <yuchao0@huawei.com>
Signed-off-by: Gao Xiang <gaoxiang25@huawei.com>
---

This patch adds a new compressed_page_t type because erofs tagged
pointer implementation defines 1-bit tag tagptr1_t, 2-bit tag
tagptr2_t, ... structures to wrap up different tagged pointer uses.
it needs to give a meaningful type name rather than purely use
tagptr1_t, tagptr2_t, ...

Thanks,
Gao Xiang

 drivers/staging/erofs/unzip_vle.c | 166 ++++++++++++++++++++++++++++----------
 1 file changed, 123 insertions(+), 43 deletions(-)

diff --git a/drivers/staging/erofs/unzip_vle.c b/drivers/staging/erofs/unzip_vle.c
index de41f7b739ac..2b494f29e2c2 100644
--- a/drivers/staging/erofs/unzip_vle.c
+++ b/drivers/staging/erofs/unzip_vle.c
@@ -21,6 +21,21 @@
  */
 #define PAGE_UNALLOCATED     ((void *)0x5F0E4B1D)
 
+/* how to allocate cached pages for a workgroup */
+enum z_erofs_cache_alloctype {
+	DONTALLOC,	/* don't allocate any cached pages */
+	DELAYEDALLOC,	/* delayed allocation (at the time of submitting io) */
+};
+
+/*
+ * tagged pointer with 1-bit tag for all compressed pages
+ * tag 0 - the page is just found with an extra page reference
+ */
+typedef tagptr1_t compressed_page_t;
+
+#define tag_compressed_page_justfound(page) \
+	tagptr_fold(compressed_page_t, page, 1)
+
 static struct workqueue_struct *z_erofs_workqueue __read_mostly;
 static struct kmem_cache *z_erofs_workgroup_cachep __read_mostly;
 
@@ -131,38 +146,58 @@ struct z_erofs_vle_work_builder {
 	{ .work = NULL, .role = Z_EROFS_VLE_WORK_PRIMARY_FOLLOWED }
 
 #ifdef EROFS_FS_HAS_MANAGED_CACHE
-
-static bool grab_managed_cache_pages(struct address_space *mapping,
-				     erofs_blk_t start,
-				     struct page **compressed_pages,
-				     int clusterblks,
-				     bool reserve_allocation)
+static void preload_compressed_pages(struct z_erofs_vle_work_builder *bl,
+				     struct address_space *mc,
+				     pgoff_t index,
+				     unsigned int clusterpages,
+				     enum z_erofs_cache_alloctype type,
+				     struct list_head *pagepool,
+				     gfp_t gfp)
 {
-	bool noio = true;
-	unsigned int i;
+	struct page **const pages = bl->compressed_pages;
+	const unsigned int remaining = bl->compressed_deficit;
+	bool standalone = true;
+	unsigned int i, j = 0;
+
+	if (bl->role < Z_EROFS_VLE_WORK_PRIMARY_FOLLOWED)
+		return;
+
+	gfp = mapping_gfp_constraint(mc, gfp) & ~__GFP_RECLAIM;
 
-	/* TODO: optimize by introducing find_get_pages_range */
-	for (i = 0; i < clusterblks; ++i) {
-		struct page *page, *found;
+	index += clusterpages - remaining;
 
-		if (READ_ONCE(compressed_pages[i]))
+	for (i = 0; i < remaining; ++i) {
+		struct page *page;
+		compressed_page_t t;
+
+		/* the compressed page was loaded before */
+		if (READ_ONCE(pages[i]))
 			continue;
 
-		page = found = find_get_page(mapping, start + i);
-		if (!found) {
-			noio = false;
-			if (!reserve_allocation)
-				continue;
-			page = PAGE_UNALLOCATED;
+		page = find_get_page(mc, index + i);
+
+		if (page) {
+			t = tag_compressed_page_justfound(page);
+		} else if (type == DELAYEDALLOC) {
+			t = tagptr_init(compressed_page_t, PAGE_UNALLOCATED);
+		} else {	/* DONTALLOC */
+			if (standalone)
+				j = i;
+			standalone = false;
+			continue;
 		}
 
-		if (!cmpxchg(compressed_pages + i, NULL, page))
+		if (!cmpxchg_relaxed(&pages[i], NULL, tagptr_cast_ptr(t)))
 			continue;
 
-		if (found)
-			put_page(found);
+		if (page)
+			put_page(page);
 	}
-	return noio;
+	bl->compressed_pages += j;
+	bl->compressed_deficit = remaining - j;
+
+	if (standalone)
+		bl->role = Z_EROFS_VLE_WORK_PRIMARY;
 }
 
 /* called by erofs_shrinker to get rid of all compressed_pages */
@@ -234,6 +269,17 @@ int erofs_try_to_free_cached_page(struct address_space *mapping,
 	}
 	return ret;
 }
+#else
+static void preload_compressed_pages(struct z_erofs_vle_work_builder *bl,
+				     struct address_space *mc,
+				     pgoff_t index,
+				     unsigned int clusterpages,
+				     enum z_erofs_cache_alloctype type,
+				     struct list_head *pagepool,
+				     gfp_t gfp)
+{
+	/* nowhere to load compressed pages from */
+}
 #endif
 
 /* page_type must be Z_EROFS_PAGE_TYPE_EXCLUSIVE */
@@ -608,6 +654,26 @@ struct z_erofs_vle_frontend {
 	.owned_head = Z_EROFS_VLE_WORKGRP_TAIL, \
 	.backmost = true, }
 
+#ifdef EROFS_FS_HAS_MANAGED_CACHE
+static inline bool
+should_alloc_managed_pages(struct z_erofs_vle_frontend *fe, erofs_off_t la)
+{
+	if (fe->backmost)
+		return true;
+
+	if (EROFS_FS_ZIP_CACHE_LVL >= 2)
+		return la < fe->headoffset;
+
+	return false;
+}
+#else
+static inline bool
+should_alloc_managed_pages(struct z_erofs_vle_frontend *fe, erofs_off_t la)
+{
+	return false;
+}
+#endif
+
 static int z_erofs_do_read_page(struct z_erofs_vle_frontend *fe,
 				struct page *page,
 				struct list_head *page_pool)
@@ -622,12 +688,7 @@ static int z_erofs_do_read_page(struct z_erofs_vle_frontend *fe,
 	bool tight = builder_is_followed(builder);
 	struct z_erofs_vle_work *work = builder->work;
 
-#ifdef EROFS_FS_HAS_MANAGED_CACHE
-	struct address_space *const mc = MNGD_MAPPING(sbi);
-	struct z_erofs_vle_workgroup *grp;
-	bool noio_outoforder;
-#endif
-
+	enum z_erofs_cache_alloctype cache_strategy;
 	enum z_erofs_page_type page_type;
 	unsigned int cur, end, spiltted, index;
 	int err = 0;
@@ -667,20 +728,16 @@ static int z_erofs_do_read_page(struct z_erofs_vle_frontend *fe,
 	if (unlikely(err))
 		goto err_out;
 
-#ifdef EROFS_FS_HAS_MANAGED_CACHE
-	grp = fe->builder.grp;
-
-	/* let's do out-of-order decompression for noio */
-	noio_outoforder = grab_managed_cache_pages(mc,
-		erofs_blknr(map->m_pa),
-		grp->compressed_pages, erofs_blknr(map->m_plen),
-		/* compressed page caching selection strategy */
-		fe->backmost | (EROFS_FS_ZIP_CACHE_LVL >= 2 ?
-				map->m_la < fe->headoffset : 0));
-
-	if (noio_outoforder && builder_is_followed(builder))
-		builder->role = Z_EROFS_VLE_WORK_PRIMARY;
-#endif
+	/* preload all compressed pages (maybe downgrade role if necessary) */
+	if (should_alloc_managed_pages(fe, map->m_la))
+		cache_strategy = DELAYEDALLOC;
+	else
+		cache_strategy = DONTALLOC;
+
+	preload_compressed_pages(builder, MNGD_MAPPING(sbi),
+				 map->m_pa / PAGE_SIZE,
+				 map->m_plen / PAGE_SIZE,
+				 cache_strategy, page_pool, GFP_KERNEL);
 
 	tight &= builder_is_followed(builder);
 	work = builder->work;
@@ -1062,6 +1119,9 @@ pickup_page_for_submission(struct z_erofs_vle_workgroup *grp,
 	struct address_space *mapping;
 	struct page *oldpage, *page;
 
+	compressed_page_t t;
+	int justfound;
+
 repeat:
 	page = READ_ONCE(grp->compressed_pages[nr]);
 	oldpage = page;
@@ -1078,6 +1138,11 @@ pickup_page_for_submission(struct z_erofs_vle_workgroup *grp,
 		goto out_allocpage;
 	}
 
+	/* process the target tagged pointer */
+	t = tagptr_init(compressed_page_t, page);
+	justfound = tagptr_unfold_tags(t);
+	page = tagptr_unfold_ptr(t);
+
 	mapping = READ_ONCE(page->mapping);
 
 	/*
@@ -1085,7 +1150,10 @@ pickup_page_for_submission(struct z_erofs_vle_workgroup *grp,
 	 * get such a cached-like page.
 	 */
 	if (nocache) {
-		/* should be locked, not uptodate, and not truncated */
+		/* if managed cache is disabled, it is impossible `justfound' */
+		DBG_BUGON(justfound);
+
+		/* and it should be locked, not uptodate, and not truncated */
 		DBG_BUGON(!PageLocked(page));
 		DBG_BUGON(PageUptodate(page));
 		DBG_BUGON(!mapping);
@@ -1102,11 +1170,22 @@ pickup_page_for_submission(struct z_erofs_vle_workgroup *grp,
 
 	lock_page(page);
 
+	/* only true if page reclaim goes wrong, should never happen */
+	DBG_BUGON(justfound && PagePrivate(page));
+
 	/* the page is still in manage cache */
 	if (page->mapping == mc) {
 		WRITE_ONCE(grp->compressed_pages[nr], page);
 
 		if (!PagePrivate(page)) {
+			/*
+			 * impossible to be !PagePrivate(page) for
+			 * the current restriction as well if
+			 * the page is already in compressed_pages[].
+			 */
+			DBG_BUGON(!justfound);
+
+			justfound = 0;
 			set_page_private(page, (unsigned long)grp);
 			SetPagePrivate(page);
 		}
@@ -1124,6 +1203,7 @@ pickup_page_for_submission(struct z_erofs_vle_workgroup *grp,
 	 * reuse this one, let's allocate a new cache-managed page.
 	 */
 	DBG_BUGON(page->mapping);
+	DBG_BUGON(!justfound);
 
 	tocache = true;
 	unlock_page(page);
-- 
2.14.4

