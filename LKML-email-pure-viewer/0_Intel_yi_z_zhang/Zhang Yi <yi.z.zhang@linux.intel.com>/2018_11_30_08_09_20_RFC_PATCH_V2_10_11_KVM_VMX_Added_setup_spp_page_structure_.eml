Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 30 Nov 2018 16:09:44 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga001.jf.intel.com (orsmga001.jf.intel.com [10.7.209.18])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 7007E580460;
	Fri, 30 Nov 2018 00:09:43 -0800 (PST)
Received: from orsmga103.jf.intel.com ([10.7.208.35])
  by orsmga001-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 30 Nov 2018 00:09:43 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3A8HASiBdDNh6ZlXIb/dORPe0MlGMj4u6mDksu8pMi?=
 =?us-ascii?q?zoh2WeGdxc6+YByN2/xhgRfzUJnB7Loc0qyK6/CmATRIyK3CmUhKSIZLWR4BhJ?=
 =?us-ascii?q?detC0bK+nBN3fGKuX3ZTcxBsVIWQwt1Xi6NU9IBJS2PAWK8TW94jEIBxrwKxd+?=
 =?us-ascii?q?KPjrFY7OlcS30P2594HObwlSizexfbB/IA+qoQnNq8IbnZZsJqEtxxXTv3BGYf?=
 =?us-ascii?q?5WxWRmJVKSmxbz+MK994N9/ipTpvws6ddOXb31cKokQ7NYCi8mM30u683wqRbD?=
 =?us-ascii?q?VwqP6WACXWgQjxFFHhLK7BD+Xpf2ryv6qu9w0zSUMMHqUbw5Xymp4qF2QxHqlS?=
 =?us-ascii?q?gHLSY0/mHJhMJtgqxVoxWvqB5xw47PeIGYNuFzcr/Bcd4AWWZMRNpdWzBHD4ih?=
 =?us-ascii?q?b4UPFe0BPeNAoob+p1sBtx2+BQaxD+ztyz9Dm3j73K070+g7EADKxg0hH9IUv3?=
 =?us-ascii?q?TSo9X4L6MSUeGzzKnVwjTPdfJW2THh6IfWdhAtu+2DXbV1ccfIz0QkCgDLjk2I?=
 =?us-ascii?q?pID7Iz+Y0v4Bv3WV4uZ+T+6jlm0qpx1rrjWuxMogkpTFi4YLxlze9Sh0wJw5KN?=
 =?us-ascii?q?2mREJhfNKpEIZcuieHPIVsWMwiWXtnuCMix70Gp5G7eC8KxYw5xx7QdfOHaZKE?=
 =?us-ascii?q?4hH9W+aLJzd3mnZldKi4hxao/kis0uz8Vs+q31ZWtidJjMXAu3QX2xDO5MWLVO?=
 =?us-ascii?q?Fx8lqi1DqTzQze6+NJLVgxlaXBKp4hxrAwloAUsUTGBiL2nET2jKmLdkQr4+So?=
 =?us-ascii?q?6ProYq/gppCCM494kwb+M6oomsOhG+Q1KRYOX2eF9uSmzrHj/lP2QK9MjvIolq?=
 =?us-ascii?q?nVqpfaJd4UpqKhGQ9azp4j6wqjDzehyNkYmXgHLFFbdx6dgInpJkrDIPT5Dfe5?=
 =?us-ascii?q?nlStny1nx/HAPr39HJrNKmLPn6vmfbZ480Rc0hY8zchD55JIDbEMOPLzVVXwtN?=
 =?us-ascii?q?zEFBA5NBa4w+b6CNpn0IMeVnmCAquYMKPUrF+J6fgjI+iKZI8Jpjn9L+Ip6OLp?=
 =?us-ascii?q?jX88gVUdZ7Wm3YMLaHCkGfRrO0eZYX3yjdsbEmcKuQw+TOrtiFCZVT5TZnCyX7?=
 =?us-ascii?q?8z5z0hCYKmC5vDSZ6pgLCbwCi7GZhWbHhcCl+QCXfoa5mEW/AUZS2IOc9hkjsE?=
 =?us-ascii?q?Vbu7R487zx6uthT3y75mLurS5y0Zuojv1Nlz5+3Pix4y8SZ4ANia02GIV2t0hH?=
 =?us-ascii?q?8HRycq3KBjpkxw0lSD3rJ+g/BCEdxT5ulGUgE1NZPHy+x6CtbyWh/Of9uTSVam?=
 =?us-ascii?q?RMmmDi81Tt4r39AOZEN9Ec24jh/fxyqqH6MVl7uTCZMp6K3cwWb+K9x9y3nc0q?=
 =?us-ascii?q?khlEcpQs1IOW2iha5/8gzTCpXNk0WYkaaqaKsd0DTM9GeF0WqBokVYXBRsXqXC?=
 =?us-ascii?q?WHAVflHWosjh5kPeU7+uDqwqPRZbxs6cNKRGcN3pgk9ARPf4JtveZXm8m2OxBR?=
 =?us-ascii?q?aO27ONY5Dme2Qb3CXBFkcElxof8mqBNQg7Hi2huX7RDCRyFVLzZEPh6fV+qHK+?=
 =?us-ascii?q?Tk8z0wGKb01g26Cp+hIPgvycUfcT3rMCuCcusDh0GFe939TLC9uPvQZhfaNcYc?=
 =?us-ascii?q?8j71dDz27Wqwt9Ppm4JaB4mlEeaxh3v1/p1xhvEIpAldYlrXw0wwtyM66Xy05B?=
 =?us-ascii?q?eC6C0pD2Ob3XLXfy8Qura67X3FHezdmX9r0O6PQ+t1XsogWpGlA+/HVg1tlfy2?=
 =?us-ascii?q?Gc6YnSDAoOTZLxVV469hhnp73AfiYx/YLV1X12PqmyvT/PwNYpBOojyha9cNZT?=
 =?us-ascii?q?KqKEFAnuE8IEA8iiMvAlm1+sbhgcJuBd6LY0P9+6d/uBwKOrJudgky68gmRd/Y?=
 =?us-ascii?q?991ViA9yxiRe7S3pYJ2u2X0RGDVzjhklihqMf3mYZfaDEWH2q/zzXkBYFLaq1z?=
 =?us-ascii?q?e4YLFXmhI8mtytpigJ7tXmZS9ES/CFMexM+pZR2SYkT93A1Rz0gWoWarmTClzz?=
 =?us-ascii?q?xyiDwpqquf3CrTw+XtbhYHO2hLRHV8glfoO4S7k9caXE2wZQgziBSl/Vr6x7Rc?=
 =?us-ascii?q?pKlnLWncW11Ifyv1L2FlSKewraCNY81M6JMptyVYTuK8bEueSr78pRsaziziE3?=
 =?us-ascii?q?FfxDA9azGlpJH5kwZmh2KaKXZ5tGDZdt1oxRfD+NzcQuZc3zocSyl/kzXXBFm8?=
 =?us-ascii?q?P9+y8NWQlpfDtP2+Vm27Wp1Sdynr0Z2PtC+h6WJ2Bh2/mui5msf7HggizS/7y9?=
 =?us-ascii?q?5qWD3LrBnmY4nnzaS6Mf99cUluC1/x8M56GoB4kosti5Acw3kahpOJ/XUZlWf/?=
 =?us-ascii?q?K8lU2aX7bHAVXz4E38bV4BT52E1kNn+JwoP5VnaHzcd7adi1fHgW2j4j4MBQE6?=
 =?us-ascii?q?ib8qZEnSRur1q8rALRZ+V9nzgHxfsv7n4an/8GuA43wiqBBbASGFFSPTbwmBSQ?=
 =?us-ascii?q?89C+sKJXaX6zcbi30Upyh9GgA6uEogFBQ3b5YZYiEDR07sV+NlLMzXLy5pvleN?=
 =?us-ascii?q?nWcdIcqBmUnw3cgOhSLZI7juAKijZ/OWLhoX0lzPY2ggd00pG9uIiHNn9h/Kak?=
 =?us-ascii?q?AhNDMj31ZsUT+izijKpEn8aW2ZyvEYtlGjkRQJToSveoGioItfv7LwaODCE8qn?=
 =?us-ascii?q?CDFLrdBwCf7Vlpr2nVH5C3LX2XJ2QZzdZ/RBmbPkNfmxsZXDE7np4/CwCry9bt?=
 =?us-ascii?q?cEZ/5jAN+FH4rgFAxf5vNxn6Sm3fvhunai8oSJiDKxpb9gFD50DIMcyH8+J8BS?=
 =?us-ascii?q?dY8oe6rAyKLGybaB9FDWUIWkyCGlDiMaOi5djG8+iEGOW+K+HCbqmJqexbT/2I?=
 =?us-ascii?q?346g0pN6/zaQMcWCJnliAOM62kpAXnB5Gt7VmzYVSywQmCLCcdSbpAqn+iBsqs?=
 =?us-ascii?q?C/8fLrWB/g5IeVCrtSN8lv9A6ygauZK+GQgyN5Iy5C1pwQ3X/I1KQf3FkKhi50?=
 =?us-ascii?q?ajmiCq4AujDNTa7Knq9XDgUWaydyNMtO8qI90RNBOc/ditPpyLF4iuQ5BEtCVV?=
 =?us-ascii?q?zkgsupf9AFI3mhNFPbA0aGLKiGJTzOw83tfaywU6FfjOVKuB22ojubF07jPjKe?=
 =?us-ascii?q?lzjmTRyvMOdMjD2FMxxaoo2ybhFtCW37RtL8dhK7KMN3jSExwbAshnLFL2gcMT?=
 =?us-ascii?q?19c0NLtrKR7CNYje95G2xO9XdlKeiEmyCE7+jXMJoWsP1rAjhqmOJe+ng117xV?=
 =?us-ascii?q?7CQXDMByzTPYqdh8olenuvOCxjpuTFxFrTMPzIeHukN+JI3T94VMVHjJ+h4A9y?=
 =?us-ascii?q?ObDBFO791uC8Hyk6pR1tTCmq/1JDpYtdnT+JgyHc/RffCHOXwueSvoUGrFDU0X?=
 =?us-ascii?q?CyyrPGfQr0hcluyCsHyTspU+7JPrncxdGfdgSFUpG6ZCWQxeF9sYLcIvUw=3D?=
 =?us-ascii?q?=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0BEAACN7wBch0O0hNFiHAEBAQQBAQcEA?=
 =?us-ascii?q?QGBUwUBAQsBgVWCFieYGSmBPwE4lzKBcxQYEwGBS4YpIjYHDQEDAQEBAQEBAgE?=
 =?us-ascii?q?TAQEBCA0JCCkvgjYigmUDAwECJFIGCQEBUANUBwkJBYMcggIFp3wzii+HbIQre?=
 =?us-ascii?q?oEcgREzAY06ApAdkAkHAg2RKRhefIUQijKIeY9egU0FggFwgzyCJxeOKjIBATE?=
 =?us-ascii?q?BAYEDAQGLNiuCIAEB?=
X-IPAS-Result: =?us-ascii?q?A0BEAACN7wBch0O0hNFiHAEBAQQBAQcEAQGBUwUBAQsBgVW?=
 =?us-ascii?q?CFieYGSmBPwE4lzKBcxQYEwGBS4YpIjYHDQEDAQEBAQEBAgETAQEBCA0JCCkvg?=
 =?us-ascii?q?jYigmUDAwECJFIGCQEBUANUBwkJBYMcggIFp3wzii+HbIQreoEcgREzAY06ApA?=
 =?us-ascii?q?dkAkHAg2RKRhefIUQijKIeY9egU0FggFwgzyCJxeOKjIBATEBAYEDAQGLNiuCI?=
 =?us-ascii?q?AEB?=
X-IronPort-AV: E=Sophos;i="5.56,297,1539673200"; 
   d="scan'208";a="54368721"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 30 Nov 2018 00:09:41 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1727433AbeK3TSG (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Fri, 30 Nov 2018 14:18:06 -0500
Received: from mga17.intel.com ([192.55.52.151]:5287 "EHLO mga17.intel.com"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1726551AbeK3TSG (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Fri, 30 Nov 2018 14:18:06 -0500
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from orsmga003.jf.intel.com ([10.7.209.27])
  by fmsmga107.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 30 Nov 2018 00:09:37 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.56,297,1539673200"; 
   d="scan'208";a="105480429"
Received: from linux.intel.com ([10.54.29.200])
  by orsmga003.jf.intel.com with ESMTP; 30 Nov 2018 00:09:37 -0800
Received: from dazhang1-ssd.sh.intel.com (unknown [10.239.48.128])
        by linux.intel.com (Postfix) with ESMTP id 882B8580460;
        Fri, 30 Nov 2018 00:09:35 -0800 (PST)
From: Zhang Yi <yi.z.zhang@linux.intel.com>
To: pbonzini@redhat.com, mdontu@bitdefender.com, ncitu@bitdefender.com
Cc: rkrcmar@redhat.com, linux-kernel@vger.kernel.org,
        kvm@vger.kernel.org, Zhang Yi <yi.z.zhang@linux.intel.com>
Subject: [RFC PATCH V2 10/11] KVM: VMX: Added setup spp page structure.
Date: Fri, 30 Nov 2018 16:09:20 +0800
Message-Id: <582cad9754a9651595385e901430ab69bbd668b6.1543481993.git.yi.z.zhang@linux.intel.com>
X-Mailer: git-send-email 2.7.4
In-Reply-To: <cover.1543481993.git.yi.z.zhang@linux.intel.com>
References: <cover.1543481993.git.yi.z.zhang@linux.intel.com>
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

The hardware uses the guest-physical address and bits 11:7 of the
address accessed to lookup the SPPT to fetch a write permission bit for
the 128 byte wide sub-page region being accessed within the 4K
guest-physical page. If the sub-page region write permission bit is set,
the write is allowed; otherwise the write is disallowed and results in
an EPT violation.

Guest-physical pages mapped via leaf EPT-paging-structures for which the
accumulated write-access bit and the SPP bits are both clear (0) generate
EPT violations on memory writes accesses. Guest-physical pages mapped via
EPT-paging-structure for which the accumulated write-access bit is set
(1) allow writes, effectively ignoring the SPP bit on the leaf EPT-paging
structure.

Software will setup the spp page table level4,3,2 as well as EPT page
structure, and fill the level1 via the 32 bit bitmap per a single 4K page.
Now it could be divided to 32 x 128 sub-pages.

Signed-off-by: Zhang Yi <yi.z.zhang@linux.intel.com>
---
 arch/x86/include/asm/kvm_host.h |   4 ++
 arch/x86/kvm/mmu.c              | 123 +++++++++++++++++++++++++++++++++++++++-
 2 files changed, 125 insertions(+), 2 deletions(-)

diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 3218d91..ce6d258 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1402,6 +1402,10 @@ int kvm_emulate_hypercall(struct kvm_vcpu *vcpu);
 
 int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gva_t gva, u64 error_code,
 		       void *insn, int insn_len);
+
+int kvm_mmu_setup_spp_structure(struct kvm_vcpu *vcpu,
+				u32 access_map, gfn_t gfn);
+
 void kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva);
 void kvm_mmu_invpcid_gva(struct kvm_vcpu *vcpu, gva_t gva, unsigned long pcid);
 void kvm_mmu_new_cr3(struct kvm_vcpu *vcpu, gpa_t new_cr3, bool skip_tlb_flush);
diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c
index d512125..287ee62 100644
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@ -206,6 +206,11 @@ static const union kvm_mmu_page_role mmu_base_role_mask = {
 		({ spte = mmu_spte_get_lockless(_walker.sptep); 1; });	\
 	     __shadow_walk_next(&(_walker), spte))
 
+#define for_each_shadow_spp_entry(_vcpu, _addr, _walker)    \
+	for (shadow_spp_walk_init(&(_walker), _vcpu, _addr);	\
+	     shadow_walk_okay(&(_walker));			\
+	     shadow_walk_next(&(_walker)))
+
 static struct kmem_cache *pte_list_desc_cache;
 static struct kmem_cache *mmu_page_header_cache;
 static struct percpu_counter kvm_total_used_mmu_pages;
@@ -476,6 +481,11 @@ static int is_shadow_present_pte(u64 pte)
 	return (pte != 0) && !is_mmio_spte(pte);
 }
 
+static int is_spp_mide_page_present(u64 pte)
+{
+	return pte & PT_PRESENT_MASK;
+}
+
 static int is_large_pte(u64 pte)
 {
 	return pte & PT_PAGE_SIZE_MASK;
@@ -495,6 +505,11 @@ static bool is_executable_pte(u64 spte)
 	return (spte & (shadow_x_mask | shadow_nx_mask)) == shadow_x_mask;
 }
 
+static bool is_spp_spte(struct kvm_mmu_page *sp)
+{
+	return sp->role.spp;
+}
+
 static kvm_pfn_t spte_to_pfn(u64 pte)
 {
 	return (pte & PT64_BASE_ADDR_MASK) >> PAGE_SHIFT;
@@ -2606,6 +2621,16 @@ static void shadow_walk_init(struct kvm_shadow_walk_iterator *iterator,
 				    addr);
 }
 
+static void shadow_spp_walk_init(struct kvm_shadow_walk_iterator *iterator,
+				 struct kvm_vcpu *vcpu, u64 addr)
+{
+	iterator->addr = addr;
+	iterator->shadow_addr = vcpu->arch.mmu->sppt_root;
+
+	/* SPP Table is a 4-level paging structure */
+	iterator->level = 4;
+}
+
 static bool shadow_walk_okay(struct kvm_shadow_walk_iterator *iterator)
 {
 	if (iterator->level < PT_PAGE_TABLE_LEVEL)
@@ -2656,6 +2681,18 @@ static void link_shadow_page(struct kvm_vcpu *vcpu, u64 *sptep,
 		mark_unsync(sptep);
 }
 
+static void link_spp_shadow_page(struct kvm_vcpu *vcpu, u64 *sptep,
+				 struct kvm_mmu_page *sp)
+{
+	u64 spte;
+
+	spte = __pa(sp->spt) | PT_PRESENT_MASK;
+
+	mmu_spte_set(sptep, spte);
+
+	mmu_page_add_parent_pte(vcpu, sp, sptep);
+}
+
 static void validate_direct_spte(struct kvm_vcpu *vcpu, u64 *sptep,
 				   unsigned direct_access)
 {
@@ -2686,7 +2723,13 @@ static bool mmu_page_zap_pte(struct kvm *kvm, struct kvm_mmu_page *sp,
 
 	pte = *spte;
 	if (is_shadow_present_pte(pte)) {
-		if (is_last_spte(pte, sp->role.level)) {
+		if (is_spp_spte(sp)) {
+			if (sp->role.level == PT_PAGE_TABLE_LEVEL)
+				//spp page do not need to release rmap.
+				return true;
+			child = page_header(pte & PT64_BASE_ADDR_MASK);
+			drop_parent_pte(child, spte);
+		} else if (is_last_spte(pte, sp->role.level)) {
 			drop_spte(kvm, spte);
 			if (is_large_pte(pte))
 				--kvm->stat.lpages;
@@ -4231,6 +4274,77 @@ static int tdp_page_fault(struct kvm_vcpu *vcpu, gva_t gpa, u32 error_code,
 	return RET_PF_RETRY;
 }
 
+static u64 format_spp_spte(u32 spp_wp_bitmap)
+{
+	u64 new_spte = 0;
+	int i = 0;
+
+	/*
+	 * One 4K page contains 32 sub-pages, in SPP table L4E, old bits
+	 * are reserved, so we need to transfer u32 subpage write
+	 * protect bitmap to u64 SPP L4E format.
+	 */
+	while (i < 32) {
+		if (spp_wp_bitmap & (1ULL << i))
+			new_spte |= 1ULL << (i * 2);
+
+		i++;
+	}
+
+	return new_spte;
+}
+
+static void mmu_spp_spte_set(u64 *sptep, u64 new_spte)
+{
+	__set_spte(sptep, new_spte);
+}
+
+int kvm_mmu_setup_spp_structure(struct kvm_vcpu *vcpu,
+				u32 access_map, gfn_t gfn)
+{
+	struct kvm_shadow_walk_iterator iter;
+	struct kvm_mmu_page *sp;
+	gfn_t pseudo_gfn;
+	u64 old_spte, spp_spte;
+	struct kvm *kvm = vcpu->kvm;
+
+	spin_lock(&kvm->mmu_lock);
+
+	/* direct_map spp start */
+
+	if (!VALID_PAGE(vcpu->arch.mmu->sppt_root))
+		goto out_unlock;
+
+	for_each_shadow_spp_entry(vcpu, (u64)gfn << PAGE_SHIFT, iter) {
+		if (iter.level == PT_PAGE_TABLE_LEVEL) {
+			spp_spte = format_spp_spte(access_map);
+			old_spte = mmu_spte_get_lockless(iter.sptep);
+			if (old_spte != spp_spte) {
+				mmu_spp_spte_set(iter.sptep, spp_spte);
+				kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
+			}
+			break;
+		}
+
+		if (!is_spp_mide_page_present(*iter.sptep)) {
+			u64 base_addr = iter.addr;
+
+			base_addr &= PT64_LVL_ADDR_MASK(iter.level);
+			pseudo_gfn = base_addr >> PAGE_SHIFT;
+			sp = kvm_mmu_get_spp_page(vcpu, pseudo_gfn,
+						  iter.level - 1);
+			link_spp_shadow_page(vcpu, iter.sptep, sp);
+		}
+	}
+
+	spin_unlock(&kvm->mmu_lock);
+	return 0;
+
+out_unlock:
+	spin_unlock(&kvm->mmu_lock);
+	return -EFAULT;
+}
+
 int kvm_mmu_get_subpages(struct kvm *kvm, struct kvm_subpage *spp_info)
 {
 	u32 *access = spp_info->access_map;
@@ -4255,9 +4369,10 @@ int kvm_mmu_set_subpages(struct kvm *kvm, struct kvm_subpage *spp_info)
 	gfn_t gfn = spp_info->base_gfn;
 	int npages = spp_info->npages;
 	struct kvm_memory_slot *slot;
+	struct kvm_vcpu *vcpu;
 	u32 *wp_map;
 	int ret;
-	int i;
+	int i, j;
 
 	for (i = 0; i < npages; i++, gfn++) {
 		slot = gfn_to_memslot(kvm, gfn);
@@ -4281,6 +4396,10 @@ int kvm_mmu_set_subpages(struct kvm *kvm, struct kvm_subpage *spp_info)
 				"Please try to disable the huge page\n", gfn);
 			return -EFAULT;
 		}
+
+		kvm_for_each_vcpu(j, vcpu, kvm)
+			kvm_mmu_setup_spp_structure(vcpu, access, gfn);
+
 		wp_map = gfn_to_subpage_wp_info(slot, gfn);
 		*wp_map = access;
 	}
-- 
2.7.4

