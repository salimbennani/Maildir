Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 28 Nov 2018 18:10:10 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga005.fm.intel.com (fmsmga005.fm.intel.com [10.253.24.32])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 2DD58580460;
	Tue, 27 Nov 2018 23:18:21 -0800 (PST)
Received: from orsmga106.jf.intel.com ([10.7.208.65])
  by fmsmga005-1.fm.intel.com with ESMTP; 27 Nov 2018 23:18:20 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AFWA4gBPzMjm3Unui0acl6mtUPXoX/o7sNwtQ0KIM?=
 =?us-ascii?q?zox0KPjzpMbcNUDSrc9gkEXOFd2Cra4c26yO6+jJYi8p2d65qncMcZhBBVcuqP?=
 =?us-ascii?q?49uEgeOvODElDxN/XwbiY3T4xoXV5h+GynYwAOQJ6tL1LdrWev4jEMBx7xKRR6?=
 =?us-ascii?q?JvjvGo7Vks+7y/2+94fcbglUhzexe69+IAmrpgjNq8cahpdvJLwswRXTuHtIfO?=
 =?us-ascii?q?pWxWJsJV2Nmhv3+9m98p1+/SlOovwt78FPX7n0cKQ+VrxYES8pM3sp683xtBnM?=
 =?us-ascii?q?VhWA630BWWgLiBVIAgzF7BbnXpfttybxq+Rw1DWGMcDwULs5Qiqp4bt1RxD0iS?=
 =?us-ascii?q?cHLz85/3/Risxsl6JQvRatqwViz4LIfI2ZMfxzdb7fc9wHX2pMRshfWSxfDI2h?=
 =?us-ascii?q?bIUPAeUOMvpFoIb/qVQOtgO+CAu3CePz1jNFnGP60bEg3ug/FwzNwQwuH8gJsH?=
 =?us-ascii?q?TRtNj5OqkcXvqvzKnSzDXMc/BW0ir55oTSbxsuofaMXbR/ccbf1EIiEB7KgU+K?=
 =?us-ascii?q?qYz/PjOayucNv3KV7upnU+KvhHUqqwZroje12sgsjpfGipgJxVDD8CV02YA4Ls?=
 =?us-ascii?q?C7Rk5jedOoDodcuiWAO4drTM4uXXtktDs5x7Eao5K2fSoHxIw6yxPQdvCLaZWE?=
 =?us-ascii?q?7x3iWeqLPDt0mXNodKihixqv90Wr1/fyWdOu0FlQqypIitnMuW4J1xzU8sWHVP?=
 =?us-ascii?q?R98Vm72TqV1ADc9PtEIUYqmqrfMZIhxaQwlpULvUTCGC/5hln2gbeIekk4/uWk?=
 =?us-ascii?q?8fnrb7v4qpOGKoN5iR3yPr4vl8G9Geg4NxIBX2mf+eSyzr3j+kj5Ta1Ojv03lK?=
 =?us-ascii?q?nZrZ/bKd0YpqGnGQ9V1Jgs6xKmAzeh3tUYm2cILEhedRKZgIjmJUvOLOr7Dfih?=
 =?us-ascii?q?mVSslilkx/TcMr3mGJXNIWDPkK39crZl905c1A0zwMhb55JVCbEOPuj/W0DstN?=
 =?us-ascii?q?HDCh85Mgq0w/voCdln14MeX36PDbGdMK/IrVCI4ecvKfGWZIAJoDb9N+Ql5/n2?=
 =?us-ascii?q?gH85g1AdfLWp0oEQaHyiHvRmPl+WYXzjgtoaFWcKvww+TPHliVGYUD5TYWqyUL?=
 =?us-ascii?q?w45j0hFI2mCoLDTJi3gLOdxCe7AoFWZmdeB1COFnfnaZ+IW/QLaCKUJM9hlScJ?=
 =?us-ascii?q?VbygS48nyBGvuxX2y7thLurI5CIYsYjv28Ry5+3WjRsy7yB7D9yB02GRSGF5hn?=
 =?us-ascii?q?kHRyQ23KB4okxx0E2D3rJ6g/FDEdxT5vVJUho1NJLGzux6DczyVRzFftuTVFmm?=
 =?us-ascii?q?RdCmCykrTt0t298Of1p9G9K6gxDAxSWqBaUZl7iKBJMu9KLc0GP8J8J8y3bAya?=
 =?us-ascii?q?kggEMqQspJNW26mKF/8xLfCJLOk0Wcj6yqb7gT3DbR9GefymqDpFxXXxRuUarb?=
 =?us-ascii?q?R3wfZlHZrdLi5kzcSb+iDrAnMghEyc6GMaZKbtzpjVNbRPbsItjeYmSxm3uuCh?=
 =?us-ascii?q?aM3L+DcI3qe2AF1iXHFEcEixwT/WqBNQUmGyiuuXzeAyJ0GVLveUzs9/J+p229?=
 =?us-ascii?q?TkIvywGKbkth16e6+xIPhPycTe8T0awAuCs7tzp0G1O91crMC9WcvwphYLlcYd?=
 =?us-ascii?q?Ql7Vdb1GLZsgt9PoCgL6FinFIebx57v0T01xVzC4VAl8cqoWguzApzL6KYzVxA?=
 =?us-ascii?q?eymZ3ZD2Jr3YNG3y8AqzZK7R31HUyMyW9bsX6PQkt1XjuxmkGVAm83p53NhazX?=
 =?us-ascii?q?ud6o/RAwoPTJ3+SEA39xt9p7HEeSQ944LU1XtxMai7qDPC2tQpBPc7xRakZdtQ?=
 =?us-ascii?q?LKSEFArqGc0AG8euMPAqm0Subh8cPOFS6bQ4MN+8e/qH2K6kJuBgnD29gGRD4Y?=
 =?us-ascii?q?B91F+M9iVmRu7J2ZYF3++X3g+dWzjgi1eht9j9mZpYajEKAmq/1S/kCZZLZq1z?=
 =?us-ascii?q?YYkEE32uLNCwxtlkgZ7iRWRY9F+6C1wawsCpfQedYELn3Q1X00QauninmSq+zz?=
 =?us-ascii?q?xpnDAltKuf3CrSw+v8cBoLIHJERG5njV30O4i7k8gaXFS0bwgujBak5Uf6y7Jb?=
 =?us-ascii?q?pahlNGnTXEFIcjPyL2FjVKuwq7WDb9RO6JMurSVYTuC8bUqGRb76phsQyznjEH?=
 =?us-ascii?q?dGxDAnazGqvY30nx5gh2KHL3Zzr33ZddtrxRjF59zcRv9R3jwYSyl+kjTXAlm8?=
 =?us-ascii?q?P9+0/dSbjZvDs+a+V36/WZ1XayXk0YSAtC6j721wHRK/h+yzmsHgEQUi0S70zd?=
 =?us-ascii?q?5qWT/KrBb9ZInmzKC6MeNhfkl1C17w8ct6GodikoQugJEcw2QVhpKQ/XAfi2f8?=
 =?us-ascii?q?Lc1b2b7ibHoKXTMLwMTa4An/1E1jM3KG3YT5VnqGz8tlZtm6ZH4W2y0n48BLDq?=
 =?us-ascii?q?eU8KJLnS9vrlWkqgLRZOB3ni0Bxvs29H4an+YJtRIoziWcAbAdB1NUPCLymBST?=
 =?us-ascii?q?89Cxsr9Xa32xfriq20pzhtShDLCEogFBV3f1YJYiHSls7sphNFLAymH86obheN?=
 =?us-ascii?q?PId9IcqgWUkwvcj+hSMJ8xiv0KhS99NWP8p3IlzfM7jQd13ZG7p4WHL2Rt/KSk?=
 =?us-ascii?q?Ah9XLDH1ZsUT+i3zgqZahMqZw4evHpB5EDURQJToVe6oEC4Vtfn/LAmBCjo8qn?=
 =?us-ascii?q?OGGbbFGQ+f9Vxrr3bOE5CtKnGWK2MVzdRkRBmBOkNfhBoYUyk9np48Dgqq3tDu?=
 =?us-ascii?q?cF9l5jAN4V71shlMxfhtNxbhSWfTvhuoZi03SJiCLxpW8wdC6F3OMcGF6uJzHi?=
 =?us-ascii?q?dY/oCurQCXK2ybYRhIAn8NWkCeG1/jObyu78Ha8+eEHuq+M+fOYbKWpO1eUPeI?=
 =?us-ascii?q?2Iuv0pZ88DaKLMmPJX5iD/s02kdYWXB5GsLZmygASiANliLNadKbqwm4+iFtss?=
 =?us-ascii?q?+/9/HrUhr15YSTE7tSLclv+xeujKeDKeGQhSV5KTVZ1p8Wxn/I0r8f3FEMhCF0?=
 =?us-ascii?q?ajmtCq8NtSrMTKLWh69WAAQXayJ1NMtU8a082hNBNtLcitPwzrR4lOI6C09ZVV?=
 =?us-ascii?q?z9ncGkfdYFI3ymO1PdGkmKNK6KJTnKw8zsZaO8SLtQjPhbthGquDabFVPjMSqH?=
 =?us-ascii?q?lzXzSx+vNuRMhjmBPBNCoIG9bgptCW/7QdL6ax27NcV7giEszbIohnPFK2gcPC?=
 =?us-ascii?q?N4c0NMqL2Q8CxZju9+G2xH8npqM+2ElzyF4OnfL5YcqeFrDThsl+JG/HQ6zKNY?=
 =?us-ascii?q?7SFeS/xznSvSr9hurEmnk+mP0DVnVhVOpy1PhIKKu0ViJKrY+oNBWXbC4BIC82?=
 =?us-ascii?q?GQBw4WqNtiD92885xXn/TGiq+7AzdD9dTZ54NIC8HKKc6vPmAmNB3zEj/ISgAC?=
 =?us-ascii?q?SGj4G3vYghkXvPiI7HyerdAY7NDTn5wBAPcPWFUvDPIRCQJ6G9oNCJBtVzggnP?=
 =?us-ascii?q?iQi8tetin2lwXYWMgP5sOPbfmVG/i6bW/B1bQ=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AFAAC5QP5bh0O0hNFkGQEBAQEBAQEBA?=
 =?us-ascii?q?QEBAQcBAQEBAQGBVAEBAQEBAQsBgVWCFieEW4gViyqCDRSNdBGLHBIBARgTAYQ?=
 =?us-ascii?q?3gysiNwYNAQMBAQEBAQECARMBAQEIDQkIKSMMgjYkAYJhAQEBAQIBAQIkEwYBA?=
 =?us-ascii?q?TcBBAEJAQEKDgoJJQMMBUkTBYMcgXoIBAGlNIFsM4J2AQEFh0kIjBKBVz+EI4R?=
 =?us-ascii?q?thW+JByAyhkSFKIpQBwICiiiGfCMKgVCIMYcDgnmEX4IbjiMCBAIEBQITAYFcg?=
 =?us-ascii?q?XdRH4M8ghsMF4NKinRRgQUBAYobASUEgiMBAQ?=
X-IPAS-Result: =?us-ascii?q?A0AFAAC5QP5bh0O0hNFkGQEBAQEBAQEBAQEBAQcBAQEBAQG?=
 =?us-ascii?q?BVAEBAQEBAQsBgVWCFieEW4gViyqCDRSNdBGLHBIBARgTAYQ3gysiNwYNAQMBA?=
 =?us-ascii?q?QEBAQECARMBAQEIDQkIKSMMgjYkAYJhAQEBAQIBAQIkEwYBATcBBAEJAQEKDgo?=
 =?us-ascii?q?JJQMMBUkTBYMcgXoIBAGlNIFsM4J2AQEFh0kIjBKBVz+EI4RthW+JByAyhkSFK?=
 =?us-ascii?q?IpQBwICiiiGfCMKgVCIMYcDgnmEX4IbjiMCBAIEBQITAYFcgXdRH4M8ghsMF4N?=
 =?us-ascii?q?KinRRgQUBAYobASUEgiMBAQ?=
X-IronPort-AV: E=Sophos;i="5.56,289,1539673200"; 
   d="scan'208";a="41855990"
X-Amp-Result: UNSCANNABLE
X-Amp-File-Uploaded: False
Unscannable: 2
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 27 Nov 2018 23:18:18 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1727552AbeK1SPx (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Wed, 28 Nov 2018 13:15:53 -0500
Received: from userp2130.oracle.com ([156.151.31.86]:33942 "EHLO
        userp2130.oracle.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1727280AbeK1SPx (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Wed, 28 Nov 2018 13:15:53 -0500
Received: from pps.filterd (userp2130.oracle.com [127.0.0.1])
        by userp2130.oracle.com (8.16.0.22/8.16.0.22) with SMTP id wAS7EaEj176923;
        Wed, 28 Nov 2018 07:15:13 GMT
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=oracle.com; h=date : from : to : cc
 : subject : message-id : references : mime-version : content-type :
 in-reply-to; s=corp-2018-07-02;
 bh=wUIMTlblzBZzIPHfhI918uDpN2bV2DB1+2zGoceuwo4=;
 b=yf+/DtD3wY92+nFJVBigBDV7X8sUTBWp+ovuGI4W+ucalQ5VmaY9Ie7PCMLj7JakYh93
 DMMGcUevgAU0aVrZlP7ieH4GADDvcs9jNFQdPh2LEgCTKfddoFtfKbuw7qfd0lN2OwAv
 N4+VN4Gt0Gr8hFYFHVJcMZwFU3EKCV6Cg3EIB4cXj1cnIPYICFgqBuAl7UDm3obwUSQY
 RZDDr/kmQ9eONiXXcTIVUMmBjMKwJJKGQ03WqgbeaDbeakR9to5SmXF20gu7VAjFosw3
 sh/klM33nq0U1fuWoV1qWHAN+SODyW18aHVz/u+9IK//Z7y7AwxtIdSnqSzfDni1KgI1 SA== 
Received: from aserv0021.oracle.com (aserv0021.oracle.com [141.146.126.233])
        by userp2130.oracle.com with ESMTP id 2nxx2u8f04-1
        (version=TLSv1.2 cipher=ECDHE-RSA-AES256-GCM-SHA384 bits=256 verify=OK);
        Wed, 28 Nov 2018 07:15:13 +0000
Received: from aserv0122.oracle.com (aserv0122.oracle.com [141.146.126.236])
        by aserv0021.oracle.com (8.14.4/8.14.4) with ESMTP id wAS7FCrv024012
        (version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256 verify=OK);
        Wed, 28 Nov 2018 07:15:12 GMT
Received: from abhmp0015.oracle.com (abhmp0015.oracle.com [141.146.116.21])
        by aserv0122.oracle.com (8.14.4/8.14.4) with ESMTP id wAS7FCEh028531;
        Wed, 28 Nov 2018 07:15:12 GMT
Received: from localhost (/67.169.218.210)
        by default (Oracle Beehive Gateway v4.0)
        with ESMTP ; Tue, 27 Nov 2018 23:15:11 -0800
Date: Tue, 27 Nov 2018 23:15:10 -0800
From: "Darrick J. Wong" <darrick.wong@oracle.com>
To: Dave Chinner <david@fromorbit.com>
Cc: Allison Henderson <allison.henderson@oracle.com>,
        linux-block@vger.kernel.org, linux-xfs@vger.kernel.org,
        linux-kernel@vger.kernel.org, linux-fsdevel@vger.kernel.org,
        martin.petersen@oracle.com, shirley.ma@oracle.com,
        bob.liu@oracle.com
Subject: Re: [RFC PATCH v1 0/7] Block/XFS: Support alternative mirror device
 retry
Message-ID: <20181128071510.GG8125@magnolia>
References: <1543376991-5764-1-git-send-email-allison.henderson@oracle.com>
 <20181128053303.GL6311@dastard>
 <20181128054923.GF8125@magnolia>
 <20181128063046.GO6311@dastard>
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: <20181128063046.GO6311@dastard>
User-Agent: Mutt/1.9.4 (2018-02-28)
X-Proofpoint-Virus-Version: vendor=nai engine=5900 definitions=9090 signatures=668685
X-Proofpoint-Spam-Details: rule=notspam policy=default score=0 suspectscore=0 malwarescore=0
 phishscore=0 bulkscore=0 spamscore=0 mlxscore=0 mlxlogscore=999
 adultscore=0 classifier=spam adjust=0 reason=mlx scancount=1
 engine=8.0.1-1810050000 definitions=main-1811280066
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On Wed, Nov 28, 2018 at 05:30:46PM +1100, Dave Chinner wrote:
> On Tue, Nov 27, 2018 at 09:49:23PM -0800, Darrick J. Wong wrote:
> > On Wed, Nov 28, 2018 at 04:33:03PM +1100, Dave Chinner wrote:
> > > On Tue, Nov 27, 2018 at 08:49:44PM -0700, Allison Henderson wrote:
> > > > Motivation:
> > > > When fs data/metadata checksum mismatch, lower block devices may have other
> > > > correct copies. e.g. If XFS successfully reads a metadata buffer off a raid1 but
> > > > decides that the metadata is garbage, today it will shut down the entire
> > > > filesystem without trying any of the other mirrors.  This is a severe
> > > > loss of service, and we propose these patches to have XFS try harder to
> > > > avoid failure.
> > > > 
> > > > This patch prototype this mirror retry idea by:
> > > > * Adding @nr_mirrors to struct request_queue which is similar as
> > > >   blk_queue_nonrot(), filesystem can grab device request queue and check max
> > > >   mirrors this block device has.
> > > >   Helper functions were also added to get/set the nr_mirrors.
> > > > 
> > > > * Expanding bi_write_hint to bi_rw_hint, now @bi_rw_hint has three meanings.
> > > >  1.Original write_hint.
> > > >  2.end_io() will update @bi_rw_hint to reflect which mirror this i/o really happened.
> > > >  3.Fs set @bi_rw_hint to force driver e.g raid1 read from a specific mirror.
> > > > 
> > > > * Modify md/raid1 to support this retry feature.
> > > > 
> > > > * Add b_rw_hint to xfs_buf
> > > >   This patch adds a new field b_rw_hint to xfs_buf.  We will use this to set the
> > > >   new bio->bi_rw_hint when submitting the read request, and also to store the
> > > >   returned mirror when the read compleates
> > > 
> > > One thing that is going to make this more complex at the XFS layer
> > > is discontiguous buffers. They require multiple IOs (and therefore
> > > bios) and so we are going to need to ensure that all the bios use
> > > the same bi_rw_hint.
> > 
> > Hmm, we hadn't thought about that.  What happens if we have a
> > discontiguous buffer mapped to multiple blocks, and there's only one
> > good copy of each block on separate disks in the whole array?
> > 
> > e.g. we have 8k directory blocks on a 4k block filesystem, only disk 0
> > has a good copy of block 0 and only disk 1 has a good copy of block 1?
> 
> Then the user has a disaster on their hands because they have
> multiple failing disks. 

Or lives in the crazy modern age, where we have rapidly autodegrading
flash storage and hard disks whose heads pop off with no warning. :D

(But seriously, ugh.)

> > I think we're just stuck with failing the whole thing because we can't
> > check the halves of the 8k block independently and there's too much of a
> > combinatoric explosion potential to try to mix and match.
> 
> Yup, user needs to fix their storage before the filesystem can
> attempt recovery.
> 
> > > > We're not planning to take over all 16 bits of the read hint field; just looking for
> > > > feedback about the sanity of the overall approach.
> > > 
> > > It seems conceptually simple enough - the biggest questions I have
> > > are:
> > > 
> > > 	- how does propagation through stacked layers work?
> > 
> > Right now it doesn't, though once we work out how to make stacking work
> > through device mapper (my guess is that simple dm targets like linear
> > and crypt can set the mirror count to min(all underlying devices).
> > 
> > > 	- is it generic/abstract enough to be able to work with
> > > 	  RAID5/6 to trigger verification/recovery from the parity
> > > 	  information in the stripe?
> > 
> > In theory we could supply a raid5 implementation, wherein rw_hint == 0
> > lets the raid do as it pleases; rw_hint == 1 reads from the stripe; and
> > rw_hint == 2 forces stripe recovery for the given block.
> 
> So more magic numbers to define complex behaviours? :P

Yes!!!

I mean... you /could/ allow devices more expansive reporting of their
redundancy capabilities so that xfs could look at its read-retry-time
budget and try mirrors in decreasing order of likelihood of a good
response:

struct blkdev_redundancy_level {
	unsigned		latency;		/* ms */
	unsigned		chance_of_success;	/* 0 to 100 */
} redundancy_levels[blk_queue_get_mirrors()] = {
	{ 10,	    90 }, /* tries another mirror */
	{ 300,      85 }, /* erasure decoding */
	{ 7000,	    30 }, /* long slow disk scraping via SCT ERC */
	{ 1000000,   5 }, /* boils the oceans looking for data */
};

So at least the indices wouldn't be *completely* magic.  But now we have
the question of how do you populate this table?  And how many callers
are going to do something smarter than the dumb loop that it's worth the
extra code?

(Anyone?  Now would be a great time to pipe up.)

> > A trickier scenario that I have no idea how to solve is the question of
> > how to handle dynamic redundancy levels.  We don't have a standard bio
> > error value that means "this mirror is temporarily offline", so if you
> 
> We can get ETIMEDOUT, ENOLINK, EBUSY and EAGAIN from the block layer
> which all indicate temporary errors (see blk_errors[]). Whether the
> specific storage layers are actually using them is another matter...

<nod>

> > have a raid1 of two disks and disk 0 goes offline, the retry loop in xfs
> > will hit the EIO and abort without even asking disk 1.  It's also
> > unclear if we need to designate a second bio error value to mean "this
> > mirror is permanently gone".
> 
> If we have a mirror based retries, we should probably consider EIO
> as "try next mirror", not as a hard failure.

Yeah.

> > [Also insert handwaving about whether or not online fsck will want to
> > control retries and automatic rewrite; I suspect the answer is that it
> > doesn't care.]
> 
> Don't care - have the storage fix itself, then check what comes
> back and fix it from there.

<nod> Admittedly, the auto retry and rewrite are dependent solely on the
lack of EIO and the verifiers giving their blessing, and for the most
part online fsck doesn't go digging through buffers that don't pass the
verifiers, so it'll likely never see any of this anyway.

> > [[Also insert severe handwaving about do we expose this to userspace so
> > that xfs_repair can use it?]]
> 
> I suspect the answer there is through the AIO interfaces....

Y{ay,uck}...

--D

> Cheers,
> 
> Dave.
> -- 
> Dave Chinner
> david@fromorbit.com
