Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by i7-8700 with POP3-SSL;
  24 Dec 2018 15:44:39 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga008.jf.intel.com (orsmga008.jf.intel.com [10.7.209.65])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 0A86A58050C;
	Thu, 20 Dec 2018 18:14:30 -0800 (PST)
Received: from orsmga102-1.jf.intel.com (HELO mga09.intel.com) ([10.7.208.27])
  by orsmga008-1.jf.intel.com with ESMTP; 20 Dec 2018 18:14:29 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AjGfGIxJe0ULmNmaBcNmcpTZWNBhigK39O0sv0rFi?=
 =?us-ascii?q?tYgULv75rarrMEGX3/hxlliBBdydt6oUzbKO+4nbGkU4qa6bt34DdJEeHzQksu?=
 =?us-ascii?q?4x2zIaPcieFEfgJ+TrZSFpVO5LVVti4m3peRMNQJW2aFLduGC94iAPERvjKwV1?=
 =?us-ascii?q?Ov71GonPhMiryuy+4ZLebxlLiTanfb9+MAi9oBnMuMURnYZsMLs6xAHTontPde?=
 =?us-ascii?q?RWxGdoKkyWkh3h+Mq+/4Nt/jpJtf45+MFOTav1f6IjTbxFFzsmKHw65NfqtRbY?=
 =?us-ascii?q?UwSC4GYXX3gMnRpJBwjF6wz6Xov0vyDnuOdxxDWWMMvrRr0vRz+s87lkRwPpiC?=
 =?us-ascii?q?cfNj427mfXitBrjKlGpB6tvgFzz5LIbI2QMvd1Y6HTcs4ARWdZUMhfVzJPDJ6/?=
 =?us-ascii?q?YYQNAeoBOulXoJXyqVsVtRuzBxKhBP/txzJSmnP6wbE23/onHArb3AIgBdUOsH?=
 =?us-ascii?q?HModjpMKcdT++0x7TIwjXCa/NW3Tb955LNchA6pvGMW697fM3LxkkrCQzEgU+c?=
 =?us-ascii?q?qILkPzOR0uQNsnKU7+97VeKojW4qsBtxrSayyccskIbJmpgZxUzD9SV82Ys4I8?=
 =?us-ascii?q?CzRk1jYdO8DpdcqyWXO5FrTs4sXW1kojs2x74atZKhfSUHyowrywDDZ/GDaYSE?=
 =?us-ascii?q?/xzuWPiLLTtlh39odqiziwix/EWm1+byTNO70ExQoSpAitTMtm4C1xjU6sWfVP?=
 =?us-ascii?q?t98Vmu2SyV2wDQ9O5EO0Y0mrTfK5I7xb4wjJUTvVzCHi/whkr2kLebels49uWs?=
 =?us-ascii?q?8ejrf7vrqoGGO4NpiQzyLr4il829DOggNwgBRWmb+eCy1L35+k35Ra1Hjvk3kq?=
 =?us-ascii?q?nfrZDbKt0Xpq2nDA9P1IYs9RK/Ay6h0NUWm3kIMkhFdQmZj4jmJV7OOur0DfSh?=
 =?us-ascii?q?jFS2ljdk2fTGMqfmApXXIXjPiK3hcqpl605A1AozyshS55ZVCrECPv3/QEDwtM?=
 =?us-ascii?q?HDAx89Mgy0xfvnCdpn2oMfX2KPHrGWMKfIvVCU4eIvJvGGZJUJtzblN/gl+/nu?=
 =?us-ascii?q?gGc5mVAHfKmpwYEYaHeiEvRmPkWWe3zsgtgaHGcOvwo+SvHqiVKYXT5SYXayQ7?=
 =?us-ascii?q?wz5jUhBI26CofDQ5ingKad0yejAp1WemdGB0iWHnfzd4WEXPQMZDiIIsB7kDwJ?=
 =?us-ascii?q?TryhS44n1RGzuw720btnLuzI+iIGsZLvzsR65+rWlRsq7zx7E9yd032RT2Fzhm?=
 =?us-ascii?q?4IRCU53KZlrUx5y1eMy654g/NDGNxX5vNJVBo6NJHGw+x7DdDyRhzOftOTRFm6?=
 =?us-ascii?q?RdWmBCk7TsgtzN8Wf0Z9B9Kigwje0CWwHb8aiaaHBJwu/aLaxHj+OcB9x3HC1K?=
 =?us-ascii?q?kiilkmRtBCNWmnhq556gjSCJTFk0Sfl6a2a6sc2DTB+3uEzWqLpEtYShJ/Ub3Z?=
 =?us-ascii?q?XXADYUvbtcn26VncT7CwE7gnMhFOyciZKqRUbN3kllFGRPblONTDbGO9gWawBR?=
 =?us-ascii?q?CUxrySaIrmYXkS3CLYCEIciQAc4W6GNRQiBiemu2/eED1uFVfoY0Pw8el/qGm3?=
 =?us-ascii?q?TlMuwwGNdU1h07u1+hgahfGHT/MT37QEuDouqjluHVa92c7WBMSEpwZ7YKpcZt?=
 =?us-ascii?q?Y96k9d1W3Frwx9IoCgL6d6i1EEdwR4oUzv2At3C4laiscqsW4lwxB0KaKb11NB?=
 =?us-ascii?q?aSiV3ZTxOr3RN2nz8wqja6/Q2lHCztmW/r0D5+g/q1XmpAupDFYt821709lJ1H?=
 =?us-ascii?q?uR/pXLDBcIXZ3tSEo29hh6qKvcYiky/I7U0XxsMa+psj7Nwd4pBe0lygq+cNdb?=
 =?us-ascii?q?Kq+LCAjyE8gCDci0NOMqg0Spbg4DPO1K7q40Pt+peOGc2KG2O+ZshjSmgnpd4I?=
 =?us-ascii?q?B8y06D6zBzSurV0Jkb2fGY2Q2HWiz4jFegtMD3hI9FaSsTHmq51SjrGopRardu?=
 =?us-ascii?q?coYMDGekO9e3yclmh57xR35Y80auBlMH2MOzeRuedUf93RBW1UkMpXynmC24zy?=
 =?us-ascii?q?F7kj0zr6qf2jDOzPrmdBYdJmFLQ2xih0/2IYeol9AaQFSobw8xmRun/0n63atb?=
 =?us-ascii?q?pKd4L2XJWkdHZSv2L2JjUqu2qLWCZdVC6JcpsSVRTeS9bkqWSr/7oxsGzSzjG3?=
 =?us-ascii?q?FSyyw8dzGvop/5hQB1iHqBLHZvq3rUYcFxyg3Y5NDGR/5R3zwGSTJ8iTnWAFi8?=
 =?us-ascii?q?Itao8c+Vl5fFruCxSWahWodPfinsyIOKrDG76nFyAR2jg/CzncXqEQsg3i/6zd?=
 =?us-ascii?q?VqVTjIowz6YonkzKm6NeNnfk91BF7z8cZ6G4d+kpcui5EUw3QVmpKV/X8fm2fp?=
 =?us-ascii?q?Ldpbwb7+bGYKRTMTxt7V4Qvl11d5InOH2Y35TWmdzdV7Z9ahfGwWwC094ttOCK?=
 =?us-ascii?q?eV6rxEgCR0rkC5rQLXffhygDMdxeEy534dhuEDoBAtwTmFArAOAUlYOjThlwiP?=
 =?us-ascii?q?79C7tqlWZXygcb6t1EplhtChDaqPogVdWHb/Z5cjEjV87sR5MFLQznLz7pvod8?=
 =?us-ascii?q?XXbdIWrheUiQvPj/BJKJItkfoHnTFoNnj6vX0h1u43lxhu3YyhsYiDKmVt+r+5?=
 =?us-ascii?q?AxFCOj30YcMT5i/ijaJEksmK2ICvG41rGi8XU5vwUfKoDDUSuOzlNwmUFT0ztG?=
 =?us-ascii?q?ybFaDDHQOF60dmsnHPE42tN36NP3kU1tFiRBibJExCjwEYRjQ6npglFg+0wMzt?=
 =?us-ascii?q?alt25jcU5lTgsBtD1vpoNwXjUmfYvAqpajA0SIWGLBpL9A5C4V3ZMdeZ7uJ1EC?=
 =?us-ascii?q?FV5ZmhrA2LKmyGaAVEF2AJWkqYB1/9Ormi/8XP8++dBuCmNfvBfa2OqfBCV/eP?=
 =?us-ascii?q?3Z+v0pVp/zePNsWMP3luFf472lBEXXB2BcvZgSgPSzcMmiLJbs6bogq8+yJtos?=
 =?us-ascii?q?C+9vTrRBzg5Y+VB7RONtVv/gi8gb2fOO6ImCZ5NTFY24sWyn/V07cfxkAdiiF0?=
 =?us-ascii?q?ezmpELQNrirNTKPWmq9KAB8XcSJzNM1U760i2glBI9LUitTw1rRglP46F09FVU?=
 =?us-ascii?q?D9msGufcEKP2C9NFbdCEmXLriJOT3LzN/xYa6nTb1QjeNUtwC/uDqBEk/jOCiD?=
 =?us-ascii?q?mCftVxy1Le5MiySbNgREuI6hahZtFXTjTNX+Zxy7KtB3izg2wbwyhnzSNG8cMS?=
 =?us-ascii?q?Jzc0VCrrCL6SNYg/N/G3FO73Z/LOmEnTqZ4PfcKpoMrfRrBSF0xKpm5yETz7ZF?=
 =?us-ascii?q?4TAMZ/VvlCLJrtNwuFKng6HbwztkeB1crj9Bn8SRoBMxF7/e88xpXXvUtC0M6W?=
 =?us-ascii?q?GRAhERoN0tXtTrtrAW1dnLkKP6LSxF9frO8MccDtSSI8WCZilyeSH1ESLZWVNW?=
 =?us-ascii?q?BQWgMnvS0gkEyKmf?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0ANAAAQTBxch0O0hNFkHAEBAQQBAQcEA?=
 =?us-ascii?q?QGBUQcBAQsBg2snjBZfixyCDRSXSoF2EhgTAYcuIjQJDQEDAQEBAQEBAgETAQE?=
 =?us-ascii?q?BCA0JCCkvgjYkAYJhAQEBAQIBAQI3PwUBCQEBCg4KCSUDDEgGEwWDHYF6CAWoN?=
 =?us-ascii?q?IoxjD+BVz+BEYMSimACiXyBRoRiN5BhBwKCJQSILYcDJIFfiBk3hy4siiePOoF?=
 =?us-ascii?q?Ggg5wgzyCJxcSjisfAQExgQUBARyNfQEB?=
X-IPAS-Result: =?us-ascii?q?A0ANAAAQTBxch0O0hNFkHAEBAQQBAQcEAQGBUQcBAQsBg2s?=
 =?us-ascii?q?njBZfixyCDRSXSoF2EhgTAYcuIjQJDQEDAQEBAQEBAgETAQEBCA0JCCkvgjYkA?=
 =?us-ascii?q?YJhAQEBAQIBAQI3PwUBCQEBCg4KCSUDDEgGEwWDHYF6CAWoNIoxjD+BVz+BEYM?=
 =?us-ascii?q?SimACiXyBRoRiN5BhBwKCJQSILYcDJIFfiBk3hy4siiePOoFGgg5wgzyCJxcSj?=
 =?us-ascii?q?isfAQExgQUBARyNfQEB?=
X-IronPort-AV: E=Sophos;i="5.56,379,1539673200"; 
   d="scan'208";a="58396716"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 20 Dec 2018 18:14:28 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S2403770AbeLUCIZ (ORCPT <rfc822;like.xu@linux.intel.com>
        + 22 others); Thu, 20 Dec 2018 21:08:25 -0500
Received: from mx1.redhat.com ([209.132.183.28]:34184 "EHLO mx1.redhat.com"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1728307AbeLUCIY (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Thu, 20 Dec 2018 21:08:24 -0500
Received: from smtp.corp.redhat.com (int-mx04.intmail.prod.int.phx2.redhat.com [10.5.11.14])
        (using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
        (No client certificate requested)
        by mx1.redhat.com (Postfix) with ESMTPS id CDEE2169747;
        Fri, 21 Dec 2018 02:08:23 +0000 (UTC)
Received: from x1.home (ovpn-116-92.phx2.redhat.com [10.3.116.92])
        by smtp.corp.redhat.com (Postfix) with ESMTP id 293DF6B64E;
        Fri, 21 Dec 2018 02:08:22 +0000 (UTC)
Date: Thu, 20 Dec 2018 19:08:21 -0700
From: Alex Williamson <alex.williamson@redhat.com>
To: Alexey Kardashevskiy <aik@ozlabs.ru>
Cc: linuxppc-dev@lists.ozlabs.org,
        David Gibson <david@gibson.dropbear.id.au>,
        kvm-ppc@vger.kernel.org, kvm@vger.kernel.org,
        Alistair Popple <alistair@popple.id.au>,
        Reza Arbab <arbab@linux.ibm.com>,
        Sam Bobroff <sbobroff@linux.ibm.com>,
        Piotr Jaroszynski <pjaroszynski@nvidia.com>,
        Leonardo Augusto =?UTF-8?B?R3Vp?= =?UTF-8?B?bWFyw6Nlcw==?=
         Garcia <lagarcia@br.ibm.com>,
        Jose Ricardo Ziviani <joserz@linux.ibm.com>,
        Daniel Henrique Barboza <danielhb413@gmail.com>,
        Paul Mackerras <paulus@ozlabs.org>,
        linux-kernel@vger.kernel.org, Christoph Hellwig <hch@infradead.org>
Subject: Re: [PATCH kernel v7 20/20] vfio_pci: Add NVIDIA GV100GL [Tesla
 V100 SXM2] subdriver
Message-ID: <20181220190821.5a408f93@x1.home>
In-Reply-To: <b296128d-be96-8683-b0c0-1eac0a7f18ca@ozlabs.ru>
References: <20181220082350.58113-1-aik@ozlabs.ru>
        <20181220082350.58113-21-aik@ozlabs.ru>
        <20181220094604.7d172cbe@x1.home>
        <8c4ee37c-8759-a618-0c38-034210858b89@ozlabs.ru>
        <20181220183750.44535f1c@x1.home>
        <b296128d-be96-8683-b0c0-1eac0a7f18ca@ozlabs.ru>
Organization: Red Hat
MIME-Version: 1.0
Content-Type: text/plain; charset=US-ASCII
Content-Transfer-Encoding: 7bit
X-Scanned-By: MIMEDefang 2.79 on 10.5.11.14
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16 (mx1.redhat.com [10.5.110.30]); Fri, 21 Dec 2018 02:08:24 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On Fri, 21 Dec 2018 12:50:00 +1100
Alexey Kardashevskiy <aik@ozlabs.ru> wrote:

> On 21/12/2018 12:37, Alex Williamson wrote:
> > On Fri, 21 Dec 2018 12:23:16 +1100
> > Alexey Kardashevskiy <aik@ozlabs.ru> wrote:
> >   
> >> On 21/12/2018 03:46, Alex Williamson wrote:  
> >>> On Thu, 20 Dec 2018 19:23:50 +1100
> >>> Alexey Kardashevskiy <aik@ozlabs.ru> wrote:
> >>>     
> >>>> POWER9 Witherspoon machines come with 4 or 6 V100 GPUs which are not
> >>>> pluggable PCIe devices but still have PCIe links which are used
> >>>> for config space and MMIO. In addition to that the GPUs have 6 NVLinks
> >>>> which are connected to other GPUs and the POWER9 CPU. POWER9 chips
> >>>> have a special unit on a die called an NPU which is an NVLink2 host bus
> >>>> adapter with p2p connections to 2 to 3 GPUs, 3 or 2 NVLinks to each.
> >>>> These systems also support ATS (address translation services) which is
> >>>> a part of the NVLink2 protocol. Such GPUs also share on-board RAM
> >>>> (16GB or 32GB) to the system via the same NVLink2 so a CPU has
> >>>> cache-coherent access to a GPU RAM.
> >>>>
> >>>> This exports GPU RAM to the userspace as a new VFIO device region. This
> >>>> preregisters the new memory as device memory as it might be used for DMA.
> >>>> This inserts pfns from the fault handler as the GPU memory is not onlined
> >>>> until the vendor driver is loaded and trained the NVLinks so doing this
> >>>> earlier causes low level errors which we fence in the firmware so
> >>>> it does not hurt the host system but still better be avoided; for the same
> >>>> reason this does not map GPU RAM into the host kernel (usual thing for
> >>>> emulated access otherwise).
> >>>>
> >>>> This exports an ATSD (Address Translation Shootdown) register of NPU which
> >>>> allows TLB invalidations inside GPU for an operating system. The register
> >>>> conveniently occupies a single 64k page. It is also presented to
> >>>> the userspace as a new VFIO device region. One NPU has 8 ATSD registers,
> >>>> each of them can be used for TLB invalidation in a GPU linked to this NPU.
> >>>> This allocates one ATSD register per an NVLink bridge allowing passing
> >>>> up to 6 registers. Due to the host firmware bug (just recently fixed),
> >>>> only 1 ATSD register per NPU was actually advertised to the host system
> >>>> so this passes that alone register via the first NVLink bridge device in
> >>>> the group which is still enough as QEMU collects them all back and
> >>>> presents to the guest via vPHB to mimic the emulated NPU PHB on the host.
> >>>>
> >>>> In order to provide the userspace with the information about GPU-to-NVLink
> >>>> connections, this exports an additional capability called "tgt"
> >>>> (which is an abbreviated host system bus address). The "tgt" property
> >>>> tells the GPU its own system address and allows the guest driver to
> >>>> conglomerate the routing information so each GPU knows how to get directly
> >>>> to the other GPUs.
> >>>>
> >>>> For ATS to work, the nest MMU (an NVIDIA block in a P9 CPU) needs to
> >>>> know LPID (a logical partition ID or a KVM guest hardware ID in other
> >>>> words) and PID (a memory context ID of a userspace process, not to be
> >>>> confused with a linux pid). This assigns a GPU to LPID in the NPU and
> >>>> this is why this adds a listener for KVM on an IOMMU group. A PID comes
> >>>> via NVLink from a GPU and NPU uses a PID wildcard to pass it through.
> >>>>
> >>>> This requires coherent memory and ATSD to be available on the host as
> >>>> the GPU vendor only supports configurations with both features enabled
> >>>> and other configurations are known not to work. Because of this and
> >>>> because of the ways the features are advertised to the host system
> >>>> (which is a device tree with very platform specific properties),
> >>>> this requires enabled POWERNV platform.
> >>>>
> >>>> The V100 GPUs do not advertise any of these capabilities via the config
> >>>> space and there are more than just one device ID so this relies on
> >>>> the platform to tell whether these GPUs have special abilities such as
> >>>> NVLinks.
> >>>>
> >>>> Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
> >>>> ---
> >>>> Changes:
> >>>> v6.1:
> >>>> * fixed outdated comment about VFIO_REGION_INFO_CAP_NVLINK2_LNKSPD
> >>>>
> >>>> v6:
> >>>> * reworked capabilities - tgt for nvlink and gpu and link-speed
> >>>> for nvlink only
> >>>>
> >>>> v5:
> >>>> * do not memremap GPU RAM for emulation, map it only when it is needed
> >>>> * allocate 1 ATSD register per NVLink bridge, if none left, then expose
> >>>> the region with a zero size
> >>>> * separate caps per device type
> >>>> * addressed AW review comments
> >>>>
> >>>> v4:
> >>>> * added nvlink-speed to the NPU bridge capability as this turned out to
> >>>> be not a constant value
> >>>> * instead of looking at the exact device ID (which also changes from system
> >>>> to system), now this (indirectly) looks at the device tree to know
> >>>> if GPU and NPU support NVLink
> >>>>
> >>>> v3:
> >>>> * reworded the commit log about tgt
> >>>> * added tracepoints (do we want them enabled for entire vfio-pci?)
> >>>> * added code comments
> >>>> * added write|mmap flags to the new regions
> >>>> * auto enabled VFIO_PCI_NVLINK2 config option
> >>>> * added 'tgt' capability to a GPU so QEMU can recreate ibm,npu and ibm,gpu
> >>>> references; there are required by the NVIDIA driver
> >>>> * keep notifier registered only for short time
> >>>> ---
> >>>>  drivers/vfio/pci/Makefile           |   1 +
> >>>>  drivers/vfio/pci/trace.h            | 102 ++++++
> >>>>  drivers/vfio/pci/vfio_pci_private.h |  14 +
> >>>>  include/uapi/linux/vfio.h           |  37 +++
> >>>>  drivers/vfio/pci/vfio_pci.c         |  27 +-
> >>>>  drivers/vfio/pci/vfio_pci_nvlink2.c | 482 ++++++++++++++++++++++++++++
> >>>>  drivers/vfio/pci/Kconfig            |   6 +
> >>>>  7 files changed, 667 insertions(+), 2 deletions(-)
> >>>>  create mode 100644 drivers/vfio/pci/trace.h
> >>>>  create mode 100644 drivers/vfio/pci/vfio_pci_nvlink2.c
> >>>>    
> >>> ...    
> >>>> diff --git a/include/uapi/linux/vfio.h b/include/uapi/linux/vfio.h
> >>>> index 8131028..5562587 100644
> >>>> --- a/include/uapi/linux/vfio.h
> >>>> +++ b/include/uapi/linux/vfio.h
> >>>> @@ -353,6 +353,21 @@ struct vfio_region_gfx_edid {
> >>>>  #define VFIO_DEVICE_GFX_LINK_STATE_DOWN  2
> >>>>  };
> >>>>  
> >>>> +/*
> >>>> + * 10de vendor sub-type
> >>>> + *
> >>>> + * NVIDIA GPU NVlink2 RAM is coherent RAM mapped onto the host address space.
> >>>> + */
> >>>> +#define VFIO_REGION_SUBTYPE_NVIDIA_NVLINK2_RAM	(1)
> >>>> +
> >>>> +/*
> >>>> + * 1014 vendor sub-type
> >>>> + *
> >>>> + * IBM NPU NVlink2 ATSD (Address Translation Shootdown) register of NPU
> >>>> + * to do TLB invalidation on a GPU.
> >>>> + */
> >>>> +#define VFIO_REGION_SUBTYPE_IBM_NVLINK2_ATSD	(1)
> >>>> +
> >>>>  /*
> >>>>   * The MSIX mappable capability informs that MSIX data of a BAR can be mmapped
> >>>>   * which allows direct access to non-MSIX registers which happened to be within
> >>>> @@ -363,6 +378,28 @@ struct vfio_region_gfx_edid {
> >>>>   */
> >>>>  #define VFIO_REGION_INFO_CAP_MSIX_MAPPABLE	3
> >>>>  
> >>>> +/*
> >>>> + * Capability with compressed real address (aka SSA - small system address)
> >>>> + * where GPU RAM is mapped on a system bus. Used by a GPU for DMA routing.
> >>>> + */
> >>>> +#define VFIO_REGION_INFO_CAP_NVLINK2_SSATGT	4
> >>>> +
> >>>> +struct vfio_region_info_cap_nvlink2_ssatgt {
> >>>> +	struct vfio_info_cap_header header;
> >>>> +	__u64 tgt;
> >>>> +};
> >>>> +
> >>>> +/*
> >>>> + * Capability with an NVLink link speed.
> >>>> + */    
> >>>
> >>> I was really hoping for something more like SSATGT above indicating the
> >>> intended users and purpose, and an update to SSATGT since it's now used
> >>> by both the GPU and NPU2.  This comment is correct, but it's basically
> >>> useless, it doesn't provide any information that isn't readily apparent
> >>> from the structure definition.  AIUI, SSATGT is used not only for the
> >>> GPU to determine where its RAM is mapped on the system bus, but also by
> >>> the NPU2 to associate itself to a GPU, right?    
> >>
> >> Correct. It could be improved by
> >>
> >> diff --git a/include/uapi/linux/vfio.h b/include/uapi/linux/vfio.h
> >> index 5562587..ff238ef9c 100644
> >> --- a/include/uapi/linux/vfio.h
> >> +++ b/include/uapi/linux/vfio.h
> >> @@ -380,7 +380,8 @@ struct vfio_region_gfx_edid {
> >>
> >>  /*
> >>   * Capability with compressed real address (aka SSA - small system address)
> >> - * where GPU RAM is mapped on a system bus. Used by a GPU for DMA routing.
> >> + * where GPU RAM is mapped on a system bus. Used by a GPU for DMA routing
> >> + * and by the userspace to associate a NVLink bridge with a GPU.
> >>   */
> >>  #define VFIO_REGION_INFO_CAP_NVLINK2_SSATGT    4
> >>
> >>
> >>  
> >>> And the link speed here
> >>> is consumed by the NPU2 in order to fill in DT information for the
> >>> guest for compatibility and possibly routing optimizations?    
> >>
> >>
> >> It is just some speed number, 8 or 9, one works and the other does not,
> >> depending on the actual system. The NVIDIA driver handles it in the
> >> binary blob. The existing comment is not much use but I am really not
> >> sure what other comment could be useful in here.  
> > 
> > So why do we need to expose it?  "Exposed on NPU2 devices for userspace
> > to export to guest VM via DT(?) or else <something bad happens/doesn't  
> > work> in the guest".  Work with me, there must be some justification  
> > for why it gets exposed, not just what it is.  Thanks,  
> 
> 
> How about this?
> 
> /*
>  * Capability with an NVLink link speed. The value is read by
>  * the NVlink2 bridge driver from the bridge's "ibm,nvlink-speed"
>  * property in the device tree. The value is fixed in the hardware
>  * and failing to provide the correct value results in the link
>  * not working with no indication from the driver why.
>  */

I'll take it.  With the above two changes,

Acked-by: Alex Williamson <alex.williamson@redhat.com>
