Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 04 Dec 2018 18:29:22 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga002.fm.intel.com (fmsmga002.fm.intel.com [10.253.24.26])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id B37CC580117;
	Mon,  3 Dec 2018 21:22:09 -0800 (PST)
Received: from orsmga106.jf.intel.com ([10.7.208.65])
  by fmsmga002-1.fm.intel.com with ESMTP; 03 Dec 2018 21:22:08 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AJLR27hOEXgDOYcOXc84l6mtUPXoX/o7sNwtQ0KIM?=
 =?us-ascii?q?zox0KPn4oMbcNUDSrc9gkEXOFd2Cra4c26yO6+jJYi8p2d65qncMcZhBBVcuqP?=
 =?us-ascii?q?49uEgeOvODElDxN/XwbiY3T4xoXV5h+GynYwAOQJ6tL1LdrWev4jEMBx7xKRR6?=
 =?us-ascii?q?JvjvGo7Vks+7y/2+94fcbglUhzexe69+IAmrpgjNq8cahpdvJLwswRXTuHtIfO?=
 =?us-ascii?q?pWxWJsJV2Nmhv3+9m98p1+/SlOovwt78FPX7n0cKQ+VrxYES8pM3sp683xtBnM?=
 =?us-ascii?q?VhWA630BWWgLiBVIAgzF7BbnXpfttybxq+Rw1DWGMcDwULs5Xymp4aV2Rx/ykC?=
 =?us-ascii?q?oJNyA3/nzZhMJzi6xUohyhqBNwzY7PeIGYNuFzcr/ecN4AWWZMRNpdWzBHD4ih?=
 =?us-ascii?q?b4UPFe0BPeNAoofhvVQOrwWxBQ+xC+P10DBDm2f40rc60us7EgHNwQstH8gIsH?=
 =?us-ascii?q?vOrNT6LqQSXPupw6nP0DrMcelZ2Srn6IjPaBAuu+uAXbVqccre0EQiER7OgFaI?=
 =?us-ascii?q?qYH9IT+ZyuAAv3KG4+duS+6jkXMrpgJtrjS1x8ohiJHFip8Jxl3K7yl13pg5KN?=
 =?us-ascii?q?miREJmYtOoCoZcuiWcOoBrWM0tWXtotzw/yrAeuZ60YiwKyJM/yh7BZPyIbZKI?=
 =?us-ascii?q?7gjgVOmPOzd4gmxqeLalixa160igxfXwVsiy0FlUsipIisfAumwJ2hDJ98SKRO?=
 =?us-ascii?q?Vx8l281TuMywzf8OBJLEIsmareMZEhw7owlpQJsUTEGy/7gEH2jK6QdkU59emk?=
 =?us-ascii?q?8ufnbav8pp+aKYB0jhjyMqMgm8y5H+s4KBMDX3aU+euizr3v5075T6tQjv0wjK?=
 =?us-ascii?q?bZtIrWJcMBpq62GwNVyJos6w6jDze619QVhX0HLFNGeB2ZlYToNEzOLev8Dfe+?=
 =?us-ascii?q?hVSsjThqy+rHPr3nHpXCMHzDnK39crZ67k5W0BAzwsxH55JIFrEBJ+r+Wk/ru9?=
 =?us-ascii?q?zZEh82KQu0z/znCNVgzIweX22PD7SdMKPTt1+I++0uL/ONZI8TpDbyNfwl6+Ty?=
 =?us-ascii?q?gn8+nF8XZbOp0ocPaHCkAvRmJF2UYXjrgtgfC2sKvQ0+TOrsiF2FSjNTY3eyX6?=
 =?us-ascii?q?Qh5jA0Eo6mDIHDRpyzj7yFxiu0AppWZmVeAFCWDXjob5mEW+sLaC+KOMBhiTwE?=
 =?us-ascii?q?Vb+iS4M7zxGhrg36xqFjLurV/C0YqJ3i2MJ05+3ViRE96zh0A96B3GGKSmF+hn?=
 =?us-ascii?q?kISCMu3KBjvUx9zU+O0atijPxeD9BT4/JJXR08NZ7T1OF6D9HyWgTcftaGUlqm?=
 =?us-ascii?q?Q9OmAS0vQdI12dMBf0F9G9C6hBDZwyWqG6MVl6CMBJEs763TxWbxKNhnx3bGzq?=
 =?us-ascii?q?YhiUImTdVJNWGhgq5/9A3TB4rSnkWdlqaqc7kc3SHX+GeCy2qOoF9XUApqXarZ?=
 =?us-ascii?q?WnAfY1Pcrc7l6UPaU7+uFbMnPxNcxsGYNKtFdMfljVVcS/flI9TRfWSxlny0BR?=
 =?us-ascii?q?aJwLOMcYXrd38c3CXbFEgLjQQT8WyaOgg5Ayeru3jeAyB2FVLzf0Ps9vFzqG+6?=
 =?us-ascii?q?Tk8xyAGKc01h1rqv9h4Jn/CcTOkT3rYFuCcnpDV5B1K939PQC9qdqAttZqRcYd?=
 =?us-ascii?q?Uh4FhZ0WLVrRByPpulL6p6nF4Rbxx3v1/y1xVwEohPi8wqo20lzQVsKaOUykhO?=
 =?us-ascii?q?dy6F0p/qPL3XKW7y/A60Zq7S21He1suW+6gV5PQ5rVXjoB+mFk44/3p71NlV1m?=
 =?us-ascii?q?OW5o/WAwoKTZLxTkE3+gB6prHHeCUy+5nY1HxsMaautD/C1MkkBO8kyhamYtde?=
 =?us-ascii?q?P7mIFA70E80GGceuLPYmlESubhIBJOpS7rI7P9u6d/ua366mJPxgnDO6gmtd+o?=
 =?us-ascii?q?Byz0WM+zB6SuHWxZYFwuiU0RedWDf4kViurNr3mYdCZTwJGmq/yC7kBJNeZ6Fo?=
 =?us-ascii?q?fIYLD3uuLNOzxtlkm5HtXHtY/ka5B1wawM+pZQaSb1vl0AxQ1EQbu3ymlTGjzz?=
 =?us-ascii?q?xplTEkta6f3C3Iw+T/exsLIG9LRG9+jVjyJYi4lcwVXE+tbwIxjhuq+V76x7RH?=
 =?us-ascii?q?pKR4N2TTQ11HfzL1L2FhVau8rKGCbNRM6JMrsCVXVvqzYVaBR775ohsa1T7jHm?=
 =?us-ascii?q?REyDA6cTGqpov2nxhghG2BK3ZzqWLTedtsyhfH+NzcWflR0yIGRSZijjnbHFq8?=
 =?us-ascii?q?P9iv/diPk5fDs+a+V3+uV5FJcCnry5+AuzW/5WFwHRK/mPWzkMX9EQcmyS/7y8?=
 =?us-ascii?q?VqVSLQoRb/eIbr1r62MeJ6fkZyAl/85NF3GoV/kos2mZER1mIWhpST/Xobj2jz?=
 =?us-ascii?q?Nc9X1r75bHoIXTQL2cLa4BD52E1/KXKE34f5VnSewsR7fdW7bH0Z2jkh789UEq?=
 =?us-ascii?q?eb96JLnTF6olejqQLRYP59nioSyPc06X4ahf0JtxQpzimHHr8SGkxYNzT2lxuU?=
 =?us-ascii?q?99C+sLlXZGG3fLis00p+mMqtDbCYrQFaRXb5YYwiHS5r48V7MVLM1mDz647+dN?=
 =?us-ascii?q?nRa9ITqgObkxPag+dJL5Ixk+IAhTB7NmLloX0l1+k7gAR10p6gu4iHL3ht8Li9?=
 =?us-ascii?q?Ah5FLTD1Y8IT+jfwjadRhMqW3oavHol/FTUPRpfnUfWoEDcKv/T9KwmOCCE8qm?=
 =?us-ascii?q?ucGbfHHw+Q8kJmoGzUE5yxK3GbPnoZzchhRBmAIkxQmhsUUS4+np44EACq2cPg?=
 =?us-ascii?q?fF145jAX+l73tB9Mxvh0OBn4V2fVvB2oZSssSJiDMBpW6RlP5kfSMcyD9+JzAj?=
 =?us-ascii?q?tX/pu7oAyWLWybYQtIAHoNWkyFAVDjI7av6cPB8+ieGuqxMf/Oba+SpuxZUveC?=
 =?us-ascii?q?3Yiv3Zd+/zaQKsWPOWFvD/0l1UpCXnB5GMXZly8MSywNkCLNYNCUpA2h+i1stc?=
 =?us-ascii?q?2/9PXrWATy5YqAEbdSMNNv+wyojqeHLeKfmCF5KTNA3JMW2XDI0KQf3EIViyx2?=
 =?us-ascii?q?bTatDK8PtTTTQ6PQgKNXCx8bZjh3NMtJ6aI8wwZMNdTaitPzyr53kPo1B01ZWl?=
 =?us-ascii?q?zmn8GjfdYKLH2lNFPbGEaLM6yLJSHKw8Hye6+zVadcg/lUtxKuvzaWCEvjPjWF?=
 =?us-ascii?q?lzn0WBGjK+BMjCeHPBNAvIGxaApiCW/mTNj+cB20LMd3jSEqwb0znn7LNXQTMT?=
 =?us-ascii?q?55c0NMqL2f9SJZgvVlFGxH4XpoNu2Emyef7+nFJZcaq/prAiJol+1E5HQ20ada?=
 =?us-ascii?q?7CZBRK89pCyHtNlko3mln/OJxz4hVwBB7n5VhZiGpkUkOrjc/4NoXXfC9QgKq2?=
 =?us-ascii?q?KKBFBCvNFiG9HHuK1KzNXL0qXpJ3MK+tTY7I0THc/PM+qDMXwoKx2vHyTbSEMY?=
 =?us-ascii?q?STDtNWjZnFdaltmW93uctJ98rYLj3NIFTbtfVVouGtsRDUhoFcEYJ41+GDQ+nv?=
 =?us-ascii?q?rTic0BzXG+th7URdhf+JfdWaG8G/LqfQyQj7RCawEBiZXxPI0Qftnh3UVzY1pS?=
 =?us-ascii?q?l4PGFkPNR9dXq2tmdABi8xYFy2R3UmBmgxGtUQiq+nJGTfM=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AQAADsDQZch0O0hNFjHAEBAQQBAQcEA?=
 =?us-ascii?q?QGBUQcBAQsBgVomgWsnjBJfizCCIZc1FIFYGxgTAYgJIjQJDQEDAQEBAQEBAgE?=
 =?us-ascii?q?TAQEBCgsJCCkjDII2IoJlAwMBAiQLAQ0BATcBBQkBAVADKSsHCggFgxyCAQEFo?=
 =?us-ascii?q?3iBbDOCdgEBBYcvCIdtgxOBHBeBQD+BEYdYHxGFZokvhgN8j1RGCZE0I4lbh0u?=
 =?us-ascii?q?JBI9ugUaCDTMaCCgIgyeCGwkDF38BAodchUc4MoEFAQGLAwEB?=
X-IPAS-Result: =?us-ascii?q?A0AQAADsDQZch0O0hNFjHAEBAQQBAQcEAQGBUQcBAQsBgVo?=
 =?us-ascii?q?mgWsnjBJfizCCIZc1FIFYGxgTAYgJIjQJDQEDAQEBAQEBAgETAQEBCgsJCCkjD?=
 =?us-ascii?q?II2IoJlAwMBAiQLAQ0BATcBBQkBAVADKSsHCggFgxyCAQEFo3iBbDOCdgEBBYc?=
 =?us-ascii?q?vCIdtgxOBHBeBQD+BEYdYHxGFZokvhgN8j1RGCZE0I4lbh0uJBI9ugUaCDTMaC?=
 =?us-ascii?q?CgIgyeCGwkDF38BAodchUc4MoEFAQGLAwEB?=
X-IronPort-AV: E=Sophos;i="5.56,312,1539673200"; 
   d="scan'208";a="42611943"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 03 Dec 2018 21:22:06 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726214AbeLDFWD (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Tue, 4 Dec 2018 00:22:03 -0500
Received: from smtp.codeaurora.org ([198.145.29.96]:48646 "EHLO
        smtp.codeaurora.org" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726184AbeLDFV7 (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Tue, 4 Dec 2018 00:21:59 -0500
Received: by smtp.codeaurora.org (Postfix, from userid 1000)
        id ADF8F602FE; Tue,  4 Dec 2018 05:21:57 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=codeaurora.org;
        s=default; t=1543900917;
        bh=OD+JsHsHQYfsKE/GcNjHX/Y/GoA+EHe0gIDxt6Ppbco=;
        h=From:To:Cc:Subject:Date:In-Reply-To:References:From;
        b=Ba/6um0CHZnjEVkBloiq9o2Rg0mTEo/vKn0nS34DWigNSosM8dU16l85seS/so64O
         s/X40kvyVus8SD3Qhpy9m09LVLh++C16/SKdTaROe7oEnmGsS6KwvnF0GwGD+lS5Xn
         tYUsh/XmNoOrWCbK5AlwkM1gjHqvXH+vmFQ7sKm8=
X-Spam-Checker-Version: SpamAssassin 3.4.0 (2014-02-07) on
        pdx-caf-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-2.7 required=2.0 tests=ALL_TRUSTED,BAYES_00,
        DKIM_INVALID,DKIM_SIGNED autolearn=no autolearn_force=no version=3.4.0
Received: from blr-ubuntu-173.qualcomm.com (blr-bdr-fw-01_globalnat_allzones-outside.qualcomm.com [103.229.18.19])
        (using TLSv1.2 with cipher ECDHE-RSA-AES128-SHA256 (128/128 bits))
        (No client certificate requested)
        (Authenticated sender: rnayak@smtp.codeaurora.org)
        by smtp.codeaurora.org (Postfix) with ESMTPSA id D5B4660BFA;
        Tue,  4 Dec 2018 05:21:48 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=codeaurora.org;
        s=default; t=1543900912;
        bh=OD+JsHsHQYfsKE/GcNjHX/Y/GoA+EHe0gIDxt6Ppbco=;
        h=From:To:Cc:Subject:Date:In-Reply-To:References:From;
        b=fDXnpnetxB1MAZTNWtbeDNit7HR7M+vPG9R3lDJk1zu2rIko7abpQ65o/2NJCeXHg
         UfcKfzMvSA+tjTHHPbs3dEH3o2O6C3ZFPSIptyL2kMd/4A6VV5Oq+174YyRFlvojmj
         ZdTgqI+5mW8mssKfTZuCE6yGX5vg+wujX46VISnY=
DMARC-Filter: OpenDMARC Filter v1.3.2 smtp.codeaurora.org D5B4660BFA
Authentication-Results: pdx-caf-mail.web.codeaurora.org; dmarc=none (p=none dis=none) header.from=codeaurora.org
Authentication-Results: pdx-caf-mail.web.codeaurora.org; spf=none smtp.mailfrom=rnayak@codeaurora.org
From: Rajendra Nayak <rnayak@codeaurora.org>
To: robh@kernel.org, viresh.kumar@linaro.org, sboyd@kernel.org,
        andy.gross@linaro.org, ulf.hansson@linaro.org,
        collinsd@codeaurora.org, mka@chromium.org
Cc: devicetree@vger.kernel.org, linux-arm-msm@vger.kernel.org,
        linux-kernel@vger.kernel.org,
        Rajendra Nayak <rnayak@codeaurora.org>
Subject: [PATCH v5 6/8] soc: qcom: rpmhpd: Add RPMh Power domain driver
Date: Tue,  4 Dec 2018 10:51:17 +0530
Message-Id: <20181204052119.806-7-rnayak@codeaurora.org>
X-Mailer: git-send-email 2.19.1
In-Reply-To: <20181204052119.806-1-rnayak@codeaurora.org>
References: <20181204052119.806-1-rnayak@codeaurora.org>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

The RPMh Power domain driver aggregates the corner votes from various
consumers for the ARC resources and communicates it to RPMh.

With RPMh we use 2 different numbering space for corners, one used
by the clients to express their performance needs, and another used
to communicate to RPMh hardware.

The clients express their performance requirements using a sparse
numbering space which are mapped to meaningful levels like RET, SVS,
NOMINAL, TURBO etc which then get mapped to another number space
between 0 and 15 which is communicated to RPMh. The sparse number space,
also referred to as vlvl is mapped to the continuous number space of 0
to 15, also referred to as hlvl, using command DB.

Some power domain clients could request a performance state only while
the CPU is active, while some others could request for a certain
performance state all the time regardless of the state of the CPU.
We handle this by internally aggregating the votes from both type of
clients and then send the aggregated votes to RPMh.

There are also 3 different types of Votes that are comunicated to RPMh
for every resource.
1. ACTIVE_ONLY: This specifies the requirement for the resource when the
CPU is active
2. SLEEP: This specifies the requirement for the resource when the CPU
is going to sleep
3. WAKE_ONLY: This specifies the requirement for the resource when the
CPU is coming out of sleep to active state

We add data for all power domains on sdm845 SoC as part of the patch.
The driver can be extended to support other SoCs which support RPMh

Signed-off-by: Rajendra Nayak <rnayak@codeaurora.org>
Reviewed-by: Ulf Hansson <ulf.hansson@linaro.org>
---
 drivers/soc/qcom/Kconfig  |   9 +
 drivers/soc/qcom/Makefile |   1 +
 drivers/soc/qcom/rpmhpd.c | 431 ++++++++++++++++++++++++++++++++++++++
 3 files changed, 441 insertions(+)
 create mode 100644 drivers/soc/qcom/rpmhpd.c

diff --git a/drivers/soc/qcom/Kconfig b/drivers/soc/qcom/Kconfig
index e9b60695f6e7..a51458022d21 100644
--- a/drivers/soc/qcom/Kconfig
+++ b/drivers/soc/qcom/Kconfig
@@ -103,6 +103,15 @@ config QCOM_RPMH
 	  of hardware components aggregate requests for these resources and
 	  help apply the aggregated state on the resource.
 
+config QCOM_RPMHPD
+	bool "Qualcomm RPMh Power domain driver"
+	depends on QCOM_RPMH && QCOM_COMMAND_DB
+	help
+	  QCOM RPMh Power domain driver to support power-domains with
+	  performance states. The driver communicates a performance state
+	  value to RPMh which then translates it into corresponding voltage
+	  for the voltage rail.
+
 config QCOM_RPMPD
 	bool "Qualcomm RPM Power domain driver"
 	depends on MFD_QCOM_RPM && QCOM_SMD_RPM
diff --git a/drivers/soc/qcom/Makefile b/drivers/soc/qcom/Makefile
index f1b25fdcf2ad..dd6ca92985ee 100644
--- a/drivers/soc/qcom/Makefile
+++ b/drivers/soc/qcom/Makefile
@@ -22,3 +22,4 @@ obj-$(CONFIG_QCOM_APR) += apr.o
 obj-$(CONFIG_QCOM_LLCC) += llcc-slice.o
 obj-$(CONFIG_QCOM_SDM845_LLCC) += llcc-sdm845.o
 obj-$(CONFIG_QCOM_RPMPD) += rpmpd.o
+obj-$(CONFIG_QCOM_RPMHPD) += rpmhpd.o
diff --git a/drivers/soc/qcom/rpmhpd.c b/drivers/soc/qcom/rpmhpd.c
new file mode 100644
index 000000000000..10b45b4f4588
--- /dev/null
+++ b/drivers/soc/qcom/rpmhpd.c
@@ -0,0 +1,431 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Copyright (c) 2018, The Linux Foundation. All rights reserved.*/
+
+#include <linux/err.h>
+#include <linux/export.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+#include <linux/pm_domain.h>
+#include <linux/slab.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/platform_device.h>
+#include <linux/pm_opp.h>
+#include <soc/qcom/cmd-db.h>
+#include <soc/qcom/rpmh.h>
+#include <dt-bindings/power/qcom-rpmpd.h>
+
+#define domain_to_rpmhpd(domain) container_of(domain, struct rpmhpd, pd)
+
+/*
+ * This is the number of bytes used for each command DB aux data entry of an
+ * ARC resource.
+ */
+#define RPMH_ARC_LEVEL_SIZE	2
+#define RPMH_ARC_MAX_LEVELS	16
+
+/**
+ * struct rpmhpd - top level RPMh power domain resource data structure
+ * @dev:		rpmh power domain controller device
+ * @pd: 		generic_pm_domain corrresponding to the power domain
+ * @peer:		A peer power domain in case Active only Voting is supported
+ * @active_only:	True if it represents an Active only peer
+ * @level:		An array of level (vlvl) to corner (hlvl) mappings derived from cmd-db
+ * @level_count:	Number of levels supported by the power domain. max being 16 (0 - 15)
+ * @enabled:		true if the power domain is enabled
+ * @res_name:		Resource name used for cmd-db lookup
+ * @addr:		Resource address as looped up using resource name from cmd-db
+ */
+struct rpmhpd {
+	struct device	*dev;
+	struct generic_pm_domain pd;
+	struct generic_pm_domain *parent;
+	struct rpmhpd	*peer;
+	const bool	active_only;
+	unsigned int	corner;
+	unsigned int	active_corner;
+	u32		level[RPMH_ARC_MAX_LEVELS];
+	int		level_count;
+	bool		enabled;
+	const char	*res_name;
+	u32		addr;
+};
+
+struct rpmhpd_desc {
+	struct rpmhpd **rpmhpds;
+	size_t num_pds;
+};
+
+static DEFINE_MUTEX(rpmhpd_lock);
+
+/* SDM845 RPMH powerdomains */
+
+static struct rpmhpd sdm845_ebi = {
+	.pd = { .name = "ebi", },
+	.res_name = "ebi.lvl",
+};
+
+static struct rpmhpd sdm845_lmx = {
+	.pd = { .name = "lmx", },
+	.res_name = "lmx.lvl",
+};
+
+static struct rpmhpd sdm845_lcx = {
+	.pd = { .name = "lcx", },
+	.res_name = "lcx.lvl",
+};
+
+static struct rpmhpd sdm845_gfx = {
+	.pd = { .name = "gfx", },
+	.res_name = "gfx.lvl",
+};
+
+static struct rpmhpd sdm845_mss = {
+	.pd = { .name = "mss", },
+	.res_name = "mss.lvl",
+};
+
+static struct rpmhpd sdm845_mx_ao;
+static struct rpmhpd sdm845_mx = {
+	.pd = { .name = "mx", },
+	.peer = &sdm845_mx_ao,
+	.res_name = "mx.lvl",
+};
+
+static struct rpmhpd sdm845_mx_ao = {
+	.pd = { .name = "mx_ao", },
+	.peer = &sdm845_mx,
+	.res_name = "mx.lvl",
+};
+
+static struct rpmhpd sdm845_cx_ao;
+static struct rpmhpd sdm845_cx = {
+	.pd = { .name = "cx", },
+	.peer = &sdm845_cx_ao,
+	.res_name = "cx.lvl",
+};
+
+static struct rpmhpd sdm845_cx_ao = {
+	.pd = { .name = "cx_ao", },
+	.peer = &sdm845_cx,
+	.res_name = "cx.lvl",
+};
+
+static struct rpmhpd *sdm845_rpmhpds[] = {
+	[SDM845_EBI] = &sdm845_ebi,
+	[SDM845_MX] = &sdm845_mx,
+	[SDM845_MX_AO] = &sdm845_mx_ao,
+	[SDM845_CX] = &sdm845_cx,
+	[SDM845_CX_AO] = &sdm845_cx_ao,
+	[SDM845_LMX] = &sdm845_lmx,
+	[SDM845_LCX] = &sdm845_lcx,
+	[SDM845_GFX] = &sdm845_gfx,
+	[SDM845_MSS] = &sdm845_mss,
+};
+
+static const struct rpmhpd_desc sdm845_desc = {
+	.rpmhpds = sdm845_rpmhpds,
+	.num_pds = ARRAY_SIZE(sdm845_rpmhpds),
+};
+
+static const struct of_device_id rpmhpd_match_table[] = {
+	{ .compatible = "qcom,sdm845-rpmhpd", .data = &sdm845_desc },
+	{ }
+};
+MODULE_DEVICE_TABLE(of, rpmhpd_match_table);
+
+static int rpmhpd_send_corner(struct rpmhpd *pd, int state,
+			      unsigned int corner, bool sync)
+{
+	struct tcs_cmd cmd = {
+		.addr = pd->addr,
+		.data = corner,
+	};
+
+	/*
+	 * Wait for an ack only when we are increasing the
+	 * perf state of the power domain
+	 */
+	if (sync)
+		return rpmh_write(pd->dev, state, &cmd, 1);
+	else
+		return rpmh_write_async(pd->dev, state, &cmd, 1);
+}
+
+static void to_active_sleep(struct rpmhpd *pd, unsigned int corner,
+			    unsigned int *active, unsigned int *sleep)
+{
+	*active = corner;
+
+	if (pd->active_only)
+		*sleep = 0;
+	else
+		*sleep = *active;
+}
+
+/*
+ * This function is used to aggregate the votes across the active only
+ * resources and its peers. The aggregated votes are sent to RPMh as
+ * ACTIVE_ONLY votes (which take effect immediately), as WAKE_ONLY votes
+ * (applied by RPMh on system wakeup) and as SLEEP votes (applied by RPMh
+ * on system sleep).
+ * We send ACTIVE_ONLY votes for resources without any peers. For others,
+ * which have an active only peer, all 3 votes are sent.
+ */
+static int rpmhpd_aggregate_corner(struct rpmhpd *pd, unsigned int corner)
+{
+	int ret;
+	struct rpmhpd *peer = pd->peer;
+	unsigned int active_corner, sleep_corner;
+	unsigned int this_active_corner = 0, this_sleep_corner = 0;
+	unsigned int peer_active_corner = 0, peer_sleep_corner = 0;
+
+	to_active_sleep(pd, corner, &this_active_corner, &this_sleep_corner);
+
+	if (peer && peer->enabled)
+		to_active_sleep(peer, peer->corner, &peer_active_corner,
+				&peer_sleep_corner);
+
+	active_corner = max(this_active_corner, peer_active_corner);
+
+	ret = rpmhpd_send_corner(pd, RPMH_ACTIVE_ONLY_STATE, active_corner,
+				 active_corner > pd->active_corner);
+	if (ret)
+		return ret;
+
+	pd->active_corner = active_corner;
+
+	if (peer) {
+		peer->active_corner = active_corner;
+
+		ret = rpmhpd_send_corner(pd, RPMH_WAKE_ONLY_STATE,
+					 active_corner, false);
+		if (ret)
+			return ret;
+
+		sleep_corner = max(this_sleep_corner, peer_sleep_corner);
+
+		return rpmhpd_send_corner(pd, RPMH_SLEEP_STATE, sleep_corner,
+					  false);
+	}
+
+	return ret;
+}
+
+static int rpmhpd_power_on(struct generic_pm_domain *domain)
+{
+	struct rpmhpd *pd = domain_to_rpmhpd(domain);
+	int ret = 0;
+
+	mutex_lock(&rpmhpd_lock);
+
+	if (pd->corner)
+		ret = rpmhpd_aggregate_corner(pd, pd->corner);
+
+	if (!ret)
+		pd->enabled = true;
+
+	mutex_unlock(&rpmhpd_lock);
+
+	return ret;
+}
+
+static int rpmhpd_power_off(struct generic_pm_domain *domain)
+{
+	struct rpmhpd *pd = domain_to_rpmhpd(domain);
+	int ret = 0;
+
+	mutex_lock(&rpmhpd_lock);
+
+	ret = rpmhpd_aggregate_corner(pd, pd->level[0]);
+
+	if (!ret)
+		pd->enabled = false;
+
+	mutex_unlock(&rpmhpd_lock);
+
+	return ret;
+}
+
+static int rpmhpd_set_performance_state(struct generic_pm_domain *domain,
+					unsigned int level)
+{
+	struct rpmhpd *pd = domain_to_rpmhpd(domain);
+	int ret = 0, i;
+
+	mutex_lock(&rpmhpd_lock);
+
+	for (i = 0; i < pd->level_count; i++)
+		if (level <= pd->level[i])
+			break;
+
+	/*
+	 * If the level requested is more than that supported by the
+	 * max corner, just set it to max anyway.
+	 */
+	if (i == pd->level_count)
+		i--;
+
+	if (pd->enabled) {
+		ret = rpmhpd_aggregate_corner(pd, i);
+		if (ret)
+			goto out;
+	}
+
+	pd->corner = i;
+out:
+	mutex_unlock(&rpmhpd_lock);
+
+	return ret;
+}
+
+static unsigned int rpmhpd_get_performance_state(struct generic_pm_domain *genpd,
+						 struct dev_pm_opp *opp)
+{
+	struct device_node *np;
+	unsigned int level = 0;
+
+	np = dev_pm_opp_get_of_node(opp);
+	if (of_property_read_u32(np, "qcom,level", &level)) {
+		pr_err("%s: missing 'qcom,level' property\n", __func__);
+		return 0;
+	}
+
+	of_node_put(np);
+
+	return level;
+}
+
+static int rpmhpd_update_level_mapping(struct rpmhpd *rpmhpd)
+{
+	u8 buf[RPMH_ARC_MAX_LEVELS * RPMH_ARC_LEVEL_SIZE];
+	int i, j, len, ret;
+
+	len = cmd_db_read_aux_data_len(rpmhpd->res_name);
+	if (len <= 0)
+		return len;
+	else if (len > RPMH_ARC_MAX_LEVELS * RPMH_ARC_LEVEL_SIZE)
+		return -EINVAL;
+
+	ret = cmd_db_read_aux_data(rpmhpd->res_name, buf, len);
+	if (ret < 0)
+		return ret;
+
+	rpmhpd->level_count = len / RPMH_ARC_LEVEL_SIZE;
+
+	for (i = 0; i < rpmhpd->level_count; i++) {
+		rpmhpd->level[i] = 0;
+		for (j = 0; j < RPMH_ARC_LEVEL_SIZE; j++)
+			rpmhpd->level[i] |=
+				buf[i * RPMH_ARC_LEVEL_SIZE + j] << (8 * j);
+
+		/*
+		 * The AUX data may be zero padded.  These 0 valued entries at
+		 * the end of the map must be ignored.
+		 */
+		if (i > 0 && rpmhpd->level[i] == 0) {
+			rpmhpd->level_count = i;
+			break;
+		}
+		pr_debug("%s: ARC hlvl=%2d --> vlvl=%4u\n", rpmhpd->res_name, i,
+			 rpmhpd->level[i]);
+	}
+
+	return 0;
+}
+
+static int rpmhpd_probe(struct platform_device *pdev)
+{
+	int i, ret;
+	size_t num_pds;
+	struct device *dev = &pdev->dev;
+	struct genpd_onecell_data *data;
+	struct rpmhpd **rpmhpds;
+	const struct rpmhpd_desc *desc;
+
+	desc = of_device_get_match_data(dev);
+	if (!desc)
+		return -EINVAL;
+
+	rpmhpds = desc->rpmhpds;
+	num_pds = desc->num_pds;
+
+	data = devm_kzalloc(dev, sizeof(*data), GFP_KERNEL);
+	if (!data)
+		return -ENOMEM;
+
+	data->domains = devm_kcalloc(dev, num_pds, sizeof(*data->domains),
+				     GFP_KERNEL);
+	if (!data->domains)
+		return -ENOMEM;
+
+	data->num_domains = num_pds;
+
+	ret = cmd_db_ready();
+	if (ret) {
+		if (ret != -EPROBE_DEFER)
+			dev_err(dev, "Command DB unavailable, ret=%d\n", ret);
+		return ret;
+	}
+
+	for (i = 0; i < num_pds; i++) {
+		if (!rpmhpds[i]) {
+			dev_warn(dev, "rpmhpds[%d] is empty\n", i);
+			continue;
+		}
+
+		rpmhpds[i]->dev = dev;
+		rpmhpds[i]->addr = cmd_db_read_addr(rpmhpds[i]->res_name);
+		if (!rpmhpds[i]->addr) {
+			dev_err(dev, "Could not find RPMh address for resource %s\n",
+				rpmhpds[i]->res_name);
+			return -ENODEV;
+		}
+
+		ret = cmd_db_read_slave_id(rpmhpds[i]->res_name);
+		if (ret != CMD_DB_HW_ARC) {
+			dev_err(dev, "RPMh slave ID mismatch\n");
+			return -EINVAL;
+		}
+
+		ret = rpmhpd_update_level_mapping(rpmhpds[i]);
+		if (ret)
+			return ret;
+
+		rpmhpds[i]->pd.power_off = rpmhpd_power_off;
+		rpmhpds[i]->pd.power_on = rpmhpd_power_on;
+		rpmhpds[i]->pd.set_performance_state = rpmhpd_set_performance_state;
+		rpmhpds[i]->pd.opp_to_performance_state = rpmhpd_get_performance_state;
+		pm_genpd_init(&rpmhpds[i]->pd, NULL, true);
+
+		data->domains[i] = &rpmhpds[i]->pd;
+	}
+
+	return of_genpd_add_provider_onecell(pdev->dev.of_node, data);
+}
+
+static struct platform_driver rpmhpd_driver = {
+	.driver = {
+		.name = "qcom-rpmhpd",
+		.of_match_table = rpmhpd_match_table,
+	},
+	.probe = rpmhpd_probe,
+};
+
+static int __init rpmhpd_init(void)
+{
+	return platform_driver_register(&rpmhpd_driver);
+}
+core_initcall(rpmhpd_init);
+
+static void __exit rpmhpd_exit(void)
+{
+	platform_driver_unregister(&rpmhpd_driver);
+}
+module_exit(rpmhpd_exit);
+
+MODULE_DESCRIPTION("Qualcomm Technologies, Inc. RPMh Power Domain Driver");
+MODULE_LICENSE("GPL v2");
+MODULE_ALIAS("platform:qcom-rpmhpd");
-- 
QUALCOMM INDIA, on behalf of Qualcomm Innovation Center, Inc. is a member
of Code Aurora Forum, hosted by The Linux Foundation

