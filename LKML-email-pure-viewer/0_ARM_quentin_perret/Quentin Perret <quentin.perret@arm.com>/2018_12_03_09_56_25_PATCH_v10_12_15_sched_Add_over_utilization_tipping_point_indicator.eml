Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 03 Dec 2018 20:24:29 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga002.fm.intel.com (fmsmga002.fm.intel.com [10.253.24.26])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 2CA07580224;
	Mon,  3 Dec 2018 02:00:44 -0800 (PST)
Received: from fmsmga102.fm.intel.com ([10.1.193.69])
  by fmsmga002-1.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 03 Dec 2018 02:00:43 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3A9dryaRwQSTzq3mjXCy+O+j09IxM/srCxBDY+r6Qd?=
 =?us-ascii?q?0e4eI/ad9pjvdHbS+e9qxAeQG9mDu7Qc06L/iOPJYSQ4+5GPsXQPItRndiQuro?=
 =?us-ascii?q?EopTEmG9OPEkbhLfTnPGQQFcVGU0J5rTngaRAGUMnxaEfPrXKs8DUcBgvwNRZv?=
 =?us-ascii?q?JuTyB4Xek9m72/q99pHPYAhEniaxba9vJxiqsAvdsdUbj5F/Iagr0BvJpXVIe+?=
 =?us-ascii?q?VSxWx2IF+Yggjx6MSt8pN96ipco/0u+dJOXqX8ZKQ4UKdXDC86PGAv5c3krgfM?=
 =?us-ascii?q?QA2S7XYBSGoWkx5IAw/Y7BHmW5r6ryX3uvZh1CScIMb7S60/Vza/4KdxUBLmiD?=
 =?us-ascii?q?kJOSM3/m/UjcJ/jqxbrx2uqRxk247ZYpqYOOZ9c67HYd8XX3ZNUtpXWidcAo28?=
 =?us-ascii?q?dYwPD+8ZMOhDsonyuV0OrQe/BQmqGejh0CFHhmXs3aIkz+QqDAbL3BU9H90Qtn?=
 =?us-ascii?q?TUsNT1NKEIXeCt0qbI1y/DYO1K2Trm8ofIaBUhreuQUrJ3dMrc0E8iHB7GgFWI?=
 =?us-ascii?q?sYHpIS+Z2+AXv2SG4edsS/ijh3Mkpg1tuDSix8UhhpHXio4IyF3I7zh1zYkpKd?=
 =?us-ascii?q?GiVUJ3fcOoHIFSui2GMYZ9X9ksTHtyuCkgz70LoZ67czYOyJQg3xPfdfOHfJaS?=
 =?us-ascii?q?4h75V+aePy14hHR7d7K7nRqy9lKgyuLkWsm11lZFsDZFn8HSunwR0xHf8NWLR/?=
 =?us-ascii?q?Vj8ku7xDqC1Bzf5vtFLE02jabbLoQuwr80lpodq0TDGSr2lV3yjK+XcEUk5+ep?=
 =?us-ascii?q?5/3kYrr4vJ+cMZF7igXnPqQplM2/B/o3MhIVUmiF9uSzyqfj8Vf6QLpUlP02lL?=
 =?us-ascii?q?fWsJTAKcQcvK65DBdZ0pw/5BanEzemzNMYkGEDLFJEexKIkZLlOl7TIP3jCfe/?=
 =?us-ascii?q?glKskCpkxvzcP73hBInNIWbHkLv7Ybl97EtcwhIpzd9D/5JUFq0BIPXrV0/xrt?=
 =?us-ascii?q?PYDwM5PBazw+r9CNV9y5kRWWSAAqKCNKPSsFmI5v8gIuWWZY8Vvir9JOYh5/L0?=
 =?us-ascii?q?kXA5nlodd7Gz3ZQLcHC4AuhmI0KBbHruhdcOD30Gvgk5TOzsjl2CViVeZ3KzX6?=
 =?us-ascii?q?I6+zE6B5iqDYbFRoCxnrOB2D23EYFRZmBDElqMC2vnd52YW/cQbyKfOtVukicE?=
 =?us-ascii?q?VbimSI8tzwuuuxX4y7d8KurU+ysYtY/s1dRv5u3Tkw0y+iJwD8iHz26NSGR0lH?=
 =?us-ascii?q?sSRzAqxKB/vVB9ylCb3Kh7mfNYE8Zf5/dIUgggM57cwPd3C9TzWgLHY9eIR0yq?=
 =?us-ascii?q?QtSgATEtUN0xx8UCbFp6G9WnlhrDxTalA6cJl7yXA5w56r/T0GLvJ8lj0XrG1L?=
 =?us-ascii?q?Muj189QsRRM22qgap/9wvWB47NiEiZk6eqdaIB3C/C7muDzGyOvF1GXw50S6nK?=
 =?us-ascii?q?QXcfZk7Op9Tj+kzCV6OuCaggMgZZ1MGNMLVKZcPzgVVGXvjjPs/ebHy3m2qrAR?=
 =?us-ascii?q?aIx7WMbJflemkH3SXdDlQEnB4X/XqcKQc+ASKhqXrEDDNyDVLvf1/s8e5mpXOn?=
 =?us-ascii?q?T080ygaKb1Fh17u14BIVmeaQS/QJ3rIAuSchrTp0EU2539LXDdqAugVgcL9dYd?=
 =?us-ascii?q?M7/FdIy2bZuxZhMZynKqBonkQefBhvv0PyyxV3DZ1NntIurHw0wwt9N6KZ3Ela?=
 =?us-ascii?q?eDOFwJ//ILvXKmr1/BCxcKPW3lDe0NCL+qYA8vg4qlPjvB23GUom6Xloz95V03?=
 =?us-ascii?q?6E7JXQEAUSSY7xUlow9xVip7DafzMx6J/O2XxtMam7qDnC290yCeshyxagecpf?=
 =?us-ascii?q?MayeGA/zFc0aG9ahKOgwl1e1aRIEOfhY9LQoMMO+a/uGxKmrMf56nDKnkWtI+p?=
 =?us-ascii?q?p93V+L9yZmTO7HwYwFw/CB0gSbTTj8iEquvd7tmYBDYzEfBW6/ySniBI5Maax+?=
 =?us-ascii?q?Z4cLCWGyI8KpwtVynYLiW3ld9FS7HVMJxNepeQaOb1z6xQBR1UUXrWanmCei1D?=
 =?us-ascii?q?x0jjcpo7Gb3CzPxeTiaRUGNnRKRGlkkVfjP4y0g8oGU0ivaggjjAGl6lrix6hH?=
 =?us-ascii?q?uKR/KHHeQUVScCjsLGFiUaywuqCZY8FV65MosiRXUOKiblCcUbP9pxoa0yX+H2?=
 =?us-ascii?q?pR3jw7djequonnkBx+km6SMHFzrH/Bc8Fq2Rjf/MDcReJW3jceRCh3kz7XCkK+?=
 =?us-ascii?q?P9mo+9WZjJPDsuG4V2K8WZxfayjrzYWctCSl4W1mGwGwn/e2mtf/Cwg1zTf718?=
 =?us-ascii?q?V2VSXPtBv8YZPk16WgPeJnY0lnHln868VhF4F6k4swgowQ2HcAipWU+3oHjXn8?=
 =?us-ascii?q?MdFB1a3ia3oNQCYBw8TJ7wj9xE1jMnWJypr8VnWcwctufcO2Y20I1SIm88BFFr?=
 =?us-ascii?q?2U46dakitvrVq1tgbRYfl7njcAxvoi8n8ag+cVuAUzyiWRGKwdHU5dPSb0jRSH?=
 =?us-ascii?q?88i+rLlLZGaoabWwyE1+ndW7ALCDuA5cXmv5eow5HS9x9ch/NFPM0Hvu6oDrYt?=
 =?us-ascii?q?XQbNQTtgGKnBfEleRaNJUxlv8Sjyp9JW39pWEly/I8jRF2xp61po+HK2Fw/K6j?=
 =?us-ascii?q?Bh5YKyb4Z8cS+jHrkKZfkdya34GpHpV9BDoLWIHkQu6vEDIXrf7nLRqBECUgqn?=
 =?us-ascii?q?eHHrrSBQ2f511jr33RCJCnLW2XKGMazdV/RxmdOUpfjxoPXDghmp45FwaqxNHu?=
 =?us-ascii?q?cUtj5zAR4EL4pQVIyu5yKxb/VWLfrh+yajgoUJifMAZW7gZa6kbVNsye8/tzHz?=
 =?us-ascii?q?xC8Z2ntgCNMWubZwJHDWwSXkyEBlbjPqSh5NXa8uiYAPa+IOXKYbmUteNeUPKI?=
 =?us-ascii?q?z4q10oR65zaMKtmPPn56Av07xEVDXHN5G8fYmzkVSCwXjSXNb8GFqxem5y13td?=
 =?us-ascii?q?uy8PDqWALp+IuOBKFeMdRp+xCqn6iDM/Scizp+KTZdzpkM32PHyKAD3F4OjCFj?=
 =?us-ascii?q?byOiEakbui7XUq3RmrVbDxoaay5oMMtI7qQ83hRCOMLBi9P10KJ4geAxC1tfSV?=
 =?us-ascii?q?PhncSpb9QQI26hLFPHGFqLNLOeKD3XwsH3ZLmwRqFKgOpItx2/pzCbHlTnPjSC?=
 =?us-ascii?q?kTnpSh+uPftNjCGdIBxRpoW9fgxxBmjkSdLscge7P8NvjT0q3b00gWvHNG4GPj?=
 =?us-ascii?q?h6dkNNr7uQ4jtZg/V/AWNB6HVlIPKAmyaY6enYN5kXveFqAiRyi+JV/nA6x6FJ?=
 =?us-ascii?q?4yFDQfwm0BfV+8Zjv1y8gMGOzDR9WRZDozoNg5iE+Q14O7rx7JtfSG3D9xEMq2?=
 =?us-ascii?q?OdF1BCo9pjF82quK1KzNXLvLz8JS0E8N/O+8YYQc/OJ4bPNHsnLAqsADv8EgQI?=
 =?us-ascii?q?V3ioOHvZikgbl+udpVOPqZ1vk4XhnoYPTPdjWUYvG+9SXl14EdoeIZExRj4+i7?=
 =?us-ascii?q?OHpMcS4Dy1qxyHF5YShYzOSv/HWaanEz2el7QRPxY=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0A3AACr/QRch0O0hNFiHQEBBQEHBQGBU?=
 =?us-ascii?q?wYBCwGBVQWCESeaJxSXNBSBXywTAYgCIjYHDQEDAQEBAQEBAgETAQEBCgsJCCk?=
 =?us-ascii?q?vgjYigmUDAwECCxkLATIUBgkBAR8YGQNUBxIFgxyCAgQBpWszih6HbYQvgVc/g?=
 =?us-ascii?q?RGHToEEhRwCiQyWX1UHAoIfBI8OI4Fbj0uJBIEDkDMNgXkzGiODPIInF38BB40?=
 =?us-ascii?q?WPwEBMYEFAQGKbAEB?=
X-IPAS-Result: =?us-ascii?q?A0A3AACr/QRch0O0hNFiHQEBBQEHBQGBUwYBCwGBVQWCESe?=
 =?us-ascii?q?aJxSXNBSBXywTAYgCIjYHDQEDAQEBAQEBAgETAQEBCgsJCCkvgjYigmUDAwECC?=
 =?us-ascii?q?xkLATIUBgkBAR8YGQNUBxIFgxyCAgQBpWszih6HbYQvgVc/gRGHToEEhRwCiQy?=
 =?us-ascii?q?WX1UHAoIfBI8OI4Fbj0uJBIEDkDMNgXkzGiODPIInF38BB40WPwEBMYEFAQGKb?=
 =?us-ascii?q?AEB?=
X-IronPort-AV: E=Sophos;i="5.56,310,1539673200"; 
   d="scan'208";a="55258153"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 03 Dec 2018 02:00:41 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726307AbeLCJ5u (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Mon, 3 Dec 2018 04:57:50 -0500
Received: from foss.arm.com ([217.140.101.70]:32884 "EHLO foss.arm.com"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1725888AbeLCJ5u (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 3 Dec 2018 04:57:50 -0500
Received: from usa-sjc-imap-foss1.foss.arm.com (unknown [10.72.51.249])
        by usa-sjc-mx-foss1.foss.arm.com (Postfix) with ESMTP id CEE901688;
        Mon,  3 Dec 2018 01:57:30 -0800 (PST)
Received: from queper01-lin.cambridge.arm.com (queper01-lin.cambridge.arm.com [10.1.195.48])
        by usa-sjc-imap-foss1.foss.arm.com (Postfix) with ESMTPSA id B2D8A3F59C;
        Mon,  3 Dec 2018 01:57:26 -0800 (PST)
From: Quentin Perret <quentin.perret@arm.com>
To: peterz@infradead.org, rjw@rjwysocki.net,
        linux-kernel@vger.kernel.org, linux-pm@vger.kernel.org
Cc: gregkh@linuxfoundation.org, mingo@redhat.com,
        dietmar.eggemann@arm.com, morten.rasmussen@arm.com,
        chris.redpath@arm.com, patrick.bellasi@arm.com,
        valentin.schneider@arm.com, vincent.guittot@linaro.org,
        thara.gopinath@linaro.org, viresh.kumar@linaro.org,
        tkjos@google.com, joel@joelfernandes.org, smuckle@google.com,
        adharmap@codeaurora.org, skannan@codeaurora.org,
        pkondeti@codeaurora.org, juri.lelli@redhat.com,
        edubezval@gmail.com, srinivas.pandruvada@linux.intel.com,
        currojerez@riseup.net, javi.merino@kernel.org,
        quentin.perret@arm.com
Subject: [PATCH v10 12/15] sched: Add over-utilization/tipping point indicator
Date: Mon,  3 Dec 2018 09:56:25 +0000
Message-Id: <20181203095628.11858-13-quentin.perret@arm.com>
X-Mailer: git-send-email 2.19.2
In-Reply-To: <20181203095628.11858-1-quentin.perret@arm.com>
References: <20181203095628.11858-1-quentin.perret@arm.com>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

From: Morten Rasmussen <morten.rasmussen@arm.com>

Energy-aware scheduling is only meant to be active while the system is
_not_ over-utilized. That is, there are spare cycles available to shift
tasks around based on their actual utilization to get a more
energy-efficient task distribution without depriving any tasks. When
above the tipping point task placement is done the traditional way based
on load_avg, spreading the tasks across as many cpus as possible based
on priority scaled load to preserve smp_nice. Below the tipping point we
want to use util_avg instead. We need to define a criteria for when we
make the switch.

The util_avg for each cpu converges towards 100% regardless of how many
additional tasks we may put on it. If we define over-utilized as:

sum_{cpus}(rq.cfs.avg.util_avg) + margin > sum_{cpus}(rq.capacity)

some individual cpus may be over-utilized running multiple tasks even
when the above condition is false. That should be okay as long as we try
to spread the tasks out to avoid per-cpu over-utilization as much as
possible and if all tasks have the _same_ priority. If the latter isn't
true, we have to consider priority to preserve smp_nice.

For example, we could have n_cpus nice=-10 util_avg=55% tasks and
n_cpus/2 nice=0 util_avg=60% tasks. Balancing based on util_avg we are
likely to end up with nice=-10 tasks sharing cpus and nice=0 tasks
getting their own as we 1.5*n_cpus tasks in total and 55%+55% is less
over-utilized than 55%+60% for those cpus that have to be shared. The
system utilization is only 85% of the system capacity, but we are
breaking smp_nice.

To be sure not to break smp_nice, we have defined over-utilization
conservatively as when any cpu in the system is fully utilized at its
highest frequency instead:

cpu_rq(any).cfs.avg.util_avg + margin > cpu_rq(any).capacity

IOW, as soon as one cpu is (nearly) 100% utilized, we switch to load_avg
to factor in priority to preserve smp_nice.

With this definition, we can skip periodic load-balance as no cpu has an
always-running task when the system is not over-utilized. All tasks will
be periodic and we can balance them at wake-up. This conservative
condition does however mean that some scenarios that could benefit from
energy-aware decisions even if one cpu is fully utilized would not get
those benefits.

For systems where some cpus might have reduced capacity on some cpus
(RT-pressure and/or big.LITTLE), we want periodic load-balance checks as
soon a just a single cpu is fully utilized as it might one of those with
reduced capacity and in that case we want to migrate it.

cc: Ingo Molnar <mingo@redhat.com>
cc: Peter Zijlstra <peterz@infradead.org>
Signed-off-by: Morten Rasmussen <morten.rasmussen@arm.com>
[ Added a comment explaining why new tasks are not accounted during
  overutilization detection ]
Signed-off-by: Quentin Perret <quentin.perret@arm.com>
---
 kernel/sched/fair.c  | 59 ++++++++++++++++++++++++++++++++++++++++++--
 kernel/sched/sched.h |  4 +++
 2 files changed, 61 insertions(+), 2 deletions(-)

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index e21f37129395..c3b2dad72c9c 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -5082,6 +5082,24 @@ static inline void hrtick_update(struct rq *rq)
 }
 #endif
 
+#ifdef CONFIG_SMP
+static inline unsigned long cpu_util(int cpu);
+static unsigned long capacity_of(int cpu);
+
+static inline bool cpu_overutilized(int cpu)
+{
+	return (capacity_of(cpu) * 1024) < (cpu_util(cpu) * capacity_margin);
+}
+
+static inline void update_overutilized_status(struct rq *rq)
+{
+	if (!READ_ONCE(rq->rd->overutilized) && cpu_overutilized(rq->cpu))
+		WRITE_ONCE(rq->rd->overutilized, SG_OVERUTILIZED);
+}
+#else
+static inline void update_overutilized_status(struct rq *rq) { }
+#endif
+
 /*
  * The enqueue_task method is called before nr_running is
  * increased. Here we update the fair scheduling stats and
@@ -5139,8 +5157,26 @@ enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 		update_cfs_group(se);
 	}
 
-	if (!se)
+	if (!se) {
 		add_nr_running(rq, 1);
+		/*
+		 * Since new tasks are assigned an initial util_avg equal to
+		 * half of the spare capacity of their CPU, tiny tasks have the
+		 * ability to cross the overutilized threshold, which will
+		 * result in the load balancer ruining all the task placement
+		 * done by EAS. As a way to mitigate that effect, do not account
+		 * for the first enqueue operation of new tasks during the
+		 * overutilized flag detection.
+		 *
+		 * A better way of solving this problem would be to wait for
+		 * the PELT signals of tasks to converge before taking them
+		 * into account, but that is not straightforward to implement,
+		 * and the following generally works well enough in practice.
+		 */
+		if (flags & ENQUEUE_WAKEUP)
+			update_overutilized_status(rq);
+
+	}
 
 	hrtick_update(rq);
 }
@@ -7940,6 +7976,9 @@ static inline void update_sg_lb_stats(struct lb_env *env,
 		if (nr_running > 1)
 			*sg_status |= SG_OVERLOAD;
 
+		if (cpu_overutilized(i))
+			*sg_status |= SG_OVERUTILIZED;
+
 #ifdef CONFIG_NUMA_BALANCING
 		sgs->nr_numa_running += rq->nr_numa_running;
 		sgs->nr_preferred_running += rq->nr_preferred_running;
@@ -8170,8 +8209,15 @@ static inline void update_sd_lb_stats(struct lb_env *env, struct sd_lb_stats *sd
 		env->fbq_type = fbq_classify_group(&sds->busiest_stat);
 
 	if (!env->sd->parent) {
+		struct root_domain *rd = env->dst_rq->rd;
+
 		/* update overload indicator if we are at root domain */
-		WRITE_ONCE(env->dst_rq->rd->overload, sg_status & SG_OVERLOAD);
+		WRITE_ONCE(rd->overload, sg_status & SG_OVERLOAD);
+
+		/* Update over-utilization (tipping point, U >= 0) indicator */
+		WRITE_ONCE(rd->overutilized, sg_status & SG_OVERUTILIZED);
+	} else if (sg_status & SG_OVERUTILIZED) {
+		WRITE_ONCE(env->dst_rq->rd->overutilized, SG_OVERUTILIZED);
 	}
 }
 
@@ -8398,6 +8444,14 @@ static struct sched_group *find_busiest_group(struct lb_env *env)
 	 * this level.
 	 */
 	update_sd_lb_stats(env, &sds);
+
+	if (static_branch_unlikely(&sched_energy_present)) {
+		struct root_domain *rd = env->dst_rq->rd;
+
+		if (rcu_dereference(rd->pd) && !READ_ONCE(rd->overutilized))
+			goto out_balanced;
+	}
+
 	local = &sds.local_stat;
 	busiest = &sds.busiest_stat;
 
@@ -9798,6 +9852,7 @@ static void task_tick_fair(struct rq *rq, struct task_struct *curr, int queued)
 		task_tick_numa(rq, curr);
 
 	update_misfit_status(curr, rq);
+	update_overutilized_status(task_rq(curr));
 }
 
 /*
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 4c1e4b73f40d..28d9209554dc 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -717,6 +717,7 @@ struct perf_domain {
 
 /* Scheduling group status flags */
 #define SG_OVERLOAD		0x1 /* More than one runnable task on a CPU. */
+#define SG_OVERUTILIZED		0x2 /* One or more CPUs are over-utilized. */
 
 /*
  * We add the notion of a root-domain which will be used to define per-domain
@@ -740,6 +741,9 @@ struct root_domain {
 	 */
 	int			overload;
 
+	/* Indicate one or more cpus over-utilized (tipping point) */
+	int			overutilized;
+
 	/*
 	 * The bit corresponding to a CPU gets set here if such CPU has more
 	 * than one runnable -deadline task (as it is below for RT tasks).
-- 
2.19.2

