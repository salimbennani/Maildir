Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 11 Dec 2018 12:55:55 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga006.fm.intel.com (fmsmga006.fm.intel.com [10.253.24.20])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id E121F5807A2;
	Mon, 10 Dec 2018 19:06:59 -0800 (PST)
Received: from fmsmga104.fm.intel.com ([10.1.193.100])
  by fmsmga006-1.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 10 Dec 2018 19:06:58 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AmgMRDR8X6lncKf9uRHKM819IXTAuvvDOBiVQ1KB9?=
 =?us-ascii?q?1+sUIJqq85mqBkHD//Il1AaPAd2Lraocw8Pt8InYEVQa5piAtH1QOLdtbDQizf?=
 =?us-ascii?q?ssogo7HcSeAlf6JvO5JwYzHcBFSUM3tyrjaRsdF8nxfUDdrWOv5jAOBBr/KRB1?=
 =?us-ascii?q?JuPoEYLOksi7ze+/94HQbglSmDaxfa55IQmrownWqsQYm5ZpJLwryhvOrHtIeu?=
 =?us-ascii?q?BWyn1tKFmOgRvy5dq+8YB6/ShItP0v68BPUaPhf6QlVrNYFygpM3o05MLwqxbO?=
 =?us-ascii?q?SxaE62YGXWUXlhpIBBXF7A3/U5zsvCb2qvZx1S+HNsDwULs6Wymt771zRRH1li?=
 =?us-ascii?q?kHOT43/mLZhMN+g61Uog6uqRNkw47MYoyYL+Z+c6DHcN8GWWZMUMRcWipcCY28?=
 =?us-ascii?q?dYsPCO8BMP5coYbnvFsOqh2+DhSsC+z1zD9IiWL90Ko40+s7CgHG2wIhEMgIsH?=
 =?us-ascii?q?Tbt9j1LrkdXv21zKbS0TXPde9Z2TD46IXRdB0qvPKCXapofMbP1UUiExnJgkie?=
 =?us-ascii?q?pID7JT+Zy+cAv3SB4+dhV++jk3Arpx11rzS128shhJfFipgIxl3H+yh12pg5KN?=
 =?us-ascii?q?6+RUVme9CrCoFQuDufN4ZuQsMtXWVouCEix70Ytp60YjIKxI4kxxHBcfyHdZaH?=
 =?us-ascii?q?4hb5WOaWOzd4i2ppeLO5hxms7Uit0vPwWtWw3VpQsyZInMfAumoQ2xHQ9sSLUP?=
 =?us-ascii?q?pw80W51TaKzQ/T6+VEIU4ularcLp4s2rowlpsVsUTeES76gUb2g7GMdkUi5Oeo?=
 =?us-ascii?q?7/3rYrLop5+aKYB0kBrzMrohmsOhG+Q0KAsOUHaB+eS6yrLj+Vf1QLJQjv05iq?=
 =?us-ascii?q?XZqozVJdwHpq6lBA9YyoIj6xe8Dzi4ytgZkmQHIUlBeBKGiYjpJl7PLOr5Dfe5?=
 =?us-ascii?q?n1SjjjNry+raMb3mB5XHNmLDn6v5fbZh905czxI+zdBF6JJVFrEOOvXzVlXxtN?=
 =?us-ascii?q?zFFBA5NQO0zv3jCNV80IMeRG2ODrWYMKPUrV+H+OYvL/OQa48SvTb3M+Il6OL2?=
 =?us-ascii?q?jX8lhV8derGk3ZkQaH+mBPhmIEKZYXzqgtcGCmoKugs+TOr3iFyNSzJTZnCyX7?=
 =?us-ascii?q?4i6TE/Eo6pEYDDRoW1irybwCi7BoFWZnxBCl2UC3jobIWEW/APaC6IOM9uiD4E?=
 =?us-ascii?q?WKOlS48g0xGuqQD7x6BmLurS5i0Xq5bj2MJp6O3UkBE47SZ0ANiF02GRU2F0mX?=
 =?us-ascii?q?sFRzws06B5u0B9zlaD3rJ+g/xXDtFT4/JJUgEnNZ/T1eB6CtbyWh7fcdeNUlqp?=
 =?us-ascii?q?XtKmATQpRNIr39AOe1p9G8mljh3bwiWlGaEVmKKRCJw06K3c2WPxKNh7y3rB0K?=
 =?us-ascii?q?khjFwmQs9UOG2ih65/8RXTBoHTn0WYkaaqaboT3CrX+GifymqOuVlSUBRsXqXd?=
 =?us-ascii?q?QXAfekzWoMzk6UzYUb+hF64rMgtbxs6EMatFdNvpgE5CRPfiPtTefm2wl32xBR?=
 =?us-ascii?q?aO2rODco7qd38B0yXaDUgOixoT8mqeNQgiGiehpHrTDCBvFVLqZEPs7el+qXOh?=
 =?us-ascii?q?Qk8o1Q2KaFdh1760+h4TivyRUPcT3rMCuCc8pDR4Ble939TKC9WeowptZrlTYd?=
 =?us-ascii?q?Q44F1fz2LWqxR9PoC8L6BlnlMRaQB3sF3h1hppCoRMi9MlrG43wwVoL6KY01RB?=
 =?us-ascii?q?dy2D0JD0O73XLHTy/R+1Z67X3FHezMiZ+qMV5PskrFXjuRmjFlA+/HV/z9lVz3?=
 =?us-ascii?q?yc643KDQoPS57+Tlw79hl6p77AZCk96JjZ1XltMamyrz/D1MglBOojyha8Ydhf?=
 =?us-ascii?q?NLmIGxP1E80fH8KuMvAlm0C1bhIYO+Bf7LQ0P8K6ePScwq6kIeFgkCigjWtZ+o?=
 =?us-ascii?q?B901uA+DZmRe7MwpkK3euY3gyBVzrniFehs8b3mZ1LZD0IH2q/zzTkC5BVZqFo?=
 =?us-ascii?q?YYkLDmKuKdWtxtpin57tR2JY9Fm7ClIG2c+lYxqTY0bm3Q1N00QauninmSq+zz?=
 =?us-ascii?q?xpnDAltKuf3CrSw+v8cBoLIHJERG5njV30O4i7k8gaXFS0bwgujBal51z1x7JY?=
 =?us-ascii?q?pKR8KGnfW1xIcDL1L2FhU6uwsKSNY8hU5ZMssCVXVvm8YF+ARr78pRsazz3sH2?=
 =?us-ascii?q?9EyD8ncDGqv43znwZmh2KFMHZzsH3ZdNlwxBjF49zQX/xR3jsARCRjjTnXB168?=
 =?us-ascii?q?P8Sm/NmOlpfDtPy+WHylVpFJbSbryoaAvjOh5WJ2GR2/g+yzmtr/HAg5yyD71s?=
 =?us-ascii?q?NmVSXVrBnmZInr2L+3MeZmfkluGV/95NB2GoB4kossmp4Q3WIWiYmS/XoCiW3z?=
 =?us-ascii?q?K8lU2bribHoRQj4G29vV4Anm2EJ5NH6IyZz2Vm6BwsR/fdm1eX0Z2jgy781RDK?=
 =?us-ascii?q?eU7bpEnTZ6o1airALRZ+R9kSkZyfc09HEahOQJshI3ziqBGrASAVVYPSv0mhSU?=
 =?us-ascii?q?6NC+qb9bZWe1fri2yUp+hsuhDLCZrwFYWXb5fIoiHCBq4sV+NlLMzGP86oX+dN?=
 =?us-ascii?q?bMatITswWekw3cgOhNNJIxiv0KiDJ8Nm3nun0lz/M7gQZq3ZGnp4WHL2Rt/KSk?=
 =?us-ascii?q?Ah9XLDH1ZsUT+i3zgqZahMqZw4evHpB5EDURQJToVe6oEC4Vtfn/NQaBCjg8pW?=
 =?us-ascii?q?mAGbbFAQ+T8kRmr2/ME5C2MXGYPmIZwM5mRBmcIkxfnQ8VUC87np4/CgCl2sjh?=
 =?us-ascii?q?fF1l6TAW417ysgFMxf5wNxnjTmffox+lazczSJiCNRpW8xtN50HPPcyF6eJzGS?=
 =?us-ascii?q?5Y84alrAyMLGybegtJAXsIWkyCG1DsIL2u6cPc/OifA+q0N+HObqmWqexCS/eI?=
 =?us-ascii?q?woqi0ot48DaNNcWPPXhiA+c42kpZWnB5FNrWmy8SSywMjC/Na8+bpBGh+ix4tM?=
 =?us-ascii?q?y/8fLrWB7x6ouLEbddLdJv+xWui6eZK+GQnDp5KSpf1p4U33DI1aMQ3EQMhCBu?=
 =?us-ascii?q?bTWtFbUAuDXJTKLRnK9XEhEaZzlyNMtO86IzwA1NNdTHhdPy075yluQ1BEtdVV?=
 =?us-ascii?q?z9hsGpYtQHI328NFPCHkqEKK6KKiHLw87pZ6OxUrlQjORStx2ttjeXCU7jPjKf?=
 =?us-ascii?q?lzb3UxCjK/1DjCaePBZGooGybg5tCXT/TNLhchC6MMV4jTszwb01gHPFLW8dMT?=
 =?us-ascii?q?hmfENLob2d9idYgvR5G2xc4XtpN+iEmyCF7+bGLpYaq+dkAiNxl7ES3HNvgZ5U?=
 =?us-ascii?q?5yFNSfg9o22aksVvuVKnlKPHnj98TDJUpzpLjZ7NtkJnb/b37J5FDFTN5xULpV?=
 =?us-ascii?q?qZDRtC89lsEdrwsqZf4sLCmKL6NHFJ9NeCrphUPNTdNM/SaClpChHuAjOBVAY?=
 =?us-ascii?q?=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0BCAAAfKQ9ch0O0hNFcCBoBAQEBAQIBA?=
 =?us-ascii?q?QEBBwIBAQEBgWWBW4IRFBODe4h4izGBYC0UiH6QMhQYEwGHbSI4EgEDAQEBAQE?=
 =?us-ascii?q?BAgETAQEBCA0JCCkvgjYigmQBAQEBAgEBAiAECwEFQQYJAQEIAhgCAiYCAgMeN?=
 =?us-ascii?q?gYNBgIBAQGDHIFqAw0IBYo1m1B8M4VAgkQNghyBC4sWF4FAP4E4gmuCV4IFgym?=
 =?us-ascii?q?CVwKJFYckkA4uCY1jPYMpBhiBf4dnEIdHLI53ig2BXYF3MxoIGxWDJ4InF38BA?=
 =?us-ascii?q?o0oMgEBMQGBBAEBHIpDAQE?=
X-IPAS-Result: =?us-ascii?q?A0BCAAAfKQ9ch0O0hNFcCBoBAQEBAQIBAQEBBwIBAQEBgWW?=
 =?us-ascii?q?BW4IRFBODe4h4izGBYC0UiH6QMhQYEwGHbSI4EgEDAQEBAQEBAgETAQEBCA0JC?=
 =?us-ascii?q?CkvgjYigmQBAQEBAgEBAiAECwEFQQYJAQEIAhgCAiYCAgMeNgYNBgIBAQGDHIF?=
 =?us-ascii?q?qAw0IBYo1m1B8M4VAgkQNghyBC4sWF4FAP4E4gmuCV4IFgymCVwKJFYckkA4uC?=
 =?us-ascii?q?Y1jPYMpBhiBf4dnEIdHLI53ig2BXYF3MxoIGxWDJ4InF38BAo0oMgEBMQGBBAE?=
 =?us-ascii?q?BHIpDAQE?=
X-IronPort-AV: E=Sophos;i="5.56,340,1539673200"; 
   d="scan'208";a="54640418"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 10 Dec 2018 19:06:56 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1729816AbeLKDGy (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Mon, 10 Dec 2018 22:06:54 -0500
Received: from mx1.redhat.com ([209.132.183.28]:41824 "EHLO mx1.redhat.com"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1728393AbeLKDGy (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 10 Dec 2018 22:06:54 -0500
Received: from smtp.corp.redhat.com (int-mx04.intmail.prod.int.phx2.redhat.com [10.5.11.14])
        (using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
        (No client certificate requested)
        by mx1.redhat.com (Postfix) with ESMTPS id 896C4C058CB0;
        Tue, 11 Dec 2018 03:06:53 +0000 (UTC)
Received: from [10.72.12.184] (ovpn-12-184.pek2.redhat.com [10.72.12.184])
        by smtp.corp.redhat.com (Postfix) with ESMTPS id C3D7A5D962;
        Tue, 11 Dec 2018 03:06:47 +0000 (UTC)
Subject: Re: [PATCH net 2/4] vhost_net: rework on the lock ordering for busy
 polling
To: "Michael S. Tsirkin" <mst@redhat.com>
Cc: kvm@vger.kernel.org, virtualization@lists.linux-foundation.org,
        netdev@vger.kernel.org, linux-kernel@vger.kernel.org,
        Tonghao Zhang <xiangxia.m.yue@gmail.com>
References: <20181210094454.21144-1-jasowang@redhat.com>
 <20181210094454.21144-3-jasowang@redhat.com>
 <20181210203119-mutt-send-email-mst@kernel.org>
From: Jason Wang <jasowang@redhat.com>
Message-ID: <f2a98f3a-a5c5-b762-8ec3-119a7708795d@redhat.com>
Date: Tue, 11 Dec 2018 11:06:43 +0800
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101
 Thunderbird/60.2.1
MIME-Version: 1.0
In-Reply-To: <20181210203119-mutt-send-email-mst@kernel.org>
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 8bit
Content-Language: en-US
X-Scanned-By: MIMEDefang 2.79 on 10.5.11.14
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16 (mx1.redhat.com [10.5.110.32]); Tue, 11 Dec 2018 03:06:53 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org


On 2018/12/11 上午9:34, Michael S. Tsirkin wrote:
> On Mon, Dec 10, 2018 at 05:44:52PM +0800, Jason Wang wrote:
>> When we try to do rx busy polling in tx path in commit 441abde4cd84
>> ("net: vhost: add rx busy polling in tx path"), we lock rx vq mutex
>> after tx vq mutex is held. This may lead deadlock so we try to lock vq
>> one by one in commit 78139c94dc8c ("net: vhost: lock the vqs one by
>> one"). With this commit, we avoid the deadlock with the assumption
>> that handle_rx() and handle_tx() run in a same process. But this
>> commit remove the protection for IOTLB updating which requires the
>> mutex of each vq to be held.
>>
>> To solve this issue, the first step is to have a exact same lock
>> ordering for vhost_net. This is done through:
>>
>> - For handle_rx(), if busy polling is enabled, lock tx vq immediately.
>> - For handle_tx(), always lock rx vq before tx vq, and unlock it if
>>    busy polling is not enabled.
>> - Remove the tricky locking codes in busy polling.
>>
>> With this, we can have a exact same lock ordering for vhost_net, this
>> allows us to safely revert commit 78139c94dc8c ("net: vhost: lock the
>> vqs one by one") in next patch.
>>
>> The patch will add two more atomic operations on the tx path during
>> each round of handle_tx(). 1 byte TCP_RR does not notice such
>> overhead.
>>
>> Fixes: commit 78139c94dc8c ("net: vhost: lock the vqs one by one")
>> Cc: Tonghao Zhang<xiangxia.m.yue@gmail.com>
>> Signed-off-by: Jason Wang<jasowang@redhat.com>
>> ---
>>   drivers/vhost/net.c | 18 +++++++++++++++---
>>   1 file changed, 15 insertions(+), 3 deletions(-)
>>
>> diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
>> index ab11b2bee273..5f272ab4d5b4 100644
>> --- a/drivers/vhost/net.c
>> +++ b/drivers/vhost/net.c
>> @@ -513,7 +513,6 @@ static void vhost_net_busy_poll(struct vhost_net *net,
>>   	struct socket *sock;
>>   	struct vhost_virtqueue *vq = poll_rx ? tvq : rvq;
>>   
>> -	mutex_lock_nested(&vq->mutex, poll_rx ? VHOST_NET_VQ_TX: VHOST_NET_VQ_RX);
>>   	vhost_disable_notify(&net->dev, vq);
>>   	sock = rvq->private_data;
>>   
>> @@ -543,8 +542,6 @@ static void vhost_net_busy_poll(struct vhost_net *net,
>>   		vhost_net_busy_poll_try_queue(net, vq);
>>   	else if (!poll_rx) /* On tx here, sock has no rx data. */
>>   		vhost_enable_notify(&net->dev, rvq);
>> -
>> -	mutex_unlock(&vq->mutex);
>>   }
>>   
>>   static int vhost_net_tx_get_vq_desc(struct vhost_net *net,
>> @@ -913,10 +910,16 @@ static void handle_tx_zerocopy(struct vhost_net *net, struct socket *sock)
>>   static void handle_tx(struct vhost_net *net)
>>   {
>>   	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];
>> +	struct vhost_net_virtqueue *nvq_rx = &net->vqs[VHOST_NET_VQ_RX];
>>   	struct vhost_virtqueue *vq = &nvq->vq;
>> +	struct vhost_virtqueue *vq_rx = &nvq_rx->vq;
>>   	struct socket *sock;
>>   
>> +	mutex_lock_nested(&vq_rx->mutex, VHOST_NET_VQ_RX);
>>   	mutex_lock_nested(&vq->mutex, VHOST_NET_VQ_TX);
>> +	if (!vq->busyloop_timeout)
>> +		mutex_unlock(&vq_rx->mutex);
>> +
>>   	sock = vq->private_data;
>>   	if (!sock)
>>   		goto out;
>> @@ -933,6 +936,8 @@ static void handle_tx(struct vhost_net *net)
>>   		handle_tx_copy(net, sock);
>>   
>>   out:
>> +	if (vq->busyloop_timeout)
>> +		mutex_unlock(&vq_rx->mutex);
>>   	mutex_unlock(&vq->mutex);
>>   }
>>   
> So rx mutex taken on tx path now.  And tx mutex is on rc path ...  This
> is just messed up. Why can't tx polling drop rx lock before
> getting the tx lock and vice versa?


Because we want to poll both tx and rx virtqueue at the same time 
(vhost_net_busy_poll()).

     while (vhost_can_busy_poll(endtime)) {
         if (vhost_has_work(&net->dev)) {
             *busyloop_intr = true;
             break;
         }

         if ((sock_has_rx_data(sock) &&
              !vhost_vq_avail_empty(&net->dev, rvq)) ||
             !vhost_vq_avail_empty(&net->dev, tvq))
             break;

         cpu_relax();

     }


And we disable kicks and notification for better performance.


>
> Or if we really wanted to force everything to be locked at
> all times, let's just use a single mutex.
>
>
>

We could, but it might requires more changes which could be done for 
-next I believe.


Thanks

