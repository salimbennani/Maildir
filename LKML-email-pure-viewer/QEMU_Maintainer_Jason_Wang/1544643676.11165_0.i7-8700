Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by i7-8700 with POP3-SSL;
  12 Dec 2018 19:41:16 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga002.fm.intel.com (fmsmga002.fm.intel.com [10.253.24.26])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id EDD745803DC;
	Tue, 11 Dec 2018 19:05:17 -0800 (PST)
Received: from orsmga101.jf.intel.com ([10.7.208.22])
  by fmsmga002-1.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 11 Dec 2018 19:05:17 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AdxHLHRcYdLQXAQehaORBBfmDlGMj4u6mDksu8pMi?=
 =?us-ascii?q?zoh2WeGdxc6+ZRaN2/xhgRfzUJnB7Loc0qyK6/CmATRIyK3CmUhKSIZLWR4BhJ?=
 =?us-ascii?q?detC0bK+nBN3fGKuX3ZTcxBsVIWQwt1Xi6NU9IBJS2PAWK8TW94jEIBxrwKxd+?=
 =?us-ascii?q?KPjrFY7OlcS30P2594HObwlSizexfbB/IA+qoQnNq8IbnZZsJqEtxxXTv3BGYf?=
 =?us-ascii?q?5WxWRmJVKSmxbz+MK994N9/ipTpvws6ddOXb31cKokQ7NYCi8mM30u683wqRbD?=
 =?us-ascii?q?VwqP6WACXWgQjxFFHhLK7BD+Xpf2ryv6qu9w0zSUMMHqUbw5Xymp4qF2QxHqlS?=
 =?us-ascii?q?gHLSY0/nzJhMx+jKxVoxyvqBJwzIHWfI6bO+F+frvfcN4BWWpMXdxcWzBdDo6y?=
 =?us-ascii?q?bYYCCfcKM+ZCr4n6olsDtR+wChO3BOPozD9Dm3/50rc80+QuDArL2w4gEMgVsH?=
 =?us-ascii?q?TTotT6LqESUe+uwanS0zrMcvNW1i3h6ITSbh8hpvSMUKt2fMHMykcvDxvIgkuM?=
 =?us-ascii?q?pYHhJT+Zy+oAv3aB4+Z9Vu+jl3QrpgBzrzS32Msglo3EipgIxl3K6yl12ps5KN?=
 =?us-ascii?q?62RUJhf9KpE51dvDyAOYRsWMMtWWRotT46yrIYvZ67ezAHyIooxxHBcfyLaYuI?=
 =?us-ascii?q?7Qz5VOaXPzh4gGhpeLWlhxa96USgy+v8Wdeo0FtSsCZJjt3BumoQ2xHd9MSLUO?=
 =?us-ascii?q?Zx80S91TqV1g3e6PlIIUUumqraL54hzKQwlp0WsUnbGi/2mUP2jLKZd0k9+ein?=
 =?us-ascii?q?9f7nYrP4qZ+YLoN0jQ//MqIwlcylGuk4LAcOUHaB+eim173s41f5QLNUgf0yiK?=
 =?us-ascii?q?XZt4raJcsDqq6jHwBVypoj6wq4Dzq+1NQYnHoHI0xfdBOIkojkIFXOIPH+Dfei?=
 =?us-ascii?q?jFWgijZrx/baPrL/BpXBNGTMkLDkfbxl8U5T1BIzzcxD55JTErwBIOj8Wk7ttN?=
 =?us-ascii?q?PCCR81KQy0w/v9B9V72YMTQmaPAq6fMKPPvl6E/OMvI++QZIALvDbxMeQq5/nr?=
 =?us-ascii?q?jXUhg18SYbGp3YcLaHC/BvlmIUKZbmT2jtcCFmcKuAw+TOvxhV2GUD5TYWuyXq?=
 =?us-ascii?q?0m6jE6DoKmEZnMRoS3jLOd2ye7G4VcZnpaBVCUDXfoa4KEVu8WZyKJIs9hlT8E?=
 =?us-ascii?q?WaK7S4A71xGjrwv6y7thLurJ9SwUr5Pj1N5p5+LNkRE+7yB7D8OY02uVVWF7gn?=
 =?us-ascii?q?sIRyMq3KB4uUF9yU2M0a5ij/xcFNxc/fVJUgghOJ7Yzux6Dc3yWw3bcteITlam?=
 =?us-ascii?q?XsupATUrQt0txN8OZl53G8++gRDbwyqqH7gVmqSWC5Mu7KLTwWL9J8ZnxHbAz6?=
 =?us-ascii?q?kukV8mTsxLNW2lg65/8xPeB4rIk0Wfiqarer4Q3C/L9Gef02WOuFtUXxJ3UaXA?=
 =?us-ascii?q?RXoffFfZrczl5kPeSL+jEakoPRFfycGcMKdKasfmjVNdRPj9PtTTeGaxm2a2BR?=
 =?us-ascii?q?aVybKAdovqe2MB3CrDDEgIiRwc/XGDNQImHCeuv3reDCByFVLoe07s7eh+qHa8?=
 =?us-ascii?q?Tk8ozwGLb1dt16av9h4Sn/ycT/IT3rQLuCo6rzV0HVC938/ZCtaapgpherlcbs?=
 =?us-ascii?q?054FtdyW3ZsAl9NIS6L69+nl4ebxh3v0T22hptDoVPj8cro20qzQZoMq2YzU5O?=
 =?us-ascii?q?eCme3ZDzPb3XNHL//BSua67Qx1Hf38ya+qYJ6PQktVrjuBulGVYl83Viy9NVyW?=
 =?us-ascii?q?eT5o3WDAoOVpL8SkY29wVgq77GeCU85oPU2mdqMam7qTLC39MpBO04yhevZdtf?=
 =?us-ascii?q?MaWEFBPsHM0eHcShNOsqm12xZBIeIO9S7LI0P9+hd/aewq6rPfpvkyi8jWta54?=
 =?us-ascii?q?BxyESM+DRmSu7JxpoK3+uX3g+aWDjillehtcb3lJtAZTETGGq/1CflCJRQZq10?=
 =?us-ascii?q?YYYEF2OuL9erydV5gp7nQ2RY+0K7B1MaxM+pfgKfblz83QFK1UUbu2enmTa+zz?=
 =?us-ascii?q?FvlzEpr6yf3DHBwuj4dRoHPHJLS3dmjVv2PYe0iNUaVlCybwc1jBul+Vr6x69D?=
 =?us-ascii?q?qaR9NWbTW0RIfyvxL214Sau/rLmCY81O6JMurypXVv+xYVSbSr77vhsb3DnvH2?=
 =?us-ascii?q?pYxDAnaT6qvo/1kAB9iGKYNHxztmbWedlsxRfD49zRXf5Q0SABRCViiznXB168?=
 =?us-ascii?q?MsKt/dWVkZfDr++/W3igVp1VbSnk04eAuDGn6m1tBB21h+qzlcH/EQgmzS/70M?=
 =?us-ascii?q?FnVSbSoxb9eIXr17m6PvhhfklnH1L878t6Godjkoo/np0Q2H4ahomL8noDi2v8?=
 =?us-ascii?q?LdJb2afmZnoXWTEL28LV4BTi2EB7K3KJxoH5WW+Hzspveda6eX8W1Twn789RFq?=
 =?us-ascii?q?iU9rNEkDBxolq5qwLRfPd8ki0cyfso9H4VneUJtBAxwSWaB7AYBVNYMjD0lxSU?=
 =?us-ascii?q?89C+q71aZHqocbi1zkZ/nMquDLefogFHQ3b5e4wvHStx7sV5LVLN32f/6oDieN?=
 =?us-ascii?q?nMc90TsgeYnAvHj+hQMJgxjOYFhTJ7OWLhun0o0/U0ggZp3ZG+oYiLMWFt/L+i?=
 =?us-ascii?q?Dx5cNz31Yd4T+z73gaZfmMaWw56gHpF7FjoXW5voSOqiECgOuvT/KwaODDo8p2?=
 =?us-ascii?q?+ZGbXFGA+Q9lxmo2jTE5yxNHGXOXoZzchkRBaHP0xSmwQUXDQ8np4kGQGm3s3h?=
 =?us-ascii?q?cEFl5j8P4l70sAdDyuVtNxPnSGfQuB+oaisoSJiYNBdX7gZC51rMMcyD9O1zGT?=
 =?us-ascii?q?tU/oamrAyLJWyWfABIDWAPWkyZCFHvJLiu5d/c8+eGAuqyNefBYbKLqeZGTfeH?=
 =?us-ascii?q?2Yqv0pd6/zaLLsiPPmdtD/w42kpAXHB1AcfZmy8ISywYiS3NddObpAyn9y1zr8?=
 =?us-ascii?q?C/9unrWQ316YuODbtSLctg+xSsjaifMO6QgT5zKSxE2ZMU2X/I1L8f0UYIhCFz?=
 =?us-ascii?q?bDatC6oPtC7XQKLUm69aFBobayJ1NMtV4KMwxAhNOcjHitzr0r50lOI6C1BAVV?=
 =?us-ascii?q?b5gMGmedQKI32hNFPAHEuLKLWGJTjRz87re6+zVb1Qg/tStxCrvTabEknjPimM?=
 =?us-ascii?q?ljXzVhCvN/1Mgz+fPBBEpI69dRNtA3D5TN36ch27LMN3jTouzLw0nHzKM2scMT?=
 =?us-ascii?q?l6c0xVr7yQ7TlXgvNwG2Fa6nplLO+EmzuW7uXCK5YWt+drDTpwl+5A/Hs6zL5V?=
 =?us-ascii?q?v2l4Q6k/vSrZrtdjpxmd1KGz0Tt9UxtI4H4fgZmXlV9vNaXQ6t9LXnOSrzwX6m?=
 =?us-ascii?q?DFKR0RoN0tNdTptOgEyNHUk736Ljhq6d/Y/cIAQcPTLZTUYzIaLRP1FWuMX0M+?=
 =?us-ascii?q?RjmxODSHig=3D=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AtAABLehBch0O0hNFdCBoBAQEBAQIBA?=
 =?us-ascii?q?QEBBwIBAQEBgWWBW4IRFBODe4h4jRMIJRSIf5AyFBgTAYcuIjgSAQMBAQEBAQE?=
 =?us-ascii?q?CARMBAQEIDQkIKS+CNiKCZQECAwECIAQLAQVBBgkBAQgCGAICJgICAx42Bg0GA?=
 =?us-ascii?q?gEBAYMcgWoDFQWKHZtQfDOFQIJHDYIcgQuLMBeBQD+BOAyCX4JXggWDKYJXAok?=
 =?us-ascii?q?XhySFO4pXLgmNZj2DKgYYgX+HaRCHSiyOeIoQgV2BdzMaCBsVgyeCJxd/AQKNK?=
 =?us-ascii?q?DIBATEBgQQBARyLNAEB?=
X-IPAS-Result: =?us-ascii?q?A0AtAABLehBch0O0hNFdCBoBAQEBAQIBAQEBBwIBAQEBgWW?=
 =?us-ascii?q?BW4IRFBODe4h4jRMIJRSIf5AyFBgTAYcuIjgSAQMBAQEBAQECARMBAQEIDQkIK?=
 =?us-ascii?q?S+CNiKCZQECAwECIAQLAQVBBgkBAQgCGAICJgICAx42Bg0GAgEBAYMcgWoDFQW?=
 =?us-ascii?q?KHZtQfDOFQIJHDYIcgQuLMBeBQD+BOAyCX4JXggWDKYJXAokXhySFO4pXLgmNZ?=
 =?us-ascii?q?j2DKgYYgX+HaRCHSiyOeIoQgV2BdzMaCBsVgyeCJxd/AQKNKDIBATEBgQQBARy?=
 =?us-ascii?q?LNAEB?=
X-IronPort-AV: E=Sophos;i="5.56,343,1539673200"; 
   d="scan'208";a="44595728"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 11 Dec 2018 19:04:09 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726344AbeLLDEG (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Tue, 11 Dec 2018 22:04:06 -0500
Received: from mx1.redhat.com ([209.132.183.28]:60544 "EHLO mx1.redhat.com"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1726218AbeLLDEG (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Tue, 11 Dec 2018 22:04:06 -0500
Received: from smtp.corp.redhat.com (int-mx01.intmail.prod.int.phx2.redhat.com [10.5.11.11])
        (using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
        (No client certificate requested)
        by mx1.redhat.com (Postfix) with ESMTPS id 43FD0307CDC2;
        Wed, 12 Dec 2018 03:04:05 +0000 (UTC)
Received: from [10.72.12.76] (ovpn-12-76.pek2.redhat.com [10.72.12.76])
        by smtp.corp.redhat.com (Postfix) with ESMTPS id 07938600C9;
        Wed, 12 Dec 2018 03:03:59 +0000 (UTC)
Subject: Re: [PATCH net 2/4] vhost_net: rework on the lock ordering for busy
 polling
To: "Michael S. Tsirkin" <mst@redhat.com>
Cc: kvm@vger.kernel.org, virtualization@lists.linux-foundation.org,
        netdev@vger.kernel.org, linux-kernel@vger.kernel.org,
        Tonghao Zhang <xiangxia.m.yue@gmail.com>
References: <20181210094454.21144-1-jasowang@redhat.com>
 <20181210094454.21144-3-jasowang@redhat.com>
 <20181210203119-mutt-send-email-mst@kernel.org>
 <f2a98f3a-a5c5-b762-8ec3-119a7708795d@redhat.com>
 <20181210230106-mutt-send-email-mst@kernel.org>
From: Jason Wang <jasowang@redhat.com>
Message-ID: <aa8f36da-1489-a094-35ce-286bb3f25243@redhat.com>
Date: Wed, 12 Dec 2018 11:03:57 +0800
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101
 Thunderbird/60.2.1
MIME-Version: 1.0
In-Reply-To: <20181210230106-mutt-send-email-mst@kernel.org>
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 8bit
Content-Language: en-US
X-Scanned-By: MIMEDefang 2.79 on 10.5.11.11
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16 (mx1.redhat.com [10.5.110.49]); Wed, 12 Dec 2018 03:04:05 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org


On 2018/12/11 下午12:04, Michael S. Tsirkin wrote:
> On Tue, Dec 11, 2018 at 11:06:43AM +0800, Jason Wang wrote:
>> On 2018/12/11 上午9:34, Michael S. Tsirkin wrote:
>>> On Mon, Dec 10, 2018 at 05:44:52PM +0800, Jason Wang wrote:
>>>> When we try to do rx busy polling in tx path in commit 441abde4cd84
>>>> ("net: vhost: add rx busy polling in tx path"), we lock rx vq mutex
>>>> after tx vq mutex is held. This may lead deadlock so we try to lock vq
>>>> one by one in commit 78139c94dc8c ("net: vhost: lock the vqs one by
>>>> one"). With this commit, we avoid the deadlock with the assumption
>>>> that handle_rx() and handle_tx() run in a same process. But this
>>>> commit remove the protection for IOTLB updating which requires the
>>>> mutex of each vq to be held.
>>>>
>>>> To solve this issue, the first step is to have a exact same lock
>>>> ordering for vhost_net. This is done through:
>>>>
>>>> - For handle_rx(), if busy polling is enabled, lock tx vq immediately.
>>>> - For handle_tx(), always lock rx vq before tx vq, and unlock it if
>>>>     busy polling is not enabled.
>>>> - Remove the tricky locking codes in busy polling.
>>>>
>>>> With this, we can have a exact same lock ordering for vhost_net, this
>>>> allows us to safely revert commit 78139c94dc8c ("net: vhost: lock the
>>>> vqs one by one") in next patch.
>>>>
>>>> The patch will add two more atomic operations on the tx path during
>>>> each round of handle_tx(). 1 byte TCP_RR does not notice such
>>>> overhead.
>>>>
>>>> Fixes: commit 78139c94dc8c ("net: vhost: lock the vqs one by one")
>>>> Cc: Tonghao Zhang<xiangxia.m.yue@gmail.com>
>>>> Signed-off-by: Jason Wang<jasowang@redhat.com>
>>>> ---
>>>>    drivers/vhost/net.c | 18 +++++++++++++++---
>>>>    1 file changed, 15 insertions(+), 3 deletions(-)
>>>>
>>>> diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c
>>>> index ab11b2bee273..5f272ab4d5b4 100644
>>>> --- a/drivers/vhost/net.c
>>>> +++ b/drivers/vhost/net.c
>>>> @@ -513,7 +513,6 @@ static void vhost_net_busy_poll(struct vhost_net *net,
>>>>    	struct socket *sock;
>>>>    	struct vhost_virtqueue *vq = poll_rx ? tvq : rvq;
>>>> -	mutex_lock_nested(&vq->mutex, poll_rx ? VHOST_NET_VQ_TX: VHOST_NET_VQ_RX);
>>>>    	vhost_disable_notify(&net->dev, vq);
>>>>    	sock = rvq->private_data;
>>>> @@ -543,8 +542,6 @@ static void vhost_net_busy_poll(struct vhost_net *net,
>>>>    		vhost_net_busy_poll_try_queue(net, vq);
>>>>    	else if (!poll_rx) /* On tx here, sock has no rx data. */
>>>>    		vhost_enable_notify(&net->dev, rvq);
>>>> -
>>>> -	mutex_unlock(&vq->mutex);
>>>>    }
>>>>    static int vhost_net_tx_get_vq_desc(struct vhost_net *net,
>>>> @@ -913,10 +910,16 @@ static void handle_tx_zerocopy(struct vhost_net *net, struct socket *sock)
>>>>    static void handle_tx(struct vhost_net *net)
>>>>    {
>>>>    	struct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];
>>>> +	struct vhost_net_virtqueue *nvq_rx = &net->vqs[VHOST_NET_VQ_RX];
>>>>    	struct vhost_virtqueue *vq = &nvq->vq;
>>>> +	struct vhost_virtqueue *vq_rx = &nvq_rx->vq;
>>>>    	struct socket *sock;
>>>> +	mutex_lock_nested(&vq_rx->mutex, VHOST_NET_VQ_RX);
>>>>    	mutex_lock_nested(&vq->mutex, VHOST_NET_VQ_TX);
>>>> +	if (!vq->busyloop_timeout)
>>>> +		mutex_unlock(&vq_rx->mutex);
>>>> +
>>>>    	sock = vq->private_data;
>>>>    	if (!sock)
>>>>    		goto out;
>>>> @@ -933,6 +936,8 @@ static void handle_tx(struct vhost_net *net)
>>>>    		handle_tx_copy(net, sock);
>>>>    out:
>>>> +	if (vq->busyloop_timeout)
>>>> +		mutex_unlock(&vq_rx->mutex);
>>>>    	mutex_unlock(&vq->mutex);
>>>>    }
>>> So rx mutex taken on tx path now.  And tx mutex is on rc path ...  This
>>> is just messed up. Why can't tx polling drop rx lock before
>>> getting the tx lock and vice versa?
>>
>> Because we want to poll both tx and rx virtqueue at the same time
>> (vhost_net_busy_poll()).
>>
>>      while (vhost_can_busy_poll(endtime)) {
>>          if (vhost_has_work(&net->dev)) {
>>              *busyloop_intr = true;
>>              break;
>>          }
>>
>>          if ((sock_has_rx_data(sock) &&
>>               !vhost_vq_avail_empty(&net->dev, rvq)) ||
>>              !vhost_vq_avail_empty(&net->dev, tvq))
>>              break;
>>
>>          cpu_relax();
>>
>>      }
>>
>>
>> And we disable kicks and notification for better performance.
> Right but it's all slow path - it happens when queue is
> otherwise empty. So this is what I am saying: let's drop the locks
> we hold around this.


Is this really safe? I looks to me it can race with SET_VRING_ADDR. And 
the codes did more:

- access sock object

- access device IOTLB

- enable and disable notification

None of above is safe without the protection of vq mutex.


>
>
>>> Or if we really wanted to force everything to be locked at
>>> all times, let's just use a single mutex.
>>>
>>>
>>>
>> We could, but it might requires more changes which could be done for -next I
>> believe.
>>
>>
>> Thanks
> I'd rather we kept the fine grained locking. E.g. people are
> looking at splitting the tx and rx threads. But if not possible
> let's fix it cleanly with a coarse-grained one. A mess here will
> just create more trouble later.
>

I believe we won't go back to coarse one. Looks like we can solve this 
by using mutex_trylock() for rxq during TX. And don't do polling for rxq 
is a IOTLB updating is pending.

Let me post V2.

Thanks

