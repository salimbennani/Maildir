Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by i7-8700 with POP3-SSL;
  13 Dec 2018 08:57:30 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga003.jf.intel.com (orsmga003.jf.intel.com [10.7.209.27])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 12B22580380;
	Wed, 12 Dec 2018 16:51:31 -0800 (PST)
Received: from orsmga106.jf.intel.com ([10.7.208.65])
  by orsmga003-1.jf.intel.com with ESMTP; 12 Dec 2018 16:51:30 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3Anq08wxft2TboPM/08WPrvVU3lGMj4u6mDksu8pMi?=
 =?us-ascii?q?zoh2WeGdxc6+YRKN2/xhgRfzUJnB7Loc0qyK6/CmATRIyK3CmUhKSIZLWR4BhJ?=
 =?us-ascii?q?detC0bK+nBN3fGKuX3ZTcxBsVIWQwt1Xi6NU9IBJS2PAWK8TW94jEIBxrwKxd+?=
 =?us-ascii?q?KPjrFY7OlcS30P2594HObwlSizexfbB/IA+qoQnNq8IbnZZsJqEtxxXTv3BGYf?=
 =?us-ascii?q?5WxWRmJVKSmxbz+MK994N9/ipTpvws6ddOXb31cKokQ7NYCi8mM30u683wqRbD?=
 =?us-ascii?q?VwqP6WACXWgQjxFFHhLK7BD+Xpf2ryv6qu9w0zSUMMHqUbw5Xymp4rx1QxH0li?=
 =?us-ascii?q?gIKz858HnWisNuiqJbvAmhrAF7z4LNfY2ZKOZycqbbcNgHR2ROQ9xRWjRPDI28?=
 =?us-ascii?q?cYUBEukPPehXoIbhulQBrxWxBRK0BO7t0TJImmP60Lcm3+g9CwzKwgotFM8Ovn?=
 =?us-ascii?q?TOq9X1Mb8fX+6vw6nW0zrIcu1b2Tf86IjOdBAuv+uMVq93fMXM00YvCQLFgUiL?=
 =?us-ascii?q?pIzrJTOV0eENs2+B7+V7VOKvl3QrpB12ojiq38ohjJTCiIENyl3c6yl13II4Kc?=
 =?us-ascii?q?elREN1f9KoCoZcuiKGO4dsQ84vQXlktSY+x7EcpJK2fSoHxI46yxLBb/GLaZaE?=
 =?us-ascii?q?7xL9WOuXPDx2nmhqeKiliBa36UWgyvPzVs2z0FtSsCpFncfDtmoX2xzQ9MeHUP?=
 =?us-ascii?q?198Vml2TqV0ADT8O5ELVg1lardNZEh3qY9moQPvUnHBCP6hUv7gLGMekk5+eWk?=
 =?us-ascii?q?9/7rb7Tkq5OEMo97kAD+MqAgmsylBuQ4NxADX2ya+eS6yb3i8lT1QLZUgf0slK?=
 =?us-ascii?q?nWrpTaKd0cpq+3BQ9azJwj5g2hDzepztsYh2MLLFFbdxKdiYjmJVXOLOr/Dfel?=
 =?us-ascii?q?jFSgiC1ryOzePr39HpXNKWDOkLPgfbZ+9UFQ0gUyzc1E6pJQC7EBJu/zW0DruN?=
 =?us-ascii?q?zZCB85LxK7w+L9BNph0YMeXHqFArWFP6PKrV+I+uUvLvGMZIAPuTb9N+Iq5/n0?=
 =?us-ascii?q?gX85hF8SZ6+p3ZQMZXC8H/RmJViZYHX2jtcAF2cKohQxTOjwhFKeVj5TYm64X7?=
 =?us-ascii?q?gg6TEjFIKmEYDDS5ivgLyGwii3BJ5WZmdAClCKFnflbIGEW/YKaCKPLc5tiD0E?=
 =?us-ascii?q?Vb69S4A/0RGirhP1y71iLuDM4C0XqYrj1MRp5+3UjRw97yd0D8Sa02GOVW10hH?=
 =?us-ascii?q?kHRz0t0aB7oEx9zEqD0Kdij/xZE9xT++1GUgMgOZHAyOx6Dsj4WhjdcdeRVFam?=
 =?us-ascii?q?XtKmDCkyTt0rxd8CeUJ9G9S4gRDF0CqnGLsVl72NBJwp/aPQxXnxJ8Bhy3nY0K?=
 =?us-ascii?q?ktlUUpQsxKNWe+nK5w6xDTB5LVk0Wej6ulaL4T3DDT+2uZzWqBpkdYUBVuXqXD?=
 =?us-ascii?q?WnwfYkjWrdH95k7ZS7+uCLInMhZOyMKYK6tKbMHpgktCRPv5JNveZGexkX+qBR?=
 =?us-ascii?q?mU3rOMcJbqe2IF0SrAEkcEjR4c8WyGNQckACehuHzRDDp1GFLrYkPs9/R+qXyh?=
 =?us-ascii?q?Qk81yQGKc1Nu176v9hEJgvycTusZ3qgYtyc5tzV0AFG90srKC9qBogphe7lcYd?=
 =?us-ascii?q?M94VtdyWLVrQt9PoakL6BjgF4efB96v0fv1xVxF4VBntImrHIszApuN62Y1ElN?=
 =?us-ascii?q?eC+f3ZD1IrfXMHX9/Aiza67K3VHTyMyZ+qMR5/U3tVrivBulGVE/83p6ydZV1X?=
 =?us-ascii?q?ic5pLUDAcJVZLxU0A39wV1pr3AYyk94Z/U2mNoMaWurjDC3NcpDvM/yhm8Z9df?=
 =?us-ascii?q?LL+EFAjqHswaGsiiMvAll0KobxIEJu9S8qE0Mtiid/uH3q6rIelhkCinjWRB/I?=
 =?us-ascii?q?ByzEaM+zBgRe7P2pYP2+uY0RefVzfgkFehtdj6mZpFZT4OBGW/0zXrBIlLaq10?=
 =?us-ascii?q?Y4kLE2auL9Srydpkg57iRmBX9ESkB1MAw8KpfRuSb1rg3Qxfz0gXoHqnmTemwD?=
 =?us-ascii?q?xwiT0msq2f3CnWyeT4aBUHInJLRHVljVr0IYm0jsoWXUiyYwgyiRul4133x6xa?=
 =?us-ascii?q?pKR5MmnSTl1EfynwL2F+TKSwsqCObNJI6JMtqS9XSvizYUiGSr7hpBsXyyHjEH?=
 =?us-ascii?q?FExDA4dDGqvY/1nwdgh2KeL3ZzrXzZdt92xRrE4NzcRPhR3icJRSViiDnXAES8?=
 =?us-ascii?q?MMet/dmOi5jDteW+XXq7VpJPaSnr0Z+AtCyj6GxqGxK/mOq/mt/mEQg8yiL709?=
 =?us-ascii?q?hqVSPVrBfzeIXr1qK6Mf55cUlsHlPz9816GoRmmIsqmJ4QwWQahomS/Xcfkmf8?=
 =?us-ascii?q?K9Rb1bj+bHoQXzELxdHV7RPh2E1iKHKJ2o34Wm+cwstne9m1fGcW1jgh4MBNDa?=
 =?us-ascii?q?ee9KZEkjdtolqksQLRZuBwnjIcyfst8nEWme8ItxQ2ziWBHL8SB1JXMjL2lxSH?=
 =?us-ascii?q?9N2+qKRXZGCycbm/zkZ+nNahDK2crQFYQnr2ZpAiHSpo5MVlLF3MyGHz6p3jeN?=
 =?us-ascii?q?TIcdITsRiUnw3BjudPLpIxi+EKhTFmOW/muX0lyug7jQFh3J2gvYiHLXlt87y9?=
 =?us-ascii?q?AhJCKjL1YMYT8Cn3jalChsaWw5yvHpJ5FzoQRpToSvaoEC8IufTjKgaDCzk8qn?=
 =?us-ascii?q?adGbrCEg6T8kZmr3TTE5+1M3GbPmUWzdJnRBOFPkxQnBgUXCkmnp4+Dg2qxtbu?=
 =?us-ascii?q?cEJn6TAK/FL4tgFAyuF1Nxn8T2ffoh2naiwvRZibLRpW6BxC5kjPPcyf6OJzAz?=
 =?us-ascii?q?9X/pm7oAORLWybYhxCDXsVVUycG1DjIr6u6MHA8uifG+q+NubObq+IqeBETPeI?=
 =?us-ascii?q?wpSv0oR98jaIN8WPOGRiDvIh1kpCW3B5B9rWmzEVRyMLkCLNataRpA2g9S1vss?=
 =?us-ascii?q?C/7PPrVRrv5YSRCrtSMtZv+xGsjaeALe6Qgyl5JihC1pMR3n/F07wf3F8UiyFz?=
 =?us-ascii?q?eDihC7UAtSjRTK3Onq9bFQIUayR2NMFQ9aIzwhFNOdLHitPyzrN3lOM1BExfVV?=
 =?us-ascii?q?P/msClf8gKI2CmOVPDBUaLMqmGJDLRz8H2Z6O8VaNfjOFOuxKsvjabFlfpPi6f?=
 =?us-ascii?q?mDnxSxCvLeZMgTmHPBxfvYGxaApiBXL/Q9LmdBK7NsR6jTk3wb0ym3PLOnQQMT?=
 =?us-ascii?q?l6c0NRsLKQ6TlUjel4G2xE9nBlN/WLmz6F7+nELZYbqftqDT5yl+Jf4XQ6zaNa?=
 =?us-ascii?q?7SBERPNvnivSocVjo1WnkumJ1zpmXwBCqjdNhIKXo0piPb/V+YVHWXbBr1ox6j?=
 =?us-ascii?q?CyBg4NotYtJdnuoLtdgozNm6byMyxP28jZ8csVG47fL8fRY1Q7Nh+8NDfKAUMu?=
 =?us-ascii?q?TT+tM2zFzxhRme+T+lWbtJ8/rYLmmYZIQbheAg9mXsgGA1hoSYRRaKx8WSkpxP?=
 =?us-ascii?q?vC1JYF?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AFAABlrBFch0O0hNFkGQEBAQEBAQEBA?=
 =?us-ascii?q?QEBAQcBAQEBAQGBUwIBAQEBAQsBg1kSJ4x0izNQAQEGgTUUiQ2EbRGJNIFzLBM?=
 =?us-ascii?q?Bhz4iNgcNAQMBAQEBAQECARMBAQEIDQkIKS+CNiQBgmEBAQEBAgEBAiQTPwUBC?=
 =?us-ascii?q?QEBCg4KCSUDDAUoIRMFGYMDgXUFBwEFqA4ziiwijBoXeIEHhCOETYNpgiYCiRk?=
 =?us-ascii?q?gAYF2hBWRRwmRTCMKkTwsmmAMgXszGggbFYMngicXjjEsMoECAwEBAwEdEwsBi?=
 =?us-ascii?q?WArgiABAQ?=
X-IPAS-Result: =?us-ascii?q?A0AFAABlrBFch0O0hNFkGQEBAQEBAQEBAQEBAQcBAQEBAQG?=
 =?us-ascii?q?BUwIBAQEBAQsBg1kSJ4x0izNQAQEGgTUUiQ2EbRGJNIFzLBMBhz4iNgcNAQMBA?=
 =?us-ascii?q?QEBAQECARMBAQEIDQkIKS+CNiQBgmEBAQEBAgEBAiQTPwUBCQEBCg4KCSUDDAU?=
 =?us-ascii?q?oIRMFGYMDgXUFBwEFqA4ziiwijBoXeIEHhCOETYNpgiYCiRkgAYF2hBWRRwmRT?=
 =?us-ascii?q?CMKkTwsmmAMgXszGggbFYMngicXjjEsMoECAwEBAwEdEwsBiWArgiABAQ?=
X-IronPort-AV: E=Sophos;i="5.56,346,1539673200"; 
   d="scan'208";a="43847071"
X-Amp-Result: UNSCANNABLE
X-Amp-File-Uploaded: False
Unscannable: 2
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 12 Dec 2018 16:51:29 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726416AbeLMAv0 (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Wed, 12 Dec 2018 19:51:26 -0500
Received: from ipmailnode02.adl6.internode.on.net ([150.101.137.148]:34832
        "EHLO ipmailnode02.adl6.internode.on.net" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S1726311AbeLMAvZ (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Wed, 12 Dec 2018 19:51:25 -0500
Received: from ppp59-167-129-252.static.internode.on.net (HELO dastard) ([59.167.129.252])
  by ipmail02.adl6.internode.on.net with ESMTP; 13 Dec 2018 11:21:19 +1030
Received: from dave by dastard with local (Exim 4.80)
        (envelope-from <david@fromorbit.com>)
        id 1gXFDj-0000xx-2b; Thu, 13 Dec 2018 11:51:19 +1100
Date: Thu, 13 Dec 2018 11:51:19 +1100
From: Dave Chinner <david@fromorbit.com>
To: Jerome Glisse <jglisse@redhat.com>
Cc: Jan Kara <jack@suse.cz>, John Hubbard <jhubbard@nvidia.com>,
        Matthew Wilcox <willy@infradead.org>,
        Dan Williams <dan.j.williams@intel.com>,
        John Hubbard <john.hubbard@gmail.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Linux MM <linux-mm@kvack.org>, tom@talpey.com,
        Al Viro <viro@zeniv.linux.org.uk>, benve@cisco.com,
        Christoph Hellwig <hch@infradead.org>,
        Christopher Lameter <cl@linux.com>,
        "Dalessandro, Dennis" <dennis.dalessandro@intel.com>,
        Doug Ledford <dledford@redhat.com>,
        Jason Gunthorpe <jgg@ziepe.ca>,
        Michal Hocko <mhocko@kernel.org>, mike.marciniszyn@intel.com,
        rcampbell@nvidia.com,
        Linux Kernel Mailing List <linux-kernel@vger.kernel.org>,
        linux-fsdevel <linux-fsdevel@vger.kernel.org>
Subject: Re: [PATCH 1/2] mm: introduce put_user_page*(), placeholder versions
Message-ID: <20181213005119.GD29416@dastard>
References: <20181205014441.GA3045@redhat.com>
 <59ca5c4b-fd5b-1fc6-f891-c7986d91908e@nvidia.com>
 <7b4733be-13d3-c790-ff1b-ac51b505e9a6@nvidia.com>
 <20181207191620.GD3293@redhat.com>
 <3c4d46c0-aced-f96f-1bf3-725d02f11b60@nvidia.com>
 <20181208022445.GA7024@redhat.com>
 <20181210102846.GC29289@quack2.suse.cz>
 <20181212150319.GA3432@redhat.com>
 <20181212214641.GB29416@dastard>
 <20181212215931.GG5037@redhat.com>
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: <20181212215931.GG5037@redhat.com>
User-Agent: Mutt/1.5.21 (2010-09-15)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On Wed, Dec 12, 2018 at 04:59:31PM -0500, Jerome Glisse wrote:
> On Thu, Dec 13, 2018 at 08:46:41AM +1100, Dave Chinner wrote:
> > On Wed, Dec 12, 2018 at 10:03:20AM -0500, Jerome Glisse wrote:
> > > On Mon, Dec 10, 2018 at 11:28:46AM +0100, Jan Kara wrote:
> > > > On Fri 07-12-18 21:24:46, Jerome Glisse wrote:
> > > > So this approach doesn't look like a win to me over using counter in struct
> > > > page and I'd rather try looking into squeezing HMM public page usage of
> > > > struct page so that we can fit that gup counter there as well. I know that
> > > > it may be easier said than done...
> > > 
> > > So i want back to the drawing board and first i would like to ascertain
> > > that we all agree on what the objectives are:
> > > 
> > >     [O1] Avoid write back from a page still being written by either a
> > >          device or some direct I/O or any other existing user of GUP.

IOWs, you need to mark pages being written to by a GUP as
PageWriteback, so all attempts to write the page will block on
wait_on_page_writeback() before trying to write the dirty page.

> > >          This would avoid possible file system corruption.

This isn't a filesystem corruption vector. At worst, it could cause
torn data writes due to updating the page while it is under IO. We
have a name for this: "stable pages". This is designed to prevent
updates to pages via mmap writes from causing corruption of things
like MD RAID due to modification of the data during RAID parity
calculations. Hence we have wait_for_stable_page() calls in all
->page_mkwrite implementations so that new mmap writes block until
writeback IO is complete on the devices that require stable pages
to prevent corruption.

IOWs, we already deal with this "delay new modification while
writeback is in progress" problem in the mmap/filesystem world and
have infrastructure to handle it. And the ->page_mkwrite code
already deals with it.

> > > 
> > >     [O2] Avoid crash when set_page_dirty() is call on a page that is
> > >          considered clean by core mm (buffer head have been remove and
> > >          with some file system this turns into an ugly mess).
> > 
> > I think that's wrong. This isn't an "avoid a crash" case, this is a
> > "prevent data and/or filesystem corruption" case. The primary goal
> > we have here is removing our exposure to potential corruption, which
> > has the secondary effect of avoiding the crash/panics that currently
> > occur as a result of inconsistent page/filesystem state.
> 
> This is O1 avoid corruption is O1

It's "avoid a specific instance of data corruption", not a general
mechanism for avoiding data/filesystem corruption.

Calling set_page_dirty() on a file backed page which has not been
correctly prepared can cause data corruption, filesystem coruption
and shutdowns, etc because we have dirty data over a region that is
not correctly mapped. Yes, it can also cause a crash (because we
really, really suck at validation and error handling in generic code
paths), but there's so, so much more that can go wrong than crash
the kernel when we do stupid shit like this.

> > i.e. The goal is to have ->page_mkwrite() called on the clean page
> > /before/ the file-backed page is marked dirty, and hence we don't
> > expose ourselves to potential corruption or crashes that are a
> > result of inappropriately calling set_page_dirty() on clean
> > file-backed pages.
> 
> Yes and this would be handle by put_user_page ie:

No, put_user_page() is too late - it's after the DMA has completed,
but we have to ensure the file has backing store allocated and the
pages are in the correct state /before/ the DMA is done.

Think ENOSPC - that has to be handled before we do the DMA, not
after. Before the DMA it is a recoverable error, after the DMA it is
data loss/corruption failure.

> put_user_page(struct page *page, bool dirty)
> {
>     if (!PageAnon(page)) {
>         if (dirty) {
>             // Do the whole dance ie page_mkwrite and all before
>             // calling set_page_dirty()
>         }
>         ...
>     }
>     ...
> }

Essentially, doing this would require a whole new "dirty a page"
infrastructure because it is in the IO path, not the page fault
path.

And, for hardware that does it's own page faults for DMA, this whole
post-DMA page setup is broken because the pages will have already
gone through ->page_mkwrite() and be set up correctly already.

> > > For [O2] i believe we can handle that case in the put_user_page()
> > > function to properly dirty the page without causing filesystem
> > > freak out.
> > 
> > I'm pretty sure you can't call ->page_mkwrite() from
> > put_user_page(), so I don't think this is workable at all.
> 
> Hu why ? i can not think of any reason whike you could not. User of

It's not a fault path, you can't safely lock pages, you can't take
fault-path only locks in the IO path (mmap_sem inversion problems),
etc.

/me has a nagging feeling this was all explained in a previous
discussions of this patchset...

Cheers,

Dave.
-- 
Dave Chinner
david@fromorbit.com
