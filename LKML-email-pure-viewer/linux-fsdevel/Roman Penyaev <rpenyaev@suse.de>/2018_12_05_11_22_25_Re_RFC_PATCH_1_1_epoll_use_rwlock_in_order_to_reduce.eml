Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 06 Dec 2018 08:51:20 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from FMSMGA003.fm.intel.com (fmsmga003.fm.intel.com [10.253.24.29])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 839905804F9;
	Wed,  5 Dec 2018 03:22:33 -0800 (PST)
Received: from fmsmga102.fm.intel.com ([10.1.193.69])
  by FMSMGA003-1.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 05 Dec 2018 03:22:33 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AlreVKR1SuguHUwppsmDT+DRfVm0co7zxezQtwd8Z?=
 =?us-ascii?q?segSI/nxwZ3uMQTl6Ol3ixeRBMOHs6IC07KempujcFRI2YyGvnEGfc4EfD4+ou?=
 =?us-ascii?q?JSoTYdBtWYA1bwNv/gYn9yNs1DUFh44yPzahANS47xaFLIv3K98yMZFAnhOgpp?=
 =?us-ascii?q?POT1HZPZg9iq2+yo9JDffwZFiCChbb9uMR67sRjfus4KjIV4N60/0AHJonxGe+?=
 =?us-ascii?q?RXwWNnO1eelAvi68mz4ZBu7T1et+ou+MBcX6r6eb84TaFDAzQ9L281/szrugLd?=
 =?us-ascii?q?QgaJ+3ART38ZkhtMAwjC8RH6QpL8uTb0u+ZhxCWXO9D9QKsqUjq+8ahkVB7oiD?=
 =?us-ascii?q?8GNzEn9mHXltdwh79frB64uhBz35LYbISTOfFjfK3SYMkaSHJOUclNWCJPDIOy?=
 =?us-ascii?q?YZUSAeUDM+ZWrIb8qFUVrRumBwShH//vxz1KhnPqwaE3yfotHAfb1wIgBdIOt3?=
 =?us-ascii?q?HUoc3pOqcVVOC61q/IxijZYfxI3zf86JLHfQ4/ofqRWr9wa9LRxlcpFwLFlFqf?=
 =?us-ascii?q?t5LlMiiO1uQQqGiU8fBsWv+xhGM+rQx6vzahxsApiobTh4IVzEjJ9SF+wIkpJt?=
 =?us-ascii?q?20UlN7YcW8HJtftiGaK4t2Qt45TG1ypCk6zbgGtJimdyYJ0JQq3wDTZ+CDfoSS?=
 =?us-ascii?q?/x7uWvidLS1liH9mZL6znRe//Eq4xuHhUsS50ExGojdLn9TMrHwByQLf5tSdRv?=
 =?us-ascii?q?dj/kqs1jCC3B3J5O5eO0A7j6/bJoYhwrEukpoTtlzOHjH5mErolq+abEYk9fay?=
 =?us-ascii?q?6+ThfLrmooWQN4huigHxKqgum8q/DvokMgUWQWSX5eCx2Kf+8UD3XrlGlOA6n6?=
 =?us-ascii?q?rFvJzAJMkWpLa1AwpP3YYi7xa/AS2m0NMdnXQfKFJFeRSHj5XmOl3XI/D3E+2/?=
 =?us-ascii?q?g1Kynzdv3vzGObPgApPTIXjZi7rhY7l95FBGyAYpztBQ+YhUCrcfL/L3QEPxs8?=
 =?us-ascii?q?bYDhAhPwyu3+nnEMl91p8ZWW+XBq+ZMaDSvkGS6uMgPumBf4sVuDf7K/g46P/i?=
 =?us-ascii?q?l345mVkBfaa32Zsbcmy3HvNjI0+Be3rjns8BEXsWvgo5VOHqiEeNUT9PZ3moWK?=
 =?us-ascii?q?Iw/DE7CJ+8AofFSYCgm7iB3Ca9Hp1LaWFKEFGMEXH0d4qaX/cAcj6dIshkkjYc?=
 =?us-ascii?q?T7iuV5ch1Q2ytA/907dnLO3U9TMCuZLg09h14evTlRYp+DxwDsSd1XyNTm5ukm?=
 =?us-ascii?q?MJQT82wL5woUhnxlif1qh4huRSFcZP6PNRTgc6KZncwvRgBNDpWgLBedSJREy8?=
 =?us-ascii?q?Qtq8AzE8VdYxw94IY0ZgFNSulBHD3yy2A7ALk7yHHoA78qXZ33LpPcZy127G1L?=
 =?us-ascii?q?U9j1khWsZPNnephqhl+wjXBo7GiUOZl6mxeKQY3S7N8nqDzGWUsEFZVg5wTbvK?=
 =?us-ascii?q?XXQFakTKqtT541vIT6WyBrQ/LgtB1cmCJ7NXZdLzk1VJWu3vONTEbGK3gGe/Gx?=
 =?us-ascii?q?CIyrSIbIrpfmUd2D7QCEwFkwAV4HaHOhIyBiano2LCEjNuEUjjbF/r8el7sHm7?=
 =?us-ascii?q?VFM7zxmWb0190Lq44gMaiuaCS/wNxL4EuD0uqzNvHFmj2dLbEN6ApwtnfKVBbt?=
 =?us-ascii?q?Ix+lZH1WTFtwNjOpysNbxthlkbcw5vpUPhyw13CplckcgttH4q0AtyKaef0FNd?=
 =?us-ascii?q?dzOZ3Yr8OqHNJmn15hCvb6/W2lfR0NuN/qcP6fI4q0jsvQ2zF0oi9Wln3MdR03?=
 =?us-ascii?q?eG+prKCw8SW4rrUkkr7xh6u63aYi4l6ozO0X1jL6a1vSHC2t4zH+Ql1wuvcM1Z?=
 =?us-ascii?q?MKOHEw/yDdYXB8yvKOwshlioYQgIPOFU9K4oIcymc+GK17KsPOZlhDiml3hI4J?=
 =?us-ascii?q?hh0kKQ8CpxUu7J34sfz/6ExAeGVjf8g02nssD4go1EYTASHmyiySnrHoJRZ6ty?=
 =?us-ascii?q?fZoVBmeqOcG42tJ+h5v1UX5C6FGjH08G2NOueReKdVz93BFf2l4NoXO6niu01T?=
 =?us-ascii?q?p0nC8zrqqexSDB3/7tdB4aNWFVXmliilHsLJOwj9AbWkiocgcomAGk5Ub826hU?=
 =?us-ascii?q?uqB/I3PPTkdPeij8N3tiXbeotrqef85P74slsCVWUOimYVGWUKX9owYc0y74G2?=
 =?us-ascii?q?texTY7dyylu5njnhx6jn6dI2h3rHbDZc5wwhLf7sTGRfFNxjoGWDV4iT7PC1i+?=
 =?us-ascii?q?Jdap59aUl5TEsuykTGKuTJ5TcSrqzYOGqiS743ZnARm+n/C1h93mHhI20S7919?=
 =?us-ascii?q?l2SyrIqAzwbZXs16S/KehnZFVnBEfg68pmHYFziosxi4sX2XQAhJWV/HwHnHz3?=
 =?us-ascii?q?MdVa36L+cXUMSSQKw97T/AjqxkljImiVyIL+U3WX2tFhaMWiYmMKxiI96NhHCK?=
 =?us-ascii?q?SO47xFhyd1oke4ohjXYfhyhTodzfou6HgHg+AGogYtzyOdAqwMEklcJyDjixOI?=
 =?us-ascii?q?79WmpqVNeGmvaaSw1FZ5nd25CbGCowJcV2zjdpYsAyBw9dl/P0zW0HLo9I7kYt?=
 =?us-ascii?q?bQYMkXth2VlRfAkudUJIgwlvoMmSpoJ2b9sWc5xO48iBxkxYu6s5SfK2Vx4KK5?=
 =?us-ascii?q?BQZVNiHyZ8MW4D3sjLxRnsCL34C0BZVhGy4GXJ/pTfKuDTITuu7rNweIED0gtH?=
 =?us-ascii?q?ibHaDTEhOY6Edjt3jPCYykN2mLJHkFytVvXAKSK1ZYgAAQQTU2hJo5Fh2xyczl?=
 =?us-ascii?q?cUd54C0R51HiphtNzOJoKwfwUmPFqAi0bTc0TYCVLABK4QFa+0fVLcue4/pxHy?=
 =?us-ascii?q?5C+52usheNJnaHZwhSDmEJR0+EB03lPra0/tnN6OyYBuu4L/vTbrSCs+1eV/GU?=
 =?us-ascii?q?xZ2x1otq5SqDNsKKPnN6Ff00xlJDXWxlG8TegzgOSysXlz/Ub86Gvhi8/Dd7rt?=
 =?us-ascii?q?u48PTtVwLi/o+PC7pUMdVy9BG6m6aDN+iMhClnLTZUzI8DxXjNyLIHxl4dlzlu?=
 =?us-ascii?q?dyWxEbQHrSPNTLzfmq5UDx4YaiN/LshI77g73glCJ8HbkM711qVjg/4xCldFU0?=
 =?us-ascii?q?Hhm86zacwLJWG9KE3IBEKROLuaIj3Lxtn9YbmgRr1IkOVUqxqwtC6BHE/+JTuD?=
 =?us-ascii?q?jSfmVhC1PeFIjSGWJxheuIC7chZwBmnvVtPmahunMNBpiT0626E7hnTPNWQEKz?=
 =?us-ascii?q?hzb1tNrqGM7SNfmvh/B21B7n9/IeaYgSqW8+nYJYgQsfZwBiR0luRa4Gk1yrdP?=
 =?us-ascii?q?7SFEQuB1lzXWrtJ0v16mleyPwCJ9UBVSsjZLmJ6LvUJ6NKTZ7JZAX3PE/BML7W?=
 =?us-ascii?q?qIChUKp8FqCtvgu61LztjPlaTzKCpN8t7O/MscAdTUJ9yDMHY7LRXpHzvUX0M5?=
 =?us-ascii?q?SmuzPHzbgkdeuPWT8GCF6Jkwr97nn59dZKVcUQkPEfYTDAxdEdoTKZppFmc+ib?=
 =?us-ascii?q?OdnccF+1KlsQLcAslX6MOUHsmOCOnifW7KxYJPYAEFlPahddwe?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0CPAgB+tAdch0O0hNFkHgEGBwaBZQKCa?=
 =?us-ascii?q?IECJ5gggg0Ukj+GbykLCAGEQIMSIjgSAQMBAQEBAQECARMBAQEIDQkIKSMMQgE?=
 =?us-ascii?q?QAYFiJAGCYQEBAQECAQECJBECPwYJAQEDBxgcEgNUEAkFA4MZAYF5CAQBCqVyM?=
 =?us-ascii?q?4omBYkEgxoXgUA/gRGDEoMeAoFhhVsCiQ8khkiQBg9GCYEVhW6DNIMgg2MIGIF?=
 =?us-ascii?q?bI4RxgnwrhxkshB+JJ4xpgXYzGggcFDuCbIInFxKDOIpUPzIBgQQBAYIdiQABA?=
 =?us-ascii?q?Q?=
X-IPAS-Result: =?us-ascii?q?A0CPAgB+tAdch0O0hNFkHgEGBwaBZQKCaIECJ5gggg0Ukj+?=
 =?us-ascii?q?GbykLCAGEQIMSIjgSAQMBAQEBAQECARMBAQEIDQkIKSMMQgEQAYFiJAGCYQEBA?=
 =?us-ascii?q?QECAQECJBECPwYJAQEDBxgcEgNUEAkFA4MZAYF5CAQBCqVyM4omBYkEgxoXgUA?=
 =?us-ascii?q?/gRGDEoMeAoFhhVsCiQ8khkiQBg9GCYEVhW6DNIMgg2MIGIFbI4Rxgnwrhxksh?=
 =?us-ascii?q?B+JJ4xpgXYzGggcFDuCbIInFxKDOIpUPzIBgQQBAYIdiQABAQ?=
X-IronPort-AV: E=Sophos;i="5.56,317,1539673200"; 
   d="scan'208";a="55607628"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 05 Dec 2018 03:22:31 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1727452AbeLELW3 (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Wed, 5 Dec 2018 06:22:29 -0500
Received: from mx2.suse.de ([195.135.220.15]:58848 "EHLO mx1.suse.de"
        rhost-flags-OK-OK-OK-FAIL) by vger.kernel.org with ESMTP
        id S1726866AbeLELW2 (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Wed, 5 Dec 2018 06:22:28 -0500
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay1.suse.de (unknown [195.135.220.254])
        by mx1.suse.de (Postfix) with ESMTP id 8368EAF89;
        Wed,  5 Dec 2018 11:22:25 +0000 (UTC)
MIME-Version: 1.0
Content-Type: text/plain; charset=US-ASCII;
 format=flowed
Content-Transfer-Encoding: 7bit
Date: Wed, 05 Dec 2018 12:22:25 +0100
From: Roman Penyaev <rpenyaev@suse.de>
To: paulmck@linux.ibm.com
Cc: Jason Baron <jbaron@akamai.com>,
        Alexander Viro <viro@zeniv.linux.org.uk>,
        Linus Torvalds <torvalds@linux-foundation.org>,
        linux-fsdevel@vger.kernel.org, linux-kernel@vger.kernel.org
Subject: Re: [RFC PATCH 1/1] epoll: use rwlock in order to reduce
 ep_poll_callback() contention
In-Reply-To: <20181204190206.GB4170@linux.ibm.com>
References: <20181203110237.14787-1-rpenyaev@suse.de>
 <45bce871-edfd-c402-acde-2e57e80cc522@akamai.com>
 <20181204190206.GB4170@linux.ibm.com>
Message-ID: <5bca219db9546e863c667d51316c9b80@suse.de>
X-Sender: rpenyaev@suse.de
User-Agent: Roundcube Webmail
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On 2018-12-04 20:02, Paul E. McKenney wrote:
> On Tue, Dec 04, 2018 at 12:23:08PM -0500, Jason Baron wrote:
>> 
>> 
>> On 12/3/18 6:02 AM, Roman Penyaev wrote:
>> > Hi all,
>> >
>> > The goal of this patch is to reduce contention of ep_poll_callback() which
>> > can be called concurrently from different CPUs in case of high events
>> > rates and many fds per epoll.  Problem can be very well reproduced by
>> > generating events (write to pipe or eventfd) from many threads, while
>> > consumer thread does polling.  In other words this patch increases the
>> > bandwidth of events which can be delivered from sources to the poller by
>> > adding poll items in a lockless way to the list.
>> >
>> > The main change is in replacement of the spinlock with a rwlock, which is
>> > taken on read in ep_poll_callback(), and then by adding poll items to the
>> > tail of the list using xchg atomic instruction.  Write lock is taken
>> > everywhere else in order to stop list modifications and guarantee that list
>> > updates are fully completed (I assume that write side of a rwlock does not
>> > starve, it seems qrwlock implementation has these guarantees).
>> >
>> > The following are some microbenchmark results based on the test [1] which
>> > starts threads which generate N events each.  The test ends when all
>> > events are successfully fetched by the poller thread:
>> >
>> > spinlock
>> > ========
>> >
>> > threads       run time    events per ms
>> > -------       ---------   -------------
>> >       8         13191ms         6064/ms
>> >      16         30758ms         5201/ms
>> >      32         44315ms         7220/ms
>> >
>> > rwlock + xchg
>> > =============
>> >
>> > threads       run time    events per ms
>> > -------       ---------   -------------
>> >       8          8581ms         9323/ms
>> >      16         13800ms        11594/ms
>> >      32         24167ms        13240/ms
>> >
>> > According to the results bandwidth of delivered events is significantly
>> > increased, thus execution time is reduced.
>> >
>> > This is RFC because I did not run any benchmarks comparing current
>> > qrwlock and spinlock implementations (4.19 kernel), although I did
>> > not notice any epoll performance degradations in other benchmarks.
>> >
>> > Also I'm not quite sure where to put very special lockless variant
>> > of adding element to the list (list_add_tail_lockless() in this
>> > patch).  Seems keeping it locally is safer.
>> >
>> > [1] https://github.com/rouming/test-tools/blob/master/stress-epoll.c
>> >
>> > Signed-off-by: Roman Penyaev <rpenyaev@suse.de>
>> > Cc: Alexander Viro <viro@zeniv.linux.org.uk>
>> > Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
>> > Cc: Linus Torvalds <torvalds@linux-foundation.org>
>> > Cc: linux-fsdevel@vger.kernel.org
>> > Cc: linux-kernel@vger.kernel.org
>> > ---
>> >  fs/eventpoll.c | 107 +++++++++++++++++++++++++++++++------------------
>> >  1 file changed, 69 insertions(+), 38 deletions(-)
>> >
>> > diff --git a/fs/eventpoll.c b/fs/eventpoll.c
>> > index 42bbe6824b4b..89debda47aca 100644
>> > --- a/fs/eventpoll.c
>> > +++ b/fs/eventpoll.c
>> > @@ -50,10 +50,10 @@
>> >   *
>> >   * 1) epmutex (mutex)
>> >   * 2) ep->mtx (mutex)
>> > - * 3) ep->wq.lock (spinlock)
>> > + * 3) ep->lock (rwlock)
>> >   *
>> >   * The acquire order is the one listed above, from 1 to 3.
>> > - * We need a spinlock (ep->wq.lock) because we manipulate objects
>> > + * We need a rwlock (ep->lock) because we manipulate objects
>> >   * from inside the poll callback, that might be triggered from
>> >   * a wake_up() that in turn might be called from IRQ context.
>> >   * So we can't sleep inside the poll callback and hence we need
>> > @@ -85,7 +85,7 @@
>> >   * of epoll file descriptors, we use the current recursion depth as
>> >   * the lockdep subkey.
>> >   * It is possible to drop the "ep->mtx" and to use the global
>> > - * mutex "epmutex" (together with "ep->wq.lock") to have it working,
>> > + * mutex "epmutex" (together with "ep->lock") to have it working,
>> >   * but having "ep->mtx" will make the interface more scalable.
>> >   * Events that require holding "epmutex" are very rare, while for
>> >   * normal operations the epoll private "ep->mtx" will guarantee
>> > @@ -182,8 +182,6 @@ struct epitem {
>> >   * This structure is stored inside the "private_data" member of the file
>> >   * structure and represents the main data structure for the eventpoll
>> >   * interface.
>> > - *
>> > - * Access to it is protected by the lock inside wq.
>> >   */
>> >  struct eventpoll {
>> >  	/*
>> > @@ -203,13 +201,16 @@ struct eventpoll {
>> >  	/* List of ready file descriptors */
>> >  	struct list_head rdllist;
>> >
>> > +	/* Lock which protects rdllist and ovflist */
>> > +	rwlock_t lock;
>> > +
>> >  	/* RB tree root used to store monitored fd structs */
>> >  	struct rb_root_cached rbr;
>> >
>> >  	/*
>> >  	 * This is a single linked list that chains all the "struct epitem" that
>> >  	 * happened while transferring ready events to userspace w/out
>> > -	 * holding ->wq.lock.
>> > +	 * holding ->lock.
>> >  	 */
>> >  	struct epitem *ovflist;
>> >
>> > @@ -697,17 +698,17 @@ static __poll_t ep_scan_ready_list(struct eventpoll *ep,
>> >  	 * because we want the "sproc" callback to be able to do it
>> >  	 * in a lockless way.
>> >  	 */
>> > -	spin_lock_irq(&ep->wq.lock);
>> > +	write_lock_irq(&ep->lock);
>> >  	list_splice_init(&ep->rdllist, &txlist);
>> >  	ep->ovflist = NULL;
>> > -	spin_unlock_irq(&ep->wq.lock);
>> > +	write_unlock_irq(&ep->lock);
>> >
>> >  	/*
>> >  	 * Now call the callback function.
>> >  	 */
>> >  	res = (*sproc)(ep, &txlist, priv);
>> >
>> > -	spin_lock_irq(&ep->wq.lock);
>> > +	write_lock_irq(&ep->lock);
>> >  	/*
>> >  	 * During the time we spent inside the "sproc" callback, some
>> >  	 * other events might have been queued by the poll callback.
>> > @@ -722,7 +723,8 @@ static __poll_t ep_scan_ready_list(struct eventpoll *ep,
>> >  		 * contain them, and the list_splice() below takes care of them.
>> >  		 */
>> >  		if (!ep_is_linked(epi)) {
>> > -			list_add_tail(&epi->rdllink, &ep->rdllist);
>> > +			/* Reverse ->ovflist, events should be in FIFO */
>> > +			list_add(&epi->rdllink, &ep->rdllist);
>> >  			ep_pm_stay_awake(epi);
>> >  		}
>> >  	}
>> > @@ -745,11 +747,11 @@ static __poll_t ep_scan_ready_list(struct eventpoll *ep,
>> >  		 * the ->poll() wait list (delayed after we release the lock).
>> >  		 */
>> >  		if (waitqueue_active(&ep->wq))
>> > -			wake_up_locked(&ep->wq);
>> > +			wake_up(&ep->wq);
>> >  		if (waitqueue_active(&ep->poll_wait))
>> >  			pwake++;
>> >  	}
>> > -	spin_unlock_irq(&ep->wq.lock);
>> > +	write_unlock_irq(&ep->lock);
>> >
>> >  	if (!ep_locked)
>> >  		mutex_unlock(&ep->mtx);
>> > @@ -789,10 +791,10 @@ static int ep_remove(struct eventpoll *ep, struct epitem *epi)
>> >
>> >  	rb_erase_cached(&epi->rbn, &ep->rbr);
>> >
>> > -	spin_lock_irq(&ep->wq.lock);
>> > +	write_lock_irq(&ep->lock);
>> >  	if (ep_is_linked(epi))
>> >  		list_del_init(&epi->rdllink);
>> > -	spin_unlock_irq(&ep->wq.lock);
>> > +	write_unlock_irq(&ep->lock);
>> >
>> >  	wakeup_source_unregister(ep_wakeup_source(epi));
>> >  	/*
>> > @@ -842,7 +844,7 @@ static void ep_free(struct eventpoll *ep)
>> >  	 * Walks through the whole tree by freeing each "struct epitem". At this
>> >  	 * point we are sure no poll callbacks will be lingering around, and also by
>> >  	 * holding "epmutex" we can be sure that no file cleanup code will hit
>> > -	 * us during this operation. So we can avoid the lock on "ep->wq.lock".
>> > +	 * us during this operation. So we can avoid the lock on "ep->lock".
>> >  	 * We do not need to lock ep->mtx, either, we only do it to prevent
>> >  	 * a lockdep warning.
>> >  	 */
>> > @@ -1023,6 +1025,7 @@ static int ep_alloc(struct eventpoll **pep)
>> >  		goto free_uid;
>> >
>> >  	mutex_init(&ep->mtx);
>> > +	rwlock_init(&ep->lock);
>> >  	init_waitqueue_head(&ep->wq);
>> >  	init_waitqueue_head(&ep->poll_wait);
>> >  	INIT_LIST_HEAD(&ep->rdllist);
>> > @@ -1112,10 +1115,38 @@ struct file *get_epoll_tfile_raw_ptr(struct file *file, int tfd,
>> >  }
>> >  #endif /* CONFIG_CHECKPOINT_RESTORE */
>> >
>> > +/*
>> > + * Adds a new entry to the tail of the list in a lockless way, i.e.
>> > + * multiple CPUs are allowed to call this function concurrently.
>> > + *
>> > + * Beware: it is necessary to prevent any other modifications of the
>> > + *         existing list until all changes are completed, in other words
>> > + *         concurrent list_add_tail_lockless() calls should be protected
>> > + *         with a read lock, where write lock acts as a barrier which
>> > + *         makes sure all list_add_tail_lockless() calls are fully
>> > + *         completed.
>> > + */
>> > +static inline void list_add_tail_lockless(struct list_head *new,
>> > +					  struct list_head *head)
>> > +{
>> > +	struct list_head *prev;
>> > +
>> > +	new->next = head;
>> > +	prev = xchg(&head->prev, new);
>> > +	prev->next = new;
>> > +	new->prev = prev;
>> > +}
>> > +
>> >  /*
>> >   * This is the callback that is passed to the wait queue wakeup
>> >   * mechanism. It is called by the stored file descriptors when they
>> >   * have events to report.
>> > + *
>> > + * This callback takes a read lock in order not to content with concurrent
>> > + * events from another file descriptors, thus all modifications to ->rdllist
>> > + * or ->ovflist are lockless.  Read lock is paired with the write lock from
>> > + * ep_scan_ready_list(), which stops all list modifications and guarantees
>> > + * that lists won't be corrupted.
>> >   */
>> >  static int ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
>> >  {
>> > @@ -1126,7 +1157,7 @@ static int ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, v
>> >  	__poll_t pollflags = key_to_poll(key);
>> >  	int ewake = 0;
>> >
>> > -	spin_lock_irqsave(&ep->wq.lock, flags);
>> > +	read_lock_irqsave(&ep->lock, flags);
>> >
>> >  	ep_set_busy_poll_napi_id(epi);
>> >
>> > @@ -1156,8 +1187,8 @@ static int ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, v
>> >  	 */
>> >  	if (unlikely(ep->ovflist != EP_UNACTIVE_PTR)) {
>> >  		if (epi->next == EP_UNACTIVE_PTR) {
>> > -			epi->next = ep->ovflist;
>> > -			ep->ovflist = epi;
>> > +			/* Atomically exchange tail */
>> > +			epi->next = xchg(&ep->ovflist, epi);
>> 
>> This also relies on the fact that the same epi can't be added to the
>> list in parallel, because the wait queue doing the wakeup will have 
>> the
>> wait_queue_head lock. That was a little confusing for me b/c we only 
>> had
>> the read_lock() above.
> 
> I missed this as well.
> 
> I was also concerned about "fly-by" wakeups where the to-be-awoken task
> never really goes to sleep, but it looks like tasks are unconditionally
> queued, which seems like it should avoid that problem.
> 
> Please do some testing with artificial delays in the lockless queuing
> code, for example, via udelay() or similar.  If there are races, this
> would help force them to happen.

Yep, that what I am doing right now, experimenting with different
polling scenarios and stressing with random delays.  Thanks.

--
Roman

