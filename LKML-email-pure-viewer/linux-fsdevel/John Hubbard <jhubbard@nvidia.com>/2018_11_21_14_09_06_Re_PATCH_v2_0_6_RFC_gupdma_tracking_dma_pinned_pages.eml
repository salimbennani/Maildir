Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from pop3.zju.edu.cn (61.164.42.155:110) by
  likexu-MOBL1.ccr.corp.intel.com with POP3; 21 Nov 2018 10:59:22 -0000
Received: from icoremail.net (unknown [209.85.214.174])
	by mail-app2 (Coremail) with SMTP id by_KCgDXvwqd9vRbWWLFAQ--.57404S3;
	Wed, 21 Nov 2018 14:09:34 +0800 (CST)
Received: from mail-pl1-f174.google.com (unknown [209.85.214.174])
	by mx2.icoremail.net (Coremail) with SMTP id AQAAfwB3Wkab9vRb17xhAA--.8434S3;
	Wed, 21 Nov 2018 14:09:31 +0800 (CST)
Received: by mail-pl1-f174.google.com with SMTP id u6so3893526plm.8
        for <xuliker@zju.edu.cn>; Tue, 20 Nov 2018 22:09:31 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:delivered-to:subject:to:cc:references:from
         :message-id:date:user-agent:mime-version:in-reply-to
         :content-language:content-transfer-encoding:dkim-signature:sender
         :precedence:list-id;
        bh=VsYY+pZmJupKW3/8MneAkLYd+1iL+ulPPvbmpYRVJjo=;
        b=NXjj8sA3+EkTVoFO8yIjpP63FfGNXOp5jL7u+7IjutvzmPqjNdVzvyLqFedgOB9qej
         VQlSeKop4Mw+0+s7lYD/o+Qhw2Xg1W0RkXjSr7TZ4CZSr7GfPi5uQR5BUThX6oG3clcy
         qBV50Q1dl+p4KG3cOgbRnLLA5pxudrODsL/Tg2n8VyShmhf0LvIskhI7BI8wRiu5FP6t
         Wy7QRbLpNDCDBm2a1WhR25vuqvdba3R92ueH8emwZN8ur0aq3UVg3ZKtj5Sb+NyIAx0d
         yoDqw5eOWCYlYpvUscG5uJUk/cg2X3/x/b6LYeQ9UE6rI8kViIQKUzwhwb+MBMiNwuEL
         tV3g==
X-Gm-Message-State: AA+aEWZL894QjWa95Y1FjvXH/MRyounZ01s8YEKb/vkzOgndrcMBjzR5
	HjoVpuhH4OOVg2MpOTqr4lxitw1DxIyIUVq9g4KmvF4UhAKPsVo=
X-Received: by 2002:a17:902:166:: with SMTP id 93-v6mr5339015plb.68.1542780571045;
        Tue, 20 Nov 2018 22:09:31 -0800 (PST)
X-Forwarded-To: xuliker@zju.edu.cn
X-Forwarded-For: liker.xu@gmail.com xuliker@zju.edu.cn
Delivered-To: liker.xu@gmail.com
Received: by 2002:a17:90a:d106:0:0:0:0 with SMTP id l6-v6csp1545269pju;
        Tue, 20 Nov 2018 22:09:29 -0800 (PST)
X-Google-Smtp-Source: AFSGD/VtfO6aidL9xxwZW0HPC6x97XgrB1RzjO43YhgPuA++dzOPy9JVnvdZibyYntZKtcUXrhdr
X-Received: by 2002:a17:902:aa84:: with SMTP id d4-v6mr5509222plr.25.1542780569689;
        Tue, 20 Nov 2018 22:09:29 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1542780569; cv=none;
        d=google.com; s=arc-20160816;
        b=g4nIG1iJ4d2fVObymS5gMQwd67DKDNhHT7PTUK4KOdBmxrpJrGXcn3KlHvjnRMc/9B
         lVVfHRBBKmfSRAqCKwHDO1m596ix5cWqyJt9CVmjHDlZs6UJlcFOg/trIacwWyaBLORO
         aiPTUS1slyfdUblicqwM6gLtr0iEoc24ruLwlTBx6A1uutKsyd8smxDkR9j5OQTYQWRw
         QdG13qVF/WbirbKrWX2SsKDtmwBYmClGQEjPMv2Mtw4QEBe6AsuMc33zbvRmtxwVRfaA
         AZWXzcCCE2Y61Xj7W1/oLEij+hY681GzAsLLVp4oN9uI1GDC2mB6G2fmaHxdWZ0FZX30
         x5Qg==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=list-id:precedence:sender:dkim-signature:content-transfer-encoding
         :content-language:in-reply-to:mime-version:user-agent:date
         :message-id:from:references:cc:to:subject;
        bh=VsYY+pZmJupKW3/8MneAkLYd+1iL+ulPPvbmpYRVJjo=;
        b=Hz1u/dxFmYjzIEtAAtZaHuiNBXvfAA+Vsd53RGgxnGvWXZToJoQd38xgAR9FXOSICP
         le4OQEHF/H11LU27ns/e4Xopq+NNjkC8BmwNvrTayABp1gAojWxTXnG/Wy7ZYVOOJK7p
         O44uXBl9rPmzfUyrYlnywtT0/WlJjQYzclkjQjbOAxLW9S9Qv8JUdsYmcbzTmv7G4WLN
         Ry66CizUDShLbdQyBiT3O6L5Se5MGuPpFQce4Ks8uLkqUjB8OBfd5Bo2WLIsWhRl6zns
         6RcRLm3LxHrVY+rI1WGnNsVM+LipfdDXxX8eYElu5e0mQGwxx7UyXJXb9/wda8P+rvDo
         rbUw==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass header.i=@nvidia.com header.s=n1 header.b="Sp2/9xy+";
       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org;
       dmarc=pass (p=NONE sp=NONE dis=NONE) header.from=nvidia.com
Received: from vger.kernel.org (vger.kernel.org. [209.132.180.67])
        by mx.google.com with ESMTP id i12si29600601pgq.466.2018.11.20.22.09.15;
        Tue, 20 Nov 2018 22:09:29 -0800 (PST)
Received-SPF: pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) client-ip=209.132.180.67;
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1727728AbeKUQmN (ORCPT <rfc822;nullbytes00@gmail.com>
        + 99 others); Wed, 21 Nov 2018 11:42:13 -0500
Received: from hqemgate15.nvidia.com ([216.228.121.64]:10513 "EHLO
        hqemgate15.nvidia.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1725939AbeKUQmN (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Wed, 21 Nov 2018 11:42:13 -0500
Received: from hqpgpgate102.nvidia.com (Not Verified[216.228.121.13]) by hqemgate15.nvidia.com (using TLS: TLSv1.2, DES-CBC3-SHA)
        id <B5bf4f6750000>; Tue, 20 Nov 2018 22:08:53 -0800
Received: from hqmail.nvidia.com ([172.20.161.6])
  by hqpgpgate102.nvidia.com (PGP Universal service);
  Tue, 20 Nov 2018 22:09:07 -0800
X-PGP-Universal: processed;
        by hqpgpgate102.nvidia.com on Tue, 20 Nov 2018 22:09:07 -0800
Received: from [10.110.48.28] (10.124.1.5) by HQMAIL101.nvidia.com
 (172.20.187.10) with Microsoft SMTP Server (TLS) id 15.0.1395.4; Wed, 21 Nov
 2018 06:09:07 +0000
Subject: Re: [PATCH v2 0/6] RFC: gup+dma: tracking dma-pinned pages
To: Tom Talpey <tom@talpey.com>, <john.hubbard@gmail.com>,
        <linux-mm@kvack.org>
CC: Andrew Morton <akpm@linux-foundation.org>,
        LKML <linux-kernel@vger.kernel.org>,
        linux-rdma <linux-rdma@vger.kernel.org>,
        <linux-fsdevel@vger.kernel.org>
References: <20181110085041.10071-1-jhubbard@nvidia.com>
 <942cb823-9b18-69e7-84aa-557a68f9d7e9@talpey.com>
X-Nvconfidentiality: public
From: John Hubbard <jhubbard@nvidia.com>
Message-ID: <97934904-2754-77e0-5fcb-83f2311362ee@nvidia.com>
Date: Tue, 20 Nov 2018 22:09:06 -0800
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101
 Thunderbird/60.3.1
MIME-Version: 1.0
In-Reply-To: <942cb823-9b18-69e7-84aa-557a68f9d7e9@talpey.com>
X-Originating-IP: [10.124.1.5]
X-ClientProxiedBy: HQMAIL108.nvidia.com (172.18.146.13) To
 HQMAIL101.nvidia.com (172.20.187.10)
Content-Type: text/plain; charset="utf-8"
Content-Language: en-US-large
Content-Transfer-Encoding: quoted-printable
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=nvidia.com; s=n1;
        t=1542780533; bh=VsYY+pZmJupKW3/8MneAkLYd+1iL+ulPPvbmpYRVJjo=;
        h=X-PGP-Universal:Subject:To:CC:References:X-Nvconfidentiality:From:
         Message-ID:Date:User-Agent:MIME-Version:In-Reply-To:
         X-Originating-IP:X-ClientProxiedBy:Content-Type:Content-Language:
         Content-Transfer-Encoding;
        b=Sp2/9xy+abdODSiNH3eWwn3W8KtLxfc9HSbN2sQGBI/x9BxqItGkK4nfNvhE5iC+j
         OwYEjI6jsz+YNFhZEyyRvnVFgHHXR+h3+EB7LDt3j1ibFIfrl6uROdGL1HspnPzcY8
         WJsAdgMLcBVBSnULcXPVadK3tn4kkjmfe72NEsmFZWH2zChz+qoKxbQKJx0sdpgPTQ
         psAeMk0ihT+g0Ky2clAoRJ+U6xz+rVL4WtTjPB867sWTBFmEjQCtArtX1si2A+G7dk
         nMoTx7j9WUoxSNTehuW+7fhc93iA8ENBwhl1r7UsIzyPAFLCCnXHpcEKD7cpMMsUlM
         LbojghgYR0PGg==
Sender: liker.xu+caf_=xuliker=zju.edu.cn@gmail.com
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-CM-TRANSID: AQAAfwB3Wkab9vRb17xhAA--.8434S3
Authentication-Results: mail-app2; spf=pass smtp.mail=liker.xu+caf_=xu
	liker=zju.edu.cn@gmail.com;
X-Coremail-Antispam: 1UD129KBjvJXoW3Gw47Wr4UJryUGry3Cr17KFg_yoWfWw1xpr
	W3JrWxW3yxG34Ikw1Iq340gr48JFy3X3Z8JFn8tr10g3W5ZryaqFy5JF4rua4xZrZ5Ca17
	XF1fXw129F4DX37anT9S1TB71UUUUUUqnTZGkaVYY2UrUUUUjbIjqfuFe4nvWSU8nxnvy2
	9KBjDU0xBIdaVrnRJUUUmab7Iv0xC_Cr1lb4IE77IF4wAFF20E14v26r1j6r4UM7CIcVAF
	z4kK6r1j6r18M28lY4IEw2IIxxk0rwA2F7IY1VAKz4vEj48ve4kI8wA2z4x0Y4vE2Ix0cI
	8IcVAFwI0_Xr0_Ar1l84ACjcxK6xIIjxv20xvEc7CjxVAFwI0_Gr0_Cr1l84ACjcxK6I8E
	87Iv67AKxVW0oVCq3wA2z4x0Y4vEx4A2jsIEc7CjxVAFwI0_GcCE3s1le2I262IYc4CY6c
	8Ij28IcVAaY2xG8wAqx4xG64xvF2IEw4CE5I8CrVC2j2WlYx0E2Ix0cI8IcVAFwI0_Jr0_
	Jr4lYx0Ex4A2jsIE14v26r4j6F4UMcvjeVCFs4IE7xkEbVWUJVW8JwACjcxG0xvEwIxGrw
	CjxxvEa2IrMxk0xIA0c2IEe2xFo4CEbIxvr21lc7CjxVAKzI0EY4vE52x082I5MxkFs20E
	Y4vE44CYbxCE4x80FwCY02Avz4vEIxC_GF1lc2IjII80xcxEwVAKI48JMxvI42IY6xIIjx
	v20xvE14v26r1I6r4UMxvI42IY6xIIjxv20xvEc7CjxVAFwI0_Jr0_Gr1lcIIF0xvEx4A2
	jsIE14v26F4UJVW0owCYIxAIcVC2z280aVCY1x0267AKxVWxJr0_GcWl42xK82IYc2Ij64
	vIr41l42xK82IY64kExVAvwVAq07x20xyl4x8a6x804xWl4I8I3I0E4IkC6x0Yz7v_Jr0_
	Gr1lx2IqxVAqx4xG67AKxVWUJVWUGwC20s026x8GjcxK67AKxVWUGVWUWwC2zVAF1VAY17
	CE14v26r1q6r43MIIYrxkI7VAKI48JMIIF0xvE42xK8VAvwI8IcIk0rVW3JVWrJrUvcSsG
	vfC2KfnxnUUI43ZEXa7IU5xKsUUUUUU==

On 11/19/18 10:57 AM, Tom Talpey wrote:
> John, thanks for the discussion at LPC. One of the concerns we
> raised however was the performance test. The numbers below are
> rather obviously tainted. I think we need to get a better baseline
> before concluding anything...
>=20
> Here's my main concern:
>=20

Hi Tom,

Thanks again for looking at this!


> On 11/10/2018 3:50 AM, john.hubbard@gmail.com wrote:
>> From: John Hubbard <jhubbard@nvidia.com>
>> ...
>> ------------------------------------------------------
>> WITHOUT the patch:
>> ------------------------------------------------------
>> reader: (g=3D0): rw=3Dread, bs=3D(R) 4096B-4096B, (W) 4096B-4096B, (T) 4=
096B-4096B, ioengine=3Dlibaio, iodepth=3D64
>> fio-3.3
>> Starting 1 process
>> Jobs: 1 (f=3D1): [R(1)][100.0%][r=3D55.5MiB/s,w=3D0KiB/s][r=3D14.2k,w=3D=
0 IOPS][eta 00m:00s]
>> reader: (groupid=3D0, jobs=3D1): err=3D 0: pid=3D1750: Tue Nov=C2=A0 6 2=
0:18:06 2018
>> =C2=A0=C2=A0=C2=A0 read: IOPS=3D13.9k, BW=3D54.4MiB/s (57.0MB/s)(1024MiB=
/18826msec)
>=20
> ~14000 4KB read IOPS is really, really low for an NVMe disk.

Yes, but Jan Kara's original config file for fio is *intended* to highlight
the get_user_pages/put_user_pages changes. It was *not* intended to get max
performance,  as you can see by the numjobs and direct IO parameters:

cat fio.conf=20
[reader]
direct=3D1
ioengine=3Dlibaio
blocksize=3D4096
size=3D1g
numjobs=3D1
rw=3Dread
iodepth=3D64


So I'm thinking that this is not a "tainted" test, but rather, we're constr=
aining
things a lot with these choices. It's hard to find a good test config to ru=
n that
allows decisions, but so far, I'm not really seeing anything that says "thi=
s
is so bad that we can't afford to fix the brokenness." I think.

After talking with you and reading this email, I did a bunch more test runs=
,=20
varying the following fio parameters:

	-- direct
	-- numjobs
	-- iodepth

...with both the baseline 4.20-rc3 kernel, and with my patches applied. (bt=
w, if
anyone cares, I'll post a github link that has a complete, testable patchse=
t--not
ready for submission as such, but it works cleanly and will allow others to=
=20
attempt to reproduce my results).

What I'm seeing is that I can get 10x or better improvements in IOPS and BW=
,
just by going to 10 threads and turning off direct IO--as expected. So in t=
he end,
I increased the number of threads, and also increased iodepth a bit.=20


Test results below...


>=20
>> =C2=A0=C2=A0 cpu=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 :=
 usr=3D2.39%, sys=3D95.30%, ctx=3D669, majf=3D0, minf=3D72
>=20
> CPU is obviously the limiting factor. At these IOPS, it should be far
> less.
>> ------------------------------------------------------
>> OR, here's a better run WITH the patch applied, and you can see that thi=
s is nearly as good
>> as the "without" case:
>> ------------------------------------------------------
>>
>> reader: (g=3D0): rw=3Dread, bs=3D(R) 4096B-4096B, (W) 4096B-4096B, (T) 4=
096B-4096B, ioengine=3Dlibaio, iodepth=3D64
>> fio-3.3
>> Starting 1 process
>> Jobs: 1 (f=3D1): [R(1)][100.0%][r=3D53.2MiB/s,w=3D0KiB/s][r=3D13.6k,w=3D=
0 IOPS][eta 00m:00s]
>> reader: (groupid=3D0, jobs=3D1): err=3D 0: pid=3D2521: Tue Nov=C2=A0 6 2=
0:01:33 2018
>> =C2=A0=C2=A0=C2=A0 read: IOPS=3D13.4k, BW=3D52.5MiB/s (55.1MB/s)(1024MiB=
/19499msec)
>=20
> Similar low IOPS.
>=20
>> =C2=A0=C2=A0 cpu=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 :=
 usr=3D3.47%, sys=3D94.61%, ctx=3D370, majf=3D0, minf=3D73
>=20
> Similar CPU saturation.
>=20
>>
>=20
> I get nearly 400,000 4KB IOPS on my tiny desktop, which has a 25W
> i7-7500 and a Samsung PM961 128GB NVMe (stock Bionic 4.15 kernel
> and fio version 3.1). Even then, the CPU saturates, so it's not
> necessarily a perfect test. I'd like to see your runs both get to
> "max" IOPS, i.e. CPU < 100%, and compare the CPU numbers. This would
> give the best comparison for making a decision.

I can get to CPU < 100% by increasing to 10 or 20 threads, although it
makes latency ever so much worse.

>=20
> Can you confirm what type of hardware you're running this test on?
> CPU, memory speed and capacity, and NVMe device especially?
>=20
> Tom.

Yes, it's a nice new system, I don't expect any strange perf problems:

CPU: Intel(R) Core(TM) i7-7800X CPU @ 3.50GHz
    (Intel X299 chipset)
Block device: nvme-Samsung_SSD_970_EVO_250GB
DRAM: 32 GB

So, here's a comparison using 20 threads, direct IO, for the baseline vs.=20
patched kernel (below). Highlights:

	-- IOPS are similar, around 60k.=20
	-- BW gets worse, dropping from 290 to 220 MB/s.
	-- CPU is well under 100%.
	-- latency is incredibly long, but...20 threads.

Baseline:

$ ./run.sh
fio configuration:
[reader]
ioengine=3Dlibaio
blocksize=3D4096
size=3D1g
rw=3Dread
group_reporting
iodepth=3D256
direct=3D1
numjobs=3D20
-------- Running fio:
reader: (g=3D0): rw=3Dread, bs=3D(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096=
B-4096B, ioengine=3Dlibaio, iodepth=3D256
...
fio-3.3
Starting 20 processes
Jobs: 4 (f=3D4): [_(8),R(2),_(2),R(1),_(1),R(1),_(5)][95.9%][r=3D244MiB/s,w=
=3D0KiB/s][r=3D62.5k,w=3D0 IOPS][eta 00m:03s]
reader: (groupid=3D0, jobs=3D20): err=3D 0: pid=3D14499: Tue Nov 20 16:20:3=
5 2018
   read: IOPS=3D74.2k, BW=3D290MiB/s (304MB/s)(20.0GiB/70644msec)
    slat (usec): min=3D26, max=3D48167, avg=3D249.27, stdev=3D1200.02
    clat (usec): min=3D42, max=3D147792, avg=3D67108.56, stdev=3D18062.46
     lat (usec): min=3D103, max=3D147943, avg=3D67358.10, stdev=3D18109.75
    clat percentiles (msec):
     |  1.00th=3D[   21],  5.00th=3D[   40], 10.00th=3D[   41], 20.00th=3D[=
   47],
     | 30.00th=3D[   58], 40.00th=3D[   65], 50.00th=3D[   70], 60.00th=3D[=
   75],
     | 70.00th=3D[   79], 80.00th=3D[   83], 90.00th=3D[   89], 95.00th=3D[=
   93],
     | 99.00th=3D[  104], 99.50th=3D[  109], 99.90th=3D[  121], 99.95th=3D[=
  125],
     | 99.99th=3D[  134]
   bw (  KiB/s): min=3D 9712, max=3D46362, per=3D5.11%, avg=3D15164.99, std=
ev=3D2242.15, samples=3D2742
   iops        : min=3D 2428, max=3D11590, avg=3D3790.94, stdev=3D560.53, s=
amples=3D2742
  lat (usec)   : 50=3D0.01%, 250=3D0.01%, 500=3D0.01%, 750=3D0.01%, 1000=3D=
0.01%
  lat (msec)   : 2=3D0.01%, 4=3D0.01%, 10=3D0.02%, 20=3D0.98%, 50=3D20.44%
  lat (msec)   : 100=3D76.95%, 250=3D1.61%
  cpu          : usr=3D1.00%, sys=3D57.65%, ctx=3D158367, majf=3D0, minf=3D=
5284
  IO depths    : 1=3D0.1%, 2=3D0.1%, 4=3D0.1%, 8=3D0.1%, 16=3D0.1%, 32=3D0.=
1%, >=3D64=3D100.0%
     submit    : 0=3D0.0%, 4=3D100.0%, 8=3D0.0%, 16=3D0.0%, 32=3D0.0%, 64=
=3D0.0%, >=3D64=3D0.0%
     complete  : 0=3D0.0%, 4=3D100.0%, 8=3D0.0%, 16=3D0.0%, 32=3D0.0%, 64=
=3D0.0%, >=3D64=3D0.1%
     issued rwts: total=3D5242880,0,0,0 short=3D0,0,0,0 dropped=3D0,0,0,0
     latency   : target=3D0, window=3D0, percentile=3D100.00%, depth=3D256

Run status group 0 (all jobs):
   READ: bw=3D290MiB/s (304MB/s), 290MiB/s-290MiB/s (304MB/s-304MB/s), io=
=3D20.0GiB (21.5GB), run=3D70644-70644msec

Disk stats (read/write):
  nvme0n1: ios=3D5240738/7, merge=3D0/7, ticks=3D1457727/5, in_queue=3D1547=
139, util=3D100.00%

--------------------------------------------------------------
Patched:

<redforge> fast_256GB $ ./run.sh=20
fio configuration:
[reader]
ioengine=3Dlibaio
blocksize=3D4096
size=3D1g
rw=3Dread
group_reporting
iodepth=3D256
direct=3D1
numjobs=3D20
-------- Running fio:
reader: (g=3D0): rw=3Dread, bs=3D(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096=
B-4096B, ioengine=3Dlibaio, iodepth=3D256
...
fio-3.3
Starting 20 processes
Jobs: 13 (f=3D8): [_(1),R(1),_(1),f(1),R(2),_(1),f(2),_(1),R(1),f(1),R(1),f=
(1),R(1),_(2),R(1),_(1),R(1)][97.9%][r=3D229MiB/s,w=3D0KiB/s][r=3D58.5k,w=
=3D0 IOPS][eta 00m:02s]
reader: (groupid=3D0, jobs=3D20): err=3D 0: pid=3D2104: Tue Nov 20 22:01:58=
 2018
   read: IOPS=3D56.8k, BW=3D222MiB/s (232MB/s)(20.0GiB/92385msec)
    slat (usec): min=3D26, max=3D50436, avg=3D337.21, stdev=3D1405.14
    clat (usec): min=3D43, max=3D178839, avg=3D88963.96, stdev=3D21745.31
     lat (usec): min=3D106, max=3D179041, avg=3D89301.43, stdev=3D21800.43
    clat percentiles (msec):
     |  1.00th=3D[   50],  5.00th=3D[   53], 10.00th=3D[   55], 20.00th=3D[=
   68],
     | 30.00th=3D[   79], 40.00th=3D[   86], 50.00th=3D[   93], 60.00th=3D[=
   99],
     | 70.00th=3D[  103], 80.00th=3D[  108], 90.00th=3D[  114], 95.00th=3D[=
  121],
     | 99.00th=3D[  134], 99.50th=3D[  140], 99.90th=3D[  150], 99.95th=3D[=
  155],
     | 99.99th=3D[  163]
   bw (  KiB/s): min=3D 4920, max=3D39733, per=3D5.07%, avg=3D11506.18, std=
ev=3D1540.18, samples=3D3650
   iops        : min=3D 1230, max=3D 9933, avg=3D2876.20, stdev=3D385.05, s=
amples=3D3650
  lat (usec)   : 50=3D0.01%, 100=3D0.01%, 250=3D0.01%, 500=3D0.01%, 750=3D0=
.01%
  lat (usec)   : 1000=3D0.01%
  lat (msec)   : 2=3D0.01%, 4=3D0.01%, 10=3D0.01%, 20=3D0.23%, 50=3D1.13%
  lat (msec)   : 100=3D63.04%, 250=3D35.57%
  cpu          : usr=3D0.65%, sys=3D58.07%, ctx=3D188963, majf=3D0, minf=3D=
5303
  IO depths    : 1=3D0.1%, 2=3D0.1%, 4=3D0.1%, 8=3D0.1%, 16=3D0.1%, 32=3D0.=
1%, >=3D64=3D100.0%
     submit    : 0=3D0.0%, 4=3D100.0%, 8=3D0.0%, 16=3D0.0%, 32=3D0.0%, 64=
=3D0.0%, >=3D64=3D0.0%
     complete  : 0=3D0.0%, 4=3D100.0%, 8=3D0.0%, 16=3D0.0%, 32=3D0.0%, 64=
=3D0.0%, >=3D64=3D0.1%
     issued rwts: total=3D5242880,0,0,0 short=3D0,0,0,0 dropped=3D0,0,0,0
     latency   : target=3D0, window=3D0, percentile=3D100.00%, depth=3D256

Run status group 0 (all jobs):
   READ: bw=3D222MiB/s (232MB/s), 222MiB/s-222MiB/s (232MB/s-232MB/s), io=
=3D20.0GiB (21.5GB), run=3D92385-92385msec

Disk stats (read/write):
  nvme0n1: ios=3D5240550/7, merge=3D0/7, ticks=3D1513681/4, in_queue=3D1636=
411, util=3D100.00%


Thoughts?


thanks,
--=20
John Hubbard
NVIDIA
