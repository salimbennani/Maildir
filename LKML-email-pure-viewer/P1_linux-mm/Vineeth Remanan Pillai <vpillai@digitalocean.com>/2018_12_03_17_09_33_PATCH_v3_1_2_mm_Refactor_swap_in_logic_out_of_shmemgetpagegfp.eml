Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 04 Dec 2018 08:55:37 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga004.jf.intel.com (orsmga004.jf.intel.com [10.7.209.38])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id EE21A58014B;
	Mon,  3 Dec 2018 09:13:03 -0800 (PST)
Received: from fmsmga103.fm.intel.com ([10.1.193.90])
  by orsmga004-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 03 Dec 2018 09:13:03 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AmVilGhUAdgqPAKc+ZWWH7d8BQ9rV8LGtZVwlr6E/?=
 =?us-ascii?q?grcLSJyIuqrYZhKGtKdThVPEFb/W9+hDw7KP9fy4CSpYud6oizMrSNR0TRgLiM?=
 =?us-ascii?q?EbzUQLIfWuLgnFFsPsdDEwB89YVVVorDmROElRH9viNRWJ+iXhpTEdFQ/iOgVr?=
 =?us-ascii?q?O+/7BpDdj9it1+C15pbffxhEiCCybL9uLxi6txndutULioZ+N6g9zQfErGFVcO?=
 =?us-ascii?q?pM32NoIlyTnxf45siu+ZNo7jpdtfE8+cNeSKv2Z6s3Q6BWAzQgKGA1+dbktQLf?=
 =?us-ascii?q?QguV53sTSXsZnxxVCAXY9h76X5Pxsizntuph3SSRIMP7QawoVTmk8qxmUwHjhj?=
 =?us-ascii?q?sZODEl8WHXks1wg7xdoBK9vBx03orYbJiIOPZiYq/ReNUXTndDUMlMTSxMGoOy?=
 =?us-ascii?q?YZUSAeQPPuhWqJL9p0MMoBalGQWgGPnixiNSi3PqwaE31fkqHwHc3AwnGtIDqH?=
 =?us-ascii?q?vbrNTzNKgMT++10KnIxijfYfxI3Dfy8o7IfQonofqRR7xwcM7RyU01GAPEk16d?=
 =?us-ascii?q?rpLlPyuU1uQJt2iU9etgWPmyhG4mpAFxoySvyd0oionOmo0a1ErL9SNjzIkpIt?=
 =?us-ascii?q?24TVd2bNi5G5VTryGXL5V6Tt8mTm1ypSo3xKMKtYSmcCUJ1Jgr3R/SZv6ff4SV?=
 =?us-ascii?q?4B/uV/ydLSpmiH9mYr6zmQi+/Ei6xuHhWcS51ktBoDBfndnWrH8N0gTe6siZRf?=
 =?us-ascii?q?t5+UeswSiP1w/N5eFeO0w0lrTUK4QnwrEukpofq0PDHjX5mEnuja+WcFsr+vSw?=
 =?us-ascii?q?5uj5frnrooWQO5J6hw3gKKgih8+yDfgiPgUPXWWX4eG826fi/U39TrVKlPo2kq?=
 =?us-ascii?q?zBvZDeJMQboLO5AgBM3oYg9Rm/FTGm38ocnXUeK1JEdhSHgJbzO1zVPvD4Aumw?=
 =?us-ascii?q?g062nDdo2f/GJLvhDYvJLnTZl7fhZ7l951ZGyAUv1dBf+45UCrYZLfL3W0/xt8?=
 =?us-ascii?q?LXAgU2Mgyp2OvnDNR91oUDWWOAGKOZMaXSsUOW6eIrOeWDeIgVuDPlIfg/+/Hu?=
 =?us-ascii?q?lWM5mUMafaSx3ZsYcnG4Huh8LEWee3bsgsoBHn0MvgoxV+HqjFyCUThOZ3e9Ra?=
 =?us-ascii?q?485zc7CJ64AofHXIyinLuB3CKjFJ1Mem9GEkyMEWvvd4icWfcMbzydLtVikjwD?=
 =?us-ascii?q?U7ihTYgh2AqqtA/7zbpnM+XV9jcZtZLlyNh6+enTmQsu+jxzCsSXy3uNQH1snm?=
 =?us-ascii?q?MUWz8227hyrlZmyleD1qh4gOZUFcZJ6PNLSQo6MZ/cz+pnC9H9QA7Bf9GJSEq4?=
 =?us-ascii?q?TdWiGz0+UtUxw9oWaUZnB9qilgzD3zatA7INlLyEHpo0/rjY33jwPcl9zXnG2b?=
 =?us-ascii?q?Ilj1knRMtPKGKnirR+9wjVG47GjUGZm7y2eqQb2S7H7H2DwnaWvEFETA5wVr3I?=
 =?us-ascii?q?Um0FaUvIs9v1/EPCQKWoCbQ8LARBz8mOKq9Jat3siVVLX/PjONXYY2KslGa8Hx?=
 =?us-ascii?q?eIxrWQbIX0f2URxjnSCE8BkwoL53aJKRA+Bju9o2LZFDFhCEjgY13y/uVkqHO0?=
 =?us-ascii?q?VEk0zxqUYE1nzLe1/h8VhfqBS/IcxL4EuSEhqylqE1a5xd7ZF92Apw95dqVGfd?=
 =?us-ascii?q?w9+EtH1X7etwFlIpygLqVihlkCcwR3v0LizQl3Bp9HkcgwqHMqzQxyKa2D3VNF?=
 =?us-ascii?q?djOY243wO7LNJmnz+hCvd7DZ2lXE3NmK/acP7ewyq0//swGxCkoi73Jn3sFP3H?=
 =?us-ascii?q?uY+JrLAxQdUJLrXkks6hh1uqvVYi8+54PTy31hKq20sj7E29I0C+op0Begf9FD?=
 =?us-ascii?q?MKyaEA/+CdEVB8+rKOYygVimcgoEPPxO9K4zJ86nd+aG1LS3M+p6nTOmjX5I4I?=
 =?us-ascii?q?Zy0k+X8yp8S+jI34sKwv2C3wuHUSv8g0mlsszthY9EYjQSFHKlySf4HI5RerFy?=
 =?us-ascii?q?fYETBGe0Is242s9xh4TwVH5f7lKjAU0J2NWoeRaLc1PyxwlQ2lkJrny9niu4yS?=
 =?us-ascii?q?d5kzUorqqZwSzPzP7udBsBOm5XWmZiiU3gLpSzj9AfREKodRQmlAO55UbmwKhW?=
 =?us-ascii?q?vKR+L2jJTUZIZST2NHxiUq2ru7qGYs5P7o4osCpNXOS9Z1CaVqDyox8A3yz/GG?=
 =?us-ascii?q?tewSgxdyu2tZXhgxx6lGWdIW5xrHXDY8FwxhTf5NvGSf5KxDUGRyp4iTjRBlei?=
 =?us-ascii?q?Odmk5tGUl5bFsuCjWGOtTJxTcS/3zYyesCu3/3FlARq6n/qrgN3oDRA60TPn19?=
 =?us-ascii?q?ltTSjJrAzzYo7x26S4MOJneFJlBFv968p8B4F/nZE8hJAW2XgGmJqV+WALnnv0?=
 =?us-ascii?q?MdVewaj+dmYCRSYXw97J5wjowE5jIWiIx47jVnWd39FuZ9+1Ym4N3iI97sZKCL?=
 =?us-ascii?q?qb7bBenCt1pEa4ohzVYfRngjgdzv4u4mYAg+4VoAot0jmdArcKEEldPCzslA6H?=
 =?us-ascii?q?48qwrapJf2avbaa/1FBlkt+/FrGCrRpRWHL4epckAC9x4d9zMFPK0H3v9I7kfM?=
 =?us-ascii?q?PcYs4Uth2Rix3AlfRaKIotlvoWgipqIX79vXogy+IhjB1hx5e6vJWcK2V2/aK0?=
 =?us-ascii?q?GRpYNjzzZ8MO9TDhl6densCK34+xGpVtACkEXJztTfiwCjIdqeznNxqSED07sn?=
 =?us-ascii?q?qUAqDQHQib6Ed7tX7PF42rOmqTJHkYy9ViWRacKFZegAASQDU1gJo5Ghq2y8zm?=
 =?us-ascii?q?dUdz/ioR6ULgqhtQ1uJoMAHyUmXFqwetdDg0U4KTLAZM4gFB+kfVMtGe7uRpEC?=
 =?us-ascii?q?FD5ZChqA2NKmqGZwVHF20JW0qEB0z9Mbmq/9XP7++YBu+mJfvUfbqOsfBeV+uP?=
 =?us-ascii?q?xZ+30opm+CuMNtySPnZ4Cf03wFFDXXd/G8TWgDgPTy0XlyTQb8+Uvhu8+yt3rt?=
 =?us-ascii?q?yh//TvQg7g+YyPC75KO9V15x+2mbuDN/KXhCthKTZXyJIMxXzLyLgexlISiDtu?=
 =?us-ascii?q?dz6iEbsery7NUbnQl7RTDx4abSNzKcRJ47g93glLJc7UlNf12qRkgf4yDldPTU?=
 =?us-ascii?q?bhld2xZcwWP2G9M0vKC1yWNLudPz3E3cH2bbm4Sb1Rl+hUsxywuTCGE07sJDiD?=
 =?us-ascii?q?lj/pVwyxPuFIli2UIBteuISleBZ3FWfjVM7magG8MNJvkT02wLg0imnWOmIGLT?=
 =?us-ascii?q?d8c11CrruL4CNcg/V/HXFB73V/IemFnSaZ8/fXKpIMvfR3BSR0kvpQ4G4mxLtN?=
 =?us-ascii?q?8CFEWPt1lTPSr9F0pVGml+iPyjx/XxtPqjZEnoSLvUp5NKXd95lAX2vE/R0X4W?=
 =?us-ascii?q?WRDRQKu8VqCtn1t69MzdjPkfG7FDAX1tvI4csRGIDrKcaMN2ppZQXoHDrFJAoD?=
 =?us-ascii?q?QySiOWzWiwpaiv7EsjW1s5c64rTtk5kPTKQTAFwoGfMWIktiG9MGLdF8WTZy1f?=
 =?us-ascii?q?ayisIS6GX2iR7LWMJc9sTOVeqXKfHuLiuJyLdDeh0EyK/5KoJVMZf0jQgqblh8?=
 =?us-ascii?q?gZSPHk/4Xs5EqS4naRU75A1J8X5jXigw1ljjZweF/nAeD7i3kwQwhw84Zv4ipx?=
 =?us-ascii?q?n25FJiGFvMqSYskAELlNDshzmXOAbwKKO5XMkCFTL1s0w4M7v3Qgl0cQS2mkdp?=
 =?us-ascii?q?PTGCTLVU2egzPVt3gRPR7MMcUcVXSrdJNVpJnama?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AUAADyYgVch0O0hNFbCB0BAQUBBwUBg?=
 =?us-ascii?q?VEIAQsBg2snjBGMDIIhaJFThHkUgVgZAQEYEwGICSI0CQ0BAwEBAQEBAQIBEwE?=
 =?us-ascii?q?BAQoLCQgpIwyCNiQBgmkCJBkBATcBBQkCGDgDMQEFASIBEgWDHIICBAGaOjyKH?=
 =?us-ascii?q?YFsM4J2AQEFhxwIEodbgxOBHBEGgUA/gRGCXYURDoVyiSgKh0qPRgmRMSOBW4g?=
 =?us-ascii?q?ANocVLIhYgQOOPwYCCQcPIYElgg0zGggbFYMnghsMF4NKilNAMoEFAQEhijYBA?=
 =?us-ascii?q?Q?=
X-IPAS-Result: =?us-ascii?q?A0AUAADyYgVch0O0hNFbCB0BAQUBBwUBgVEIAQsBg2snjBG?=
 =?us-ascii?q?MDIIhaJFThHkUgVgZAQEYEwGICSI0CQ0BAwEBAQEBAQIBEwEBAQoLCQgpIwyCN?=
 =?us-ascii?q?iQBgmkCJBkBATcBBQkCGDgDMQEFASIBEgWDHIICBAGaOjyKHYFsM4J2AQEFhxw?=
 =?us-ascii?q?IEodbgxOBHBEGgUA/gRGCXYURDoVyiSgKh0qPRgmRMSOBW4gANocVLIhYgQOOP?=
 =?us-ascii?q?wYCCQcPIYElgg0zGggbFYMnghsMF4NKilNAMoEFAQEhijYBAQ?=
X-IronPort-AV: E=Sophos;i="5.56,311,1539673200"; 
   d="scan'208";a="54307561"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 03 Dec 2018 09:13:01 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726919AbeLCRM6 (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Mon, 3 Dec 2018 12:12:58 -0500
Received: from mail-io1-f66.google.com ([209.85.166.66]:42682 "EHLO
        mail-io1-f66.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726756AbeLCRM6 (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 3 Dec 2018 12:12:58 -0500
Received: by mail-io1-f66.google.com with SMTP id x6so11105107ioa.9
        for <linux-kernel@vger.kernel.org>; Mon, 03 Dec 2018 09:12:56 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=digitalocean.com; s=google;
        h=from:to:cc:subject:date:message-id;
        bh=enrtGIRyRGRmz7VvuQLDMwrJiutPSrHTYcUse/DECXU=;
        b=R71WRj5iTljH5EUejjmBIV3pNv3JD+gm3gkgjtHXEMwYFoDET/qFpMiFdowOkR9cTb
         dGvBvnsXm/ZWXb70gmLdgAhKQ9RcdaUHd31xir9I+wmCgMfBqyHshl0VadtLTBt9TrLi
         TjaLAh+fDR9IUHyTR8sW4FB4Zbzm4ZP19+iXc=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:from:to:cc:subject:date:message-id;
        bh=enrtGIRyRGRmz7VvuQLDMwrJiutPSrHTYcUse/DECXU=;
        b=VhXipbZri/eMFkIGPllShYSqB5b2P9YiGyATrhWlXKPDsvseGhKep0MRlcmmwQMvPl
         /4RVRiDES4uvLCd1lfdh2cUh36Zm5mS55IIvqsYjTa3DTIoE4Kmvipv+5iy7wRPANBME
         hbVXFsPO49eJYrFgx1+N01H7hc53pTWGIkdFGlN2RGQ2ULbaub5SI6j4abJ6FLa9guwP
         xzVIU2gYEm+9nsADqEhyGP6jyWy5iRNhEX+mgefcgYwy98OFGBhonOvijlYqmTlhCEa6
         MPiIzqORHkTPMO85SfId7RIhriOzJMynHuMjGh4DYGNbHDyZeFgE4Y31GsiT7eJ7iV+O
         sj2w==
X-Gm-Message-State: AA+aEWY5dWtPbI0hNR07JrlRdgDI5e0c6thM9xz6St5/Cgl6ZalRn1sP
        +pRBwSFjI6KJHFun58vw3h55TQ==
X-Google-Smtp-Source: AFSGD/W93JmLhiUJnMB0VCAK7Tgh7d4wW+bRDW5udo20vgJOBsz/9If+bNMKqwTga7LczdVGq4BvMw==
X-Received: by 2002:a6b:b717:: with SMTP id h23mr13824446iof.14.1543857175545;
        Mon, 03 Dec 2018 09:12:55 -0800 (PST)
Received: from swap-tester ([178.128.225.14])
        by smtp.gmail.com with ESMTPSA id h140sm4258144itb.23.2018.12.03.09.12.54
        (version=TLS1_2 cipher=ECDHE-RSA-CHACHA20-POLY1305 bits=256/256);
        Mon, 03 Dec 2018 09:12:54 -0800 (PST)
From: Vineeth Remanan Pillai <vpillai@digitalocean.com>
To: Matthew Wilcox <willy@infradead.org>,
        Hugh Dickins <hughd@google.com>,
        Andrew Morton <akpm@linux-foundation.org>, linux-mm@kvack.org,
        linux-kernel@vger.kernel.org
Cc: Vineeth Remanan Pillai <vpillai@digitalocean.com>,
        Kelley Nielsen <kelleynnn@gmail.com>,
        Rik van Riel <riel@surriel.com>
Subject: [PATCH v3 1/2] mm: Refactor swap-in logic out of shmem_getpage_gfp
Date: Mon,  3 Dec 2018 17:09:33 +0000
Message-Id: <20181203170934.16512-1-vpillai@digitalocean.com>
X-Mailer: git-send-email 2.17.1
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

swap-in logic could be reused independently without rest of the logic
in shmem_getpage_gfp. So lets refactor it out as an independent
function.

Signed-off-by: Vineeth Remanan Pillai <vpillai@digitalocean.com>
---
 mm/shmem.c | 449 +++++++++++++++++++++++++++++------------------------
 1 file changed, 244 insertions(+), 205 deletions(-)

diff --git a/mm/shmem.c b/mm/shmem.c
index cddc72ac44d8..035ea2c10f54 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -121,6 +121,10 @@ static unsigned long shmem_default_max_inodes(void)
 static bool shmem_should_replace_page(struct page *page, gfp_t gfp);
 static int shmem_replace_page(struct page **pagep, gfp_t gfp,
 				struct shmem_inode_info *info, pgoff_t index);
+static int shmem_swapin_page(struct inode *inode, pgoff_t index,
+			     struct page **pagep, enum sgp_type sgp,
+			     gfp_t gfp, struct vm_area_struct *vma,
+			     vm_fault_t *fault_type);
 static int shmem_getpage_gfp(struct inode *inode, pgoff_t index,
 		struct page **pagep, enum sgp_type sgp,
 		gfp_t gfp, struct vm_area_struct *vma,
@@ -1575,6 +1579,116 @@ static int shmem_replace_page(struct page **pagep, gfp_t gfp,
 	return error;
 }
 
+/*
+ * Swap in the page pointed to by *pagep.
+ * Caller has to make sure that *pagep contains a valid swapped page.
+ * Returns 0 and the page in pagep if success. On failure, returns the
+ * the error code and NULL in *pagep.
+ */
+static int shmem_swapin_page(struct inode *inode, pgoff_t index,
+			     struct page **pagep, enum sgp_type sgp,
+			     gfp_t gfp, struct vm_area_struct *vma,
+			     vm_fault_t *fault_type)
+{
+	struct address_space *mapping = inode->i_mapping;
+	struct shmem_inode_info *info = SHMEM_I(inode);
+	struct mm_struct *charge_mm = vma ? vma->vm_mm : current->mm;
+	struct mem_cgroup *memcg;
+	struct page *page;
+	swp_entry_t swap;
+	int error;
+
+	VM_BUG_ON(!*pagep || !xa_is_value(*pagep));
+	swap = radix_to_swp_entry(*pagep);
+	*pagep = NULL;
+
+	/* Look it up and read it in.. */
+	page = lookup_swap_cache(swap, NULL, 0);
+	if (!page) {
+		/* Or update major stats only when swapin succeeds?? */
+		if (fault_type) {
+			*fault_type |= VM_FAULT_MAJOR;
+			count_vm_event(PGMAJFAULT);
+			count_memcg_event_mm(charge_mm, PGMAJFAULT);
+		}
+		/* Here we actually start the io */
+		page = shmem_swapin(swap, gfp, info, index);
+		if (!page) {
+			error = -ENOMEM;
+			goto failed;
+		}
+	}
+
+	/* We have to do this with page locked to prevent races */
+	lock_page(page);
+	if (!PageSwapCache(page) || page_private(page) != swap.val ||
+	    !shmem_confirm_swap(mapping, index, swap)) {
+		error = -EEXIST;
+		goto unlock;
+	}
+	if (!PageUptodate(page)) {
+		error = -EIO;
+		goto failed;
+	}
+	wait_on_page_writeback(page);
+
+	if (shmem_should_replace_page(page, gfp)) {
+		error = shmem_replace_page(&page, gfp, info, index);
+		if (error)
+			goto failed;
+	}
+
+	error = mem_cgroup_try_charge_delay(page, charge_mm, gfp, &memcg,
+					    false);
+	if (!error) {
+		error = shmem_add_to_page_cache(page, mapping, index,
+						swp_to_radix_entry(swap), gfp);
+		/*
+		 * We already confirmed swap under page lock, and make
+		 * no memory allocation here, so usually no possibility
+		 * of error; but free_swap_and_cache() only trylocks a
+		 * page, so it is just possible that the entry has been
+		 * truncated or holepunched since swap was confirmed.
+		 * shmem_undo_range() will have done some of the
+		 * unaccounting, now delete_from_swap_cache() will do
+		 * the rest.
+		 */
+		if (error) {
+			mem_cgroup_cancel_charge(page, memcg, false);
+			delete_from_swap_cache(page);
+		}
+	}
+	if (error)
+		goto failed;
+
+	mem_cgroup_commit_charge(page, memcg, true, false);
+
+	spin_lock_irq(&info->lock);
+	info->swapped--;
+	shmem_recalc_inode(inode);
+	spin_unlock_irq(&info->lock);
+
+	if (sgp == SGP_WRITE)
+		mark_page_accessed(page);
+
+	delete_from_swap_cache(page);
+	set_page_dirty(page);
+	swap_free(swap);
+
+	*pagep = page;
+	return 0;
+failed:
+	if (!shmem_confirm_swap(mapping, index, swap))
+		error = -EEXIST;
+unlock:
+	if (page) {
+		unlock_page(page);
+		put_page(page);
+	}
+
+	return error;
+}
+
 /*
  * shmem_getpage_gfp - find page in cache, or get from swap, or allocate
  *
@@ -1596,7 +1710,6 @@ static int shmem_getpage_gfp(struct inode *inode, pgoff_t index,
 	struct mm_struct *charge_mm;
 	struct mem_cgroup *memcg;
 	struct page *page;
-	swp_entry_t swap;
 	enum sgp_type sgp_huge = sgp;
 	pgoff_t hindex = index;
 	int error;
@@ -1608,17 +1721,23 @@ static int shmem_getpage_gfp(struct inode *inode, pgoff_t index,
 	if (sgp == SGP_NOHUGE || sgp == SGP_HUGE)
 		sgp = SGP_CACHE;
 repeat:
-	swap.val = 0;
+	if (sgp <= SGP_CACHE &&
+	    ((loff_t)index << PAGE_SHIFT) >= i_size_read(inode)) {
+		return -EINVAL;
+	}
+
+	sbinfo = SHMEM_SB(inode->i_sb);
+	charge_mm = vma ? vma->vm_mm : current->mm;
+
 	page = find_lock_entry(mapping, index);
 	if (xa_is_value(page)) {
-		swap = radix_to_swp_entry(page);
-		page = NULL;
-	}
+		error = shmem_swapin_page(inode, index, &page,
+					  sgp, gfp, vma, fault_type);
+		if (error == -EEXIST)
+			goto repeat;
 
-	if (sgp <= SGP_CACHE &&
-	    ((loff_t)index << PAGE_SHIFT) >= i_size_read(inode)) {
-		error = -EINVAL;
-		goto unlock;
+		*pagep = page;
+		return error;
 	}
 
 	if (page && sgp == SGP_WRITE)
@@ -1632,7 +1751,7 @@ static int shmem_getpage_gfp(struct inode *inode, pgoff_t index,
 		put_page(page);
 		page = NULL;
 	}
-	if (page || (sgp == SGP_READ && !swap.val)) {
+	if (page || sgp == SGP_READ) {
 		*pagep = page;
 		return 0;
 	}
@@ -1641,215 +1760,138 @@ static int shmem_getpage_gfp(struct inode *inode, pgoff_t index,
 	 * Fast cache lookup did not find it:
 	 * bring it back from swap or allocate.
 	 */
-	sbinfo = SHMEM_SB(inode->i_sb);
-	charge_mm = vma ? vma->vm_mm : current->mm;
-
-	if (swap.val) {
-		/* Look it up and read it in.. */
-		page = lookup_swap_cache(swap, NULL, 0);
-		if (!page) {
-			/* Or update major stats only when swapin succeeds?? */
-			if (fault_type) {
-				*fault_type |= VM_FAULT_MAJOR;
-				count_vm_event(PGMAJFAULT);
-				count_memcg_event_mm(charge_mm, PGMAJFAULT);
-			}
-			/* Here we actually start the io */
-			page = shmem_swapin(swap, gfp, info, index);
-			if (!page) {
-				error = -ENOMEM;
-				goto failed;
-			}
-		}
-
-		/* We have to do this with page locked to prevent races */
-		lock_page(page);
-		if (!PageSwapCache(page) || page_private(page) != swap.val ||
-		    !shmem_confirm_swap(mapping, index, swap)) {
-			error = -EEXIST;	/* try again */
-			goto unlock;
-		}
-		if (!PageUptodate(page)) {
-			error = -EIO;
-			goto failed;
-		}
-		wait_on_page_writeback(page);
-
-		if (shmem_should_replace_page(page, gfp)) {
-			error = shmem_replace_page(&page, gfp, info, index);
-			if (error)
-				goto failed;
-		}
 
-		error = mem_cgroup_try_charge_delay(page, charge_mm, gfp, &memcg,
-				false);
-		if (!error) {
-			error = shmem_add_to_page_cache(page, mapping, index,
-						swp_to_radix_entry(swap), gfp);
-			/*
-			 * We already confirmed swap under page lock, and make
-			 * no memory allocation here, so usually no possibility
-			 * of error; but free_swap_and_cache() only trylocks a
-			 * page, so it is just possible that the entry has been
-			 * truncated or holepunched since swap was confirmed.
-			 * shmem_undo_range() will have done some of the
-			 * unaccounting, now delete_from_swap_cache() will do
-			 * the rest.
-			 * Reset swap.val? No, leave it so "failed" goes back to
-			 * "repeat": reading a hole and writing should succeed.
-			 */
-			if (error) {
-				mem_cgroup_cancel_charge(page, memcg, false);
-				delete_from_swap_cache(page);
-			}
-		}
-		if (error)
-			goto failed;
-
-		mem_cgroup_commit_charge(page, memcg, true, false);
-
-		spin_lock_irq(&info->lock);
-		info->swapped--;
-		shmem_recalc_inode(inode);
-		spin_unlock_irq(&info->lock);
-
-		if (sgp == SGP_WRITE)
-			mark_page_accessed(page);
-
-		delete_from_swap_cache(page);
-		set_page_dirty(page);
-		swap_free(swap);
-
-	} else {
-		if (vma && userfaultfd_missing(vma)) {
-			*fault_type = handle_userfault(vmf, VM_UFFD_MISSING);
-			return 0;
-		}
+	if (vma && userfaultfd_missing(vma)) {
+		*fault_type = handle_userfault(vmf, VM_UFFD_MISSING);
+		return 0;
+	}
 
-		/* shmem_symlink() */
-		if (mapping->a_ops != &shmem_aops)
-			goto alloc_nohuge;
-		if (shmem_huge == SHMEM_HUGE_DENY || sgp_huge == SGP_NOHUGE)
-			goto alloc_nohuge;
-		if (shmem_huge == SHMEM_HUGE_FORCE)
+	/* shmem_symlink() */
+	if (mapping->a_ops != &shmem_aops)
+		goto alloc_nohuge;
+	if (shmem_huge == SHMEM_HUGE_DENY || sgp_huge == SGP_NOHUGE)
+		goto alloc_nohuge;
+	if (shmem_huge == SHMEM_HUGE_FORCE)
+		goto alloc_huge;
+	switch (sbinfo->huge) {
+		loff_t i_size;
+		pgoff_t off;
+	case SHMEM_HUGE_NEVER:
+		goto alloc_nohuge;
+	case SHMEM_HUGE_WITHIN_SIZE:
+		off = round_up(index, HPAGE_PMD_NR);
+		i_size = round_up(i_size_read(inode), PAGE_SIZE);
+		if (i_size >= HPAGE_PMD_SIZE &&
+		    i_size >> PAGE_SHIFT >= off)
 			goto alloc_huge;
-		switch (sbinfo->huge) {
-			loff_t i_size;
-			pgoff_t off;
-		case SHMEM_HUGE_NEVER:
-			goto alloc_nohuge;
-		case SHMEM_HUGE_WITHIN_SIZE:
-			off = round_up(index, HPAGE_PMD_NR);
-			i_size = round_up(i_size_read(inode), PAGE_SIZE);
-			if (i_size >= HPAGE_PMD_SIZE &&
-					i_size >> PAGE_SHIFT >= off)
-				goto alloc_huge;
-			/* fallthrough */
-		case SHMEM_HUGE_ADVISE:
-			if (sgp_huge == SGP_HUGE)
-				goto alloc_huge;
-			/* TODO: implement fadvise() hints */
-			goto alloc_nohuge;
-		}
+		/* fallthrough */
+	case SHMEM_HUGE_ADVISE:
+		if (sgp_huge == SGP_HUGE)
+			goto alloc_huge;
+		/* TODO: implement fadvise() hints */
+		goto alloc_nohuge;
+	}
 
 alloc_huge:
-		page = shmem_alloc_and_acct_page(gfp, inode, index, true);
-		if (IS_ERR(page)) {
-alloc_nohuge:		page = shmem_alloc_and_acct_page(gfp, inode,
-					index, false);
-		}
-		if (IS_ERR(page)) {
-			int retry = 5;
-			error = PTR_ERR(page);
-			page = NULL;
-			if (error != -ENOSPC)
-				goto failed;
-			/*
-			 * Try to reclaim some spece by splitting a huge page
-			 * beyond i_size on the filesystem.
-			 */
-			while (retry--) {
-				int ret;
-				ret = shmem_unused_huge_shrink(sbinfo, NULL, 1);
-				if (ret == SHRINK_STOP)
-					break;
-				if (ret)
-					goto alloc_nohuge;
-			}
-			goto failed;
-		}
-
-		if (PageTransHuge(page))
-			hindex = round_down(index, HPAGE_PMD_NR);
-		else
-			hindex = index;
+	page = shmem_alloc_and_acct_page(gfp, inode, index, true);
+	if (IS_ERR(page)) {
+alloc_nohuge:
+		page = shmem_alloc_and_acct_page(gfp, inode,
+						 index, false);
+	}
+	if (IS_ERR(page)) {
+		int retry = 5;
 
-		if (sgp == SGP_WRITE)
-			__SetPageReferenced(page);
+		error = PTR_ERR(page);
+		page = NULL;
+		if (error != -ENOSPC)
+			goto unlock;
+		/*
+		 * Try to reclaim some space by splitting a huge page
+		 * beyond i_size on the filesystem.
+		 */
+		while (retry--) {
+			int ret;
 
-		error = mem_cgroup_try_charge_delay(page, charge_mm, gfp, &memcg,
-				PageTransHuge(page));
-		if (error)
-			goto unacct;
-		error = shmem_add_to_page_cache(page, mapping, hindex,
-						NULL, gfp & GFP_RECLAIM_MASK);
-		if (error) {
-			mem_cgroup_cancel_charge(page, memcg,
-					PageTransHuge(page));
-			goto unacct;
+			ret = shmem_unused_huge_shrink(sbinfo, NULL, 1);
+			if (ret == SHRINK_STOP)
+				break;
+			if (ret)
+				goto alloc_nohuge;
 		}
-		mem_cgroup_commit_charge(page, memcg, false,
-				PageTransHuge(page));
-		lru_cache_add_anon(page);
+		goto unlock;
+	}
 
-		spin_lock_irq(&info->lock);
-		info->alloced += 1 << compound_order(page);
-		inode->i_blocks += BLOCKS_PER_PAGE << compound_order(page);
-		shmem_recalc_inode(inode);
-		spin_unlock_irq(&info->lock);
-		alloced = true;
+	if (PageTransHuge(page))
+		hindex = round_down(index, HPAGE_PMD_NR);
+	else
+		hindex = index;
 
-		if (PageTransHuge(page) &&
-				DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE) <
-				hindex + HPAGE_PMD_NR - 1) {
-			/*
-			 * Part of the huge page is beyond i_size: subject
-			 * to shrink under memory pressure.
-			 */
-			spin_lock(&sbinfo->shrinklist_lock);
-			/*
-			 * _careful to defend against unlocked access to
-			 * ->shrink_list in shmem_unused_huge_shrink()
-			 */
-			if (list_empty_careful(&info->shrinklist)) {
-				list_add_tail(&info->shrinklist,
-						&sbinfo->shrinklist);
-				sbinfo->shrinklist_len++;
-			}
-			spin_unlock(&sbinfo->shrinklist_lock);
-		}
+	if (sgp == SGP_WRITE)
+		__SetPageReferenced(page);
+
+	error = mem_cgroup_try_charge_delay(page, charge_mm, gfp, &memcg,
+					    PageTransHuge(page));
+	if (error)
+		goto unacct;
+	error = shmem_add_to_page_cache(page, mapping, hindex,
+					NULL, gfp & GFP_RECLAIM_MASK);
+	if (error) {
+		mem_cgroup_cancel_charge(page, memcg,
+					 PageTransHuge(page));
+		goto unacct;
+	}
+	mem_cgroup_commit_charge(page, memcg, false,
+				 PageTransHuge(page));
+	lru_cache_add_anon(page);
+
+	spin_lock_irq(&info->lock);
+	info->alloced += 1 << compound_order(page);
+	inode->i_blocks += BLOCKS_PER_PAGE << compound_order(page);
+	shmem_recalc_inode(inode);
+	spin_unlock_irq(&info->lock);
+	alloced = true;
 
+	if (PageTransHuge(page) &&
+	    DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE) <
+			hindex + HPAGE_PMD_NR - 1) {
 		/*
-		 * Let SGP_FALLOC use the SGP_WRITE optimization on a new page.
+		 * Part of the huge page is beyond i_size: subject
+		 * to shrink under memory pressure.
 		 */
-		if (sgp == SGP_FALLOC)
-			sgp = SGP_WRITE;
-clear:
+		spin_lock(&sbinfo->shrinklist_lock);
 		/*
-		 * Let SGP_WRITE caller clear ends if write does not fill page;
-		 * but SGP_FALLOC on a page fallocated earlier must initialize
-		 * it now, lest undo on failure cancel our earlier guarantee.
+		 * _careful to defend against unlocked access to
+		 * ->shrink_list in shmem_unused_huge_shrink()
 		 */
-		if (sgp != SGP_WRITE && !PageUptodate(page)) {
-			struct page *head = compound_head(page);
-			int i;
+		if (list_empty_careful(&info->shrinklist)) {
+			list_add_tail(&info->shrinklist,
+				      &sbinfo->shrinklist);
+			sbinfo->shrinklist_len++;
+		}
+		spin_unlock(&sbinfo->shrinklist_lock);
+	}
 
-			for (i = 0; i < (1 << compound_order(head)); i++) {
-				clear_highpage(head + i);
-				flush_dcache_page(head + i);
-			}
-			SetPageUptodate(head);
+	/*
+	 * Let SGP_FALLOC use the SGP_WRITE optimization on a new page.
+	 */
+	if (sgp == SGP_FALLOC)
+		sgp = SGP_WRITE;
+clear:
+	/*
+	 * Let SGP_WRITE caller clear ends if write does not fill page;
+	 * but SGP_FALLOC on a page fallocated earlier must initialize
+	 * it now, lest undo on failure cancel our earlier guarantee.
+	 */
+	if (sgp != SGP_WRITE && !PageUptodate(page)) {
+		struct page *head = compound_head(page);
+		int i;
+
+		for (i = 0; i < (1 << compound_order(head)); i++) {
+			clear_highpage(head + i);
+			flush_dcache_page(head + i);
 		}
+		SetPageUptodate(head);
 	}
 
 	/* Perhaps the file has been truncated since we checked */
@@ -1879,9 +1921,6 @@ alloc_nohuge:		page = shmem_alloc_and_acct_page(gfp, inode,
 		put_page(page);
 		goto alloc_nohuge;
 	}
-failed:
-	if (swap.val && !shmem_confirm_swap(mapping, index, swap))
-		error = -EEXIST;
 unlock:
 	if (page) {
 		unlock_page(page);
-- 
2.17.1

