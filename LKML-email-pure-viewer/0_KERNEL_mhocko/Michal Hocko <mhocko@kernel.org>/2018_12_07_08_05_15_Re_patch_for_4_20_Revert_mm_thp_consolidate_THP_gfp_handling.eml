Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 07 Dec 2018 16:18:55 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga003.jf.intel.com (orsmga003.jf.intel.com [10.7.209.27])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id EFF22580375;
	Fri,  7 Dec 2018 00:05:25 -0800 (PST)
Received: from orsmga106.jf.intel.com ([10.7.208.65])
  by orsmga003-1.jf.intel.com with ESMTP; 07 Dec 2018 00:05:25 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3A6xfCQhbN8d+b+FVU8Cen/Jv/LSx+4OfEezUN459i?=
 =?us-ascii?q?sYplN5qZpcm/Zh7h7PlgxGXEQZ/co6odzbaO4+a4ASQp2tWoiDg6aptCVhsI24?=
 =?us-ascii?q?09vjcLJ4q7M3D9N+PgdCcgHc5PBxdP9nC/NlVJSo6lPwWB6nK94iQPFRrhKAF7?=
 =?us-ascii?q?Ovr6GpLIj8Swyuu+54Dfbx9HiTahYr5+Ngm6oRnMvcQKnIVuLbo8xAHUqXVSYe?=
 =?us-ascii?q?RWwm1oJVOXnxni48q74YBu/SdNtf8/7sBMSar1cbg2QrxeFzQmLns65Nb3uhnZ?=
 =?us-ascii?q?TAuA/WUTX2MLmRdVGQfF7RX6XpDssivms+d2xSeXMdHqQb0yRD+v9LlgRgP2hy?=
 =?us-ascii?q?gbNj456GDXhdJ2jKJHuxKquhhzz5fJbI2JKPZye6XQds4YS2VcRMZcTyJPDIOi?=
 =?us-ascii?q?YYUSDOQBM+lXoJXgqFQMoxS+HhGsCeH0xz9UmnP7x7E23/g7HA3Y2gErAtIAsG?=
 =?us-ascii?q?7TrNXwLKocVuG1w7XIzTrZa/NdxDXz6I/UfRAipvGHQLV9cc/RyUkoCwzFjVKQ?=
 =?us-ascii?q?pJfmPzyLzOQNtXaU7+56WeKokW4npBh8rz6yzckijYnJg5gaylHC9ShhxYY6O9?=
 =?us-ascii?q?i4SElhYd+kCpdfqziWO5JvTsw4W21ovTg1yrgeuZ68eCgKyYgoxh7FZ/ObaoSE?=
 =?us-ascii?q?+wvvW/yJLTdjhHJlfaywhxOo/Ue80+HwT9C430xMoyFYkdfMrmgA2wLP5sWDUP?=
 =?us-ascii?q?dx40ms1SiV2wzO6exIPVo4mKvZJpI537I9kpoevV7eEiL4mEj6lrGaels49uSy?=
 =?us-ascii?q?9ejqYrrrq5mBPIFukA7+KL4hmsmnDOQ4LAcOW2+b9Pyi1L3s40L5Wq9Gjv4ona?=
 =?us-ascii?q?nDtpDVO8AbqrS+Aw9P3YYv8xe/DzG439QEhXQLMk5JdRadg4T0NVzCPur0Aeq8?=
 =?us-ascii?q?jliwijtmxvLLMqXkAprXL3jDlLnhfax6605Z0AczydFf55RJCrAOOf7zWVH+tM?=
 =?us-ascii?q?beDhAnNwy42uHnCdt71owAQ2KCGbGZMKzMvl+S/O4vIPeDZJUTuDnjL/gp/fnu?=
 =?us-ascii?q?jWU2mVMFZ6mmwYMXaGykHvRhO0iZZXvsgtQfHmsQsQs+UffniFmDUT5VenazUL?=
 =?us-ascii?q?gw5jA9CIK6E4jDQpqhj6CG3Ce+BpdWfHxJCkiQEXf0cIWJQ/cMZziTIs99iDME?=
 =?us-ascii?q?UqKtS4881R60sg/6xKFqLu7V+i0eqJLi28J55+zVlREu6zN0C96R3H2KT2Fxhm?=
 =?us-ascii?q?kIXSM53LhjoUxhzVeOyap4g/tGGtBJ5PNJVQE6NZjbz+FhD9DyWwTBfsqGSVq8?=
 =?us-ascii?q?Q9WmBy0xQcw1w9MUf0l9HNCihAjZ3yW2G78Vi6CLBJss/63GxHjxJ8F9y3Xc2K?=
 =?us-ascii?q?k7lVYmQNBCNWmnhq556gjSCJTFk0Sfl6a2a6sc2DTB+3uEzWqLpEtYShJ/Ub3Z?=
 =?us-ascii?q?XXADYUvbtdf56VnET7O0DbQnMxFOyciNKqZRbt3pjFNGROrsOdjEYmKxnXuwCg?=
 =?us-ascii?q?iMxr+WcIXqfGAd1j3HCEcYiwAT4WqGNQ8mCyi8uW3eEiJhGUjvY0z29+l+s220?=
 =?us-ascii?q?TkkzwwGObE1h0r619wURhfydTfMTw70FtD0gqzVyAFaywdbWB8CcqApmeaVWec?=
 =?us-ascii?q?k970tf1WLFqwx9OYStIL14iV4AbQt7pUPu2A9xCoVbj8cqqmgnzA5zKaKezVNA?=
 =?us-ascii?q?eCmU3ZH2Or3LNGby+AqjZLLR2lHbyNyW4LsA6Owkq1X/uwGkDlYi83Ri09lSz3?=
 =?us-ascii?q?uc5pXLABAOUZ7rVUY37Rx6p7DcYiky/I7U0XxsMa+psj7Nwd4pBe0lygq+cNdb?=
 =?us-ascii?q?Kq+LCAjyE8gCDci0NOMqg0Spbg4DPO1K7qE0O92pdvSY16+rJupvhy+mgnld74?=
 =?us-ascii?q?B70UKM8DR8R/XM35YExfGYwwSGWy39jFenrsD4h4REaSsOEWq4zCjuHJRRabFq?=
 =?us-ascii?q?fYYXFWeuJNW6x9Vjh5L3R3FU7l+iB1Mc18+vdhqfdFj93QxW1UQKrn2rgyq4zz?=
 =?us-ascii?q?pokz43qqqTxjDBw+PndBAfIG5EWHFijUvwIYizl90aRkmoYBQzmxu/+En6wLJX?=
 =?us-ascii?q?pKJhIGnJQEdEZjT5IHtmUqStqLWCeclP6JUzvCVTUeS8Z02aS7Hnrxsb1SPjA3?=
 =?us-ascii?q?VRxDQhez62vZX5mgRwiHiBI3ZrsHrZZcZwyA/f5dzdX/JQ3iAKRDJliTnRHVW8?=
 =?us-ascii?q?O9ip/dOJl5bMqOy+VmShVoFNfinv14+PqCy75WhyCx2lg/+zgsHnERQ90SLj1d?=
 =?us-ascii?q?llTz/IrBX/Yonszai6KvhofkprBF/97cp1BId+n5A0hJER33gam5qU8WAGkWf1?=
 =?us-ascii?q?LdVUx6b+YGARSj4Mxt7f+BLl11F7LnKV24L5UW2Qw8t7aNm9eG8W2CM978ZRBa?=
 =?us-ascii?q?eQ7bxEmzZ1o1WioQLQZ/h9gikSyf805HEGhOEJvRInzj+BDbAKAUlYISvsmgyL?=
 =?us-ascii?q?79+kraVbfmavcaWq20p4ktCsF7WCogBaWHbkdZYuBy5w7sNjMF3S1H3/8J3reN?=
 =?us-ascii?q?7VbdgLrB2bjw/Aj/RJKJI2jvcKgCtnOWHnsnE/xe80kwdu3Y2ks4idMGpi4ri5?=
 =?us-ascii?q?AhFDOz3xZsMT/CztjKlEksaX2YCvAotuGjERUJT0SvKoFSoYte77OAaWDD08tn?=
 =?us-ascii?q?CbFKLDHQ+Y7UdqtXPOHIqtN3GKP3kZ1tRiSQKZJExehgAUQTo7koQ4FgCs2Mzu?=
 =?us-ascii?q?bkN56ioN6V7/rxtG0vhoOAXnUmfDuAeobS85R4OFIxpR6gFC+l3ZMciD7u9oGy?=
 =?us-ascii?q?FY/5uhrBGCK2CBZgRIC30JVVKAB1z5Irau4tzA+fCCBuWiN/vOfamOqetGWveK?=
 =?us-ascii?q?352v15Vp/y2WOsWTJHViDOA72kleXXB/AcvZgC4CSygWlyLLcs6aqw2w+ixxrs?=
 =?us-ascii?q?Cj7vvrXBjj6peIC7tXKd9v4Qy5gb+fN+6MgyZ0MTZZ1pQRxX/R1bgQxlgShzt1?=
 =?us-ascii?q?dzmqF7QAujXATKbRmq9REh4aZDl/NMpO76IgwAZNPdTXhc/y1r59lvQ1EUtKVU?=
 =?us-ascii?q?T9msG1YswHO2G9O03GBEaIN7SGJCfHw8LtYaO7Rr1fkv9UtwCruTuAF0/jPzKD?=
 =?us-ascii?q?lyTmVhy1MOFMij2bMwJauI2nbhltDm3jRsr8ahKnKN93kSE2wbosi3zQKG4TKi?=
 =?us-ascii?q?J8fF1Nr7GK6SNYg+5yG2hA7npjMOmFlDyV7+jeKpYKr/RrBj54mP5d4HQ/07FV?=
 =?us-ascii?q?9j1LRORpmCvOqd5ju02mkuiKyjZ9TBVCsCpEhIKVskVkIqjZ8phAWXDZ/BMC92?=
 =?us-ascii?q?mQChIKp8d7Bd3roaxf1t/PlKfrIjdY793U5dccB9TTKM+fMHsuKxzpGDvVDAsD?=
 =?us-ascii?q?VTGqNGHfiFZbkPGd7XCVqpk6qp7xmJsBULNbVVo1Fu8EBURhBtANPJB3XjZ32Y?=
 =?us-ascii?q?Kc2eIB+3uy5CPQScNGvJ3cHqaIAPjpNDSUlpFLYBwHxb6+JoMWYN7VwUtnP3x7?=
 =?us-ascii?q?l4TDHwLwWttOpWU1aw8zq0ZK2HtzVGs+3wTicAz7syxbLuK9ghNj0lg2Wu8q7j?=
 =?us-ascii?q?q5pg5vflc=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AGAADwKApch0O0hNFjGgEBAQEBAgEBA?=
 =?us-ascii?q?QEHAgEBAQGBVAIBAQEBCwGBWoIRJ5owFJJBhh4DTykTAYdZIjcGDQEDAQEBAQE?=
 =?us-ascii?q?BAgETAQEBCA0JCCkvgjYkAYJhAQEBAQIBAQIkEz8FAQkBAQoYCRMHCwMMBUkTB?=
 =?us-ascii?q?YMcgXoIBAGmNDOKMIwfF4FAP4ERgxKKXAKJNoFvlT0Jij+HAAsYiW2HSSyaSla?=
 =?us-ascii?q?BITMaCCgIgyeCJxcSbQEJjRU/MoEFAQGKPwEB?=
X-IPAS-Result: =?us-ascii?q?A0AGAADwKApch0O0hNFjGgEBAQEBAgEBAQEHAgEBAQGBVAI?=
 =?us-ascii?q?BAQEBCwGBWoIRJ5owFJJBhh4DTykTAYdZIjcGDQEDAQEBAQEBAgETAQEBCA0JC?=
 =?us-ascii?q?CkvgjYkAYJhAQEBAQIBAQIkEz8FAQkBAQoYCRMHCwMMBUkTBYMcgXoIBAGmNDO?=
 =?us-ascii?q?KMIwfF4FAP4ERgxKKXAKJNoFvlT0Jij+HAAsYiW2HSSyaSlaBITMaCCgIgyeCJ?=
 =?us-ascii?q?xcSbQEJjRU/MoEFAQGKPwEB?=
X-IronPort-AV: E=Sophos;i="5.56,324,1539673200"; 
   d="scan'208";a="43131285"
X-Amp-Result: UNSCANNABLE
X-Amp-File-Uploaded: False
Unscannable: 2
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 07 Dec 2018 00:05:24 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726030AbeLGIFV (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Fri, 7 Dec 2018 03:05:21 -0500
Received: from mx2.suse.de ([195.135.220.15]:42506 "EHLO mx1.suse.de"
        rhost-flags-OK-OK-OK-FAIL) by vger.kernel.org with ESMTP
        id S1725967AbeLGIFU (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Fri, 7 Dec 2018 03:05:20 -0500
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay2.suse.de (unknown [195.135.220.254])
        by mx1.suse.de (Postfix) with ESMTP id 4EABFABEF;
        Fri,  7 Dec 2018 08:05:17 +0000 (UTC)
Date: Fri, 7 Dec 2018 09:05:15 +0100
From: Michal Hocko <mhocko@kernel.org>
To: David Rientjes <rientjes@google.com>
Cc: Linus Torvalds <torvalds@linux-foundation.org>,
        Andrea Arcangeli <aarcange@redhat.com>,
        mgorman@techsingularity.net, Vlastimil Babka <vbabka@suse.cz>,
        ying.huang@intel.com, s.priebe@profihost.ag,
        Linux List Kernel Mailing <linux-kernel@vger.kernel.org>,
        alex.williamson@redhat.com, lkp@01.org, kirill@shutemov.name,
        Andrew Morton <akpm@linux-foundation.org>,
        zi.yan@cs.rutgers.edu
Subject: Re: [patch for-4.20] Revert "mm, thp: consolidate THP gfp handling
 into alloc_hugepage_direct_gfpmask"
Message-ID: <20181207080515.GT1286@dhcp22.suse.cz>
References: <alpine.DEB.2.21.1812051445390.35948@chino.kir.corp.google.com>
 <alpine.DEB.2.21.1812061341130.144733@chino.kir.corp.google.com>
 <alpine.DEB.2.21.1812061343240.144733@chino.kir.corp.google.com>
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: <alpine.DEB.2.21.1812061343240.144733@chino.kir.corp.google.com>
User-Agent: Mutt/1.10.1 (2018-07-13)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On Thu 06-12-18 14:00:20, David Rientjes wrote:
> This reverts commit 89c83fb539f95491be80cdd5158e6f0ce329e317.
> 
> There are a couple of issues with 89c83fb539f9 independent of its partial 
> revert in 2f0799a0ffc0 ("mm, thp: restore node-local hugepage 
> allocations"):
> 
> Firstly, the interaction between alloc_hugepage_direct_gfpmask() and 
> alloc_pages_vma() is racy wrt __GFP_THISNODE and MPOL_BIND.  
> alloc_hugepage_direct_gfpmask() makes sure not to set __GFP_THISNODE for 
> an MPOL_BIND policy but the policy used in alloc_pages_vma() may not be 
> the same for shared vma policies, triggering the WARN_ON_ONCE() in 
> policy_node().

Could you share a test case?

> Secondly, prior to 89c83fb539f9, alloc_pages_vma() implemented a somewhat 
> different policy for hugepage allocations, which were allocated through 
> alloc_hugepage_vma().  For hugepage allocations, if the allocating 
> process's node is in the set of allowed nodes, allocate with 
> __GFP_THISNODE for that node (for MPOL_PREFERRED, use that node with 
> __GFP_THISNODE instead).

Why is it wrong to fallback to an explicitly configured mbind mask?

> This was changed for shmem_alloc_hugepage() to 
> allow fallback to other nodes in 89c83fb539f9 as it did for new_page() in 
> mm/mempolicy.c which is functionally different behavior and removes the 
> requirement to only allocate hugepages locally.

Also why should new_page behave any differently for THP from any other
pages? There is no fallback allocation for those and so the mbind could
easily fail. So what is the actual rationale?

> The latter should have been reverted as part of 2f0799a0ffc0 as well.
> 
> Fully revert 89c83fb539f9 so that hugepage allocation policy is fully 
> restored and there is no race between alloc_hugepage_direct_gfpmask() and 
> alloc_pages_vma().
> 
> Fixes: 89c83fb539f9 ("mm, thp: consolidate THP gfp handling into alloc_hugepage_direct_gfpmask")
> Fixes: 2f0799a0ffc0 ("mm, thp: restore node-local hugepage allocations")
> Signed-off-by: David Rientjes <rientjes@google.com>
> ---
>  include/linux/gfp.h | 12 ++++++++----
>  mm/huge_memory.c    | 27 +++++++++++++--------------
>  mm/mempolicy.c      | 32 +++++++++++++++++++++++++++++---
>  mm/shmem.c          |  2 +-
>  4 files changed, 51 insertions(+), 22 deletions(-)
> 
> diff --git a/include/linux/gfp.h b/include/linux/gfp.h
> --- a/include/linux/gfp.h
> +++ b/include/linux/gfp.h
> @@ -510,18 +510,22 @@ alloc_pages(gfp_t gfp_mask, unsigned int order)
>  }
>  extern struct page *alloc_pages_vma(gfp_t gfp_mask, int order,
>  			struct vm_area_struct *vma, unsigned long addr,
> -			int node);
> +			int node, bool hugepage);
> +#define alloc_hugepage_vma(gfp_mask, vma, addr, order) \
> +	alloc_pages_vma(gfp_mask, order, vma, addr, numa_node_id(), true)
>  #else
>  #define alloc_pages(gfp_mask, order) \
>  		alloc_pages_node(numa_node_id(), gfp_mask, order)
> -#define alloc_pages_vma(gfp_mask, order, vma, addr, node)\
> +#define alloc_pages_vma(gfp_mask, order, vma, addr, node, false)\
> +	alloc_pages(gfp_mask, order)
> +#define alloc_hugepage_vma(gfp_mask, vma, addr, order) \
>  	alloc_pages(gfp_mask, order)
>  #endif
>  #define alloc_page(gfp_mask) alloc_pages(gfp_mask, 0)
>  #define alloc_page_vma(gfp_mask, vma, addr)			\
> -	alloc_pages_vma(gfp_mask, 0, vma, addr, numa_node_id())
> +	alloc_pages_vma(gfp_mask, 0, vma, addr, numa_node_id(), false)
>  #define alloc_page_vma_node(gfp_mask, vma, addr, node)		\
> -	alloc_pages_vma(gfp_mask, 0, vma, addr, node)
> +	alloc_pages_vma(gfp_mask, 0, vma, addr, node, false)
>  
>  extern unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order);
>  extern unsigned long get_zeroed_page(gfp_t gfp_mask);
> diff --git a/mm/huge_memory.c b/mm/huge_memory.c
> --- a/mm/huge_memory.c
> +++ b/mm/huge_memory.c
> @@ -629,30 +629,30 @@ static vm_fault_t __do_huge_pmd_anonymous_page(struct vm_fault *vmf,
>   *	    available
>   * never: never stall for any thp allocation
>   */
> -static inline gfp_t alloc_hugepage_direct_gfpmask(struct vm_area_struct *vma, unsigned long addr)
> +static inline gfp_t alloc_hugepage_direct_gfpmask(struct vm_area_struct *vma)
>  {
>  	const bool vma_madvised = !!(vma->vm_flags & VM_HUGEPAGE);
> -	const gfp_t gfp_mask = GFP_TRANSHUGE_LIGHT | __GFP_THISNODE;
>  
>  	/* Always do synchronous compaction */
>  	if (test_bit(TRANSPARENT_HUGEPAGE_DEFRAG_DIRECT_FLAG, &transparent_hugepage_flags))
> -		return GFP_TRANSHUGE | __GFP_THISNODE |
> -		       (vma_madvised ? 0 : __GFP_NORETRY);
> +		return GFP_TRANSHUGE | (vma_madvised ? 0 : __GFP_NORETRY);
>  
>  	/* Kick kcompactd and fail quickly */
>  	if (test_bit(TRANSPARENT_HUGEPAGE_DEFRAG_KSWAPD_FLAG, &transparent_hugepage_flags))
> -		return gfp_mask | __GFP_KSWAPD_RECLAIM;
> +		return GFP_TRANSHUGE_LIGHT | __GFP_KSWAPD_RECLAIM;
>  
>  	/* Synchronous compaction if madvised, otherwise kick kcompactd */
>  	if (test_bit(TRANSPARENT_HUGEPAGE_DEFRAG_KSWAPD_OR_MADV_FLAG, &transparent_hugepage_flags))
> -		return gfp_mask | (vma_madvised ? __GFP_DIRECT_RECLAIM :
> -						  __GFP_KSWAPD_RECLAIM);
> +		return GFP_TRANSHUGE_LIGHT |
> +			(vma_madvised ? __GFP_DIRECT_RECLAIM :
> +					__GFP_KSWAPD_RECLAIM);
>  
>  	/* Only do synchronous compaction if madvised */
>  	if (test_bit(TRANSPARENT_HUGEPAGE_DEFRAG_REQ_MADV_FLAG, &transparent_hugepage_flags))
> -		return gfp_mask | (vma_madvised ? __GFP_DIRECT_RECLAIM : 0);
> +		return GFP_TRANSHUGE_LIGHT |
> +		       (vma_madvised ? __GFP_DIRECT_RECLAIM : 0);
>  
> -	return gfp_mask;
> +	return GFP_TRANSHUGE_LIGHT;
>  }
>  
>  /* Caller must hold page table lock. */
> @@ -724,8 +724,8 @@ vm_fault_t do_huge_pmd_anonymous_page(struct vm_fault *vmf)
>  			pte_free(vma->vm_mm, pgtable);
>  		return ret;
>  	}
> -	gfp = alloc_hugepage_direct_gfpmask(vma, haddr);
> -	page = alloc_pages_vma(gfp, HPAGE_PMD_ORDER, vma, haddr, numa_node_id());
> +	gfp = alloc_hugepage_direct_gfpmask(vma);
> +	page = alloc_hugepage_vma(gfp, vma, haddr, HPAGE_PMD_ORDER);
>  	if (unlikely(!page)) {
>  		count_vm_event(THP_FAULT_FALLBACK);
>  		return VM_FAULT_FALLBACK;
> @@ -1295,9 +1295,8 @@ vm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf, pmd_t orig_pmd)
>  alloc:
>  	if (transparent_hugepage_enabled(vma) &&
>  	    !transparent_hugepage_debug_cow()) {
> -		huge_gfp = alloc_hugepage_direct_gfpmask(vma, haddr);
> -		new_page = alloc_pages_vma(huge_gfp, HPAGE_PMD_ORDER, vma,
> -				haddr, numa_node_id());
> +		huge_gfp = alloc_hugepage_direct_gfpmask(vma);
> +		new_page = alloc_hugepage_vma(huge_gfp, vma, haddr, HPAGE_PMD_ORDER);
>  	} else
>  		new_page = NULL;
>  
> diff --git a/mm/mempolicy.c b/mm/mempolicy.c
> --- a/mm/mempolicy.c
> +++ b/mm/mempolicy.c
> @@ -1116,8 +1116,8 @@ static struct page *new_page(struct page *page, unsigned long start)
>  	} else if (PageTransHuge(page)) {
>  		struct page *thp;
>  
> -		thp = alloc_pages_vma(GFP_TRANSHUGE, HPAGE_PMD_ORDER, vma,
> -				address, numa_node_id());
> +		thp = alloc_hugepage_vma(GFP_TRANSHUGE, vma, address,
> +					 HPAGE_PMD_ORDER);
>  		if (!thp)
>  			return NULL;
>  		prep_transhuge_page(thp);
> @@ -2011,6 +2011,7 @@ static struct page *alloc_page_interleave(gfp_t gfp, unsigned order,
>   * 	@vma:  Pointer to VMA or NULL if not available.
>   *	@addr: Virtual Address of the allocation. Must be inside the VMA.
>   *	@node: Which node to prefer for allocation (modulo policy).
> + *	@hugepage: for hugepages try only the preferred node if possible
>   *
>   * 	This function allocates a page from the kernel page pool and applies
>   *	a NUMA policy associated with the VMA or the current process.
> @@ -2021,7 +2022,7 @@ static struct page *alloc_page_interleave(gfp_t gfp, unsigned order,
>   */
>  struct page *
>  alloc_pages_vma(gfp_t gfp, int order, struct vm_area_struct *vma,
> -		unsigned long addr, int node)
> +		unsigned long addr, int node, bool hugepage)
>  {
>  	struct mempolicy *pol;
>  	struct page *page;
> @@ -2039,6 +2040,31 @@ alloc_pages_vma(gfp_t gfp, int order, struct vm_area_struct *vma,
>  		goto out;
>  	}
>  
> +	if (unlikely(IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE) && hugepage)) {
> +		int hpage_node = node;
> +
> +		/*
> +		 * For hugepage allocation and non-interleave policy which
> +		 * allows the current node (or other explicitly preferred
> +		 * node) we only try to allocate from the current/preferred
> +		 * node and don't fall back to other nodes, as the cost of
> +		 * remote accesses would likely offset THP benefits.
> +		 *
> +		 * If the policy is interleave, or does not allow the current
> +		 * node in its nodemask, we allocate the standard way.
> +		 */
> +		if (pol->mode == MPOL_PREFERRED && !(pol->flags & MPOL_F_LOCAL))
> +			hpage_node = pol->v.preferred_node;
> +
> +		nmask = policy_nodemask(gfp, pol);
> +		if (!nmask || node_isset(hpage_node, *nmask)) {
> +			mpol_cond_put(pol);
> +			page = __alloc_pages_node(hpage_node,
> +						gfp | __GFP_THISNODE, order);
> +			goto out;
> +		}
> +	}
> +
>  	nmask = policy_nodemask(gfp, pol);
>  	preferred_nid = policy_node(gfp, pol, node);
>  	page = __alloc_pages_nodemask(gfp, order, preferred_nid, nmask);
> diff --git a/mm/shmem.c b/mm/shmem.c
> --- a/mm/shmem.c
> +++ b/mm/shmem.c
> @@ -1439,7 +1439,7 @@ static struct page *shmem_alloc_hugepage(gfp_t gfp,
>  
>  	shmem_pseudo_vma_init(&pvma, info, hindex);
>  	page = alloc_pages_vma(gfp | __GFP_COMP | __GFP_NORETRY | __GFP_NOWARN,
> -			HPAGE_PMD_ORDER, &pvma, 0, numa_node_id());
> +			HPAGE_PMD_ORDER, &pvma, 0, numa_node_id(), true);
>  	shmem_pseudo_vma_destroy(&pvma);
>  	if (page)
>  		prep_transhuge_page(page);

-- 
Michal Hocko
SUSE Labs
