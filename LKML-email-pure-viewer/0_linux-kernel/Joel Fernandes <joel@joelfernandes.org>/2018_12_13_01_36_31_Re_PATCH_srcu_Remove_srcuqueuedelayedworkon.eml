Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by i7-8700 with POP3-SSL;
  13 Dec 2018 09:34:08 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga001.fm.intel.com (fmsmga001.fm.intel.com [10.253.24.23])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id A346D580380;
	Wed, 12 Dec 2018 17:36:41 -0800 (PST)
Received: from orsmga103.jf.intel.com ([10.7.208.35])
  by fmsmga001-1.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 12 Dec 2018 17:36:41 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3ATBBQ+BAAirmjQhpcvMfYUyQJP3N1i/DPJgcQr6Af?=
 =?us-ascii?q?oPdwSP7+r8+wAkXT6L1XgUPTWs2DsrQY07qQ6/iocFdDyK7JiGoFfp1IWk1Nou?=
 =?us-ascii?q?QttCtkPvS4D1bmJuXhdS0wEZcKflZk+3amLRodQ56mNBXdrXKo8DEdBAj0OxZr?=
 =?us-ascii?q?KeTpAI7SiNm82/yv95HJbAhEmDmwbaluIBmqsA7cqtQYjYx+J6gr1xDHuGFIe+?=
 =?us-ascii?q?NYxWNpIVKcgRPx7dqu8ZBg7ipdpesv+9ZPXqvmcas4S6dYDCk9PGAu+MLrrxjD?=
 =?us-ascii?q?QhCR6XYaT24bjwBHAwnB7BH9Q5fxri73vfdz1SWGIcH7S60/VC+85Kl3VhDnlC?=
 =?us-ascii?q?YHNyY48G7JjMxwkLlbqw+lqxBm3oLYfJ2ZOP94c6jAf90VWHBBU95fWSJBHI2y?=
 =?us-ascii?q?cogBD+QOMulEsobypVUBrQCmBQSuH+7v1iNEi2Xq0aEmyektDwfL1xEgEdIUt3?=
 =?us-ascii?q?TUqc34OqMVUe+ryKnD0DXNYO1M2Tf78ofIdA0uru+XXbltdsfRy04vGB3BjliL?=
 =?us-ascii?q?q4zlOC2a1uADs2eF9epgU/igi2g6pA5vuTij3MAsipPGho8MzF3P6Ct3wIEwJd?=
 =?us-ascii?q?KiSU57Z8apEJ9RtyGcKot3TdsiQ2V1uCY/0bIJp4S7fDMWx5QgwR7fZeaLc4+S?=
 =?us-ascii?q?4hLsUuuaPDR2hGp9db6hmxq/9VKsxvDyW8WqylpGsylInsXWun0M1RHf8taLR/?=
 =?us-ascii?q?pj8ku93DuDywXe5vxYLUwoiabWLoMtz78smpYOtEnOEDX5l1v4jKKTeEgo5+el?=
 =?us-ascii?q?6+rib7r9opKTKZV7hwT9P6ktnsG/D+I1ORUUUWeB4+Szzrjj8FX5QLpUiv02lb?=
 =?us-ascii?q?HUsI7VJcsFvK61GQxV3Zg56xa5ETim1M4UnX4dLFJKYB6Hjo7pNE/SIP3gE/uz?=
 =?us-ascii?q?n1ChnC12y/3IILHtGIjBI3vfnLv7fLtw6VZQyA8pwtBe45JUBKsBIPX2WkLpsN?=
 =?us-ascii?q?zYDxk5MxG7wur+C9VyyJkeWWSRDa+dKa/StlGJ5uQxLOmWf4IVpjn9JOY/5/L0?=
 =?us-ascii?q?jn82h0Udfa+30psTcny4Ge5mI0qBbXr2ntgBCXsKvhY5TOHyjF2CUD1TaGioU6?=
 =?us-ascii?q?Mz+zE2E4amDYbFRoCwj72Nxia7HptKZm9YDlCAC2vnd4KBW/0UciKdPtdhkiAY?=
 =?us-ascii?q?VbimU4Ih1QuhtA7my7V9KerY4C0YtY/529hz6O3ejhUy9T1yD8SA3GCBVWB0nm?=
 =?us-ascii?q?UURzAo2KBzu1ByylCG0aJgmfxXCcRT5+9VUgc9LZPczet6BM7oVgLCedeJTlCm?=
 =?us-ascii?q?QtK9DDE1T9IxxcIOYklnF9WjiBDDwzSlA7sPm7OXA5w097rW32LtKMZl13bGyK?=
 =?us-ascii?q?4hgkE8TctUNW2mga1/+xLJB4HTkUWUjKKqdaUa3C7Q+2aP12uOvEdEUAFuVaXJ?=
 =?us-ascii?q?R2wQZkzTrd7h/EPNU6euCag7MgtG0cOCKbFFatvzgVVCRffsItLeY22qlme0BB?=
 =?us-ascii?q?aIwK6MbYXwd2Uc2iXdFFYLkwQJ8XmaMgg+Az+ro3jCAzx2CVLvf0Ts/PFjp3yh?=
 =?us-ascii?q?UE870RuGb0172Lqz4R4am/qcR/QX3rIHvSchry55HFK839LQFtqBqBBtfKRaYd?=
 =?us-ascii?q?Mh/lhH0XjVuBB6PpylN6pinEIRcxxrv0Py0BV6EphPntI0rHw01gZyKbiX0FVa?=
 =?us-ascii?q?dzyG25D9IbnXKmj0/BCyZK/awFDe0NCK+qgR7PQ0sUnsvAasFkA66XVoz8FV02?=
 =?us-ascii?q?eA5pXNFAcSTZPxUkMw9xhmp7HbYjMx55/O2X1rK6m0tj7C29QmBOY+zhagftFf?=
 =?us-ascii?q?ML6LFQPoEs0aAdSuJ/Ium1Szch0EO+VS/rYuP8y6b/uGxLKrPOF4kT28iWRI/o?=
 =?us-ascii?q?983VyM9iZmUeHIwosKzOue3guEUDf8kkysssT2mYBCeDETEXCzySniBI5NeKJy?=
 =?us-ascii?q?eZwHBnupI82y3t9+nYLiW2ZE9F6/AFMLwM+oeQeIY1PhxwFR1EQXrmahmSu31D?=
 =?us-ascii?q?F0lzAprqyC3C3B2ejidRwHOnJVS2lml1vjPY+0j9UCVkiycwcpjAel5Vr9x6VD?=
 =?us-ascii?q?vqR/LnXcTl1SfyfrKGFuSK2wuaSYY85O8Z8nrT9YUOChbl+EUL79pB0a3jjnH2?=
 =?us-ascii?q?tfwjA7aj6rto/4nxx8lGKSMnJzoGDFdsF3wBfV/MbcSuJJ3joaWCl4jiHaB1i9?=
 =?us-ascii?q?P9Wz/dSYjYzDsv24V2+6Up1TcC/rzZ6PtSeh5G1qBwG/kO63mtH9DQc61ir73c?=
 =?us-ascii?q?FwVSrUtBb8fpXr16OiPOJlZEZoHlz868l9Go1kiYs/npIQ2XsbhpWT43UHl3z+?=
 =?us-ascii?q?MdFa2aL4cXoMSiQHw9/T4Aj5xkJjKmiFyJ7+VnWY2sFhfcW1YnsK2iIh6MBHEK?=
 =?us-ascii?q?eV46ZDnSdvoFq4rATRbON5njccz/su9XEbj/sIuAor0iWSHLQSEVNEMizrkhSC?=
 =?us-ascii?q?98q+o7lPZGazbbiw01JzndO7A7GDuA1cWGv5dY0kHS9/9ch/NFPM0Hvu6oDrYt?=
 =?us-ascii?q?XQbNQTtgGKnBfEleRaNJUxlv8Sjyp9JW39pWEly/I8jRF22JG6vYuHJH938KO3?=
 =?us-ascii?q?HBFYLSH1aN0J+j72k6lehMmW0purHpVgHDULQZTpQeipEDIUqfToKQKOHCcgpX?=
 =?us-ascii?q?ecHLrVBRWf51t+r3LTD5CrMGmaJGUezdVnXhWcJVZQgAYJXDUhhZ45FxunxMjg?=
 =?us-ascii?q?cEd/+zAQ6UT0qhpKyuJ0KRb/Vn3TqxuvajcxUJKfNgZZ7hle50fJNsyT9vl8ED?=
 =?us-ascii?q?tf/p2lsQyBMGibZxlTAGENW0yEAU3jP7a06dnB9eiYGvSxL//UbbqSruxeUu+C?=
 =?us-ascii?q?xYiz3Yt+4zaMKsKPM2FgD/Ih30pMQ2t5F97FmzkVSCwXiiHNb9OdpBim4S16tc?=
 =?us-ascii?q?S/8PXtWALy6oqDEbpSMdNz+x+ohaePLfKfhCF8KTxAzJMD2WfIyKQD3F4Vkyxv?=
 =?us-ascii?q?dz6tEakZtS7QVq3QnLVbDwUcayN1O8tF9KY83ghLOc7GhdL5zL94jvgpC1hbUV?=
 =?us-ascii?q?ztgN2mZcsPI2ulLlPIGF6LNKiaJT3M28z3Yb2zSbxTjOVXth28ozWbE1XkPjSM?=
 =?us-ascii?q?iTbpTQ2vMfpXgSGfPRxevpy9cxl3BWjiStLmdgO0MNttgTIqxr00g2vANXQAPj?=
 =?us-ascii?q?hkb0NNsrqQ4DtCjfViAGNB9GRqLOmelyad7unVMZIWsfptAiRpmONW+nU6y71J?=
 =?us-ascii?q?7C5aQPx5gjfdrtlro1u+iOmA1iJnUAZSqjZMnI+LoURiOaDD+pZcQ3rL4BIN4n?=
 =?us-ascii?q?uWCxkRo9tlC9vvu71fy9TVlaLzLitC/MzQ/ccGG8fUL8eHOmI7MRX1AD7UEBcF?=
 =?us-ascii?q?TTmzOGDfnUNdlvSS9nyTrpciq5nsmIABSrlUVFEuEvMaC0JlHMENIZttXzMkl6?=
 =?us-ascii?q?Kbg9AM5XaksBbRQ8Ba7dj7UafYOfzyLH60y/FhbDgFx7X1PM5bYpXyxkhnYVpS?=
 =?us-ascii?q?mIXMBlqWXNpI5CZma1lnjl9K9S1cSWA3k2zocAWk5nMeDrbgnBk7jiN8Zukp+j?=
 =?us-ascii?q?r25V4xKlfR4iwqnx9iyp3enTmNfWupf++LVoZMBn+x7hBpPw=3D=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0ABAABHtxFch0O0hNFkGQEBAQEBAQEBA?=
 =?us-ascii?q?QEBAQcBAQEBAQGBUQQBAQEBAQsBgTAqgQ9LNyeMFYwSgg0UaJFbhHwUgWIPAQE?=
 =?us-ascii?q?YDQYBhz4iNAkNAQMBAQEBAQECARMBAQEIDQkIKSMMgjYkAYJiAQIBAgECJBMGA?=
 =?us-ascii?q?QE3AQUJAQEKGAklAwwFIAEFASITBYJRSwGCAQEECps3PIodgWwzgnYBAQWBMAG?=
 =?us-ascii?q?FegMFEosOgRwXPoFBgRABgl01gx4BAoEtARIBg1SCJok/gXOFE5BJCYInhGSKQ?=
 =?us-ascii?q?QsYYHyFGoJzh10sjXGKewYCCQcPIYElgR1xTTAIgyeCGwwXiF6FYB8ygQIDAQE?=
 =?us-ascii?q?hE4l5gj4BAQ?=
X-IPAS-Result: =?us-ascii?q?A0ABAABHtxFch0O0hNFkGQEBAQEBAQEBAQEBAQcBAQEBAQG?=
 =?us-ascii?q?BUQQBAQEBAQsBgTAqgQ9LNyeMFYwSgg0UaJFbhHwUgWIPAQEYDQYBhz4iNAkNA?=
 =?us-ascii?q?QMBAQEBAQECARMBAQEIDQkIKSMMgjYkAYJiAQIBAgECJBMGAQE3AQUJAQEKGAk?=
 =?us-ascii?q?lAwwFIAEFASITBYJRSwGCAQEECps3PIodgWwzgnYBAQWBMAGFegMFEosOgRwXP?=
 =?us-ascii?q?oFBgRABgl01gx4BAoEtARIBg1SCJok/gXOFE5BJCYInhGSKQQsYYHyFGoJzh10?=
 =?us-ascii?q?sjXGKewYCCQcPIYElgR1xTTAIgyeCGwwXiF6FYB8ygQIDAQEhE4l5gj4BAQ?=
X-IronPort-AV: E=Sophos;i="5.56,346,1539673200"; 
   d="scan'208";a="56136250"
X-Amp-Result: UNSCANNABLE
X-Amp-File-Uploaded: False
Unscannable: 2
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 12 Dec 2018 17:36:38 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726555AbeLMBgg (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Wed, 12 Dec 2018 20:36:36 -0500
Received: from mail-pl1-f196.google.com ([209.85.214.196]:41928 "EHLO
        mail-pl1-f196.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726372AbeLMBgf (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Wed, 12 Dec 2018 20:36:35 -0500
Received: by mail-pl1-f196.google.com with SMTP id u6so214242plm.8
        for <linux-kernel@vger.kernel.org>; Wed, 12 Dec 2018 17:36:34 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=joelfernandes.org; s=google;
        h=date:from:to:cc:subject:message-id:references:mime-version
         :content-disposition:in-reply-to:user-agent;
        bh=IoHQwpL0rJhSTqL2suZk1mgmpF/tiPl3STGKg8CTiaA=;
        b=c4UvT2w2MEndzVyE+k1z0yEy1iOvuTm1GHlUoszuZCY2/NWFt86Q3L3oFy667yfBBF
         TozgbnvdkYaG/u9vcXe2DgqAlZ74lwDl6L2VAAyBSNR/a2QJ3vTyj3lne4YzzImxceA0
         FRQuZ4h0F0Db99vl8lRq4GuPi0wP9xoB6fcFI=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:date:from:to:cc:subject:message-id:references
         :mime-version:content-disposition:in-reply-to:user-agent;
        bh=IoHQwpL0rJhSTqL2suZk1mgmpF/tiPl3STGKg8CTiaA=;
        b=p3YSy0g4fLKiqObHKjPH8cAj92c8acblyMUS7rPTH5t9sD8wQlI+MGc2cshRCpH481
         zklKUSYm2a29FCX81h7Mq+QOXjT+QT4zgAHqJNeqF8dYXRWlZVuv4KMAuX1VALWpq46G
         WT/wl361/xgKbZl0sAp4NhWSyzYzvD/MkMOB44ZJrJYj4QRnP99ub4PTDrPrYch85Wwa
         ucEMsmmUpCCbQly1C0rqriceJQnj7a9lRdUkuOAzH/kESBymktFBLphU2cWBjSJevWln
         Or9GsbzIKsIybjwjtraL0CtV6XLmluuthegunE3wmhJI30YbTbh/POUHvAtaG5zaUcRN
         vnMA==
X-Gm-Message-State: AA+aEWZyp0xZyzB6/0OHJNsZTZ3hxqn40t8EGpnTdzQdUgvDPqmd70Sa
        0ATvSUg6evCrBStE6NCSXPA1tw==
X-Google-Smtp-Source: AFSGD/UMBOTkJ/KJYhWyrisyFQbLdtP2olaksPKsPMXEo0nJpUvr6oKjhB3ydlwyx/77nj5Z5tL1xw==
X-Received: by 2002:a17:902:8687:: with SMTP id g7mr21759643plo.96.1544664993205;
        Wed, 12 Dec 2018 17:36:33 -0800 (PST)
Received: from localhost ([2620:0:1000:1601:3aef:314f:b9ea:889f])
        by smtp.gmail.com with ESMTPSA id b10sm301250pfj.183.2018.12.12.17.36.31
        (version=TLS1_2 cipher=ECDHE-RSA-CHACHA20-POLY1305 bits=256/256);
        Wed, 12 Dec 2018 17:36:32 -0800 (PST)
Date: Wed, 12 Dec 2018 17:36:31 -0800
From: Joel Fernandes <joel@joelfernandes.org>
To: "Paul E. McKenney" <paulmck@linux.ibm.com>
Cc: Boqun Feng <boqun.feng@gmail.com>,
        Sebastian Andrzej Siewior <bigeasy@linutronix.de>,
        linux-kernel@vger.kernel.org,
        Lai Jiangshan <jiangshanlai@gmail.com>,
        Josh Triplett <josh@joshtriplett.org>,
        Steven Rostedt <rostedt@goodmis.org>,
        Mathieu Desnoyers <mathieu.desnoyers@efficios.com>,
        tglx@linutronix.de, tj@kernel.org
Subject: Re: [PATCH] srcu: Remove srcu_queue_delayed_work_on()
Message-ID: <20181213013631.GF89903@joelaf.mtv.corp.google.com>
References: <20181211111238.13474-1-bigeasy@linutronix.de>
 <20181212014016.GA97542@google.com>
 <20181212140519.GA10937@tardis>
 <20181212155546.GY4170@linux.ibm.com>
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: <20181212155546.GY4170@linux.ibm.com>
User-Agent: Mutt/1.10.1 (2018-07-13)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On Wed, Dec 12, 2018 at 07:55:46AM -0800, Paul E. McKenney wrote:
> On Wed, Dec 12, 2018 at 10:05:19PM +0800, Boqun Feng wrote:
> > Hi Joel,
> > 
> > On Tue, Dec 11, 2018 at 05:40:16PM -0800, Joel Fernandes wrote:
> > > On Tue, Dec 11, 2018 at 12:12:38PM +0100, Sebastian Andrzej Siewior wrote:
> > > > srcu_queue_delayed_work_on() disables preemption (and therefore CPU
> > > > hotplug in RCU's case) and then checks based on its own accounting if a
> > > > CPU is online. If the CPU is online it uses queue_delayed_work_on()
> > > > otherwise it fallbacks to queue_delayed_work().
> > > > The problem here is that queue_work() on -RT does not work with disabled
> > > > preemption.
> > > > 
> > > > queue_work_on() works also on an offlined CPU. queue_delayed_work_on()
> > > > has the problem that it is possible to program a timer on an offlined
> > > > CPU. This timer will fire once the CPU is online again. But until then,
> > > > the timer remains programmed and nothing will happen.
> > > > Add a local timer which will fire (as requested per delay) on the local
> > > > CPU and then enqueue the work on the specific CPU.
> > > > 
> > > > RCUtorture testing with SRCU-P for 24h showed no problems.
> > > > 
> > > > Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
> > > > ---
> > > >  include/linux/srcutree.h |  3 ++-
> > > >  kernel/rcu/srcutree.c    | 57 ++++++++++++++++++----------------------
> > > >  kernel/rcu/tree.c        |  4 ---
> > > >  kernel/rcu/tree.h        |  8 ------
> > > >  4 files changed, 27 insertions(+), 45 deletions(-)
> > > > 
> > > > diff --git a/include/linux/srcutree.h b/include/linux/srcutree.h
> > > > index 6f292bd3e7db7..0faa978c98807 100644
> > > > --- a/include/linux/srcutree.h
> > > > +++ b/include/linux/srcutree.h
> > > > @@ -45,7 +45,8 @@ struct srcu_data {
> > > >  	unsigned long srcu_gp_seq_needed;	/* Furthest future GP needed. */
> > > >  	unsigned long srcu_gp_seq_needed_exp;	/* Furthest future exp GP. */
> > > >  	bool srcu_cblist_invoking;		/* Invoking these CBs? */
> > > > -	struct delayed_work work;		/* Context for CB invoking. */
> > > > +	struct timer_list delay_work;		/* Delay for CB invoking */
> > > > +	struct work_struct work;		/* Context for CB invoking. */
> > > >  	struct rcu_head srcu_barrier_head;	/* For srcu_barrier() use. */
> > > >  	struct srcu_node *mynode;		/* Leaf srcu_node. */
> > > >  	unsigned long grpmask;			/* Mask for leaf srcu_node */
> > > > diff --git a/kernel/rcu/srcutree.c b/kernel/rcu/srcutree.c
> > > > index 3600d88d8956b..7f041f2435df9 100644
> > > > --- a/kernel/rcu/srcutree.c
> > > > +++ b/kernel/rcu/srcutree.c
> > > > @@ -58,6 +58,7 @@ static bool __read_mostly srcu_init_done;
> > > >  static void srcu_invoke_callbacks(struct work_struct *work);
> > > >  static void srcu_reschedule(struct srcu_struct *ssp, unsigned long delay);
> > > >  static void process_srcu(struct work_struct *work);
> > > > +static void srcu_delay_timer(struct timer_list *t);
> > > >  
> > > >  /* Wrappers for lock acquisition and release, see raw_spin_lock_rcu_node(). */
> > > >  #define spin_lock_rcu_node(p)					\
> > > > @@ -156,7 +157,8 @@ static void init_srcu_struct_nodes(struct srcu_struct *ssp, bool is_static)
> > > >  			snp->grphi = cpu;
> > > >  		}
> > > >  		sdp->cpu = cpu;
> > > > -		INIT_DELAYED_WORK(&sdp->work, srcu_invoke_callbacks);
> > > > +		INIT_WORK(&sdp->work, srcu_invoke_callbacks);
> > > > +		timer_setup(&sdp->delay_work, srcu_delay_timer, 0);
> > > >  		sdp->ssp = ssp;
> > > >  		sdp->grpmask = 1 << (cpu - sdp->mynode->grplo);
> > > >  		if (is_static)
> > > > @@ -386,13 +388,19 @@ void _cleanup_srcu_struct(struct srcu_struct *ssp, bool quiesced)
> > > >  	} else {
> > > >  		flush_delayed_work(&ssp->work);
> > > >  	}
> > > > -	for_each_possible_cpu(cpu)
> > > > +	for_each_possible_cpu(cpu) {
> > > > +		struct srcu_data *sdp = per_cpu_ptr(ssp->sda, cpu);
> > > > +
> > > >  		if (quiesced) {
> > > > -			if (WARN_ON(delayed_work_pending(&per_cpu_ptr(ssp->sda, cpu)->work)))
> > > > +			if (WARN_ON(timer_pending(&sdp->delay_work)))
> > > > +				return; /* Just leak it! */
> > > > +			if (WARN_ON(work_pending(&sdp->work)))
> > > >  				return; /* Just leak it! */
> > > >  		} else {
> > > > -			flush_delayed_work(&per_cpu_ptr(ssp->sda, cpu)->work);
> > > > +			del_timer_sync(&sdp->delay_work);
> > > > +			flush_work(&sdp->work);
> > > >  		}
> > > > +	}
> > > >  	if (WARN_ON(rcu_seq_state(READ_ONCE(ssp->srcu_gp_seq)) != SRCU_STATE_IDLE) ||
> > > >  	    WARN_ON(srcu_readers_active(ssp))) {
> > > >  		pr_info("%s: Active srcu_struct %p state: %d\n",
> > > > @@ -463,39 +471,23 @@ static void srcu_gp_start(struct srcu_struct *ssp)
> > > >  	WARN_ON_ONCE(state != SRCU_STATE_SCAN1);
> > > >  }
> > > >  
> > > > -/*
> > > > - * Track online CPUs to guide callback workqueue placement.
> > > > - */
> > > > -DEFINE_PER_CPU(bool, srcu_online);
> > > >  
> > > > -void srcu_online_cpu(unsigned int cpu)
> > > > +static void srcu_delay_timer(struct timer_list *t)
> > > >  {
> > > > -	WRITE_ONCE(per_cpu(srcu_online, cpu), true);
> > > > +	struct srcu_data *sdp = container_of(t, struct srcu_data, delay_work);
> > > > +
> > > > +	queue_work_on(sdp->cpu, rcu_gp_wq, &sdp->work);
> > > >  }
> > > >  
> > > > -void srcu_offline_cpu(unsigned int cpu)
> > > > -{
> > > > -	WRITE_ONCE(per_cpu(srcu_online, cpu), false);
> > > > -}
> > > > -
> > > > -/*
> > > > - * Place the workqueue handler on the specified CPU if online, otherwise
> > > > - * just run it whereever.  This is useful for placing workqueue handlers
> > > > - * that are to invoke the specified CPU's callbacks.
> > > > - */
> > > > -static bool srcu_queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
> > > > -				       struct delayed_work *dwork,
> > > > +static void srcu_queue_delayed_work_on(struct srcu_data *sdp,
> > > >  				       unsigned long delay)
> > > >  {
> > > > -	bool ret;
> > > > +	if (!delay) {
> > > > +		queue_work_on(sdp->cpu, rcu_gp_wq, &sdp->work);
> > > > +		return;
> > > > +	}
> > > >  
> > > > -	preempt_disable();
> > > > -	if (READ_ONCE(per_cpu(srcu_online, cpu)))
> > > > -		ret = queue_delayed_work_on(cpu, wq, dwork, delay);
> > > > -	else
> > > > -		ret = queue_delayed_work(wq, dwork, delay);
> > > > -	preempt_enable();
> > > 
> > > The deleted code looks like 'cpu' could be offlined.
> > > 
> > > Question for my clarification: According to sync_rcu_exp_select_cpus, we have
> > > to disable preemption before calling queue_work_on to ensure the CPU is online.
> > > 
> > > Also same is said in Boqun's presentation on the topic:
> > > https://linuxplumbersconf.org/event/2/contributions/158/attachments/68/79/workqueue_and_cpu_hotplug.pdf
> > > 
> > > Calling queue_work_on on an offline CPU sounds like could be problematic. So
> > > in your patch, don't you still need to disable preemption around
> > > queue_work_on, or the very least check if said CPU is online before calling
> > > queue_work_on?
> > > 
> > 
> > I should be the one who answers this ;-)
> > So I found something after my LPC topic, that is queue_work_on() may
> > work well even if racing with a cpu offline. The reason is that in the
> > cpu offline hook for workqueue only switches the percpu pool into an
> > unbound one, so as long as the related cpu has been online once, we are
> > fine here. 
> > 
> > Please note this is only my own analysis, and I'm the one who told
> > Sebastian and Paul about this.
> > 
> > I should send an email to check with workqueue maintainers about this,
> > but I didn't find the time. Sorry about this.. But let's do this right
> > now, while we at it.
> > 
> > (Cc TJ)
> > 
> > So Jiangshan and TJ, what's your opion on this one? If we call a
> > queue_work_on() at a place where that target cpu may be offlined, I
> > think we have the guarantee that the work will be eventually executed
> > even if the cpu is never online again, right? In other words, if a cpu
> > has been online once, queue_work_on() on it will be free from racing
> > with cpu hotplug.
> > 
> > Am I right about this, or did I miss something subtle?
> 
> Well, what I was relying on was Thomas Gleixner's assertion that it should
> work and would be fixed if it didn't.  ;-)

FWIW I did a simple manual test calling queue_work_on on an offline CPU to
see what happens and it appears to be working fine. On a 4 CPU system, I
offline CPU 3 and the work ends up executing on CPU 0 instead. But I haven't
done any stress tests to know of any possible races with doing so.

thanks,

- Joel

