Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by i7-8700 with POP3-SSL;
  19 Dec 2018 08:37:09 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga006.fm.intel.com (fmsmga006.fm.intel.com [10.253.24.20])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id D910C5805F0;
	Tue, 18 Dec 2018 05:47:49 -0800 (PST)
Received: from orsmga102-1.jf.intel.com (HELO mga09.intel.com) ([10.7.208.27])
  by fmsmga006-1.fm.intel.com with ESMTP; 18 Dec 2018 05:47:49 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3Az9PhmBGGhs3z+dPXLj+s7Z1GYnF86YWxBRYc798d?=
 =?us-ascii?q?s5kLTJ75oc6zbnLW6fgltlLVR4KTs6sC17KG9fi4EUU7or+5+EgYd5JNUxJXwe?=
 =?us-ascii?q?43pCcHRPC/NEvgMfTxZDY7FskRHHVs/nW8LFQHUJ2mPw6arXK99yMdFQviPgRp?=
 =?us-ascii?q?OOv1BpTSj8Oq3Oyu5pHfeQpFiCa+bL9oMBm6sRjau9ULj4dlNqs/0AbCrGFSe+?=
 =?us-ascii?q?RRy2NoJFaTkAj568yt4pNt8Dletuw4+cJYXqr0Y6o3TbpDDDQ7KG81/9HktQPC?=
 =?us-ascii?q?TQSU+HQRVHgdnwdSDAjE6BH6WYrxsjf/u+Fg1iSWIdH6QLYpUjm58axlVAHnhz?=
 =?us-ascii?q?sGNz4h8WHYlMpwjL5AoBm8oxBz2pPYbJ2JOPZ7eK7WYNEUSndbXstJVyJPHJ6y?=
 =?us-ascii?q?YIkBD+QPPuhXoJXyp0AWrRa8HgSsGP/jxyVUinPqwaE2z+IsGhzG0gw6GNIOtW?=
 =?us-ascii?q?zZotL0NKgOUeC61rfHzTHeZP1Z3Tf97JbHcgokof6WW7J7bM3cyUw3FwzblVif?=
 =?us-ascii?q?t4jlPzeL2eQXtmiU9exgWfiui2E6sQ1+uCWvy94qh4LUhYwV0kjJ+TtlzIsxP9?=
 =?us-ascii?q?G0VUB2bcC+HJdNtCyWK5F6T8IgTm1wpio21rMLtYSmcCQX0pgqxQPTZ+aaf4WO?=
 =?us-ascii?q?/xntTvyeIS1ii3JgYL+/hwi98UynyuDkSMm030hFrjBfntnPqH8NzRrT5daDSv?=
 =?us-ascii?q?dn+UehwzmP2xjS6uFCP080ibLWJ4A9zrM0jJYfrErOEjHslEnrj6Kaal8o9+mq?=
 =?us-ascii?q?5uj/Z7XpvJ6cN4t6igHkNaQun9SyAeA5MggIQmia9v2w1L798k3jRrVFkPk2nr?=
 =?us-ascii?q?DesJHUI8QUuLS5DhRL0oYs9Rm/FS2q0NcGknkdKlJKZhaHg5LuO1HUL/D0Fe2/?=
 =?us-ascii?q?jEi0kDd32/DGOaXsApHMLnjAjrjtZ7l861NHxQo3zNBf4Y9UC74bLPLyXE/xqM?=
 =?us-ascii?q?LXDhsjPwOoxObnDc131pkCVmKXHq+ZLKTSvEeI5u01IumMeJUauDHnJ/gl+v7h?=
 =?us-ascii?q?l3k5mVAGcKmt3JsXbm24H/t8L0WYZ3rsnskOEWMQsgUiS+zqjUWIUSRPaHaqQ6?=
 =?us-ascii?q?I8+jY7BZqkDYfEWI+hmr+B3CC9Hp1QYWBLEVSMEXbud4WZVPYAciOSIsl9kjMa?=
 =?us-ascii?q?UbitUZMu1RartAXi0bpoMvLU+jEEtZLkzNV6/fbTlRE19Tx3FcidyXuCT2Nvk2?=
 =?us-ascii?q?MMRj822r1/oENnxleC16h4n+JXFdhJ6/xVVQc6MIbWz/ZmBNDqRgLBYtCJRU6l?=
 =?us-ascii?q?Qtq8BzE9VNYxw94UbEZ7FNWvlRTD3yusA78ImL2HHp008qTA33fvI8Zx0WrJ1K?=
 =?us-ascii?q?4kj1M+WMtAKXWmhrJj9wjUH4PGjl+Wl7i0eqgG3C7C7mGDzXGQs0FeVwJwVabF?=
 =?us-ascii?q?XXUbZkbNqdT550XCT6KhCLg9MwtBz9KCJbVOatHzkVpGQ/LjMszEY22tg2ewGQ?=
 =?us-ascii?q?qIxrSUYYX3YWodwjvSBFIEkw8J+3aGLhYxBiG6rmLaDTxuE0/vYkz2/el/rnO7?=
 =?us-ascii?q?UlE7zwWQY0J90Lq1/wYfheaARPMLwrIEpCAhpi1oHFa82tLWDMaApwphfalGfd?=
 =?us-ascii?q?Mx+lBH1Xjdtwx8OJygILtvhlofcwRxokPv2A97CoRGkcg2snwqyBB+Jr6f0FNE?=
 =?us-ascii?q?bzmYx4z/OqXLKmnu+xCic7TZ2lXA39eZ5KgO6O40pE7+vA60DEUi9XZn095L03?=
 =?us-ascii?q?aH4pXKDQwSUY/+U0ot9hh6oa3abTc554/OyXJsNqy0uCfY2901HOsl1gqgf9BH?=
 =?us-ascii?q?PaKECQ/+CdEVC9KvKewqgVepaB0EMftW9K41OcOmavSH1LSqPOZmgDKpk2BH7J?=
 =?us-ascii?q?ph3UKL8ip2UvTI0Iodw/GEwguHUC/xjE2gss/rg49EfywdHm2lxSjiGoFRfKxy?=
 =?us-ascii?q?cZ8XBmi0J82428txh4TqW35e71OjA1IG2Mm0eRuddVD93AtQ1VgJrnyjgye30z?=
 =?us-ascii?q?t0kzQxpKqFwCPO2/jidAYAOmNTRGhijE3gIIiug9ABQUioaRMklB+k5Ub82qha?=
 =?us-ascii?q?q75zL2jVQUdUYSf2K3tuXbe3trqHe8RP8o8nsT1LUOSgZlCXUrz9rAEA0yPgHG?=
 =?us-ascii?q?tewyo3dyqwtZX6nBx6iWSdI2h1rHfCfcFwxBHf5MHTRPJL3zoGQjV4hifTBlSm?=
 =?us-ascii?q?I9ap+tCUnY/Zsu+iT2KhSoFTcS7zwIKAriS74ndmAR++n/C1gdDnFQk60Snm19?=
 =?us-ascii?q?hlTynIrRD8YpX12KS+K+5oYk5oBFrk4cpgBo5+ipcwhI0X2XUChZWa53sHnX3z?=
 =?us-ascii?q?MdVGw63+anUNSCUPw97U5gjlxUJiImiIx4L/SnWS3M9ha8OmbWMR3yI399pKB7?=
 =?us-ascii?q?uM7LxYgSt1pUK1rQfQYfh+hDgR0/Uv52ABg+EVpgUt1D6SDa4IHUlXJiHskxWI?=
 =?us-ascii?q?79ajrKRYfmqvcL6w1FZgktClFr2NvgZcWHPhcJc4ASBw9tl/ME7L0HDr9oHkf9?=
 =?us-ascii?q?zQYcgStxKOlRfAkvNVKIkwlvcRgSpnOGT9vWAqyuIhjBxu2426s5aDK2l34K25?=
 =?us-ascii?q?BRtYPCXvZ8wP4jHtkbpensGO0o+0BJphHTELXIbyQfKsDTIfrvDnNweIED0hpX?=
 =?us-ascii?q?aXA7vfHQmD6Eh4q3LDCYykN3aSJHMB19VtWAGdJFBDgAATRDg6nIQ2FgW0y8z6?=
 =?us-ascii?q?akt5+iod5l3lqhtP1+JoMQPwUnzEqQesazc0ToWfLRVM4gFD4UfVLdKR7uZpEy?=
 =?us-ascii?q?5E+Z2hqRSHKnaHaARQEWEJRkuECkjjP7mp+NnA6vWYCfC4L/fUerWOrupeV/iT?=
 =?us-ascii?q?yJKr04tm+SuMN8qVMnljCf07xlRMXXRjF8vFnDUPTjQdlzjRYM6DuBe85ip3o9?=
 =?us-ascii?q?i/8Pv1WQLv5oiPC7pIPdVs4RC2hqiDOPCKhCZkMjZVzZcMxX7OyLgC018ekSBu?=
 =?us-ascii?q?dz+xEbsesS7BVr7fmqhSDxQDcSN8KNNI774g3glKIcPUkM712aVijvErC1ZJT1?=
 =?us-ascii?q?jhmsCyaMwOImG9Mk7HBUmROLSHIz3L39/4YaemRbJMi+VUsgW6uSyHHE/7IjSD?=
 =?us-ascii?q?iz7pWgizMeFNiSGXJgBRtJuhfRZtF2fjSsnrahm6MN9xkD03zqc4hnLMNW4ALz?=
 =?us-ascii?q?d8d1lBoaGX7SNdmv9/AXBO7mJ5LemYnCaU9+nZJYwQsftuAyR0kfpV4XU6y7RP?=
 =?us-ascii?q?6iFES+d4mC/Trt5ovlGnnfODyjthUBpStDlLgJiHslllOaXcpdF8XiOO1RQA4H?=
 =?us-ascii?q?6QQzcXotJ/Qv6pheoYntHJma/oKHFB7tfd5+MYBs7JOISGOn9nOh3sTnqcBQgd?=
 =?us-ascii?q?SBavNGfClwlTl+yU+nSJr5887J/2l95Gb75QVFUxXtMdAUdiVIgHIJB8XTQMkr?=
 =?us-ascii?q?OBisMMo32ko0+VDOxes4DKR7qxCO/jIS2ZgKdfZBoZiefxLIc7MZ//0kZ5LENn?=
 =?us-ascii?q?ysCCA0PNWc1WijZoawU95kJX/zw2YmQt2k6tRQKn+3gVXaq4lwA3hiNxYO428z?=
 =?us-ascii?q?Gq700wPlDHrTF2kUV3kMiz0h6LdzukCaa7XYhbD2LZs0w8KJbyRU4haBW/tUFg?=
 =?us-ascii?q?MDreQrtXybpsM2l23lyP8aBTEOJRGPUXKCQbwuuaMrBxiQxR?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0APAAAw+hhch0O0hNFkHAEBAQQBAQcEA?=
 =?us-ascii?q?QGBUQcBAQsBggCBayeMFV+LGoINFGiRYYR9gXMVGBMBhx0iNAkNAQMBAQEBAQE?=
 =?us-ascii?q?CARMBAQEKCwkIKS+CNiQBgmIBAgIBAQIkUgUBCQEBCiElAwwBRwYBEgWDHYF5B?=
 =?us-ascii?q?wEFqEozijOMPxeBf4ERgl01imACiTyHZZADCZF4kVcsiRiQL4FGgg5NHxmDJ4I?=
 =?us-ascii?q?nF44xKwEyAYEDAQEBjXMBAQ?=
X-IPAS-Result: =?us-ascii?q?A0APAAAw+hhch0O0hNFkHAEBAQQBAQcEAQGBUQcBAQsBggC?=
 =?us-ascii?q?BayeMFV+LGoINFGiRYYR9gXMVGBMBhx0iNAkNAQMBAQEBAQECARMBAQEKCwkIK?=
 =?us-ascii?q?S+CNiQBgmIBAgIBAQIkUgUBCQEBCiElAwwBRwYBEgWDHYF5BwEFqEozijOMPxe?=
 =?us-ascii?q?Bf4ERgl01imACiTyHZZADCZF4kVcsiRiQL4FGgg5NHxmDJ4InF44xKwEyAYEDA?=
 =?us-ascii?q?QEBjXMBAQ?=
X-IronPort-AV: E=Sophos;i="5.56,368,1539673200"; 
   d="scan'208";a="57939983"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 18 Dec 2018 05:47:47 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726713AbeLRNrp (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Tue, 18 Dec 2018 08:47:45 -0500
Received: from ozlabs.org ([203.11.71.1]:52921 "EHLO ozlabs.org"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1726426AbeLRNro (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Tue, 18 Dec 2018 08:47:44 -0500
Received: from authenticated.ozlabs.org (localhost [127.0.0.1])
        (using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
        (No client certificate requested)
        by ozlabs.org (Postfix) with ESMTPSA id 43JzqH4lV2z9sC7;
        Wed, 19 Dec 2018 00:47:39 +1100 (AEDT)
Authentication-Results: ozlabs.org; dmarc=none (p=none dis=none) header.from=ellerman.id.au
From: Michael Ellerman <mpe@ellerman.id.au>
To: "Aneesh Kumar K.V" <aneesh.kumar@linux.ibm.com>,
        akpm@linux-foundation.org, Michal Hocko <mhocko@kernel.org>,
        Alexey Kardashevskiy <aik@ozlabs.ru>, paulus@samba.org,
        David Gibson <david@gibson.dropbear.id.au>
Cc: linux-mm@kvack.org, linux-kernel@vger.kernel.org,
        linuxppc-dev@lists.ozlabs.org,
        "Aneesh Kumar K.V" <aneesh.kumar@linux.ibm.com>
Subject: Re: [PATCH V4 1/3] mm: Add get_user_pages_cma_migrate
In-Reply-To: <20181121092259.16482-2-aneesh.kumar@linux.ibm.com>
References: <20181121092259.16482-1-aneesh.kumar@linux.ibm.com> <20181121092259.16482-2-aneesh.kumar@linux.ibm.com>
Date: Wed, 19 Dec 2018 00:47:39 +1100
Message-ID: <871s6ft090.fsf@concordia.ellerman.id.au>
MIME-Version: 1.0
Content-Type: text/plain
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

"Aneesh Kumar K.V" <aneesh.kumar@linux.ibm.com> writes:

> This helper does a get_user_pages_fast and if it find pages in the CMA area
> it will try to migrate them before taking page reference. This makes sure that
> we don't keep non-movable pages (due to page reference count) in the CMA area.
> Not able to move pages out of CMA area result in CMA allocation failures.
>
> Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
> ---
>  include/linux/hugetlb.h |   2 +
>  include/linux/migrate.h |   3 +
>  mm/hugetlb.c            |   4 +-
>  mm/migrate.c            | 132 ++++++++++++++++++++++++++++++++++++++++
>  4 files changed, 139 insertions(+), 2 deletions(-)

I'd rather not merge this much mm/ code via the powerpc tree without
acks.

Anyone?

cheers


> diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h
> index 087fd5f48c91..1eed0cdaec0e 100644
> --- a/include/linux/hugetlb.h
> +++ b/include/linux/hugetlb.h
> @@ -371,6 +371,8 @@ struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,
>  				nodemask_t *nmask);
>  struct page *alloc_huge_page_vma(struct hstate *h, struct vm_area_struct *vma,
>  				unsigned long address);
> +struct page *alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,
> +				     int nid, nodemask_t *nmask);
>  int huge_add_to_page_cache(struct page *page, struct address_space *mapping,
>  			pgoff_t idx);
>  
> diff --git a/include/linux/migrate.h b/include/linux/migrate.h
> index f2b4abbca55e..d82b35afd2eb 100644
> --- a/include/linux/migrate.h
> +++ b/include/linux/migrate.h
> @@ -286,6 +286,9 @@ static inline int migrate_vma(const struct migrate_vma_ops *ops,
>  }
>  #endif /* IS_ENABLED(CONFIG_MIGRATE_VMA_HELPER) */
>  
> +extern int get_user_pages_cma_migrate(unsigned long start, int nr_pages, int write,
> +				      struct page **pages);
> +
>  #endif /* CONFIG_MIGRATION */
>  
>  #endif /* _LINUX_MIGRATE_H */
> diff --git a/mm/hugetlb.c b/mm/hugetlb.c
> index 7f2a28ab46d5..faf3102ae45e 100644
> --- a/mm/hugetlb.c
> +++ b/mm/hugetlb.c
> @@ -1585,8 +1585,8 @@ static struct page *alloc_surplus_huge_page(struct hstate *h, gfp_t gfp_mask,
>  	return page;
>  }
>  
> -static struct page *alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,
> -		int nid, nodemask_t *nmask)
> +struct page *alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,
> +				     int nid, nodemask_t *nmask)
>  {
>  	struct page *page;
>  
> diff --git a/mm/migrate.c b/mm/migrate.c
> index f7e4bfdc13b7..b0e47e2c5347 100644
> --- a/mm/migrate.c
> +++ b/mm/migrate.c
> @@ -2946,3 +2946,135 @@ int migrate_vma(const struct migrate_vma_ops *ops,
>  }
>  EXPORT_SYMBOL(migrate_vma);
>  #endif /* defined(MIGRATE_VMA_HELPER) */
> +
> +static struct page *new_non_cma_page(struct page *page, unsigned long private)
> +{
> +	/*
> +	 * We want to make sure we allocate the new page from the same node
> +	 * as the source page.
> +	 */
> +	int nid = page_to_nid(page);
> +	gfp_t gfp_mask = GFP_USER | __GFP_THISNODE;
> +
> +	if (PageHighMem(page))
> +		gfp_mask |= __GFP_HIGHMEM;
> +
> +#ifdef CONFIG_HUGETLB_PAGE
> +	if (PageHuge(page)) {
> +		struct hstate *h = page_hstate(page);
> +		/*
> +		 * We don't want to dequeue from the pool because pool pages will
> +		 * mostly be from the CMA region.
> +		 */
> +		return alloc_migrate_huge_page(h, gfp_mask, nid, NULL);
> +	}
> +#endif
> +	if (PageTransHuge(page)) {
> +		struct page *thp;
> +		gfp_t thp_gfpmask = GFP_TRANSHUGE | __GFP_THISNODE;
> +
> +		/*
> +		 * Remove the movable mask so that we don't allocate from
> +		 * CMA area again.
> +		 */
> +		thp_gfpmask &= ~__GFP_MOVABLE;
> +		thp = __alloc_pages_node(nid, thp_gfpmask, HPAGE_PMD_ORDER);
> +		if (!thp)
> +			return NULL;
> +		prep_transhuge_page(thp);
> +		return thp;
> +	}
> +
> +	return __alloc_pages_node(nid, gfp_mask, 0);
> +}
> +
> +/**
> + * get_user_pages_cma_migrate() - pin user pages in memory by migrating pages in CMA region
> + * @start:	starting user address
> + * @nr_pages:	number of pages from start to pin
> + * @write:	whether pages will be written to
> + * @pages:	array that receives pointers to the pages pinned.
> + *		Should be at least nr_pages long.
> + *
> + * Attempt to pin user pages in memory without taking mm->mmap_sem.
> + * If not successful, it will fall back to taking the lock and
> + * calling get_user_pages().
> + *
> + * If the pinned pages are backed by CMA region, we migrate those pages out,
> + * allocating new pages from non-CMA region. This helps in avoiding keeping
> + * pages pinned in the CMA region for a long time thereby resulting in
> + * CMA allocation failures.
> + *
> + * Returns number of pages pinned. This may be fewer than the number
> + * requested. If nr_pages is 0 or negative, returns 0. If no pages
> + * were pinned, returns -errno.
> + */
> +
> +int get_user_pages_cma_migrate(unsigned long start, int nr_pages, int write,
> +			       struct page **pages)
> +{
> +	int i, ret;
> +	bool drain_allow = true;
> +	bool migrate_allow = true;
> +	LIST_HEAD(cma_page_list);
> +
> +get_user_again:
> +	ret = get_user_pages_fast(start, nr_pages, write, pages);
> +	if (ret <= 0)
> +		return ret;
> +
> +	for (i = 0; i < ret; ++i) {
> +		/*
> +		 * If we get a page from the CMA zone, since we are going to
> +		 * be pinning these entries, we might as well move them out
> +		 * of the CMA zone if possible.
> +		 */
> +		if (is_migrate_cma_page(pages[i]) && migrate_allow) {
> +
> +			struct page *head = compound_head(pages[i]);
> +
> +			if (PageHuge(head))
> +				isolate_huge_page(head, &cma_page_list);
> +			else {
> +				if (!PageLRU(head) && drain_allow) {
> +					lru_add_drain_all();
> +					drain_allow = false;
> +				}
> +
> +				if (!isolate_lru_page(head)) {
> +					list_add_tail(&head->lru, &cma_page_list);
> +					mod_node_page_state(page_pgdat(head),
> +							    NR_ISOLATED_ANON +
> +							    page_is_file_cache(head),
> +							    hpage_nr_pages(head));
> +				}
> +			}
> +		}
> +	}
> +	if (!list_empty(&cma_page_list)) {
> +		/*
> +		 * drop the above get_user_pages reference.
> +		 */
> +		for (i = 0; i < ret; ++i)
> +			put_page(pages[i]);
> +
> +		if (migrate_pages(&cma_page_list, new_non_cma_page,
> +				  NULL, 0, MIGRATE_SYNC, MR_CONTIG_RANGE)) {
> +			/*
> +			 * some of the pages failed migration. Do get_user_pages
> +			 * without migration.
> +			 */
> +			migrate_allow = false;
> +
> +			if (!list_empty(&cma_page_list))
> +				putback_movable_pages(&cma_page_list);
> +		}
> +		/*
> +		 * We did migrate all the pages, Try to get the page references again
> +		 * migrating any new CMA pages which we failed to isolate earlier.
> +		 */
> +		drain_allow = true;
> +		goto get_user_again;
> +	}
> +	return ret;
> +}
> -- 
> 2.17.2
