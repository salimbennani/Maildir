Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 04 Dec 2018 08:59:19 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga008.jf.intel.com (orsmga008.jf.intel.com [10.7.209.65])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id D645558014B;
	Mon,  3 Dec 2018 15:35:52 -0800 (PST)
Received: from fmsmga103.fm.intel.com ([10.1.193.90])
  by orsmga008-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 03 Dec 2018 15:35:52 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AIH6J/hZ0zACeQdCj6xSQJHv/LSx+4OfEezUN459i?=
 =?us-ascii?q?sYplN5qZpc+zZB7h7PlgxGXEQZ/co6odzbaO4+a4ASQp2tWoiDg6aptCVhsI24?=
 =?us-ascii?q?09vjcLJ4q7M3D9N+PgdCcgHc5PBxdP9nC/NlVJSo6lPwWB6nK94iQPFRrhKAF7?=
 =?us-ascii?q?Ovr6GpLIj8Swyuu+54Dfbx9HiTahYr5+Ngm6oRnMvcQKnIVuLbo8xAHUqXVSYe?=
 =?us-ascii?q?RWwm1oJVOXnxni48q74YBu/SdNtf8/7sBMSar1cbg2QrxeFzQmLns65Nb3uhnZ?=
 =?us-ascii?q?TAuA/WUTX2MLmRdVGQfF7RX6XpDssivms+d2xSeXMdHqQb0yRD+v9LlgRgP2hy?=
 =?us-ascii?q?gbNj456GDXhdJ2jKJHuxKquhhzz5fJbI2JKPZye6XQds4YS2VcRMZcTyxPDJ2h?=
 =?us-ascii?q?YYUBDOQOP+hYoIbhqFUBthu+HQuhCfjzyjJLnHL6wbE23/gjHAzAwQcuH8gOsH?=
 =?us-ascii?q?PRrNjtNqcSXuS1zKjTzTXYcfxbwi3955bVfRAhu/6MQah8ftbWyUkqEQ7Ok1ae?=
 =?us-ascii?q?qZT9Mj+LyugAt3KX4/duWO6xkWIrtgJ8riS1ysotiITFnpwZxkzA+Clj3Yo4K8?=
 =?us-ascii?q?O0RFRmbdOnHpZcrT+WOohwT8g/WW9nojw6xacDuZOjfCgF1pAnxxnHZvyZfIiH?=
 =?us-ascii?q?/AjjWPySIThmnnJlfqywhxKo/Uin0O38WdG40FdMriVbjtnBrm4B2wDX58SdV/?=
 =?us-ascii?q?dw8Fmt1SyS2w3Q9u1IO0E5mKjDJ54k2LEwl54TsUrZHi/xnUX7lKuWdkQi+ui1?=
 =?us-ascii?q?5OXre7bmqYGGN49ylA7+Nr0imsuxAOQ+LAcORXOW+fqz1LL95031WrZKgeMskq?=
 =?us-ascii?q?nfrp/aId4XpqmjAw9ayooj8QqwDy+60NQEmnkKNFZFeBOEj4f3IV3PL+34AOy7?=
 =?us-ascii?q?g1Stljdr2v/HMqfgApXLMnjMjrPhcaxh5E5bzQo51cpf6I5MCrEdPPLzXVf8tN?=
 =?us-ascii?q?zCAR84Lwy72eHnBM991oMDQ22PBKCZPbjWsV+J4OIvPuaNaJUUuDb7N/gq+fru?=
 =?us-ascii?q?gWUlll8aeKmjxYEXZ2ygHvR6P0WZZmLhgtUGEWgUpAY+T+vqiFuEUT5UfHuyW6?=
 =?us-ascii?q?M85jcmCIOpF4vDR4atgKCf0ye/BJFZemdGClWUG3fya4qEQ+sMaD6VIsJ5iDME?=
 =?us-ascii?q?TrihS4gi1R20rg/10bhnI/HQ+i0Zs5Ljydd06/fSlRE06Tx7EcCd33uRQGFzm2?=
 =?us-ascii?q?MCXyU207xnoUxh1leD1rB1g/9CGtxV/f9JVgY6OoTaz+x1EN3yXgPBftGUSFep?=
 =?us-ascii?q?WNmmADcxTs4vzN8KeUpyB9KijhXb1SqwH7AVj6CLBIAz8q/Ewnf+Pdh9xGjc2K?=
 =?us-ascii?q?khlVYmRNBPOnOghqJ48wjTBIvJk0GCmqaudKQc2jPN9WiZwWqPukFYTBB/UaHf?=
 =?us-ascii?q?UX8DYUvWqMzz5lneQL+2FbQnLgxBxNafJaRQdN3mk09KRPf5N9TYeGK+hWGwCA?=
 =?us-ascii?q?2MxrONaorqZmoc0D/cCEgCjwAc43KGORIiCSempmLUFCZuGk73Y0Pw7el+r2u2?=
 =?us-ascii?q?TlM1zwGPaE1hy7q1+x4PifyAUf8T2agEuCMgqzVyE1awxNbWC9uGpwp8c6RQe9?=
 =?us-ascii?q?I94FFb1W3HswxxJICvL6dnhlQGaQR4o1vu1wlrCoVHicUltncqwxR9KK2G0FJN?=
 =?us-ascii?q?bSiX3Y3tNbLNLGny/xeva7DN11Ha0daW/LoP6fsip1XiugGpClQt83F939ZJ1H?=
 =?us-ascii?q?uc443ADBAOXpLpTkY36x96qqnaYykg/Y/byWdgMKi0sjDY3dIpC/AoyhKhf9dZ?=
 =?us-ascii?q?LaOFGxX+E8wcB8iyNuMqn0KlYQ4DPOBX7KQ0Jd+pd+Oa2K63O+ZthCimjWVb74?=
 =?us-ascii?q?9nyEKD6i18RvTO35kb2fGY3xKLWCvmg1elr8/4h5pEZTYUHmekzSjkBYhRZrB9?=
 =?us-ascii?q?fIoRCGeuJdG3yct6h5L3R3FY81ujDUsc2MC1YRqSc0D93QpI2EUXoHyrgyu5wy?=
 =?us-ascii?q?ZvkzE0qKqSxyjOw+XkdBoaNW9HXmhijVHwIYeqi9AWRlSnbw8slBG9/0b127Bb?=
 =?us-ascii?q?pLhjL2nUWUpIfTL5L3thUquzsbqOedVP54krsSVUUeS8Z06VSrjmrhse0iPjGX?=
 =?us-ascii?q?ZexT8hezGrvJX5gwJ1iGaHIHlvq3rZfNl6xQ3D69zEWf5Rwj0GSTF4iDbNAViw?=
 =?us-ascii?q?Jdup/dSSl5rYtuC+VmShVoBccCXxzIOAsje76nNuARGlg/+zndjnGxAg0SDnz9?=
 =?us-ascii?q?lqSTnIrBHkb4nozau6MPhrfkl1BFDm7cp6FZpzko8xhJEWxHgbiY+Z/XsBkWfv?=
 =?us-ascii?q?L9pb3bjyY2YKRT4O29TV+hTq2FV/LnKVwIL0TnCdwspiZ9ageG8X1Dwy79tWCK?=
 =?us-ascii?q?iK9rxLhjV6okC3rQ/KZfh9nzEdyeYh6XIAguEJvhYtwTuZArwIAUZYOinsnQyS?=
 =?us-ascii?q?79+itKVXeHqvcb+o2UpimtChCauOowBGV3b/Z5ciBjRw7sRkPVLI0X3z7Jzked?=
 =?us-ascii?q?bKYdISsB2UjwnPj+xPJJ0tkfoKgDJtOXjhsn091+47kRtu0Imhs4edLGVt+L+5?=
 =?us-ascii?q?DgRcNj3ofM4T/jDtjaBDnseZxYyvH5NhGikVU5vsV/6nDDUStfH/PQaUDDI8sm?=
 =?us-ascii?q?ubGabYHQKH6kdptXfPHI6wN32NOHkV19ZiRASeJExChgAbRi46k4U9FgCrwszh?=
 =?us-ascii?q?bUh46ioQ5l7+thtD1OZoOwPjXWfYoQeici00R4SHLBpK8gFC4F/YMMyZ7uJuBi?=
 =?us-ascii?q?FY44eurAqXJmyAYARIC24JV1eAB1DiOLmu+NbB//KZBuq4M/vBf7GOpfZCWPeP?=
 =?us-ascii?q?wJKlypFm8CqUNsWTInliCOU22kpdUnB4GMTZmDQPRzYUlyLXaM6bqwmz+jdqrs?=
 =?us-ascii?q?Ck6/nrXAPv5Y2SC7pdK9lv+ha2gbudOO6Unip2NTFY1pYUz3/S1LcfxEISiz1p?=
 =?us-ascii?q?dzS1C7sAqDXCTLjOla5XFRIbbyJzOdBM76I92AlNJMHahsn01r5+kv46FVNFWU?=
 =?us-ascii?q?b9lcGuYMwAO3u9O0/fBEaXKLSGIiXGw932YaO5U7FfkP9YuAGwuTmFFU/uJTCD?=
 =?us-ascii?q?lzjvVxCyPuBAliCbPBpCuI6jdhZhE3TsTNXjahejKt94kSU2waEohnPNLWMdMS?=
 =?us-ascii?q?Jzc0VIrr2R6yNXmvR+G2xb4XpjIumJgCKZ7+jeKpYLvvpnGCV0l+RG4Hskz7tZ?=
 =?us-ascii?q?9j1LRPtwmHiaktk7p1Cgj/nKzz9PUwRHoTUNg5iE7mt4PqCM9JZEWG3f8ToX4G?=
 =?us-ascii?q?mQAghMrNxgWY6ngLxZ1tWazPG7EzxF6d+BuJJEX8U=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0ADAAAhvQVch0O0hNFjGgEBAQEBAgEBA?=
 =?us-ascii?q?QEHAgEBAQGBUwMBAQEBCwGDayeDOj+Id409FGiWTBSBWxgYEwGBS4J1g0kiNgc?=
 =?us-ascii?q?NAQMBAQEBAQECARMBAQEKCwkIKS+CNiQBgmIDAwECCRcPAUYGCQEBCBcFAhEVA?=
 =?us-ascii?q?gIDVBkFgxyCAgWKNZtQgS+JAoEsgQuITYJEF4F/gRGCXYRxYiKCRYI1IgKJDBI?=
 =?us-ascii?q?OMoFBhQuQFgmGQYpwCxiJW4dLh2qRA4FNB4F/cFCCbIInF447IQEBMQGBBAEBH?=
 =?us-ascii?q?Io+AQE?=
X-IPAS-Result: =?us-ascii?q?A0ADAAAhvQVch0O0hNFjGgEBAQEBAgEBAQEHAgEBAQGBUwM?=
 =?us-ascii?q?BAQEBCwGDayeDOj+Id409FGiWTBSBWxgYEwGBS4J1g0kiNgcNAQMBAQEBAQECA?=
 =?us-ascii?q?RMBAQEKCwkIKS+CNiQBgmIDAwECCRcPAUYGCQEBCBcFAhEVAgIDVBkFgxyCAgW?=
 =?us-ascii?q?KNZtQgS+JAoEsgQuITYJEF4F/gRGCXYRxYiKCRYI1IgKJDBIOMoFBhQuQFgmGQ?=
 =?us-ascii?q?YpwCxiJW4dLh2qRA4FNB4F/cFCCbIInF447IQEBMQGBBAEBHIo+AQE?=
X-IronPort-AV: E=Sophos;i="5.56,312,1539673200"; 
   d="scan'208";a="54364935"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 03 Dec 2018 15:35:50 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726129AbeLCXfr (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Mon, 3 Dec 2018 18:35:47 -0500
Received: from mx1.redhat.com ([209.132.183.28]:36140 "EHLO mx1.redhat.com"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1725915AbeLCXfq (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 3 Dec 2018 18:35:46 -0500
Received: from smtp.corp.redhat.com (int-mx01.intmail.prod.int.phx2.redhat.com [10.5.11.11])
        (using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
        (No client certificate requested)
        by mx1.redhat.com (Postfix) with ESMTPS id 1CCEFC036770;
        Mon,  3 Dec 2018 23:35:45 +0000 (UTC)
Received: from localhost.localdomain.com (ovpn-120-188.rdu2.redhat.com [10.10.120.188])
        by smtp.corp.redhat.com (Postfix) with ESMTP id 90B2D605A8;
        Mon,  3 Dec 2018 23:35:36 +0000 (UTC)
From: jglisse@redhat.com
To: linux-mm@kvack.org
Cc: Andrew Morton <akpm@linux-foundation.org>,
        linux-kernel@vger.kernel.org,
        =?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= <jglisse@redhat.com>,
        "Rafael J . Wysocki" <rafael@kernel.org>,
        Ross Zwisler <ross.zwisler@linux.intel.com>,
        Dan Williams <dan.j.williams@intel.com>,
        Dave Hansen <dave.hansen@intel.com>,
        Haggai Eran <haggaie@mellanox.com>,
        Balbir Singh <balbirs@au1.ibm.com>,
        "Aneesh Kumar K . V" <aneesh.kumar@linux.ibm.com>,
        Benjamin Herrenschmidt <benh@kernel.crashing.org>,
        Felix Kuehling <felix.kuehling@amd.com>,
        Philip Yang <Philip.Yang@amd.com>,
        =?UTF-8?q?Christian=20K=C3=B6nig?= <christian.koenig@amd.com>,
        Paul Blinzer <Paul.Blinzer@amd.com>,
        Logan Gunthorpe <logang@deltatee.com>,
        John Hubbard <jhubbard@nvidia.com>,
        Ralph Campbell <rcampbell@nvidia.com>,
        Michal Hocko <mhocko@kernel.org>,
        Jonathan Cameron <jonathan.cameron@huawei.com>,
        Mark Hairgrove <mhairgrove@nvidia.com>,
        Vivek Kini <vkini@nvidia.com>,
        Mel Gorman <mgorman@techsingularity.net>,
        Dave Airlie <airlied@redhat.com>,
        Ben Skeggs <bskeggs@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>
Subject: [RFC PATCH 02/14] mm/hms: heterogenenous memory system (HMS) documentation
Date: Mon,  3 Dec 2018 18:34:57 -0500
Message-Id: <20181203233509.20671-3-jglisse@redhat.com>
In-Reply-To: <20181203233509.20671-1-jglisse@redhat.com>
References: <20181203233509.20671-1-jglisse@redhat.com>
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
X-Scanned-By: MIMEDefang 2.79 on 10.5.11.11
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16 (mx1.redhat.com [10.5.110.31]); Mon, 03 Dec 2018 23:35:45 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

From: Jérôme Glisse <jglisse@redhat.com>

Add documentation to what is HMS and what it is for (see patch content).

Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
Cc: Rafael J. Wysocki <rafael@kernel.org>
Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
Cc: Dan Williams <dan.j.williams@intel.com>
Cc: Dave Hansen <dave.hansen@intel.com>
Cc: Haggai Eran <haggaie@mellanox.com>
Cc: Balbir Singh <balbirs@au1.ibm.com>
Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Cc: Felix Kuehling <felix.kuehling@amd.com>
Cc: Philip Yang <Philip.Yang@amd.com>
Cc: Christian König <christian.koenig@amd.com>
Cc: Paul Blinzer <Paul.Blinzer@amd.com>
Cc: Logan Gunthorpe <logang@deltatee.com>
Cc: John Hubbard <jhubbard@nvidia.com>
Cc: Ralph Campbell <rcampbell@nvidia.com>
Cc: Michal Hocko <mhocko@kernel.org>
Cc: Jonathan Cameron <jonathan.cameron@huawei.com>
Cc: Mark Hairgrove <mhairgrove@nvidia.com>
Cc: Vivek Kini <vkini@nvidia.com>
Cc: Mel Gorman <mgorman@techsingularity.net>
Cc: Dave Airlie <airlied@redhat.com>
Cc: Ben Skeggs <bskeggs@redhat.com>
Cc: Andrea Arcangeli <aarcange@redhat.com>
---
 Documentation/vm/hms.rst | 275 ++++++++++++++++++++++++++++++++++-----
 1 file changed, 246 insertions(+), 29 deletions(-)

diff --git a/Documentation/vm/hms.rst b/Documentation/vm/hms.rst
index dbf0f71918a9..bd7c9e8e7077 100644
--- a/Documentation/vm/hms.rst
+++ b/Documentation/vm/hms.rst
@@ -4,32 +4,249 @@
 Heterogeneous Memory System (HMS)
 =================================
 
-System with complex memory topology needs a more versatile memory topology
-description than just node where a node is a collection of memory and CPU.
-In heterogeneous memory system we consider four types of object::
-   - target: which is any kind of memory
-   - initiator: any kind of device or CPU
-   - inter-connect: any kind of links that connects target and initiator
-   - bridge: a link between two inter-connects
-
-Properties (like bandwidth, latency, bus width, ...) are define per bridge
-and per inter-connect. Property of an inter-connect apply to all initiators
-which are link to that inter-connect. Not all initiators are link to all
-inter-connect and thus not all initiators can access all memory (this apply
-to CPU too ie some CPU might not be able to access all memory).
-
-Bridges allow initiators (that can use the bridge) to access target for
-which they do not have a direct link with (ie they do not share a common
-inter-connect with the target).
-
-Through this four types of object we can describe any kind of system memory
-topology. To expose this to userspace we expose a new sysfs hierarchy (that
-co-exist with the existing one)::
-   - /sys/bus/hms/target* all targets in the system
-   - /sys/bus/hms/initiator* all initiators in the system
-   - /sys/bus/hms/interconnect* all inter-connects in the system
-   - /sys/bus/hms/bridge* all bridges in the system
-
-Inside each bridge or inter-connect directory they are symlinks to targets
-and initiators that are linked to that bridge or inter-connect. Properties
-are defined inside bridge and inter-connect directory.
+Heterogeneous memory system are becoming more and more the norm, in
+those system there is not only the main system memory for each node,
+but also device memory and|or memory hierarchy to consider. Device
+memory can comes from a device like GPU, FPGA, ... or from a memory
+only device (persistent memory, or high density memory device).
+
+Memory hierarchy is when you not only have the main memory but also
+other type of memory like HBM (High Bandwidth Memory often stack up
+on CPU die or GPU die), peristent memory or high density memory (ie
+something slower then regular DDR DIMM but much bigger).
+
+On top of this diversity of memories you also have to account for the
+system bus topology ie how all CPUs and devices are connected to each
+others. Userspace do not care about the exact physical topology but
+care about topology from behavior point of view ie what are all the
+paths between an initiator (anything that can initiate memory access
+like CPU, GPU, FGPA, network controller ...) and a target memory and
+what are all the properties of each of those path (bandwidth, latency,
+granularity, ...).
+
+This means that it is no longer sufficient to consider a flat view
+for each node in a system but for maximum performance we need to
+account for all of this new memory but also for system topology.
+This is why this proposal is unlike the HMAT proposal [1] which
+tries to extend the existing NUMA for new type of memory. Here we
+are tackling a much more profound change that depart from NUMA.
+
+
+One of the reasons for radical change is the advance of accelerator
+like GPU or FPGA means that CPU is no longer the only piece where
+computation happens. It is becoming more and more common for an
+application to use a mix and match of different accelerator to
+perform its computation. So we can no longer satisfy our self with
+a CPU centric and flat view of a system like NUMA and NUMA distance.
+
+
+HMS tackle this problems through three aspects:
+    1 - Expose complex system topology and various kind of memory
+        to user space so that application have a standard way and
+        single place to get all the information it cares about.
+    2 - A new API for user space to bind/provide hint to kernel on
+        which memory to use for range of virtual address (a new
+        mbind() syscall).
+    3 - Kernel side changes for vm policy to handle this changes
+
+
+The rest of this documents is splits in 3 sections, the first section
+talks about complex system topology: what it is, how it is use today
+and how to describe it tomorrow. The second sections talks about
+new API to bind/provide hint to kernel for range of virtual address.
+The third section talks about new mechanism to track bind/hint
+provided by user space or device driver inside the kernel.
+
+
+1) Complex system topology and representing them
+================================================
+
+Inside a node you can have a complex topology of memory, for instance
+you can have multiple HBM memory in a node, each HBM memory tie to a
+set of CPUs (all of which are in the same node). This means that you
+have a hierarchy of memory for CPUs. The local fast HBM but which is
+expected to be relatively small compare to main memory and then the
+main memory. New memory technology might also deepen this hierarchy
+with another level of yet slower memory but gigantic in size (some
+persistent memory technology might fall into that category). Another
+example is device memory, and device themself can have a hierarchy
+like HBM on top of device core and main device memory.
+
+On top of that you can have multiple path to access each memory and
+each path can have different properties (latency, bandwidth, ...).
+Also there is not always symmetry ie some memory might only be
+accessible by some device or CPU ie not accessible by everyone.
+
+So a flat hierarchy for each node is not capable of representing this
+kind of complexity. To simplify discussion and because we do not want
+to single out CPU from device, from here on out we will use initiator
+to refer to either CPU or device. An initiator is any kind of CPU or
+device that can access memory (ie initiate memory access).
+
+At this point a example of such system might help:
+    - 2 nodes and for each node:
+        - 1 CPU per node with 2 complex of CPUs cores per CPU
+        - one HBM memory for each complex of CPUs cores (200GB/s)
+        - CPUs cores complex are linked to each other (100GB/s)
+        - main memory is (90GB/s)
+        - 4 GPUs each with:
+            - HBM memory for each GPU (1000GB/s) (not CPU accessible)
+            - GDDR memory for each GPU (500GB/s) (CPU accessible)
+            - connected to CPU root controller (60GB/s)
+            - connected to other GPUs (even GPUs from the second
+              node) with GPU link (400GB/s)
+
+In this example we restrict our self to bandwidth and ignore bus width
+or latency, this is just to simplify discussions but obviously they
+also factor in.
+
+
+Userspace very much would like to know about this information, for
+instance HPC folks have develop complex library to manage this and
+there is wide research on the topics [2] [3] [4] [5]. Today most of
+the work is done by hardcoding thing for specific platform. Which is
+somewhat acceptable for HPC folks where the platform stays the same
+for a long period of time.
+
+Roughly speaking i see two broads use case for topology information.
+First is for virtualization and vm where you want to segment your
+hardware properly for each vm (binding memory, CPU and GPU that are
+all close to each others). Second is for application, many of which
+can partition their workload to minimize exchange between partition
+allowing each partition to be bind to a subset of device and CPUs
+that are close to each others (for maximum locality). Here it is much
+more than just NUMA distance, you can leverage the memory hierarchy
+and  the system topology all-together (see [2] [3] [4] [5] for more
+references and details).
+
+So this is not exposing topology just for the sake of cool graph in
+userspace. They are active user today of such information and if we
+want to growth and broaden the usage we should provide a unified API
+to standardize how that information is accessible to every one.
+
+
+One proposal so far to handle new type of memory is to user CPU less
+node for those [6]. While same idea can apply for device memory, it is
+still hard to describe multiple path with different property in such
+scheme. While it is backward compatible and have minimum changes, it
+simplify can not convey complex topology (think any kind of random
+graph, not just a tree like graph).
+
+So HMS use a new way to expose to userspace the system topology. It
+relies on 4 types of objects:
+    - target: any kind of memory (main memory, HBM, device, ...)
+    - initiator: CPU or device (anything that can access memory)
+    - link: anything that link initiator and target
+    - bridges: anything that allow group of initiator to access
+      remote target (ie target they are not connected with directly
+      through an link)
+
+Properties like bandwidth, latency, ... are all sets per bridges and
+links. All initiators connected to an link can access any target memory
+also connected to the same link and all with the same link properties.
+
+Link do not need to match physical hardware ie you can have a single
+physical link match a single or multiples software expose link. This
+allows to model device connected to same physical link (like PCIE
+for instance) but not with same characteristics (like number of lane
+or lane speed in PCIE). The reverse is also true ie having a single
+software expose link match multiples physical link.
+
+Bridges allows initiator to access remote link. A bridges connect two
+links to each others and is also specific to list of initiators (ie
+not all initiators connected to each of the link can use the bridge).
+Bridges have their own properties (bandwidth, latency, ...) so that
+the actual property value for each property is the lowest common
+denominator between bridge and each of the links.
+
+
+This model allows to describe any kind of directed graph and thus
+allows to describe any kind of topology we might see in the future.
+It is also easier to add new properties to each object type.
+
+Moreover it can be use to expose devices capable to do peer to peer
+between them. For that simply have all devices capable to peer to
+peer to have a common link or use the bridge object if the peer to
+peer capabilities is only one way for instance.
+
+
+HMS use the above scheme to expose system topology through sysfs under
+/sys/bus/hms/ with:
+    - /sys/bus/hms/devices/v%version-%id-target/ : a target memory,
+      each has a UID and you can usual value in that folder (node id,
+      size, ...)
+
+    - /sys/bus/hms/devices/v%version-%id-initiator/ : an initiator
+      (CPU or device), each has a HMS UID but also a CPU id for CPU
+      (which match CPU id in (/sys/bus/cpu/). For device you have a
+      path that can be PCIE BUS ID for instance)
+
+    - /sys/bus/hms/devices/v%version-%id-link : an link, each has a
+      UID and a file per property (bandwidth, latency, ...) you also
+      find a symlink to every target and initiator connected to that
+      link.
+
+    - /sys/bus/hms/devices/v%version-%id-bridge : a bridge, each has
+      a UID and a file per property (bandwidth, latency, ...) you
+      also find a symlink to all initiators that can use that bridge.
+
+To help with forward compatibility each object as a version value and
+it is mandatory for user space to only use target or initiator with
+version supported by the user space. For instance if user space only
+knows about what version 1 means and sees a target with version 2 then
+the user space must ignore that target as if it does not exist.
+
+Mandating that allows the additions of new properties that break back-
+ward compatibility ie user space must know how this new property affect
+the object to be able to use it safely.
+
+Main memory of each node is expose under a common target. For now
+device driver are responsible to register memory they want to expose
+through that scheme but in the future that information might come from
+the system firmware (this is a different discussion).
+
+
+
+2) hbind() bind range of virtual address to heterogeneous memory
+================================================================
+
+So instead of using a bitmap, hbind() take an array of uid and each uid
+is a unique memory target inside the new memory topology description.
+User space also provide an array of modifiers. Modifier can be seen as
+the flags parameter of mbind() but here we use an array so that user
+space can not only supply a modifier but also value with it. This should
+allow the API to grow more features in the future. Kernel should return
+-EINVAL if it is provided with an unkown modifier and just ignore the
+call all together, forcing the user space to restrict itself to modifier
+supported by the kernel it is running on (i know i am dreaming about well
+behave user space).
+
+
+Note that none of this is exclusive of automatic memory placement like
+autonuma. I also believe that we will see something similar to autonuma
+for device memory.
+
+
+3) Tracking and applying heterogeneous memory policies
+======================================================
+
+Current memory policy infrastructure is node oriented, instead of
+changing that and risking breakage and regression HMS adds a new
+heterogeneous policy tracking infra-structure. The expectation is
+that existing application can keep using mbind() and all existing
+infrastructure under-disturb and unaffected, while new application
+will use the new API and should avoid mix and matching both (as they
+can achieve the same thing with the new API).
+
+Also the policy is not directly tie to the vma structure for a few
+reasons:
+    - avoid having to split vma for policy that do not cover full vma
+    - avoid changing too much vma code
+    - avoid growing the vma structure with an extra pointer
+
+The overall design is simple, on hbind() call a hms policy structure
+is created for the supplied range and hms use the callback associated
+with the target memory. This callback is provided by device driver
+for device memory or by core HMS for regular main memory. The callback
+can decide to migrate the range to the target memories or do nothing
+(this can be influenced by flags provided to hbind() too).
-- 
2.17.2

