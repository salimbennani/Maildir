Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 04 Dec 2018 08:59:22 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga008.jf.intel.com (orsmga008.jf.intel.com [10.7.209.65])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id C500358014B;
	Mon,  3 Dec 2018 15:36:53 -0800 (PST)
Received: from orsmga102-1.jf.intel.com (HELO mga09.intel.com) ([10.7.208.27])
  by orsmga008-1.jf.intel.com with ESMTP; 03 Dec 2018 15:36:53 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AE6+OEBK10OyDLQSZDtmcpTZWNBhigK39O0sv0rFi?=
 =?us-ascii?q?tYgUKfX7rarrMEGX3/hxlliBBdydt6oUzbKO+4nbGkU4qa6bt34DdJEeHzQksu?=
 =?us-ascii?q?4x2zIaPcieFEfgJ+TrZSFpVO5LVVti4m3peRMNQJW2aFLduGC94iAPERvjKwV1?=
 =?us-ascii?q?Ov71GonPhMiryuy+4ZLebxlLiTanfb9+MAi9oBnMuMURnYZsMLs6xAHTontPde?=
 =?us-ascii?q?RWxGdoKkyWkh3h+Mq+/4Nt/jpJtf45+MFOTav1f6IjTbxFFzsmKHw65NfqtRbY?=
 =?us-ascii?q?UwSC4GYXX3gMnRpJBwjF6wz6Xov0vyDnuOdxxDWWMMvrRr0vRz+s87lkRwPpiC?=
 =?us-ascii?q?cfNj427mfXitBrjKlGpB6tvgFzz5LIbI2QMvd1Y6HTcs4ARWdZXshfSTFPDI2/?=
 =?us-ascii?q?YYQNAeoOMvpXoYbmp1sWrxazHhWsCeD1xzNUmnP6wa833uI8Gg/GxgwgGNcOvW?=
 =?us-ascii?q?zWrNX6MKcSUPu1zLLWwjjYdfNZxyry6IjSfRA9u/2DQbVwcc/XxEIyFA3Flk2d?=
 =?us-ascii?q?pZL5Mz6RzOgBrmaW4/R6We6yiGMrtxt9rzmty8s0lIXFmoYYxkrZ+Sh33oo5P8?=
 =?us-ascii?q?C0RU11bNK+DpddtCeXPJZsTMw4WWFnoiM6x6UGuZGleCgKz4wqxwDQa/OZaYiE?=
 =?us-ascii?q?+BHjW/iLITd+mn1lfKizhxGo8Uiv0uH8V8+030hWriddjNXAqnQA2wbO5sWJVP?=
 =?us-ascii?q?dx5Fqt1DWT2wzJ6+xJI1g4la/BJJ4gxr4wmIATsUPGHiLunEX2jamWdlgr++Sx?=
 =?us-ascii?q?6OTofK/mppmCOI9wkw3+NaovmsqhDuQiKQUOQWeb9vqm1LH5/k32Xq9Kjvsona?=
 =?us-ascii?q?ndqpzaIt4bpqGhDw9Pzokj8wq/Dyuh0NkAmXkHLVFFdwydg4nmJlHDO/T4Dfa5?=
 =?us-ascii?q?g1SxnzZn3fHGPrv9AprTKnjPiqvufbF460RE0go80chf545ICrEGOP/zWlX+tN?=
 =?us-ascii?q?3EDhAjNAy42ebnCMhn2YMYVmKCGauZMKLUsV+V6eMjOeiMZIkJuDnjL/gp/eLh?=
 =?us-ascii?q?jXg8mVUFZ6mmwYMXaGykHvRhO0iZY33sjckbHWcJuQo+SurqiFqZXD5XZnayWb?=
 =?us-ascii?q?885z4hBIKnC4fDWp6igLib0CinGZ1WY3hMCkqQHnfwa4WER/AMZTqILc96kjwE?=
 =?us-ascii?q?UruhR5U71R60tg/30L5nLuvS+i0FupPvztl15+vPlR4s8Tx4FdiS02aIT2tshG?=
 =?us-ascii?q?MHWyc23LxjoUx60lqD0rJ3g/pCGdxX5vNGSAE6NZHHwux+CtDyXB/Bf9iTRFan?=
 =?us-ascii?q?RNWmHS8+TtYrz9ASZEZ9Hs2ojgrf0CqyH78Vi7uLCYQp/a3GwXj+Odxxy3bc26?=
 =?us-ascii?q?knlFkpXM1POWqihq588gjTA5XEk0GYl6asaKQd0zTB9GaFzWqSok5YVBR8XrnC?=
 =?us-ascii?q?XXAafkHWt8j25lveT7+yDrQqKhZOxtScKqRUcNHpjU9JRPH4ONvAZWK8gGOwBR?=
 =?us-ascii?q?eOxrORY4vmYWQd3CPBCEcalwAf52qJNQ87Bi25uWLRECRuFU7zY0Pr6eR+tHK7?=
 =?us-ascii?q?TkozzwGWb01g16C5+gIPifybUfMT2rMEuCEuqzhvGFa93tTWC8ePpgZ7faVcZ8?=
 =?us-ascii?q?8970lD1W7DqwN9OZmgJbh4hlECawR3o1/u1xJvB4palcglsnwrwxRyKK6Cy1xB?=
 =?us-ascii?q?ai6X0or2Or3ULWny4guia6rX2lHYzdaX9b0D6PU+q1X/og6pElAu/Glg09lQy3?=
 =?us-ascii?q?Gc/InFDBIOUZLtVUY67xh7p6zAbiYh/Y/V1H1sPrOysj/NwN8pAOolyhC9f9ZQ?=
 =?us-ascii?q?Kq+EFQnyE9EECMiqMuAlh1+pbhccNuBI6KE0J9+md+eB2KOzJuZgnS6pgn5d4I?=
 =?us-ascii?q?9g1UKA7Sx8RfDS35YE2v2XwhGIVzPhg1i/qMD3noZEaCoWHmq+zyjkGYFQardz?=
 =?us-ascii?q?fYYNFWehPcm3ys9iiJ7qXn5S7ESjCE8e2M+1ZRqSaETw3A1R1UgNoX2rgzC4zy?=
 =?us-ascii?q?Fykzwyqqqf3SrOw/ntdRYdO25LQnVigknoIYSuk98aW02oZRAzlBS5/Ub627Rb?=
 =?us-ascii?q?pKNnImnRW0hIZSv2L2JlUqeqrbqNec1P6JApsSVRTuu8Z0uXSrr8oxscziPiEH?=
 =?us-ascii?q?FSxDE9dzG2pJr5mwZ2h36aLHZ2tHDZY91/xQ/D5NzAQv5cxjoHSzN/iTXNBFi8?=
 =?us-ascii?q?Pt+p8M6Ql5fCtOC+Sm2gWodScSnt0YOPqi+76Xd2Dh24mvC5gsfnHhQi0S/ny9?=
 =?us-ascii?q?lqUj3FrBXmbYnqzai6Mf9ncVNuBF/z8Md6HoB+ko0thJAfw3Qah5OV/WYZnmf3?=
 =?us-ascii?q?K9lUxaX+bH8VTz4R39HV+BTl2FFkLn+RwoL5V3adzdF7a9i0fGwWwT494N5QCK?=
 =?us-ascii?q?iP8rNEhyR1r0G8rQLQZ/h9gzgcxeEv6H4cn+EGpg4twj+BDbAVGElSJTbsmAiQ?=
 =?us-ascii?q?79CisKVXY36icaS31Ep7h9ygDauNogdBWHbiYZciHDR97sF+MFLKzX3y5ZvoeN?=
 =?us-ascii?q?jWbdIPqBKUlw3MgPRSKJI0jvAKnzZoOXrhvX05zO42lQdh0ou9vIiDNmVh5qa5?=
 =?us-ascii?q?AgNDOz3xZsMT/CztjKlEksaX2YCvAotuGjERUJT0SvKoFSoYte77OAaWDD08tn?=
 =?us-ascii?q?CbFKLdHQCF7UdpsWnPH4q3N32NJ3kZ0NZiRASbJENFmwAZRzE6np8/FgC3y83t?=
 =?us-ascii?q?akZ55jYN5lHmrhtA0P5nNx76Um3Hvgendi80SISDLBpR9gxD50DVMdGH7uJuBS?=
 =?us-ascii?q?5Y+IetrBeKKmydaARFFmUJWk2CB1D+Mbii/9jA8+6EBuWgK/vCe6mBqetbV/2Q?=
 =?us-ascii?q?35KgzpNm/yqQNsWIJnRiD+M02ktAXXB6GsTVgTYPSzYQlyLCccGbvgq8+jZso8?=
 =?us-ascii?q?C78fTrXh/v5ISVB7tTN9Vv5w65gaOZO+GMgyZ5LC5S1okQyn/Q1LgfwFkShjl0?=
 =?us-ascii?q?eDmwCrsArzDCTaLKlq9TDh4WcCdzNMpO76Ih0QhBI8/bitXp1rFmiv45EUtKVV?=
 =?us-ascii?q?vkmsuxf8wFP3m9NE/bBEaMLLmGOTzLw8TtbaygU7FfkOVUuAO2uTuAF0/jPzKD?=
 =?us-ascii?q?lyTmVhy1MOFMij2bMwJauI2nbhltDm3jRsr8ahKnKN93kSE2wbosi3LKL2EcNz?=
 =?us-ascii?q?18c0JLrrKI7CJYgu9wG2pO7nd+KemEmiCZ7/TXK5oMsPtrBDh0mPxe4Hgg17RV?=
 =?us-ascii?q?6yREF7RJn37Wr9hzsxSjl8GM1DNsU1xJsDkYqpiMuBBgNqHY7YJNcWzJ8BIE8S?=
 =?us-ascii?q?ObDBFZ9JNeFtTztvUImZD0n6XpJWIHqoqM8A=3D=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AUAAAhvQVch0O0hNFjHAEBAQQBAQcEA?=
 =?us-ascii?q?QGBUgYBAQsBgVqCESeDOj+Id409FJc0FIFYGxgTAYRAg0kiNQgNAQMBAQEBAQE?=
 =?us-ascii?q?CARMBAQEKCwkIKS+CNiQBgmIDAwECCRcECwFGBgkBAQgXBQImAgIDVBkFglFLg?=
 =?us-ascii?q?gIFijWbUHwziQKBLIELiE2CRBeBf4ERh1wgNIJnglcCix+FC5AWCYZBinALGIl?=
 =?us-ascii?q?bh0uYbYFIAYIKcIM8gicXEo4pIQEBMQGBBAEBHIdxgk0BAQ?=
X-IPAS-Result: =?us-ascii?q?A0AUAAAhvQVch0O0hNFjHAEBAQQBAQcEAQGBUgYBAQsBgVq?=
 =?us-ascii?q?CESeDOj+Id409FJc0FIFYGxgTAYRAg0kiNQgNAQMBAQEBAQECARMBAQEKCwkIK?=
 =?us-ascii?q?S+CNiQBgmIDAwECCRcECwFGBgkBAQgXBQImAgIDVBkFglFLggIFijWbUHwziQK?=
 =?us-ascii?q?BLIELiE2CRBeBf4ERh1wgNIJnglcCix+FC5AWCYZBinALGIlbh0uYbYFIAYIKc?=
 =?us-ascii?q?IM8gicXEo4pIQEBMQGBBAEBHIdxgk0BAQ?=
X-IronPort-AV: E=Sophos;i="5.56,312,1539673200"; 
   d="scan'208";a="55923552"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 03 Dec 2018 15:36:51 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726083AbeLCXgT (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Mon, 3 Dec 2018 18:36:19 -0500
Received: from mx1.redhat.com ([209.132.183.28]:36332 "EHLO mx1.redhat.com"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1726237AbeLCXgR (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 3 Dec 2018 18:36:17 -0500
Received: from smtp.corp.redhat.com (int-mx01.intmail.prod.int.phx2.redhat.com [10.5.11.11])
        (using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
        (No client certificate requested)
        by mx1.redhat.com (Postfix) with ESMTPS id 7CA51C05004D;
        Mon,  3 Dec 2018 23:36:15 +0000 (UTC)
Received: from localhost.localdomain.com (ovpn-120-188.rdu2.redhat.com [10.10.120.188])
        by smtp.corp.redhat.com (Postfix) with ESMTP id 0B7B1600C7;
        Mon,  3 Dec 2018 23:36:11 +0000 (UTC)
From: jglisse@redhat.com
To: linux-mm@kvack.org
Cc: Andrew Morton <akpm@linux-foundation.org>,
        linux-kernel@vger.kernel.org,
        =?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= <jglisse@redhat.com>,
        "Rafael J . Wysocki" <rafael@kernel.org>,
        Ross Zwisler <ross.zwisler@linux.intel.com>,
        Dan Williams <dan.j.williams@intel.com>,
        Dave Hansen <dave.hansen@intel.com>,
        Haggai Eran <haggaie@mellanox.com>,
        Balbir Singh <balbirs@au1.ibm.com>,
        "Aneesh Kumar K . V" <aneesh.kumar@linux.ibm.com>,
        Benjamin Herrenschmidt <benh@kernel.crashing.org>,
        Felix Kuehling <felix.kuehling@amd.com>,
        Philip Yang <Philip.Yang@amd.com>,
        =?UTF-8?q?Christian=20K=C3=B6nig?= <christian.koenig@amd.com>,
        Paul Blinzer <Paul.Blinzer@amd.com>,
        Logan Gunthorpe <logang@deltatee.com>,
        John Hubbard <jhubbard@nvidia.com>,
        Ralph Campbell <rcampbell@nvidia.com>,
        Michal Hocko <mhocko@kernel.org>,
        Jonathan Cameron <jonathan.cameron@huawei.com>,
        Mark Hairgrove <mhairgrove@nvidia.com>,
        Vivek Kini <vkini@nvidia.com>,
        Mel Gorman <mgorman@techsingularity.net>,
        Dave Airlie <airlied@redhat.com>,
        Ben Skeggs <bskeggs@redhat.com>,
        Andrea Arcangeli <aarcange@redhat.com>
Subject: [RFC PATCH 10/14] mm/hbind: add heterogeneous memory policy tracking infrastructure
Date: Mon,  3 Dec 2018 18:35:05 -0500
Message-Id: <20181203233509.20671-11-jglisse@redhat.com>
In-Reply-To: <20181203233509.20671-1-jglisse@redhat.com>
References: <20181203233509.20671-1-jglisse@redhat.com>
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
X-Scanned-By: MIMEDefang 2.79 on 10.5.11.11
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16 (mx1.redhat.com [10.5.110.31]); Mon, 03 Dec 2018 23:36:16 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

From: Jérôme Glisse <jglisse@redhat.com>

This patch add infrastructure to track heterogeneous memory policy
within the kernel. Policy are defined over range of virtual address
of a process and attach to the correspond mm_struct.

User can reset to default policy for range of virtual address using
hbind() default commands for the range.

Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
Cc: Rafael J. Wysocki <rafael@kernel.org>
Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
Cc: Dan Williams <dan.j.williams@intel.com>
Cc: Dave Hansen <dave.hansen@intel.com>
Cc: Haggai Eran <haggaie@mellanox.com>
Cc: Balbir Singh <balbirs@au1.ibm.com>
Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Cc: Felix Kuehling <felix.kuehling@amd.com>
Cc: Philip Yang <Philip.Yang@amd.com>
Cc: Christian König <christian.koenig@amd.com>
Cc: Paul Blinzer <Paul.Blinzer@amd.com>
Cc: Logan Gunthorpe <logang@deltatee.com>
Cc: John Hubbard <jhubbard@nvidia.com>
Cc: Ralph Campbell <rcampbell@nvidia.com>
Cc: Michal Hocko <mhocko@kernel.org>
Cc: Jonathan Cameron <jonathan.cameron@huawei.com>
Cc: Mark Hairgrove <mhairgrove@nvidia.com>
Cc: Vivek Kini <vkini@nvidia.com>
Cc: Mel Gorman <mgorman@techsingularity.net>
Cc: Dave Airlie <airlied@redhat.com>
Cc: Ben Skeggs <bskeggs@redhat.com>
Cc: Andrea Arcangeli <aarcange@redhat.com>
---
 include/linux/hms.h        |  46 ++++++
 include/linux/mm_types.h   |   6 +
 include/uapi/linux/hbind.h |   8 +
 kernel/fork.c              |   3 +
 mm/hms.c                   | 306 ++++++++++++++++++++++++++++++++++++-
 5 files changed, 368 insertions(+), 1 deletion(-)

diff --git a/include/linux/hms.h b/include/linux/hms.h
index 511b5363d8f2..f39c390b3afb 100644
--- a/include/linux/hms.h
+++ b/include/linux/hms.h
@@ -20,6 +20,8 @@
 
 #include <linux/device.h>
 #include <linux/types.h>
+#include <linux/mm_types.h>
+#include <linux/mmu_notifier.h>
 
 
 struct hms_target;
@@ -34,6 +36,10 @@ struct hms_target_hbind {
 #if IS_ENABLED(CONFIG_HMS)
 
 
+#include <linux/interval_tree.h>
+#include <linux/rwsem.h>
+
+
 #define to_hms_object(device) container_of(device, struct hms_object, device)
 
 enum hms_type {
@@ -133,6 +139,42 @@ void hms_bridge_register(struct hms_bridge **bridgep,
 void hms_bridge_unregister(struct hms_bridge **bridgep);
 
 
+struct hms_policy_targets {
+	struct hms_target **targets;
+	unsigned ntargets;
+	struct kref kref;
+};
+
+struct hms_policy_range {
+	struct hms_policy_targets *ptargets;
+	struct interval_tree_node node;
+	struct kref kref;
+};
+
+struct hms_policy {
+	struct rb_root_cached ranges;
+	struct rw_semaphore sem;
+	struct mmu_notifier mn;
+};
+
+static inline unsigned long hms_policy_range_start(struct hms_policy_range *r)
+{
+	return r->node.start;
+}
+
+static inline unsigned long hms_policy_range_end(struct hms_policy_range *r)
+{
+	return r->node.last + 1;
+}
+
+static inline void hms_policy_init(struct mm_struct *mm)
+{
+	mm->hpolicy = NULL;
+}
+
+void hms_policy_fini(struct mm_struct *mm);
+
+
 int hms_init(void);
 
 
@@ -163,6 +205,10 @@ int hms_init(void);
 #define hms_bridge_unregister(bridgep)
 
 
+#define hms_policy_init(mm)
+#define hms_policy_fini(mm)
+
+
 static inline int hms_init(void)
 {
 	return 0;
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 5ed8f6292a53..3da91767c689 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -26,6 +26,7 @@ typedef int vm_fault_t;
 
 struct address_space;
 struct mem_cgroup;
+struct hms_policy;
 struct hmm;
 
 /*
@@ -491,6 +492,11 @@ struct mm_struct {
 		/* HMM needs to track a few things per mm */
 		struct hmm *hmm;
 #endif
+
+#if IS_ENABLED(CONFIG_HMS)
+		/* Heterogeneous Memory System policy */
+		struct hms_policy *hpolicy;
+#endif
 	} __randomize_layout;
 
 	/*
diff --git a/include/uapi/linux/hbind.h b/include/uapi/linux/hbind.h
index a9aba17ab142..cc4687587f5a 100644
--- a/include/uapi/linux/hbind.h
+++ b/include/uapi/linux/hbind.h
@@ -39,6 +39,14 @@ struct hbind_params {
 #define HBIND_ATOM_GET_CMD(v) ((v) & 0xfffff)
 #define HBIND_ATOM_SET_CMD(v) ((v) & 0xfffff)
 
+/*
+ * HBIND_CMD_DEFAULT restore default policy ie undo any of the previous policy.
+ *
+ * Additional dwords:
+ *      NONE (DWORDS MUST BE 0 !)
+ */
+#define HBIND_CMD_DEFAULT 0
+
 
 #define HBIND_IOCTL		_IOWR('H', 0x00, struct hbind_params)
 
diff --git a/kernel/fork.c b/kernel/fork.c
index 07cddff89c7b..bc40edcadc69 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -38,6 +38,7 @@
 #include <linux/mman.h>
 #include <linux/mmu_notifier.h>
 #include <linux/hmm.h>
+#include <linux/hms.h>
 #include <linux/fs.h>
 #include <linux/mm.h>
 #include <linux/vmacache.h>
@@ -671,6 +672,7 @@ void __mmdrop(struct mm_struct *mm)
 	mm_free_pgd(mm);
 	destroy_context(mm);
 	hmm_mm_destroy(mm);
+	hms_policy_fini(mm);
 	mmu_notifier_mm_destroy(mm);
 	check_mm(mm);
 	put_user_ns(mm->user_ns);
@@ -989,6 +991,7 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 	RCU_INIT_POINTER(mm->exe_file, NULL);
 	mmu_notifier_mm_init(mm);
 	hmm_mm_init(mm);
+	hms_policy_init(mm);
 	init_tlb_flush_pending(mm);
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
 	mm->pmd_huge_pte = NULL;
diff --git a/mm/hms.c b/mm/hms.c
index bf328bd577dc..be2c4e526f25 100644
--- a/mm/hms.c
+++ b/mm/hms.c
@@ -24,6 +24,7 @@
 #include <linux/slab.h>
 #include <linux/init.h>
 #include <linux/hms.h>
+#include <linux/mm.h>
 #include <linux/fs.h>
 
 #include <uapi/linux/hbind.h>
@@ -31,7 +32,6 @@
 
 #define HBIND_FIX_ARRAY 64
 
-
 static ssize_t hbind_read(struct file *file, char __user *buf,
 			size_t count, loff_t *ppos)
 {
@@ -44,6 +44,300 @@ static ssize_t hbind_write(struct file *file, const char __user *buf,
 	return -EINVAL;
 }
 
+
+static void hms_policy_targets_get(struct hms_policy_targets *ptargets)
+{
+	kref_get(&ptargets->kref);
+}
+
+static void hms_policy_targets_free(struct kref *kref)
+{
+	struct hms_policy_targets *ptargets;
+
+	ptargets = container_of(kref, struct hms_policy_targets, kref);
+	kfree(ptargets->targets);
+	kfree(ptargets);
+}
+
+static void hms_policy_targets_put(struct hms_policy_targets *ptargets)
+{
+	kref_put(&ptargets->kref, &hms_policy_targets_free);
+}
+
+static struct hms_policy_targets* hms_policy_targets_new(const uint32_t *targets,
+							 unsigned ntargets)
+{
+	struct hms_policy_targets *ptargets;
+	void *_targets;
+	unsigned i, c;
+
+	_targets = kzalloc(ntargets * sizeof(void *), GFP_KERNEL);
+	if (_targets == NULL)
+		return NULL;
+
+	ptargets = kmalloc(sizeof(*ptargets), GFP_KERNEL);
+	if (ptargets == NULL) {
+		kfree(_targets);
+		return NULL;
+	}
+
+	kref_init(&ptargets->kref);
+	ptargets->targets = _targets;
+	ptargets->ntargets = ntargets;
+
+	for (i = 0, c = 0; i < ntargets; ++i) {
+		ptargets->targets[c] = hms_target_find(targets[i]);
+		c += !!((long)ptargets->targets[i]);
+	}
+
+	/* Ignore NULL targets[i] */
+	ptargets->ntargets = c;
+
+	if (!c) {
+		/* No valid targets pointless to waste memory ... */
+		hms_policy_targets_put(ptargets);
+		return NULL;
+	}
+
+	return ptargets;
+}
+
+
+static void hms_policy_range_get(struct hms_policy_range *prange)
+{
+	kref_get(&prange->kref);
+}
+
+static void hms_policy_range_free(struct kref *kref)
+{
+	struct hms_policy_range *prange;
+
+	prange = container_of(kref, struct hms_policy_range, kref);
+	hms_policy_targets_put(prange->ptargets);
+	kfree(prange);
+}
+
+static void hms_policy_range_put(struct hms_policy_range *prange)
+{
+	kref_put(&prange->kref, &hms_policy_range_free);
+}
+
+static struct hms_policy_range *hms_policy_range_new(const uint32_t *targets,
+						     unsigned long start,
+						     unsigned long end,
+						     unsigned ntargets)
+{
+	struct hms_policy_targets *ptargets;
+	struct hms_policy_range *prange;
+
+	ptargets = hms_policy_targets_new(targets, ntargets);
+	if (ptargets == NULL)
+		return NULL;
+
+	prange = kmalloc(sizeof(*prange), GFP_KERNEL);
+	if (prange == NULL)
+		return NULL;
+
+	prange->node.start = start & PAGE_MASK;
+	prange->node.last = PAGE_ALIGN(end) - 1;
+	prange->ptargets = ptargets;
+	kref_init(&prange->kref);
+
+	return prange;
+}
+
+static struct hms_policy_range *
+hms_policy_range_dup(struct hms_policy_range *_prange)
+{
+	struct hms_policy_range *prange;
+
+	prange = kmalloc(sizeof(*prange), GFP_KERNEL);
+	if (prange == NULL)
+		return NULL;
+
+	hms_policy_targets_get(_prange->ptargets);
+	prange->node.start = _prange->node.start;
+	prange->node.last = _prange->node.last;
+	prange->ptargets = _prange->ptargets;
+	kref_init(&prange->kref);
+
+	return prange;
+}
+
+
+void hms_policy_fini(struct mm_struct *mm)
+{
+	struct hms_policy *hpolicy = READ_ONCE(mm->hpolicy);
+	struct interval_tree_node *node;
+
+	spin_lock(&mm->page_table_lock);
+	hpolicy = READ_ONCE(mm->hpolicy);
+	mm->hpolicy = NULL;
+	spin_unlock(&mm->page_table_lock);
+
+	/* No active heterogeneous policy structure so nothing to cleanup. */
+	if (hpolicy == NULL)
+		return;
+
+	mmu_notifier_unregister_no_release(&hpolicy->mn, mm);
+
+	down_write(&hpolicy->sem);
+	node = interval_tree_iter_first(&hpolicy->ranges, 0, -1UL);
+	while (node) {
+		struct hms_policy_range *prange;
+		struct interval_tree_node *next;
+
+		prange = container_of(node, struct hms_policy_range, node);
+		next = interval_tree_iter_next(node, 0, -1UL);
+		interval_tree_remove(node, &hpolicy->ranges);
+		hms_policy_range_put(prange);
+		node = next;
+	}
+	up_write(&hpolicy->sem);
+
+	kfree(hpolicy);
+}
+
+
+static int hbind_default_locked(struct hms_policy *hpolicy,
+				struct hbind_params *params)
+{
+	struct interval_tree_node *node;
+	unsigned long start, last;
+	int ret = 0;
+
+	start = params->start;
+	last = params->end - 1UL;
+
+	node = interval_tree_iter_first(&hpolicy->ranges, start, last);
+	while (node) {
+		struct hms_policy_range *prange;
+		struct interval_tree_node *next;
+
+		prange = container_of(node, struct hms_policy_range, node);
+		next = interval_tree_iter_next(node, start, last);
+		if (node->start < start && node->last > last) {
+			/* Node is split in 2 */
+			struct hms_policy_range *_prange;
+			_prange = hms_policy_range_dup(prange);
+			if (_prange == NULL) {
+				ret = -ENOMEM;
+				break;
+			}
+			prange->node.last = start - 1;
+			_prange->node.start = last + 1;
+			interval_tree_insert(&_prange->node, &hpolicy->ranges);
+			break;
+		} else if (node->start < start) {
+			prange->node.last = start - 1;
+		} else if (node->last > last) {
+			prange->node.start = last + 1;
+		} else {
+			/* Fully inside [start, last] */
+			interval_tree_remove(node, &hpolicy->ranges);
+		}
+
+		node = next;
+	}
+
+	return ret;
+}
+
+static int hbind_default(struct mm_struct *mm, struct hbind_params *params,
+			 const uint32_t *targets, uint32_t *atoms)
+{
+	struct hms_policy *hpolicy = READ_ONCE(mm->hpolicy);
+	int ret;
+
+	/* No active heterogeneous policy structure so no range to reset. */
+	if (hpolicy == NULL)
+		return 0;
+
+	down_write(&hpolicy->sem);
+	ret = hbind_default_locked(hpolicy, params);
+	up_write(&hpolicy->sem);
+
+	return ret;
+}
+
+
+static void hms_policy_notifier_release(struct mmu_notifier *mn,
+					struct mm_struct *mm)
+{
+	hms_policy_fini(mm);
+}
+
+static int hms_policy_notifier_invalidate_range_start(struct mmu_notifier *mn,
+				       const struct mmu_notifier_range *range)
+{
+	if (range->event == MMU_NOTIFY_UNMAP) {
+		struct hbind_params params;
+
+		if (!range->blockable)
+			return -EBUSY;
+
+		params.natoms = 0;
+		params.ntargets = 0;
+		params.end = range->end;
+		params.start = range->start;
+		hbind_default(range->mm, &params, NULL, NULL);
+	}
+
+	return 0;
+}
+
+static const struct mmu_notifier_ops hms_policy_notifier_ops = {
+	.release = hms_policy_notifier_release,
+	.invalidate_range_start = hms_policy_notifier_invalidate_range_start,
+};
+
+static struct hms_policy *hms_policy_get(struct mm_struct *mm)
+{
+	struct hms_policy *hpolicy = READ_ONCE(mm->hpolicy);
+	bool mmu_notifier = false;
+
+	/*
+	 * The hpolicy struct can only be freed once the mm_struct goes away,
+	 * hence only pre-allocate if none is attach yet.
+	 */
+	if (hpolicy)
+		return hpolicy;
+
+	hpolicy = kzalloc(sizeof(*hpolicy), GFP_KERNEL);
+	if (hpolicy == NULL)
+		return NULL;
+
+	init_rwsem(&hpolicy->sem);
+
+	spin_lock(&mm->page_table_lock);
+	if (!mm->hpolicy) {
+		mm->hpolicy = hpolicy;
+		mmu_notifier = true;
+		hpolicy = NULL;
+	}
+	spin_unlock(&mm->page_table_lock);
+
+	if (mmu_notifier) {
+		int ret;
+
+		hpolicy->mn.ops = &hms_policy_notifier_ops;
+		ret = mmu_notifier_register(&hpolicy->mn, mm);
+		if (ret) {
+			spin_lock(&mm->page_table_lock);
+			hpolicy = mm->hpolicy;
+			mm->hpolicy = NULL;
+			spin_unlock(&mm->page_table_lock);
+		}
+	}
+
+	if (hpolicy)
+		kfree(hpolicy);
+
+	/* At this point mm->hpolicy is valid */
+	return mm->hpolicy;
+}
+
+
 static long hbind_ioctl(struct file *file, unsigned cmd, unsigned long arg)
 {
 	uint32_t *targets, *_dtargets = NULL, _ftargets[HBIND_FIX_ARRAY];
@@ -114,6 +408,16 @@ static long hbind_ioctl(struct file *file, unsigned cmd, unsigned long arg)
 	for (i = 0, ndwords = 1; i < params.natoms; i += ndwords) {
 		ndwords = 1 + HBIND_ATOM_GET_DWORDS(atoms[i]);
 		switch (HBIND_ATOM_GET_CMD(atoms[i])) {
+		case HBIND_CMD_DEFAULT:
+			if (ndwords != 1) {
+				ret = -EINVAL;
+				goto out_mm;
+			}
+			ret = hbind_default(current->mm, &params,
+					    targets, atoms);
+			if (ret)
+				goto out_mm;
+			break;
 		default:
 			ret = -EINVAL;
 			goto out_mm;
-- 
2.17.2

