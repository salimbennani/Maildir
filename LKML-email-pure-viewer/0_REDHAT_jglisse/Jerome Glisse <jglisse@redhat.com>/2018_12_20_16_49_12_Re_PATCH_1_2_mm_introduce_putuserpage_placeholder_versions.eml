Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by i7-8700 with POP3-SSL;
  24 Dec 2018 15:38:33 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga002.fm.intel.com (fmsmga002.fm.intel.com [10.253.24.26])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 7389A5805F0;
	Thu, 20 Dec 2018 08:49:23 -0800 (PST)
Received: from fmsmga105.fm.intel.com ([10.1.193.10])
  by fmsmga002-1.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 20 Dec 2018 08:49:22 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AMlnBgRDJlekRRq9bx3cvUyQJP3N1i/DPJgcQr6Af?=
 =?us-ascii?q?oPdwSP74r82wAkXT6L1XgUPTWs2DsrQY07qQ6/iocFdDyK7JiGoFfp1IWk1Nou?=
 =?us-ascii?q?QttCtkPvS4D1bmJuXhdS0wEZcKflZk+3amLRodQ56mNBXdrXKo8DEdBAj0OxZr?=
 =?us-ascii?q?KeTpAI7SiNm82/yv95HJbAhEmDmwbaluIBmqsA7cqtQYjYx+J6gr1xDHuGFIe+?=
 =?us-ascii?q?NYxWNpIVKcgRPx7dqu8ZBg7ipdpesv+9ZPXqvmcas4S6dYDCk9PGAu+MLrrxjD?=
 =?us-ascii?q?QhCR6XYaT24bjwBHAwnB7BH9Q5fxri73vfdz1SWGIcH7S60/VC+85Kl3VhDnlC?=
 =?us-ascii?q?YHNyY48G7JjMxwkLlbqw+lqxBm3oLYfJ2ZOP94c6jAf90VWHBBU95fWSJBHI2y?=
 =?us-ascii?q?cogBD+QOMulEsobzqFkBohWlBQm0Bu7i0SNIi3zs0KEmz+gsCxzK0Qo9FNwOqn?=
 =?us-ascii?q?TUq9D1Ob8WX++r1qnIyjDDYO1L0jn87IjIcwshoeqSUrltdsfRy0YvFwfEjlWL?=
 =?us-ascii?q?sozqISiY1v4TvGeG7+pvT/6vh3Q7pAF2pzii38EhgZTKiIIN0l3I6zl1zYIvKd?=
 =?us-ascii?q?GlRkN3f8SoHIZTui2GLYd7Q8EvT3l2tComzrAKo4O3cSYUxJg92hLSaPKKf5KW?=
 =?us-ascii?q?7h/gUuuaPC12i2h/eL2lgha/6UigxfP4VsmzyFtKsCVFncfWtnwX1Bzc9NKHSv?=
 =?us-ascii?q?1j8UelwzqP0BrT6u5cLUA1k6rUNYIhz6YumpYPtUnPBDL6lUvogKOMa0kp+fSk?=
 =?us-ascii?q?5/7mb7jkvpOcMpV7igD6MqQggMy/BuE4PxAKX2ia/+S8ybLi8VT6QLpUlP02lL?=
 =?us-ascii?q?fWsJTDKcQcqK+5BRFa0pw45hahADepzs4YkWMELF1bYhKHiZbmO1XULPD/F/e/?=
 =?us-ascii?q?jEygkC13yPDeIr3hHpLNI2DHkLfgfrZy9VRQyQUuzd1E45JUC7cBIO/8W0Prtd?=
 =?us-ascii?q?zYCAM5PBKww+r9FNp90YYeU3qVAqCFKKPSrUOI5uU3LumMfoAVuSr9JOIi5/L0?=
 =?us-ascii?q?jX85hEUSfa+m3ZYMbHC4H/JmI1iWYHb2g9cBF3sKsRQ6TODwlFKCVjtTbW6oX6?=
 =?us-ascii?q?0g/jE7FJ6mDYDbS4+3m7yB3CC7Hp5MamBcEF+ME2zld4GFW/cKdSKTLdVtkj0C?=
 =?us-ascii?q?Vbi9VYAh0QuiuxP9y7piNuDU4DEXtYr/1Nhp4O3ejRIy+iZyD8iH12GNTnt7nm?=
 =?us-ascii?q?UHRzIt2KB/oEp9ykqM0KRigvxYE8BT6O1NUgsgKZHcyOl6AcjoWg3dZteJVEqm?=
 =?us-ascii?q?QtK+DD4sVN0x3cEBb1x9G9q4iBDDxDSlA7kSm7yPB5w096bc0mP1J8Z8zXbGya?=
 =?us-ascii?q?Ygg0MnQstJKW2pmKp/+xLPCI7OlkWTj7yqergE3C7R6GeDynKDs1xCXw5uT6rJ?=
 =?us-ascii?q?R3AfaVHQrdTi+EzCSaSjCbAmMgtH1M6DJbFGatzvjVVaWvjjPM7SbH62m2e1HR?=
 =?us-ascii?q?yI3K+DbJL2e2UB2yXQEFQEkwEW/XaBKQg+Biegr3jCDDB0ElLveUfs8eh4qHO0?=
 =?us-ascii?q?VUI0ywCKb0t817u64BIVhPqcS+8N0bIAoisutzJ0HFOl1dLMF9WAvxZhfLlbYd?=
 =?us-ascii?q?4l+lhH0XzWtgNjMZ2gNaxtnUMefBltsEPo1BV3DZtAnNMurHMrygpyNK2Z3ElA?=
 =?us-ascii?q?dzOewZD/JLnXJnPu8xCobq7cwkve38qO+qcT9PQ4rE3uvQG0FkY473prydhU03?=
 =?us-ascii?q?uG6ZXMAwofSpbxUkcx9xhnqLDWeCg954XI1XJyNam4qCPN29UsBOE90BavY89f?=
 =?us-ascii?q?ML+YFA/1C8AaHcmuJ/AwlFizcx0EO/pe9KgqMMO8dvuKwbKkM/xknD27k2tH+o?=
 =?us-ascii?q?d90kSX+ip4S+7I2YsFwv6C0guGUTf8kEmussTtlY9YYjESG3K1yTL4C45Jeq1y?=
 =?us-ascii?q?YYELBH+uIs223Nl/h4ThW2VF9F6lHF4G3M6peRyPb13yxwFQ1EIXoWC5liu81T?=
 =?us-ascii?q?B7jzYprq+H1izU3+vibAYHOnJMRGR6j1fjO5K0j9MZXEivdQQpjwGq5UX5x6ha?=
 =?us-ascii?q?oqRwMW/TTFxMfyj3KWFiT6SxuqCDY85J9JMnryFXXP6gblCdT773uwEa3D/7H2?=
 =?us-ascii?q?tC2DA7cCmnu5XjkBx9km6dLndzrHzCdMF0xBff4sHcRPFL0joHQil4lSfYBlym?=
 =?us-ascii?q?M9a1+tWUko/JsvqiWGK5Sp1TbS7rwJuDtCSh5G1mGx2/n+2pld3hHgg3yiv719?=
 =?us-ascii?q?hsVSXVoxfweIjr16KmMe15eklkHkPz681/GotmiIs/mIkQ2WQGhpWS5XcHlGbz?=
 =?us-ascii?q?MdZB1q7kYnsCWyULw8LL7wj/w0JsMGiJx43iW3WZw8thYcS6Y2wM1iI86cBKFL?=
 =?us-ascii?q?mb7LhekSRppVq4qBrbYeJhkTcF1fsu9HkajvkVuAoqyyWRGLETElNePSzxjBSI?=
 =?us-ascii?q?9NG+ob5TZGaud7iwyUV/kcqgDLGEvgFTRnL5do0+Ei9368V1KEjM32Hr6oH4ZN?=
 =?us-ascii?q?nQasoeuQaOnBfHi+hVNYg9lv4XhSd8PWL9vHsly/M0jBB03JG6upSHJHtp/K6j?=
 =?us-ascii?q?HhFYMTj1bdsJ+j7xlaZegtqW34e3E5VjADoLWYXnTemyHDIOr/jnNBuOED4hqn?=
 =?us-ascii?q?edA7XfBhSS6ENnr3LJDpCqOGubJHgfzdV+WhadIFZTjxwTXDU/hpQ5DBylxNT9?=
 =?us-ascii?q?cEdl4TAc/l75qhxRyuJxKhnwSGHfqB2zajczTpifIwFb7gVD50fTLMyf4fh/Hy?=
 =?us-ascii?q?Be/p29sgONLnaXaBhPDWEMQkaEHUzsPqGy5dnc9OiVHuq+IOXPYbWNquxeVu2E?=
 =?us-ascii?q?xZG10oth8DaDKN+PPmR5D/Ag3kpDXHZ5G9nWmjkVSiwXkT7NYNCfpBum5iJ3qc?=
 =?us-ascii?q?W/+uzxWA3z/YuPF6dSMdJ39h+snKeMLPSQizh5KDpC0pMMxGTFyLwe3F4UliFv?=
 =?us-ascii?q?eCOhEbUGtS7RUq3QnrVbAAIcayN2LMFI9b4z3hFROc7HjdP4zqJ4jv8wC1tfSV?=
 =?us-ascii?q?PtgN2mZcwULGG7L17HAEeLNLKbJTzE2c33YKW8SaFOg+VQrRG/pTGbE0r7NDSZ?=
 =?us-ascii?q?izbpTwyvMf1LjCyDPB1Rooe9chNsCWjlVN3nax27P8VxjT052rA0gnLKNWgBMT?=
 =?us-ascii?q?lzaU9Nr7uQ7T9GjfV7AWBO8n1lLezX0xqeusvRLN41uOFiSnB2nulZyG8n0LYT?=
 =?us-ascii?q?5zMSF9Jvny6HjNN0pFSr2saLyyF8XVIaqDlPhZmQsG14NKnZ/4UGUnHBqkFepV?=
 =?us-ascii?q?6MAggH8oM2QubkvLpdn52WzPr+?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AHAAB+xxtch0O0hNFlGQEBAQEBAQEBA?=
 =?us-ascii?q?QEBAQcBAQEBAQGBVAEBAQEBAQsBAYFUghaNHIscgg0Ul12BdCsBhy4iNwYNAQM?=
 =?us-ascii?q?BAQEBAQECARMBAQEIDQkIKS9CAQ4BgWQkAYJhAQEBAQIBAQIJGwsBRgYJAQEKD?=
 =?us-ascii?q?goJDBkDDAUpIBgWgweBeggFqF0ziiaMPxeBQD+EI4Q8gQSCeoImAolHgXuEGn+?=
 =?us-ascii?q?QYgmRWQwYgV+IGTeHLiyZYYFcgXgzGggbFYMogiYXjjshAQGBNgEBHIZGhnsDA?=
 =?us-ascii?q?QE?=
X-IPAS-Result: =?us-ascii?q?A0AHAAB+xxtch0O0hNFlGQEBAQEBAQEBAQEBAQcBAQEBAQG?=
 =?us-ascii?q?BVAEBAQEBAQsBAYFUghaNHIscgg0Ul12BdCsBhy4iNwYNAQMBAQEBAQECARMBA?=
 =?us-ascii?q?QEIDQkIKS9CAQ4BgWQkAYJhAQEBAQIBAQIJGwsBRgYJAQEKDgoJDBkDDAUpIBg?=
 =?us-ascii?q?WgweBeggFqF0ziiaMPxeBQD+EI4Q8gQSCeoImAolHgXuEGn+QYgmRWQwYgV+IG?=
 =?us-ascii?q?TeHLiyZYYFcgXgzGggbFYMogiYXjjshAQGBNgEBHIZGhnsDAQE?=
X-IronPort-AV: E=Sophos;i="5.56,377,1539673200"; 
   d="scan'208";a="142956373"
X-Amp-Result: UNKNOWN
X-Amp-Original-Verdict: FILE UNKNOWN
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 20 Dec 2018 08:49:21 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1732016AbeLTQtS (ORCPT <rfc822;like.xu@linux.intel.com>
        + 22 others); Thu, 20 Dec 2018 11:49:18 -0500
Received: from mx1.redhat.com ([209.132.183.28]:58818 "EHLO mx1.redhat.com"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1730884AbeLTQtR (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Thu, 20 Dec 2018 11:49:17 -0500
Received: from smtp.corp.redhat.com (int-mx07.intmail.prod.int.phx2.redhat.com [10.5.11.22])
        (using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
        (No client certificate requested)
        by mx1.redhat.com (Postfix) with ESMTPS id 77475AB409;
        Thu, 20 Dec 2018 16:49:16 +0000 (UTC)
Received: from redhat.com (ovpn-123-95.rdu2.redhat.com [10.10.123.95])
        by smtp.corp.redhat.com (Postfix) with ESMTPS id 27E771055006;
        Thu, 20 Dec 2018 16:49:14 +0000 (UTC)
Date: Thu, 20 Dec 2018 11:49:12 -0500
From: Jerome Glisse <jglisse@redhat.com>
To: Jan Kara <jack@suse.cz>
Cc: John Hubbard <jhubbard@nvidia.com>,
        Matthew Wilcox <willy@infradead.org>,
        Dave Chinner <david@fromorbit.com>,
        Dan Williams <dan.j.williams@intel.com>,
        John Hubbard <john.hubbard@gmail.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Linux MM <linux-mm@kvack.org>, tom@talpey.com,
        Al Viro <viro@zeniv.linux.org.uk>, benve@cisco.com,
        Christoph Hellwig <hch@infradead.org>,
        Christopher Lameter <cl@linux.com>,
        "Dalessandro, Dennis" <dennis.dalessandro@intel.com>,
        Doug Ledford <dledford@redhat.com>,
        Jason Gunthorpe <jgg@ziepe.ca>,
        Michal Hocko <mhocko@kernel.org>, mike.marciniszyn@intel.com,
        rcampbell@nvidia.com,
        Linux Kernel Mailing List <linux-kernel@vger.kernel.org>,
        linux-fsdevel <linux-fsdevel@vger.kernel.org>
Subject: Re: [PATCH 1/2] mm: introduce put_user_page*(), placeholder versions
Message-ID: <20181220164912.GB3963@redhat.com>
References: <20181212150319.GA3432@redhat.com>
 <20181212214641.GB29416@dastard>
 <20181214154321.GF8896@quack2.suse.cz>
 <20181216215819.GC10644@dastard>
 <20181217181148.GA3341@redhat.com>
 <20181217183443.GO10600@bombadil.infradead.org>
 <20181218093017.GB18032@quack2.suse.cz>
 <9f43d124-2386-7bfd-d90b-4d0417f51ccd@nvidia.com>
 <20181219020723.GD4347@redhat.com>
 <20181219110856.GA18345@quack2.suse.cz>
MIME-Version: 1.0
Content-Type: text/plain; charset=iso-8859-1
Content-Disposition: inline
Content-Transfer-Encoding: 8bit
In-Reply-To: <20181219110856.GA18345@quack2.suse.cz>
User-Agent: Mutt/1.10.1 (2018-07-13)
X-Scanned-By: MIMEDefang 2.84 on 10.5.11.22
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16 (mx1.redhat.com [10.5.110.28]); Thu, 20 Dec 2018 16:49:17 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On Wed, Dec 19, 2018 at 12:08:56PM +0100, Jan Kara wrote:
> On Tue 18-12-18 21:07:24, Jerome Glisse wrote:
> > On Tue, Dec 18, 2018 at 03:29:34PM -0800, John Hubbard wrote:
> > > OK, so let's take another look at Jerome's _mapcount idea all by itself (using
> > > *only* the tracking pinned pages aspect), given that it is the lightest weight
> > > solution for that.  
> > > 
> > > So as I understand it, this would use page->_mapcount to store both the real
> > > mapcount, and the dma pinned count (simply added together), but only do so for
> > > file-backed (non-anonymous) pages:
> > > 
> > > 
> > > __get_user_pages()
> > > {
> > > 	...
> > > 	get_page(page);
> > > 
> > > 	if (!PageAnon)
> > > 		atomic_inc(page->_mapcount);
> > > 	...
> > > }
> > > 
> > > put_user_page(struct page *page)
> > > {
> > > 	...
> > > 	if (!PageAnon)
> > > 		atomic_dec(&page->_mapcount);
> > > 
> > > 	put_page(page);
> > > 	...
> > > }
> > > 
> > > ...and then in the various consumers of the DMA pinned count, we use page_mapped(page)
> > > to see if any mapcount remains, and if so, we treat it as DMA pinned. Is that what you 
> > > had in mind?
> > 
> > Mostly, with the extra two observations:
> >     [1] We only need to know the pin count when a write back kicks in
> >     [2] We need to protect GUP code with wait_for_write_back() in case
> >         GUP is racing with a write back that might not the see the
> >         elevated mapcount in time.
> > 
> > So for [2]
> > 
> > __get_user_pages()
> > {
> >     get_page(page);
> > 
> >     if (!PageAnon) {
> >         atomic_inc(page->_mapcount);
> > +       if (PageWriteback(page)) {
> > +           // Assume we are racing and curent write back will not see
> > +           // the elevated mapcount so wait for current write back and
> > +           // force page fault
> > +           wait_on_page_writeback(page);
> > +           // force slow path that will fault again
> > +       }
> >     }
> > }
> 
> This is not needed AFAICT. __get_user_pages() gets page reference (and it
> should also increment page->_mapcount) under PTE lock. So at that point we
> are sure we have writeable PTE nobody can change. So page_mkclean() has to
> block on PTE lock to make PTE read-only and only after going through all
> PTEs like this, it can check page->_mapcount. So the PTE lock provides
> enough synchronization.

This is needed, file back page can be map in any number of page table
and thus no PTE lock gonna protect anything in the end. More over with
GUP fast we really have to assume there is no lock that force ordering.

In fact in the above snipet that mapcount should not happen if there
is an on going write back.


> > For [1] only needing pin count during write back turns page_mkclean into
> > the perfect spot to check for that so:
> > 
> > int page_mkclean(struct page *page)
> > {
> >     int cleaned = 0;
> > +   int real_mapcount = 0;
> >     struct address_space *mapping;
> >     struct rmap_walk_control rwc = {
> >         .arg = (void *)&cleaned,
> >         .rmap_one = page_mkclean_one,
> >         .invalid_vma = invalid_mkclean_vma,
> > +       .mapcount = &real_mapcount,
> >     };
> > 
> >     BUG_ON(!PageLocked(page));
> > 
> >     if (!page_mapped(page))
> >         return 0;
> > 
> >     mapping = page_mapping(page);
> >     if (!mapping)
> >         return 0;
> > 
> >     // rmap_walk need to change to count mapping and return value
> >     // in .mapcount easy one
> >     rmap_walk(page, &rwc);
> > 
> >     // Big fat comment to explain what is going on
> > +   if ((page_mapcount(page) - real_mapcount) > 0) {
> > +       SetPageDMAPined(page);
> > +   } else {
> > +       ClearPageDMAPined(page);
> > +   }
> 
> This is the detail I'm not sure about: Why cannot rmap_walk_file() race
> with e.g. zap_pte_range() which decrements page->_mapcount and thus the
> check we do in page_mkclean() is wrong?

Ok so i thought about this here is what we have:
    mp1 = page_mapcount(page);
    // let name rc1 the number of real count at mp1 time (this is
    // an ideal value that we can not get)

    rmap_walk(page, &rwc);
    // at this point let's name frc the number of real map count
    // found by rmap_walk

    mp2 = page_mapcount(page);
    // let name rc2 the number of real count at mp2 time (this is
    // an ideal value that we can not get)


So we have
    rc1 >= frc >= rc2
    pc1 = mp1 - rc1     // pin count at mp1 time
    pc2 = mp2 - rc2     // pin count at mp2 time

So we have:
    mp1 - rc1 <= mp1 - frc
    mp2 - rc2 >= mp2 - frc

From the above:
    mp1 - frc <  0 impossible value mapcount can only go down so
                   frc <= mp1
    mp1 - frc == 0 -> the page is not pin
U1  mp1 - frc >  0 -> the page might be pin

U2  mp2 - frc <= 0 -> the page might be pin
    mp2 - frc >  0 -> the page is pin

They are two unknowns [U1] and [U2]:
    [U1]    a zap raced before rmap_walk() could account the zaped
            mapping (frc < rc1)
    [U2]    a zap raced after rmap_walk() accounted the zaped
            mapping (frc > rc2)

In both cases we can detect the race but we can not ascertain if page
is pin or not.

So we can do 2 things here:
    - try to recount the real mapping (it is bound to end as no
      new mapping can be added and thus mapcount can only go down)
    - assume false positive and uselessly bounce page that would
      not need bouncing if we were not unlucky

We could mitigate this with a flag GUP unconditionaly set it and page
mkclean clears it when mp1 - frc == 0 this way we never bounce page
that were never GUPed but we might keep bouncing a page that was GUPed
once in its lifetime until there is not race for it in page_mkclean.

I will ponder a bit more and see if i can get an idea on how to close
that race ie either close U1 or close U2.


> >     // Maybe we want to leverage the int nature of return value so that
> >     // we can express more than cleaned/truncated and express cleaned/
> >     // truncated/pinned for benefit of caller and that way we do not
> >     // even need one bit as page flags above.
> > 
> >     return cleaned;
> > }
> > 
> > You do not want to change page_mapped() i do not see a need for that.
> > 
> > Then the whole discussion between Jan and Dave seems to indicate that
> > the bounce mechanism will need to be in the fs layer and that we can
> > not reuse the bio bounce mechanism. This means that more work is needed
> > at the fs level for that (so that fs do not freak on bounce page).
> > 
> > Note that they are few gotcha where we need to preserve the pin count
> > ie mostly in truncate code path that can remove page from page cache
> > and overwrite the mapcount in the process, this would need to be fixed
> > to not overwrite mapcount so that put_user_page does not set the map
> > count to an invalid value turning the page into a bad state that will
> > at one point trigger kernel BUG_ON();
> >
> > I am not saying block truncate, i am saying make sure it does not
> > erase pin count and keep truncating happily. The how to handle truncate
> > is a per existing GUP user discussion to see what they want to do for
> > that.
> > 
> > Obviously a bit deeper analysis of all spot that use mapcount is needed
> > to check that we are not breaking anything but from the top of my head
> > i can not think of anything bad (migrate will abort and other things will
> > assume the page is mapped even it is only in hardware page table, ...).
> 
> Hum, grepping for page_mapped() and page_mapcount(), this is actually going
> to be non-trivial to get right AFAICT.

No that's not that scary a good chunk of all those are for anonymous
memory and many are obvious (like migrate, ksm, ...).

Cheers,
Jérôme
