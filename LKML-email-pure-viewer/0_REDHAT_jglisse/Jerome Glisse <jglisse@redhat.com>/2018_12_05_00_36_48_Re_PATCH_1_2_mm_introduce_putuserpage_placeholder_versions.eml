Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 05 Dec 2018 08:46:01 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga007.jf.intel.com (orsmga007.jf.intel.com [10.7.209.58])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 99872580375;
	Tue,  4 Dec 2018 16:37:00 -0800 (PST)
Received: from orsmga102-1.jf.intel.com (HELO mga09.intel.com) ([10.7.208.27])
  by orsmga007-1.jf.intel.com with ESMTP; 04 Dec 2018 16:37:00 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3Ae4V8VxQtsIdUMS1YDDojTqbOi9psv+yvbD5Q0YIu?=
 =?us-ascii?q?jvd0So/mwa64YRWAt8tkgFKBZ4jH8fUM07OQ7/iwHzRYqb+681k6OKRWUBEEjc?=
 =?us-ascii?q?hE1ycBO+WiTXPBEfjxciYhF95DXlI2t1uyMExSBdqsLwaK+i764jEdAAjwOhRo?=
 =?us-ascii?q?LerpBIHSk9631+ev8JHPfglEnjWwba9xIRmssQndqtQdjJd/JKo21hbHuGZDdf?=
 =?us-ascii?q?5MxWNvK1KTnhL86dm18ZV+7SleuO8v+tBZX6nicKs2UbJXDDI9M2Ao/8LrrgXM?=
 =?us-ascii?q?TRGO5nQHTGoblAdDDhXf4xH7WpfxtTb6tvZ41SKHM8D6Uaw4VDK/5KpwVhTmlD?=
 =?us-ascii?q?kIOCI48GHPi8x/kqRboA66pxdix4LYeZyZOOZicq/Ye94RWGhPUdtLVyFZDI2y?=
 =?us-ascii?q?b5UBAfcCM+ZWoIbyu0YBoxS8CgaiH+Pv0j1Fi2Tq3aA5yektDR3K0RY9E98IrX?=
 =?us-ascii?q?/arM/1NKAXUe2tyKfH0y/Db/RT2Tjj9YPGcxQhofCXXbJrb8Xa1E4iFwHKjlWU?=
 =?us-ascii?q?qIzlJCiV2foWvmiB8eVvSOKvhHQ7qw1rvjevwcIsh5DPi4kIxF7E8iB5z5w0Jd?=
 =?us-ascii?q?2+UEN7YsCrEIFWty6EM4t6WMQiQ3tnuCoizr0Ht5i7cDIMyJs52x7SbeGMfYuQ?=
 =?us-ascii?q?4h/7SuqdPTN1iGh4dL+xmRq+61Wsx+7gWsWu0VtHrDJJnsfQun0JzRDf98aKRu?=
 =?us-ascii?q?Vn8ku82juC2Rrf5v9HLE0yiKHVMYQuwqQqmZoWqUnDHjH5mEHxjKKOaEUk9fan?=
 =?us-ascii?q?6/79brXluJCcLYl0hR/6Mqg0ncy/G+s4PhAPX2id5+u8yKXu8VPlTLhOlPE6j6?=
 =?us-ascii?q?fUvI7AKcgGpaO1HxVZ34ch5hqnCjepytUYnX0JLFJffxKHipDkO1XPIPD+EPe+?=
 =?us-ascii?q?jE2gkDR1yPDcOL3uHJHNImHEkLbve7Zy9VRcxREtzdBQ+Z1UEKsNIPHtVU/rst?=
 =?us-ascii?q?zXEBs5Pxazw+b9B9VxzpkeVn6XAq+FLKPStkeF5uYuI+mPeoAZojn8K+U+6v7q?=
 =?us-ascii?q?jH85n0IdfKaz0ZsWbnC4AuppI0GDbXXwhdcBFH8AvhAiQ+zylF2CTTlTam6yX6?=
 =?us-ascii?q?0m5zE7FJipDYDZSoCtnbyOxiG7HpJNa2BCC1CMF2rodoqeV/cNbiKSPtFukjge?=
 =?us-ascii?q?Wbe9TI8h0AmktBXmxLp/MurU5ioYuIrh1Nhy+eLfjxIy9TtyD8Sb1GGAVGV0nm?=
 =?us-ascii?q?IORz8r06Fzu019ylGf0admh/xUD8Bc5/RMUg0iL57T0/R6C8zuWgLGZtqGUk2m?=
 =?us-ascii?q?QtWhATEyVN4x2cUBY0RmFtWmjxDD2TeqArAPm7yKApw07rzT33zrK8lhzHbG0b?=
 =?us-ascii?q?Erj0M6TctXKW2mmql/+hDQB4HTlUWVjaKqdaUG0y7L+2eO1m6OvEBeUA5tXqTJ?=
 =?us-ascii?q?R3EfZk3Krdvn4kPOVaOhCbMiMgFZ086NNrNKasH1jVVBXPrsJc7RY3yvlGuqBR?=
 =?us-ascii?q?aH3LWMbJH0dGUb2yndDEsEkwUX/XudMQg+ByGho3/RDTB0FFLvZV/s/vd6qH+h?=
 =?us-ascii?q?UkA0yASKZVV717Wp4h4VmeCcS/QL070ZoightSt7EEy9393MDdqAvBRufKNHbN?=
 =?us-ascii?q?M54VdH03/ZtgNnMpyhKaBimkARcwBts0zy0BV3D51KkdI2o3My0ApyNaWY3Utd?=
 =?us-ascii?q?dzOZ2JDwPaHXKmny/Ry1d67awFbe0MyS+qcO7vQ4pE7uvAWoFkok7nVm3MNZ03?=
 =?us-ascii?q?qa5pXWEgUSVYj9XVow9xh/v7vaeDUy55vI1X1wNqm5qiXN29Y3C+oq1Bmhf81T?=
 =?us-ascii?q?P7iZFADvCcIaAcuuKOs0m1WyahIEPeZS9LM7Ps+8dvuG3rKrM/hknD68kWtH54?=
 =?us-ascii?q?V92FqW9yVgUu7Iw4oFw/aA0wqHSjfwlkuuvtr2mIBEfz4SGGW/xDPgBI5QYK1y?=
 =?us-ascii?q?YIkKBX2vI82x2tVxmZrtV2RE+16kAlMMwNWpdgaKb1zhwQ1Q0lwaoWammSSk1T?=
 =?us-ascii?q?N0iSwmrq2F0CzI3evibhsHNndXS2Z4iVfjPJa7j8odXEiudAUpkBql5UDnx6lU?=
 =?us-ascii?q?vqh/Lm/TQVtWcCjyNW1tTqywtr+aac5V9JwoqTlXUPi7YV2CSr/9ogEW0iP5E2?=
 =?us-ascii?q?tF2TA7cSqnupH4nxx8lWKcI2x/rHvfecFs2xjf4MbQSuJW3joDXCN4kyXYBkCg?=
 =?us-ascii?q?P9m1+tWZj5XDvfqkV2KiVZ1TdjPnzZiauyu45m1qABu/kO61mtD8FQg60Cn718?=
 =?us-ascii?q?RlVCnSrRb8ZJXr2Lq+Me59YkZoA1r84dJgGo5iioswmI0Q2X8Ci5SV53UHln3/?=
 =?us-ascii?q?Mdda2aL4d3cNQT8Lw9jI4AnqwkFjL3SJx57nWXWZ2Mdue966YmYO0CIn889KEL?=
 =?us-ascii?q?uU7KBDnSZtoVq3twPRbeJ9njcA0/Qu7nEajvoNuAosyCWdH78TEVNZPSzqixSH?=
 =?us-ascii?q?8dS+oL9La2aodLi6zFB+ksy5DLGevgFcX270epIjHS9z9MpzKlzN32Po6oH4Zt?=
 =?us-ascii?q?bQd8kcuQOOkxfPlOVVLJMxlvwXhStoI279vHsly/Ilghxqx521oI+HK2B19qKj?=
 =?us-ascii?q?HhFYLiH1Z98U+jz1kaZemcOW05quH5p7HDULQYDoTempED8JsfTnNgCOECAzq3?=
 =?us-ascii?q?uBGLrfGxOf51lir37VD5+rMHSXLmEDzdp+XBmdOFBfgAcMUTohhJE5EQSqxNH7?=
 =?us-ascii?q?fEd9+zAc/Vr4qhpKyuJ1OBjzSGbfpAG0ajgqTJiTNgZZ7gZH503NK8yR8vpzHz?=
 =?us-ascii?q?1E/p2mtAGNKHaUZwFWAmESQECEG0rvPrqz5dnD7uiYAPCxL+DVbLWKqOxeUeqI?=
 =?us-ascii?q?xJ213otn+TaMKtuAPn14A/In3UpDWGhzG97FlDUXVywXiyXNYtabpRe75y13t9?=
 =?us-ascii?q?2//+7tWA71/ouPDL1SMdp09hCygKeDMfOQhSljJTZZ0JMM2WHHyLwF0FEOjCFu?=
 =?us-ascii?q?ciGnEa4cui7VUKLQhqhXAgYbayxtNctI7KE83ghXNc7YkNP10bF4geAvC1dYTl?=
 =?us-ascii?q?zsgcWpZc0MI2GgO1LLHkeLNLKaJTLVx8H7e7+zSbpVjO9MrR2/pS6bE1P/PjSE?=
 =?us-ascii?q?jzTmTQ2vPvpWjC6FPB1SooW9chd2BGjnTdLmbAC7MdBtgT03x700mm3FNWoGPT?=
 =?us-ascii?q?dgdENNq6Wa7TlEjfVnB2xB8n1lIPGYmyaY6unUMIoZveF3DSR0ie1a5m83y79U?=
 =?us-ascii?q?7CFCWfx0lzHert9oo1G6jOaPziBrXwZJqjZOnIiLp1ltOb3F9plcXnbJ5BIM4n?=
 =?us-ascii?q?+WCxQPpttlDMXguqFQytfVkqLzJyxP89bV/csaGsjVJ9iLMHsnMRr1BjHUCBEJ?=
 =?us-ascii?q?QiKsNWHa1ARhl6S+/3vdkZk+rpnlnYFGHrNSUho3UOwbDkBkFdkZCJZxQj4g17?=
 =?us-ascii?q?WciZhby2C5qUzzSd9du52PefaTGu7iYGKbgr1FfAAF6an1IYQaKsvw3Ek0OQoy?=
 =?us-ascii?q?p5jDB0eFBYMFmSZmdAJh5RwVqHU=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AKAABrHAdch0O0hNFcCBoBAQEBAQIBA?=
 =?us-ascii?q?QEBBwIBAQEBgVQCAQEBAQsBAYFUBYEPgQInjHGLL4INFIh+kCoUGAsIAYdMIjc?=
 =?us-ascii?q?GDQEDAQEBAQEBAgETAQEBCA0JCCkjDEIBDgGBZCQBgmEBAQEBAwECCSYBNREGC?=
 =?us-ascii?q?QEBChUDCSUDDAUNHCATBYJRSwGBZwIDFQUKpj2ELQELAYNMDYIXBYweF4FAP4N?=
 =?us-ascii?q?uNYJXRwEDgTqGAAKJHw6Bc4UOj2wuCYcDhxCDIwsYgVuIAIdLLIldg2WBDIl8g?=
 =?us-ascii?q?VyBdzMaCBsVgycTDIpCO4VdIQEBMQEBgQMBARyFXoJdK4IgAQE?=
X-IPAS-Result: =?us-ascii?q?A0AKAABrHAdch0O0hNFcCBoBAQEBAQIBAQEBBwIBAQEBgVQ?=
 =?us-ascii?q?CAQEBAQsBAYFUBYEPgQInjHGLL4INFIh+kCoUGAsIAYdMIjcGDQEDAQEBAQEBA?=
 =?us-ascii?q?gETAQEBCA0JCCkjDEIBDgGBZCQBgmEBAQEBAwECCSYBNREGCQEBChUDCSUDDAU?=
 =?us-ascii?q?NHCATBYJRSwGBZwIDFQUKpj2ELQELAYNMDYIXBYweF4FAP4NuNYJXRwEDgTqGA?=
 =?us-ascii?q?AKJHw6Bc4UOj2wuCYcDhxCDIwsYgVuIAIdLLIldg2WBDIl8gVyBdzMaCBsVgyc?=
 =?us-ascii?q?TDIpCO4VdIQEBMQEBgQMBARyFXoJdK4IgAQE?=
X-IronPort-AV: E=Sophos;i="5.56,316,1539673200"; 
   d="scan'208";a="56117439"
X-Amp-Result: UNSCANNABLE
X-Amp-File-Uploaded: False
Unscannable: 2
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 04 Dec 2018 16:36:56 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726537AbeLEAgy (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Tue, 4 Dec 2018 19:36:54 -0500
Received: from mx1.redhat.com ([209.132.183.28]:43412 "EHLO mx1.redhat.com"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1725979AbeLEAgx (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Tue, 4 Dec 2018 19:36:53 -0500
Received: from smtp.corp.redhat.com (int-mx03.intmail.prod.int.phx2.redhat.com [10.5.11.13])
        (using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
        (No client certificate requested)
        by mx1.redhat.com (Postfix) with ESMTPS id E6CA5308403D;
        Wed,  5 Dec 2018 00:36:52 +0000 (UTC)
Received: from redhat.com (unknown [10.20.6.215])
        by smtp.corp.redhat.com (Postfix) with ESMTPS id 914896B8EA;
        Wed,  5 Dec 2018 00:36:50 +0000 (UTC)
Date: Tue, 4 Dec 2018 19:36:48 -0500
From: Jerome Glisse <jglisse@redhat.com>
To: Dan Williams <dan.j.williams@intel.com>
Cc: John Hubbard <jhubbard@nvidia.com>,
        John Hubbard <john.hubbard@gmail.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Linux MM <linux-mm@kvack.org>, Jan Kara <jack@suse.cz>,
        tom@talpey.com, Al Viro <viro@zeniv.linux.org.uk>, benve@cisco.com,
        Christoph Hellwig <hch@infradead.org>,
        Christopher Lameter <cl@linux.com>,
        "Dalessandro, Dennis" <dennis.dalessandro@intel.com>,
        Doug Ledford <dledford@redhat.com>,
        Jason Gunthorpe <jgg@ziepe.ca>,
        Matthew Wilcox <willy@infradead.org>,
        Michal Hocko <mhocko@kernel.org>, mike.marciniszyn@intel.com,
        rcampbell@nvidia.com,
        Linux Kernel Mailing List <linux-kernel@vger.kernel.org>,
        linux-fsdevel <linux-fsdevel@vger.kernel.org>
Subject: Re: [PATCH 1/2] mm: introduce put_user_page*(), placeholder versions
Message-ID: <20181205003648.GT2937@redhat.com>
References: <20181204001720.26138-1-jhubbard@nvidia.com>
 <20181204001720.26138-2-jhubbard@nvidia.com>
 <CAPcyv4h99JVHAS7Q7k3iPPUq+oc1NxHdyBHMjpgyesF1EjVfWA@mail.gmail.com>
 <a0adcf7c-5592-f003-abc5-a2645eb1d5df@nvidia.com>
 <CAPcyv4iNtamDAY9raab=iXhSZByecedBpnGybjLM+PuDMwq7SQ@mail.gmail.com>
MIME-Version: 1.0
Content-Type: text/plain; charset=iso-8859-1
Content-Disposition: inline
Content-Transfer-Encoding: 8bit
In-Reply-To: <CAPcyv4iNtamDAY9raab=iXhSZByecedBpnGybjLM+PuDMwq7SQ@mail.gmail.com>
User-Agent: Mutt/1.10.0 (2018-05-17)
X-Scanned-By: MIMEDefang 2.79 on 10.5.11.13
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16 (mx1.redhat.com [10.5.110.40]); Wed, 05 Dec 2018 00:36:53 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On Tue, Dec 04, 2018 at 03:03:02PM -0800, Dan Williams wrote:
> On Tue, Dec 4, 2018 at 1:56 PM John Hubbard <jhubbard@nvidia.com> wrote:
> >
> > On 12/4/18 12:28 PM, Dan Williams wrote:
> > > On Mon, Dec 3, 2018 at 4:17 PM <john.hubbard@gmail.com> wrote:
> > >>
> > >> From: John Hubbard <jhubbard@nvidia.com>
> > >>
> > >> Introduces put_user_page(), which simply calls put_page().
> > >> This provides a way to update all get_user_pages*() callers,
> > >> so that they call put_user_page(), instead of put_page().
> > >>
> > >> Also introduces put_user_pages(), and a few dirty/locked variations,
> > >> as a replacement for release_pages(), and also as a replacement
> > >> for open-coded loops that release multiple pages.
> > >> These may be used for subsequent performance improvements,
> > >> via batching of pages to be released.
> > >>
> > >> This is the first step of fixing the problem described in [1]. The steps
> > >> are:
> > >>
> > >> 1) (This patch): provide put_user_page*() routines, intended to be used
> > >>    for releasing pages that were pinned via get_user_pages*().
> > >>
> > >> 2) Convert all of the call sites for get_user_pages*(), to
> > >>    invoke put_user_page*(), instead of put_page(). This involves dozens of
> > >>    call sites, and will take some time.
> > >>
> > >> 3) After (2) is complete, use get_user_pages*() and put_user_page*() to
> > >>    implement tracking of these pages. This tracking will be separate from
> > >>    the existing struct page refcounting.
> > >>
> > >> 4) Use the tracking and identification of these pages, to implement
> > >>    special handling (especially in writeback paths) when the pages are
> > >>    backed by a filesystem. Again, [1] provides details as to why that is
> > >>    desirable.
> > >
> > > I thought at Plumbers we talked about using a page bit to tag pages
> > > that have had their reference count elevated by get_user_pages()? That
> > > way there is no need to distinguish put_page() from put_user_page() it
> > > just happens internally to put_page(). At the conference Matthew was
> > > offering to free up a page bit for this purpose.
> > >
> >
> > ...but then, upon further discussion in that same session, we realized that
> > that doesn't help. You need a reference count. Otherwise a random put_page
> > could affect your dma-pinned pages, etc, etc.
> 
> Ok, sorry, I mis-remembered. So, you're effectively trying to capture
> the end of the page pin event separate from the final 'put' of the
> page? Makes sense.
> 
> > I was not able to actually find any place where a single additional page
> > bit would help our situation, which is why this still uses LRU fields for
> > both the two bits required (the RFC [1] still applies), and the dma_pinned_count.
> 
> Except the LRU fields are already in use for ZONE_DEVICE pages... how
> does this proposal interact with those?
> 
> > [1] https://lore.kernel.org/r/20181110085041.10071-7-jhubbard@nvidia.com
> >
> > >> [1] https://lwn.net/Articles/753027/ : "The Trouble with get_user_pages()"
> > >>
> > >> Reviewed-by: Jan Kara <jack@suse.cz>
> > >
> > > Wish, you could have been there Jan. I'm missing why it's safe to
> > > assume that a single put_user_page() is paired with a get_user_page()?
> > >
> >
> > A put_user_page() per page, or a put_user_pages() for an array of pages. See
> > patch 0002 for several examples.
> 
> Yes, however I was more concerned about validation and trying to
> locate missed places where put_page() is used instead of
> put_user_page().
> 
> It would be interesting to see if we could have a debug mode where
> get_user_pages() returned dynamically allocated pages from a known
> address range and catch drivers that operate on a user-pinned page
> without using the proper helper to 'put' it. I think we might also
> need a ref_user_page() for drivers that may do their own get_page()
> and expect the dma_pinned_count to also increase.

Total crazy idea for this, but this is the right time of day
for this (for me at least it is beer time :)) What about mapping
all struct page in two different range of kernel virtual address
and when get user space is use it returns a pointer from the second
range of kernel virtual address to the struct page. Then in put_page
you know for sure if the code putting the page got it from GUP or
from somewhere else. page_to_pfn() would need some trickery to
handle that.

Dunno if we are running out of kernel virtual address (outside
32bits that i believe we are trying to shot down quietly behind
the bar).

Cheers,
Jérôme
