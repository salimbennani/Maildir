Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by i7-8700 with POP3-SSL;
  18 Dec 2018 08:47:58 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga001.jf.intel.com (orsmga001.jf.intel.com [10.7.209.18])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id A10EF5805CF;
	Mon, 17 Dec 2018 10:15:05 -0800 (PST)
Received: from fmsmga104.fm.intel.com ([10.1.193.100])
  by orsmga001-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 17 Dec 2018 10:15:04 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3Ax7UNTRwAgLY70n/XCy+O+j09IxM/srCxBDY+r6Qd?=
 =?us-ascii?q?0e4TIvad9pjvdHbS+e9qxAeQG9mDu7Qc06L/iOPJYSQ4+5GPsXQPItRndiQuro?=
 =?us-ascii?q?EopTEmG9OPEkbhLfTnPGQQFcVGU0J5rTngaRAGUMnxaEfPrXKs8DUcBgvwNRZv?=
 =?us-ascii?q?JuTyB4Xek9m72/q99pHPYAhEniaxba9vJxiqsAvdsdUbj5F/Iagr0BvJpXVIe+?=
 =?us-ascii?q?VSxWx2IF+Yggjx6MSt8pN96ipco/0u+dJOXqX8ZKQ4UKdXDC86PGAv5c3krgfM?=
 =?us-ascii?q?QA2S7XYBSGoWkx5IAw/Y7BHmW5r6ryX3uvZh1CScIMb7Vq4/Vyi84Kh3SR/okC?=
 =?us-ascii?q?YHOCA/8GHLkcx7kaZXrAu8qxBj34LYZYeYP+d8cKzAZ9MXXWRPUMZPWSJcAY28?=
 =?us-ascii?q?YYQAAPYcMuhXrYbyqUAOrQO8CAS3GOPiySVFimPq0aAgzugsFxzN0gw6H9IJtX?=
 =?us-ascii?q?TZtNH7O7kIUeCyyanH0yjIYfJS2Tf884jIaQ4uquyLULJyfsrRzUgvFxjejlqO?=
 =?us-ascii?q?soHlJS2a2fkNs2eB8+psT/6gi2kiqwxopDWk28QiipHRi44L1lzJ8T91zJs7KN?=
 =?us-ascii?q?GmUkJ3fN2pHIdKuyybNYZ6Wt0uT31stSog17ELt4C3cDIXxJkkyRPTceKLfouO?=
 =?us-ascii?q?7xn+TuieOy14i2hgeL+nhxa970ygyurkW8mq31ZFsDBFnsPPtn8TzRzT7NaISv?=
 =?us-ascii?q?9n8kemwzaP2Bjf6uBCIU8qiarWM4AtzqI0m5YJrEjOEDH6lF/rgKKVakko4Oml?=
 =?us-ascii?q?5ub/brXjvJCcNot0ig/kMqQpn8yyGeA4MgkIX2iG9uWwzb7j8lPjQLVMkPI2lr?=
 =?us-ascii?q?DVsJfUJMQduKG5GRRY0pgs6xmhFTeqytcYkmcdLFJDZh2Hi5LlO1bUIPD3Ffu/?=
 =?us-ascii?q?mUijkC93x/DaOb3sGpHNLnnAkLj/Z7p85FNcxRE3zdBe4ZJUF74ALOjyWk/3qN?=
 =?us-ascii?q?zXEBs5PxaozObgDdV3zpkeVn6XAq+FLKPStkeF5uI1LOmNeI8aojH9J+Il5/7z?=
 =?us-ascii?q?l3A5n1AdcLKt3ZsWbnC4A/tnL1+YYXrqntcOD2MKshAiQ+ztjV2ISSRTaGqqX6?=
 =?us-ascii?q?Ig+jE7D5qrDYXERo+zmrCB3yC7HptQZmBBEV2MFXbod4OZW/YDci6SI8lhkiAa?=
 =?us-ascii?q?WrilUYMuyRautAriwbp9MuXU4jEYtY7k1NVt/eLTjhEy9Tt3D8iHyWGCVWN0k3?=
 =?us-ascii?q?gMRz832qB/vEN8xk2C0ah+n/xXC9hT6+lVXQc9MJ7W1/Z6BMzqWgLdYteJT06r?=
 =?us-ascii?q?Qta8DjE3VN4xx94ObFx7G9WtlR3D2yuqA7kIl72EHpA086Tc32TvKMZ50XrJyK?=
 =?us-ascii?q?4hj1w+SMtVKWKmnrJ/9xTUB4PRjkqWjbiqeroG0C7N7miDy3GOs19eUAJ3VaXF?=
 =?us-ascii?q?XnUfZk/NoNT950PCSaKuCLs9PgtAz86CNrVFatnzgVpaQ/fjPczUY3itlGeoGR?=
 =?us-ascii?q?aI2rSMYZL3dGoHwiXSFlIIkwAJ8naALggxGCGhrnnaDDxvE1Lvfkzt/fN/qHO9?=
 =?us-ascii?q?Uk870QWKY1d92Lqy/x4fneacRO8L3rIYpCchrC15HEq839LTDNqAuwphfaVGbd?=
 =?us-ascii?q?Mh+ltH0njZtwh8PpymIKBvnVoecwVxv0Pz2BR7EIRAkc42rHw0yAp+M76X0FRE?=
 =?us-ascii?q?dzmAx5D/JqXXKnXu/BCoc6PZwFXe38iZ+6gR6PU0sU7svBy0GUU49XVn0N5V02?=
 =?us-ascii?q?WH65XODQoSV4/xU0kt+xh7obHafjcy54fO2XJwNqm0tyfI28g1C+s91hagY9Bf?=
 =?us-ascii?q?PbuEFQ/vCcEVG9KiKe0qm1ezaBIEM/tf9Ko1P8OgavuH17SnPOdmnDK6k2tH5J?=
 =?us-ascii?q?px3V6L9yp5UuTIxYoKw+mE3gubUDfxlE2hssHrlo9efzEdA22/xTLiBIFPfK1y?=
 =?us-ascii?q?fJ8HBnu0LM2z29pxmYTtW3le9FO4A1MG2cmpeQedblDn3A1Q01gXrmKjmSei0z?=
 =?us-ascii?q?N0lDQppLKF3CPS2+TiaAYHOmlTSWhijFfgO4i1g8oBXEi1aQgkjx+l5Uf8x6hG?=
 =?us-ascii?q?q6VzNWjTQUFUfyfoK2FuSLe/tr2HY8RX8pMnrT1XUPigYVCdUrP9oQEV0zngH2?=
 =?us-ascii?q?tdwzA3bSqqtY/6nxx5iWKdKmh8rHzCdMF0xBff4sHcRPFL0joHQil4lSfYBlym?=
 =?us-ascii?q?M9a1+tWUko/JsvqiWGK5Sp1TbS7rwJuAtSSh4m1mGx+/n/G1mtD8FQg60Cn718?=
 =?us-ascii?q?RlVCnSrRb8ZJXr2Lq+Me59YkZoA1r84dJgGo5iioswmI0Q2X8Ci5WW53UHkH3/?=
 =?us-ascii?q?MdVG2a3kanoNSiUGw9rU4AjjxU1iIWiFx4P/VnWB3MRhY8O2bX8R2iI498pKEr?=
 =?us-ascii?q?ub7KRYnStppVq1tQfRYfl+njgH0/cv5mAVg/oVuAUz1CWSGa4dHVNXPSH3kxSI?=
 =?us-ascii?q?7ta+rLhYZWq1cLiw0lZ+ks6lDL2Yvg5cX3P5cI84HSBs9sV/LE7M0Hrr54H4f9?=
 =?us-ascii?q?nQaMgftxyOnBfGkuhVM4kxlvsRiCpjOGL9u2AlyuEhgRxv25G6oJaIK2F38K2l?=
 =?us-ascii?q?BR5YMyX/Z9kP9TH1kaZegsGW0pi0EZp7HTULWIboQeisEDIPrvnnMweOEDshqn?=
 =?us-ascii?q?aUA7bfHAmf6Ft4oHLLCZykK3aXJHwBx9V4WBadPFBfgBwTXDginJ42DAWqy9L6?=
 =?us-ascii?q?cEtj+jAd/F34qgZPyuJ1MRnwSHzfqRysajc1TpifMRVX4htD50fTLcyR8OZzEz?=
 =?us-ascii?q?tE8Z2mqQyHMnabaBhQDWEVRkyEAEjuP7mp5dnd6uiYG/CxL/3UbbWVruxeUfiI?=
 =?us-ascii?q?yImr0otn+TaMK8qOMmNjD/09xkpMQ3R5F97FlDUITiwdjzjNYNKDpBeg5i13qd?=
 =?us-ascii?q?iy8PT1VwLu5ouPCLpSPc9s+xCshqeDOPCfhDxkKTZDzZ4MwX7IyL4C3F8dkS1u?=
 =?us-ascii?q?dj+tEageui7JVq7fhqhXDxsDYSNpKMRI97483hVKOcPDkNz1y6V3juQrBFZFT1?=
 =?us-ascii?q?DhnsCpaNcOI2G8MlPHGUmKOK6HJT3N38H4f6e8RadMg+VTsh26oSybHFP7PjSf?=
 =?us-ascii?q?iznpUAiiMf1NjCGeJhBRpJuxfQptCWf9StLrcRm7MN5xjT0rzrw4nHLKNWgAMT?=
 =?us-ascii?q?did0NBtKGf7SRdgv9nAWxO8mJlLfWYmyae9+TZKowZsf1uAiR1keJV+HU7y7tP?=
 =?us-ascii?q?4yFCS/x4gy/Srt9oo1G7neiD0DtnUBxSqjlVgIKHp1ltOaLc9soIZXGR0BsT4C?=
 =?us-ascii?q?27CxUModZ/Qonjsr5RxvDBjqX2Ly1I8s6S9swZUZv6MsWCZVgsKx3sFXb7CAoe?=
 =?us-ascii?q?VjPjYWPQg01Gi/y67HCZrpEm7JPrncxdGfdgSFUpG6ZCWQxeF9sYLcIyB2t8nA?=
 =?us-ascii?q?=3D=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AHAABg5hdch0O0hNFjGQEBAQEBAQEBA?=
 =?us-ascii?q?QEBAQcBAQEBAQGBVAEBAQEBAQsBAYFUBYIRjRuLGIINFJlLKwGBS4YLIjcGDQE?=
 =?us-ascii?q?DAQEBAQEBAgETAQEBCgsJCCkvQgEOAYFkJAGCYQEBAQECAQECCRsLAUYFAQkBA?=
 =?us-ascii?q?QoOCgklAwwFKSAYFgOCOUuBeQgFqT4ziiKMPheBQD+BEYMShD0Qg22CJgKJHiC?=
 =?us-ascii?q?BeIUVkE4JiBaJOQsYgV2IFjeHKCyZOIFcgXgzGggbFYMogiYXjjshAQGBNgEBH?=
 =?us-ascii?q?IZPhwEDAQE?=
X-IPAS-Result: =?us-ascii?q?A0AHAABg5hdch0O0hNFjGQEBAQEBAQEBAQEBAQcBAQEBAQG?=
 =?us-ascii?q?BVAEBAQEBAQsBAYFUBYIRjRuLGIINFJlLKwGBS4YLIjcGDQEDAQEBAQEBAgETA?=
 =?us-ascii?q?QEBCgsJCCkvQgEOAYFkJAGCYQEBAQECAQECCRsLAUYFAQkBAQoOCgklAwwFKSA?=
 =?us-ascii?q?YFgOCOUuBeQgFqT4ziiKMPheBQD+BEYMShD0Qg22CJgKJHiCBeIUVkE4JiBaJO?=
 =?us-ascii?q?QsYgV2IFjeHKCyZOIFcgXgzGggbFYMogiYXjjshAQGBNgEBHIZPhwEDAQE?=
X-IronPort-AV: E=Sophos;i="5.56,366,1539673200"; 
   d="scan'208";a="55514488"
X-Amp-Result: UNKNOWN
X-Amp-Original-Verdict: FILE UNKNOWN
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 17 Dec 2018 10:15:02 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S2388622AbeLQSMA (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Mon, 17 Dec 2018 13:12:00 -0500
Received: from mx1.redhat.com ([209.132.183.28]:44172 "EHLO mx1.redhat.com"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1733076AbeLQSL6 (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 17 Dec 2018 13:11:58 -0500
Received: from smtp.corp.redhat.com (int-mx06.intmail.prod.int.phx2.redhat.com [10.5.11.16])
        (using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
        (No client certificate requested)
        by mx1.redhat.com (Postfix) with ESMTPS id 625A6804F5;
        Mon, 17 Dec 2018 18:11:56 +0000 (UTC)
Received: from redhat.com (ovpn-125-170.rdu2.redhat.com [10.10.125.170])
        by smtp.corp.redhat.com (Postfix) with ESMTPS id A73135C579;
        Mon, 17 Dec 2018 18:11:52 +0000 (UTC)
Date: Mon, 17 Dec 2018 13:11:50 -0500
From: Jerome Glisse <jglisse@redhat.com>
To: Dave Chinner <david@fromorbit.com>
Cc: Jan Kara <jack@suse.cz>, John Hubbard <jhubbard@nvidia.com>,
        Matthew Wilcox <willy@infradead.org>,
        Dan Williams <dan.j.williams@intel.com>,
        John Hubbard <john.hubbard@gmail.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Linux MM <linux-mm@kvack.org>, tom@talpey.com,
        Al Viro <viro@zeniv.linux.org.uk>, benve@cisco.com,
        Christoph Hellwig <hch@infradead.org>,
        Christopher Lameter <cl@linux.com>,
        "Dalessandro, Dennis" <dennis.dalessandro@intel.com>,
        Doug Ledford <dledford@redhat.com>,
        Jason Gunthorpe <jgg@ziepe.ca>,
        Michal Hocko <mhocko@kernel.org>, mike.marciniszyn@intel.com,
        rcampbell@nvidia.com,
        Linux Kernel Mailing List <linux-kernel@vger.kernel.org>,
        linux-fsdevel <linux-fsdevel@vger.kernel.org>
Subject: Re: [PATCH 1/2] mm: introduce put_user_page*(), placeholder versions
Message-ID: <20181217181148.GA3341@redhat.com>
References: <59ca5c4b-fd5b-1fc6-f891-c7986d91908e@nvidia.com>
 <7b4733be-13d3-c790-ff1b-ac51b505e9a6@nvidia.com>
 <20181207191620.GD3293@redhat.com>
 <3c4d46c0-aced-f96f-1bf3-725d02f11b60@nvidia.com>
 <20181208022445.GA7024@redhat.com>
 <20181210102846.GC29289@quack2.suse.cz>
 <20181212150319.GA3432@redhat.com>
 <20181212214641.GB29416@dastard>
 <20181214154321.GF8896@quack2.suse.cz>
 <20181216215819.GC10644@dastard>
MIME-Version: 1.0
Content-Type: text/plain; charset=iso-8859-1
Content-Disposition: inline
Content-Transfer-Encoding: 8bit
In-Reply-To: <20181216215819.GC10644@dastard>
User-Agent: Mutt/1.10.1 (2018-07-13)
X-Scanned-By: MIMEDefang 2.79 on 10.5.11.16
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16 (mx1.redhat.com [10.5.110.27]); Mon, 17 Dec 2018 18:11:57 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On Mon, Dec 17, 2018 at 08:58:19AM +1100, Dave Chinner wrote:
> On Fri, Dec 14, 2018 at 04:43:21PM +0100, Jan Kara wrote:
> > Hi!
> > 
> > On Thu 13-12-18 08:46:41, Dave Chinner wrote:
> > > On Wed, Dec 12, 2018 at 10:03:20AM -0500, Jerome Glisse wrote:
> > > > On Mon, Dec 10, 2018 at 11:28:46AM +0100, Jan Kara wrote:
> > > > > On Fri 07-12-18 21:24:46, Jerome Glisse wrote:
> > > > > So this approach doesn't look like a win to me over using counter in struct
> > > > > page and I'd rather try looking into squeezing HMM public page usage of
> > > > > struct page so that we can fit that gup counter there as well. I know that
> > > > > it may be easier said than done...
> > > > 
> > > > So i want back to the drawing board and first i would like to ascertain
> > > > that we all agree on what the objectives are:
> > > > 
> > > >     [O1] Avoid write back from a page still being written by either a
> > > >          device or some direct I/O or any other existing user of GUP.
> > > >          This would avoid possible file system corruption.
> > > > 
> > > >     [O2] Avoid crash when set_page_dirty() is call on a page that is
> > > >          considered clean by core mm (buffer head have been remove and
> > > >          with some file system this turns into an ugly mess).
> > > 
> > > I think that's wrong. This isn't an "avoid a crash" case, this is a
> > > "prevent data and/or filesystem corruption" case. The primary goal
> > > we have here is removing our exposure to potential corruption, which
> > > has the secondary effect of avoiding the crash/panics that currently
> > > occur as a result of inconsistent page/filesystem state.
> > > 
> > > i.e. The goal is to have ->page_mkwrite() called on the clean page
> > > /before/ the file-backed page is marked dirty, and hence we don't
> > > expose ourselves to potential corruption or crashes that are a
> > > result of inappropriately calling set_page_dirty() on clean
> > > file-backed pages.
> > 
> > I agree that [O1] - i.e., avoid corrupting fs data - is more important and
> > [O2] is just one consequence of [O1].
> > 
> > > > For [O1] and [O2] i believe a solution with mapcount would work. So
> > > > no new struct, no fake vma, nothing like that. In GUP for file back
> > > > pages we increment both refcount and mapcount (we also need a special
> > > > put_user_page to decrement mapcount when GUP user are done with the
> > > > page).
> > > 
> > > I don't see how a mapcount can prevent anyone from calling
> > > set_page_dirty() inappropriately.
> > > 
> > > > Now for [O1] the write back have to call page_mkclean() to go through
> > > > all reverse mapping of the page and map read only. This means that
> > > > we can count the number of real mapping and see if the mapcount is
> > > > bigger than that. If mapcount is bigger than page is pin and we need
> > > > to use a bounce page to do the writeback.
> > > 
> > > Doesn't work. Generally filesystems have already mapped the page
> > > into bios before they call clear_page_dirty_for_io(), so it's too
> > > late for the filesystem to bounce the page at that point.
> > 
> > Yes, for filesystem it is too late. But the plan we figured back in October
> > was to do the bouncing in the block layer. I.e., mark the bio (or just the
> > particular page) as needing bouncing and then use the existing page
> > bouncing mechanism in the block layer to do the bouncing for us. Ext3 (when
> > it was still a separate fs driver) has been using a mechanism like this to
> > make DIF/DIX work with its metadata.
> 
> Sure, that's a possibility, but that doesn't close off any race
> conditions because there can be DMA into the page in progress while
> the page is being bounced, right? AFAICT this ext3+DIF/DIX case is
> different in that there is no 3rd-party access to the page while it
> is under IO (ext3 arbitrates all access to it's metadata), and so
> nothing can actually race for modification of the page between
> submission and bouncing at the block layer.
> 
> In this case, the moment the page is unlocked, anyone else can map
> it and start (R)DMA on it, and that can happen before the bio is
> bounced by the block layer. So AFAICT, block layer bouncing doesn't
> solve the problem of racing writeback and DMA direct to the page we
> are doing IO on. Yes, it reduces the race window substantially, but
> it doesn't get rid of it.

So the event flow is:
    - userspace create object that match a range of virtual address
      against a given kernel sub-system (let's say infiniband) and
      let's assume that the range is an mmap() of a regular file
    - device driver do GUP on the range (let's assume it is a write
      GUP) so if the page is not already map with write permission
      in the page table than a page fault is trigger and page_mkwrite
      happens
    - Once GUP return the page to the device driver and once the
      device driver as updated the hardware states to allow access
      to this page then from that point on hardware can write to the
      page at _any_ time, it is fully disconnected from any fs event
      like write back, it fully ignore things like page_mkclean

This is how it is to day, we allowed people to push upstream such
users of GUP. This is a fact we have to live with, we can not stop
hardware access to the page, we can not force the hardware to follow
page_mkclean and force a page_mkwrite once write back ends. This is
the situation we are inheriting (and i am personnaly not happy with
that).

From my point of view we are left with 2 choices:
    [C1] break all drivers that do not abide by the page_mkclean and
         page_mkwrite
    [C2] mitigate as much as possible the issue

For [C2] the idea is to keep track of GUP per page so we know if we
can expect the page to be written to at any time. Here is the event
flow:
    - driver GUP the page and program the hardware, page is mark as
      GUPed
    ...
    - write back kicks in on the dirty page, lock the page and every
      thing as usual , sees it is GUPed and inform the block layer to
      use a bounce page
    - block layer copy the page to a bounce page effectively creating
      a snapshot of what is the content of the real page. This allows
      everything in block layer that need stable content to work on
      the bounce page (raid, stripping, encryption, ...)
    - once write back is done the page is not marked clean but stays
      dirty, this effectively disable things like COW for filesystem
      and other feature that expect page_mkwrite between write back.
      AFAIK it is believe that it is something acceptable

The whole write back sequence will repeat until all GUP users calls
put_user_page, once the page is no longer pin by any GUP then things
resume back to the normal flow ie next write back will mark the page
clean and it will force a page_mkwrite on next write fault.



> 
> /me points to wait_for_stable_page() in ->page_mkwrite as the
> mechanism we already have to avoid races between dirtying mapped
> pages and page writeback....

Saddly some devices can not abide by that rules. They over interpreted
what GUP means and what are its guarantee.


> > > > For [O2] i believe we can handle that case in the put_user_page()
> > > > function to properly dirty the page without causing filesystem
> > > > freak out.
> > > 
> > > I'm pretty sure you can't call ->page_mkwrite() from
> > > put_user_page(), so I don't think this is workable at all.
> > 
> > Yes, calling ->page_mkwrite() in put_user_page() is not only technically
> > complicated but also too late - DMA has already modified page contents.
> > What we planned to do (again discussed back in October) was to never allow
> > the pinned page to become clean. I.e., clear_page_dirty_for_io() would
> > leave pinned pages dirty. Also we would skip pinned pages for WB_SYNC_NONE
> > writeback as there's no point in that really. That way MM and filesystems
> > would be aware of the real page state - i.e., what's in memory is not in
> > sync (potentially) with what's on disk. I was thinking whether this
> > permanently-dirty state couldn't confuse filesystem in some way but I
> > didn't find anything serious - the worst I could think of are places that
> > do filemap_write_and_wait() and then invalidate page cache e.g. before hole
> > punching or extent shifting.
> 
> If it's permanently dirty, how do we trigger new COW operations
> after writeback has "cleaned" the page? i.e. we still need a
> ->page_mkwrite call to run before we allow the next write to the
> page to be done, regardless of whether the page is "permanently
> dirty" or not....

For as long as they are GUP reference on the page we are effectively
in some way disabling page cleaning ie we still write back content so
that data loss is still unlikely but pages stays dirty and it disables
anything that rely on page_mkwrite including COW.

From fs point of view it is as if page is frozen in being dirty and
under write for undefined period of time. We only mitigate the data
loss by allowing write to happen using a bounce page (so that layer
down the stack get a page with stable content ie a snapshot).

In other word we can not block the device from writing to the page,
some hardware are just not capable of that and saddly we allowed GUP
to be use for those.


> > But these should work fine as is (page cache
> > invalidation will just happily truncate dirty pages). DIO might get
> > confused by the inability to invalidate dirty pages but then user combining
> > RDMA with DIO on the same file at one moment gets what he deserves...
> 
> I'm almost certain this will do something that will occur. i.e.
> permanently mapped RDMA file, filesystem backup program uses DIO....

DIO being software i believe it can be told to understand this special
case.

Cheers,
Jérôme
