Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 09 Dec 2018 18:54:39 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga008.jf.intel.com (orsmga008.jf.intel.com [10.7.209.65])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 262B558052C;
	Fri,  7 Dec 2018 18:24:59 -0800 (PST)
Received: from fmsmga105.fm.intel.com ([10.1.193.10])
  by orsmga008-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 07 Dec 2018 18:24:58 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AczANThzdGCToLtTXCy+O+j09IxM/srCxBDY+r6Qd?=
 =?us-ascii?q?0e4TIvad9pjvdHbS+e9qxAeQG9mDu7Qc06L/iOPJYSQ4+5GPsXQPItRndiQuro?=
 =?us-ascii?q?EopTEmG9OPEkbhLfTnPGQQFcVGU0J5rTngaRAGUMnxaEfPrXKs8DUcBgvwNRZv?=
 =?us-ascii?q?JuTyB4Xek9m72/q99pHPYAhEniaxba9vJxiqsAvdsdUbj5F/Iagr0BvJpXVIe+?=
 =?us-ascii?q?VSxWx2IF+Yggjx6MSt8pN96ipco/0u+dJOXqX8ZKQ4UKdXDC86PGAv5c3krgfM?=
 =?us-ascii?q?QA2S7XYBSGoWkx5IAw/Y7BHmW5r6ryX3uvZh1CScIMb7Vq4/Vyi84Kh3SR/okC?=
 =?us-ascii?q?YHOCA/8GHLkcx7kaZXrAu8qxBj34LYZYeYP+d8cKzAZ9MXXWRPUMZPWSJcAY28?=
 =?us-ascii?q?YYQAAPYcMuhXrYbyqUAOrQO8CAS3GOPiySVFimPq0aAgzugsFxzN0gw6H9IJtX?=
 =?us-ascii?q?TZtNH7O7kIUeCyyanH0yjIYfJS2Tf884jIaQ4uquyLULJyfsrRzUgvFxjejlqO?=
 =?us-ascii?q?soHlJS2a2fkNs2eB8+psT/6gi2kiqwxopDWk28QiipHRi44L1lzJ8T91zJs7KN?=
 =?us-ascii?q?GmUkJ3fN2pHIdKuyybNYZ6Wt0uT31stSog17ELt4C3cDIXxJkkyRPTceKLfouO?=
 =?us-ascii?q?7xn+TuieOy14i2hgeL+nhxa970ygyurkW8mq31ZFsDBFnsPPtn8TzRzT7NaISv?=
 =?us-ascii?q?9n8kemwzaP2Bjf6uBCIU8qiarWM4AtzqI0m5YJrEjOEDH6lF/rgKKVakko4Oml?=
 =?us-ascii?q?5ub/brXjvJCcNot0ig/kMqQpn8yyGeA4MgkIX2iG9uWwzb7j8lPjQLVMkPI2lr?=
 =?us-ascii?q?DVsJfUJMQduKG5GRRY0pgs6xmhFTeqytcYkmcdLFJDZh2Hi5LlO1bUIPD3Ffu/?=
 =?us-ascii?q?mUijkC93x/DaOb3sGpHNLnnAkLj/Z7p85FNcxRE3zdBe4ZJUF74ALOjyWk/3qN?=
 =?us-ascii?q?zXEBs5PxaozObgDdV3zpkeVn6XAq+FLKPStkeF5uI1LOmNeI8aojH9J+Il5/7z?=
 =?us-ascii?q?l3A5n1AdcLKt3ZsWbnC4A/tnL1+YYXrqntcOD2MKshAiQ+ztjV2ISSRTaGqqX6?=
 =?us-ascii?q?Ig+jE7D5qrDYXERo+zmrCB3yC7HptQZmBBEV2MFXbod4OZW/YDci6SI8lhkiAa?=
 =?us-ascii?q?WrilUYMuyRautAriwbp9MuXU4jEYtY7k1NVt/eLTjhEy9Tt3D8iHyWGCVWN0k3?=
 =?us-ascii?q?gMRz832qB/vEN8xk2C0ah+n/xXC9hT6+lVXQc9MJ7W1/Z6BMzqWgLdYteJT06r?=
 =?us-ascii?q?Qta8DjE3VN4xx94ObFx7G9WtlR3D2yuqA7kIl72EHpA086Tc32TvKMZ50XrJyK?=
 =?us-ascii?q?4hj1w+SMtVKWKmnrJ/9xTUB4PRjkqWjbiqeroG0C7N7miDy3GOs19eUAJ3VaXF?=
 =?us-ascii?q?XnUfZk/NoNT950PCSaKuCLs9PgtAz86CNrVFatnzgVpaQ/fjPczUY3itlGeoGR?=
 =?us-ascii?q?aI2rSMYZL3dGoHwiXSFlIIkwAJ8naALggxGCGhrnnaDDxvE1Lvfkzt/fN/qHO9?=
 =?us-ascii?q?Uk870QWKY1d92Lqy/x4fneacRO8L3rIYpCchrC15HEq839LTDNqAuwphfaVGbd?=
 =?us-ascii?q?Mh+ltH0njZtwh8PpymIKBvnVoecwVxv0Pz2BR7EIRAkc42rHw0yAp+M76X0FRE?=
 =?us-ascii?q?dzmAx5D/JqXXKnXu/BCoc6PZwFXe38iZ+6gR6PU0sU7svBy0GUU49XVn0N5V02?=
 =?us-ascii?q?WH65XODQoSV4/xU0kt+xh7obHafjcy54fO2XJwNqm0tyfI28g1C+s91hagY9Bf?=
 =?us-ascii?q?PbuEFQ/vCcEVG9KiKe0qm1ezaBIEM/tf9Ko1P8OgavuH17SnPOdmnDK6k2tH5J?=
 =?us-ascii?q?px3V6L9yp5UuTIxYoKw+mE3gubUDfxlE2hssHrlo9efzEdA22/xTLiBIFPfK1y?=
 =?us-ascii?q?fJ8HBnu0LM2z29pxmYTtW3le9FO4A1MG2cmpeQedblDn3A1Q01gXrmKjmSei0z?=
 =?us-ascii?q?N0lDQppLKF3CPS2+TiaAYHOmlTSWhijFfgO4i1g8oBXEi1aQgkjx+l5Uf8x6hG?=
 =?us-ascii?q?q6VzNWjTQUFUfyfoK2FuSLe/tr2HY8RX8pMnrT1XUPigYVCdUrP9oQEV0zngH2?=
 =?us-ascii?q?tdwzA3bSqqtY/6nxx5iWKdKmh8rHzCdMF0xBff4sHcRPFL0joHQil4lSfYBlym?=
 =?us-ascii?q?M9a1+tWUko/JsvqiWGK5Sp1TbS7rwJuAtSSh4m1mGx+/n/G1mtD8FQg60Cn718?=
 =?us-ascii?q?RlVCnSrRb8ZJXr2Lq+Me59YkZoA1r84dJgGo5iioswmI0Q2X8Ci5WW53UHkH3/?=
 =?us-ascii?q?MdVG2a3kanoNSiUGw9rU4AjjxU1iIWiFx4P/VnWB3MRhY8O2bX8R2iI498pKEr?=
 =?us-ascii?q?ub7KRYnStppVq1tQfRYfl+njgH0/cv5mAVg/oVuAUz1CWSGa4dHVNXPSH3kxSI?=
 =?us-ascii?q?7ta+rLhYZWq1cLiw0lZ+ks6lDL2Yvg5cX3P5cI84HSBs9sV/LE7M0Hrr54H4f9?=
 =?us-ascii?q?nQaMgftxyOnBfGkuhVM4kxlvsRiCpjOGL9u2AlyuEhgRxv25G6oJaIK2F38K2l?=
 =?us-ascii?q?BR5YMyX/Z9kP9TH1kaZegsGW0pi0EZp7HTULWIboQeisEDIPrvnnMweOEDshqn?=
 =?us-ascii?q?aUA7bfHAmf6Ft4oHLLCZykK3aXJHwBx9V4WBadPFBfgBwTXDginJ42DAWqy9L6?=
 =?us-ascii?q?cEtj+jAd/F34qgZPyuJ1MRnwSHzfqRysajc1TpifMRVX4htD50fTLcyR8OZzEz?=
 =?us-ascii?q?tE8Z2mqQyHMnabaBhQDWEVRkyEAEjuP7mp5dnd6uiYG/CxL/3UbbWVruxeUfiI?=
 =?us-ascii?q?yImr0otn+TaMK8qOMmNjD/09xkpMQ3R5F97FlDUITiwdjzjNYNKDpBeg5i13qd?=
 =?us-ascii?q?iy8PT1VwLu5ouPCLpSPc9s+xCshqeDOPCfhDxkKTZDzZ4MwX7IyL4C3F8dkS1u?=
 =?us-ascii?q?dj+tEageui7JVq7fhqhXDxsDYSNpKMRI97483hVKOcPDkNz1y6V3juQrBFZFT1?=
 =?us-ascii?q?DhnsCpaNcOI2G8MlPHGUmKOK6HJT3N38H4f6e8RadMg+VTsh26oSybHFP7PjSf?=
 =?us-ascii?q?iznpUAiiMf1NjCGeJhBRpJuxfQptCWf9StLrcRm7MN5xjT0rzrw4nHLKNWgAMT?=
 =?us-ascii?q?did0NBtKGf7SRdgv9nAWxO8mJlLfWYmyae9+TZKowZsf1uAiR1keJV+HU7y7tP?=
 =?us-ascii?q?4yFCS/x4gy/Srt9oo1G7neiD0DtnUBxSqjlVgIKHp1ltOaLc9soIZXGR3hUN6y?=
 =?us-ascii?q?2wFh4ArsFpQontu71awNHVmIr1NjZO8tuS9swZUZv6MsWCZVgsKx3sFXb7CAoe?=
 =?us-ascii?q?VjPjYWPQg01Gi/y67HCZrpEm7JPrncxdGfdgSFUpG6ZCWQxeF9sYLcIyB2t8nA?=
 =?us-ascii?q?=3D=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AHAACvKgtch0O0hNFkGQEBAQEBAQEBA?=
 =?us-ascii?q?QEBAQcBAQEBAQGBVAEBAQEBAQsBAYFUghaNGY0/FJlEKwGHXSI3Bg0BAwEBAQE?=
 =?us-ascii?q?BAQIBEwEBAQoLCQgpL0IBDgGBZCQBgmIBAgMBAgkbCwFGBgkBAQoOCgklAwwFK?=
 =?us-ascii?q?SAYgxyCAgWpMjOKMYwiF4FAP4ERgxKINoImAokTIDKBQoUQkDAJij+HAQsYgVy?=
 =?us-ascii?q?FFoJxNocfLJh/gVyBdzMaCBsVO4JtgiYXjjshAQGBNgEBHIV9gmUoAoIgAwEB?=
X-IPAS-Result: =?us-ascii?q?A0AHAACvKgtch0O0hNFkGQEBAQEBAQEBAQEBAQcBAQEBAQG?=
 =?us-ascii?q?BVAEBAQEBAQsBAYFUghaNGY0/FJlEKwGHXSI3Bg0BAwEBAQEBAQIBEwEBAQoLC?=
 =?us-ascii?q?QgpL0IBDgGBZCQBgmIBAgMBAgkbCwFGBgkBAQoOCgklAwwFKSAYgxyCAgWpMjO?=
 =?us-ascii?q?KMYwiF4FAP4ERgxKINoImAokTIDKBQoUQkDAJij+HAQsYgVyFFoJxNocfLJh/g?=
 =?us-ascii?q?VyBdzMaCBsVO4JtgiYXjjshAQGBNgEBHIV9gmUoAoIgAwEB?=
X-IronPort-AV: E=Sophos;i="5.56,328,1539673200"; 
   d="scan'208";a="141287021"
X-Amp-Result: UNKNOWN
X-Amp-Original-Verdict: FILE UNKNOWN
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 07 Dec 2018 18:24:56 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726109AbeLHCYw (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Fri, 7 Dec 2018 21:24:52 -0500
Received: from mx1.redhat.com ([209.132.183.28]:42980 "EHLO mx1.redhat.com"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1726065AbeLHCYw (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Fri, 7 Dec 2018 21:24:52 -0500
Received: from smtp.corp.redhat.com (int-mx05.intmail.prod.int.phx2.redhat.com [10.5.11.15])
        (using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
        (No client certificate requested)
        by mx1.redhat.com (Postfix) with ESMTPS id D87B585541;
        Sat,  8 Dec 2018 02:24:50 +0000 (UTC)
Received: from redhat.com (ovpn-125-106.rdu2.redhat.com [10.10.125.106])
        by smtp.corp.redhat.com (Postfix) with ESMTPS id 641B56154A;
        Sat,  8 Dec 2018 02:24:48 +0000 (UTC)
Date: Fri, 7 Dec 2018 21:24:46 -0500
From: Jerome Glisse <jglisse@redhat.com>
To: John Hubbard <jhubbard@nvidia.com>
Cc: Matthew Wilcox <willy@infradead.org>,
        Dan Williams <dan.j.williams@intel.com>,
        John Hubbard <john.hubbard@gmail.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Linux MM <linux-mm@kvack.org>, Jan Kara <jack@suse.cz>,
        tom@talpey.com, Al Viro <viro@zeniv.linux.org.uk>, benve@cisco.com,
        Christoph Hellwig <hch@infradead.org>,
        Christopher Lameter <cl@linux.com>,
        "Dalessandro, Dennis" <dennis.dalessandro@intel.com>,
        Doug Ledford <dledford@redhat.com>,
        Jason Gunthorpe <jgg@ziepe.ca>,
        Michal Hocko <mhocko@kernel.org>, mike.marciniszyn@intel.com,
        rcampbell@nvidia.com,
        Linux Kernel Mailing List <linux-kernel@vger.kernel.org>,
        linux-fsdevel <linux-fsdevel@vger.kernel.org>
Subject: Re: [PATCH 1/2] mm: introduce put_user_page*(), placeholder versions
Message-ID: <20181208022445.GA7024@redhat.com>
References: <CAPcyv4h99JVHAS7Q7k3iPPUq+oc1NxHdyBHMjpgyesF1EjVfWA@mail.gmail.com>
 <a0adcf7c-5592-f003-abc5-a2645eb1d5df@nvidia.com>
 <CAPcyv4iNtamDAY9raab=iXhSZByecedBpnGybjLM+PuDMwq7SQ@mail.gmail.com>
 <3c91d335-921c-4704-d159-2975ff3a5f20@nvidia.com>
 <20181205011519.GV10377@bombadil.infradead.org>
 <20181205014441.GA3045@redhat.com>
 <59ca5c4b-fd5b-1fc6-f891-c7986d91908e@nvidia.com>
 <7b4733be-13d3-c790-ff1b-ac51b505e9a6@nvidia.com>
 <20181207191620.GD3293@redhat.com>
 <3c4d46c0-aced-f96f-1bf3-725d02f11b60@nvidia.com>
MIME-Version: 1.0
Content-Type: text/plain; charset=iso-8859-1
Content-Disposition: inline
Content-Transfer-Encoding: 8bit
In-Reply-To: <3c4d46c0-aced-f96f-1bf3-725d02f11b60@nvidia.com>
User-Agent: Mutt/1.10.1 (2018-07-13)
X-Scanned-By: MIMEDefang 2.79 on 10.5.11.15
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16 (mx1.redhat.com [10.5.110.28]); Sat, 08 Dec 2018 02:24:51 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On Fri, Dec 07, 2018 at 04:52:42PM -0800, John Hubbard wrote:
> On 12/7/18 11:16 AM, Jerome Glisse wrote:
> > On Thu, Dec 06, 2018 at 06:45:49PM -0800, John Hubbard wrote:
> >> On 12/4/18 5:57 PM, John Hubbard wrote:
> >>> On 12/4/18 5:44 PM, Jerome Glisse wrote:
> >>>> On Tue, Dec 04, 2018 at 05:15:19PM -0800, Matthew Wilcox wrote:
> >>>>> On Tue, Dec 04, 2018 at 04:58:01PM -0800, John Hubbard wrote:
> >>>>>> On 12/4/18 3:03 PM, Dan Williams wrote:
> >>>>>>> Except the LRU fields are already in use for ZONE_DEVICE pages... how
> >>>>>>> does this proposal interact with those?
> >>>>>>
> >>>>>> Very badly: page->pgmap and page->hmm_data both get corrupted. Is there an entire
> >>>>>> use case I'm missing: calling get_user_pages() on ZONE_DEVICE pages? Said another
> >>>>>> way: is it reasonable to disallow calling get_user_pages() on ZONE_DEVICE pages?
> >>>>>>
> >>>>>> If we have to support get_user_pages() on ZONE_DEVICE pages, then the whole 
> >>>>>> LRU field approach is unusable.
> >>>>>
> >>>>> We just need to rearrange ZONE_DEVICE pages.  Please excuse the whitespace
> >>>>> damage:
> >>>>>
> >>>>> +++ b/include/linux/mm_types.h
> >>>>> @@ -151,10 +151,12 @@ struct page {
> >>>>>  #endif
> >>>>>                 };
> >>>>>                 struct {        /* ZONE_DEVICE pages */
> >>>>> +                       unsigned long _zd_pad_2;        /* LRU */
> >>>>> +                       unsigned long _zd_pad_3;        /* LRU */
> >>>>> +                       unsigned long _zd_pad_1;        /* uses mapping */
> >>>>>                         /** @pgmap: Points to the hosting device page map. */
> >>>>>                         struct dev_pagemap *pgmap;
> >>>>>                         unsigned long hmm_data;
> >>>>> -                       unsigned long _zd_pad_1;        /* uses mapping */
> >>>>>                 };
> >>>>>  
> >>>>>                 /** @rcu_head: You can use this to free a page by RCU. */
> >>>>>
> >>>>> You don't use page->private or page->index, do you Dan?
> >>>>
> >>>> page->private and page->index are use by HMM DEVICE page.
> >>>>
> >>>
> >>> OK, so for the ZONE_DEVICE + HMM case, that leaves just one field remaining for 
> >>> dma-pinned information. Which might work. To recap, we need:
> >>>
> >>> -- 1 bit for PageDmaPinned
> >>> -- 1 bit, if using LRU field(s), for PageDmaPinnedWasLru.
> >>> -- N bits for a reference count
> >>>
> >>> Those *could* be packed into a single 64-bit field, if really necessary.
> >>>
> >>
> >> ...actually, this needs to work on 32-bit systems, as well. And HMM is using a lot.
> >> However, it is still possible for this to work.
> >>
> >> Matthew, can I have that bit now please? I'm about out of options, and now it will actually
> >> solve the problem here.
> >>
> >> Given:
> >>
> >> 1) It's cheap to know if a page is ZONE_DEVICE, and ZONE_DEVICE means not on the LRU.
> >> That, in turn, means only 1 bit instead of 2 bits (in addition to a counter) is required, 
> >> for that case. 
> >>
> >> 2) There is an independent bit available (according to Matthew). 
> >>
> >> 3) HMM uses 4 of the 5 struct page fields, so only one field is available for a counter 
> >>    in that case.
> > 
> > To expend on this, HMM private page are use for anonymous page
> > so the index and mapping fields have the value you expect for
> > such pages. Down the road i want also to support file backed
> > page with HMM private (mapping, private, index).
> > 
> > For HMM public both anonymous and file back page are supported
> > today (HMM public is only useful on platform with something like
> > OpenCAPI, CCIX or NVlink ... so PowerPC for now).
> > 
> >> 4) get_user_pages() must work on ZONE_DEVICE and HMM pages.
> > 
> > get_user_pages() only need to work with HMM public page not the
> > private one as we can not allow _anyone_ to pin HMM private page.
> > So on get_user_pages() on HMM private we get a page fault and
> > it is migrated back to regular memory.
> > 
> > 
> >> 5) For a proper atomic counter for both 32- and 64-bit, we really do need a complete
> >> unsigned long field.
> >>
> >> So that leads to the following approach:
> >>
> >> -- Use a single unsigned long field for an atomic reference count for the DMA pinned count.
> >> For normal pages, this will be the *second* field of the LRU (in order to avoid PageTail bit).
> >>
> >> For ZONE_DEVICE pages, we can also line up the fields so that the second LRU field is 
> >> available and reserved for this DMA pinned count. Basically _zd_pad_1 gets move up and
> >> optionally renamed:
> >>
> >> diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
> >> index 017ab82e36ca..b5dcd9398cae 100644
> >> --- a/include/linux/mm_types.h
> >> +++ b/include/linux/mm_types.h
> >> @@ -90,8 +90,8 @@ struct page {
> >>                                  * are in use.
> >>                                  */
> >>                                 struct {
> >> -                                       unsigned long dma_pinned_flags;
> >> -                                       atomic_t      dma_pinned_count;
> >> +                                       unsigned long dma_pinned_flags; /* LRU.next */
> >> +                                       atomic_t      dma_pinned_count; /* LRU.prev */
> >>                                 };
> >>                         };
> >>                         /* See page-flags.h for PAGE_MAPPING_FLAGS */
> >> @@ -161,9 +161,9 @@ struct page {
> >>                 };
> >>                 struct {        /* ZONE_DEVICE pages */
> >>                         /** @pgmap: Points to the hosting device page map. */
> >> -                       struct dev_pagemap *pgmap;
> >> -                       unsigned long hmm_data;
> >> -                       unsigned long _zd_pad_1;        /* uses mapping */
> >> +                       struct dev_pagemap *pgmap;      /* LRU.next */
> >> +                       unsigned long _zd_pad_1;        /* LRU.prev or dma_pinned_count */
> >> +                       unsigned long hmm_data;         /* uses mapping */
> > 
> > This breaks HMM today as hmm_data would alias with mapping field.
> > hmm_data can only be in LRU.prev
> > 
> 
> I see. OK, HMM has done an efficient job of mopping up unused fields, and now we are
> completely out of space. At this point, after thinking about it carefully, it seems clear
> that it's time for a single, new field:
> 
> diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
> index 5ed8f6292a53..1c789e324da8 100644
> --- a/include/linux/mm_types.h
> +++ b/include/linux/mm_types.h
> @@ -182,6 +182,9 @@ struct page {
>         /* Usage count. *DO NOT USE DIRECTLY*. See page_ref.h */
>         atomic_t _refcount;
>  
> +       /* DMA usage count. See get_user_pages*(), put_user_page*(). */
> +       atomic_t _dma_pinned_count;
> +
>  #ifdef CONFIG_MEMCG
>         struct mem_cgroup *mem_cgroup;
>  #endif
> 
> 
> ...because after all, the reason this is so difficult is that this fix has to work
> in pretty much every configuration. get_user_pages() use is widespread, it's a very
> general facility, and...it needs fixing.  And we're out of space. 
> 
> I'm going to send out an updated RFC that shows the latest, and I think it's going
> to include the above.

Another crazy idea, why not treating GUP as another mapping of the page
and caller of GUP would have to provide either a fake anon_vma struct or
a fake vma struct (or both for PRIVATE mapping of a file where you can
have a mix of both private and file page thus only if it is a read only
GUP) that would get added to the list of existing mapping.

So the flow would be:
    somefunction_thatuse_gup()
    {
        ...
        GUP(_fast)(vma, ..., fake_anon, fake_vma);
        ...
    }

    GUP(vma, ..., fake_anon, fake_vma)
    {
        if (vma->flags == ANON) {
            // Add the fake anon vma to the anon vma chain as a child
            // of current vma
        } else {
            // Add the fake vma to the mapping tree
        }

        // The existing GUP except that now it inc mapcount and not
        // refcount
        GUP_old(..., &nanonymous, &nfiles);

        atomic_add(&fake_anon->refcount, nanonymous);
        atomic_add(&fake_vma->refcount, nfiles);

        return nanonymous + nfiles;
    }

I believe all call place of GUP could be updated they fall into 2
categories:
    - fake_anon/fake_vma on stack (direct I/O and few other who
      just do GUP inside their work function and drop reference
      their too)
    - fake_anon/fake_vma as part of the object they have ie GUP
      user that have some kind of struct where they keep the result
      of the GUP around (most user in driver directory fall under
      that)


Few nice bonus:
    [B1] GUP_pin <= (mapcount - refcount) ie it gives a boundary for
         number of GUP on the page (some other part of the kernel might
         still temporarily inc the refcount without a mapcount increase)
    [B2] can add a revoke call back as part of the fake anon_vma/
         vma structure (if the existing GUP user can do that or maybe
         something like an emergency revoke when the memory is poisonous)
    [B3] extra cost is once per GUP call not per page so the impact
         on performance should definitly be better
    [B4] no need to modify LRU or complexify the inner of GUP code
         only the pre-ambule.

Few issues with that proposal:
    [I1] need to check mapcount in page free code path to avoid
         freeing the page if refcount reach 0 before all the GUP
         user unmap the page, page is no in some zombie state ie
         refcount = 0 and mapcount > 0
    [I2] KVM seems to use GUP for weird reasons, it might be better
         to convert KVM to use something else than GUP that have the
         same end result from KVM point of view (i believe it uses it
         to force page fault on the host page). Maybe we can work
         with KVM folks and see if we can provide them with the API
         that actualy do what they want instead of them using GUP
         for its side effect
    [I3] ANON page will need special handling as this will confuse
         mm code path that deal with COW pages ... the page is not
         COW but still has mapcount > 1
    [I4] GUP must be per vma (not an issue everywhere) we can provide
         helpers to iterate over virtual address by vma
    [I5] to ascertain that a page is under GUP might be costly code
         would look like:
            bool page_is_guped(struct page *page)
            {
                if (page_mapcount(page) > page_refcount(page)) {
                    return true;
                }
                // Unknown have to walk the reverse mapping to see
                // if they are any fake anon or fake vma and even
                // if there is we could not say for sure if they
                // apply to the page under consideration we would
                // have to assume so unless:
                //
                // GUP user keep around the array they used to store
                // the GUP results then we can check if the page is
                // in there.
            }

Probably other issues i can not think of right now ...


Maybe even better would be to add a pointer to struct address_space
and re-arrange struct anon_vma to move unsigned degree at the top and
define some flag in it (i don't think we want to grow anon_vam struct)
so that we can differentiate fake anon_vma from others by looking at
first word.

Patchset would probably looks like:
    [1-N] Convert all put_page to put_user_page() with an extra void
          pointer as first step (to allow converting using one at a
          time)
    [N-M] convet every GUP user to provide the fake struct (from
          stack or as part of their object that track GUP result)
    [M-O] patches to add all the helpers and changes inside mm to
          handle fake vma and fake anon vma and the implication of
          having mapcount > refcount (not all time)
    [P] convert GUP to add the fake to anon_vma/mapping and convert
        GUP to inc mapcount and not refcount

Note that the GUP user do not necessarily need to keep the fake
anon or vma struct as part of their own struct. It can use a key
system ie:
    put_user_page_with_key(page, key);
    versus
    put_user_page(page, fake_anon/fake_vma);

Then put_user_page would walk the list of mapping of the page
until it finds the fake anon or fake vma that have the matching
key and dec the refcount of that fake struct and free it once
it reaches zero ...

Anyway they are few thing we can do to alievate the pain for the
GUP users.


Maybe this is crazy but this is what i have without needing to add
a field to struct page.

Cheers,
Jérôme
