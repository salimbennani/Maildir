Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by i7-8700 with POP3-SSL;
  13 Dec 2018 08:56:11 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga005.fm.intel.com (fmsmga005.fm.intel.com [10.253.24.32])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id A212858079D;
	Wed, 12 Dec 2018 14:12:07 -0800 (PST)
Received: from fmsmga101.fm.intel.com ([10.1.193.65])
  by fmsmga005-1.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 12 Dec 2018 14:12:07 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AXvBP2hK2GmQQBxhQXdmcpTZWNBhigK39O0sv0rFi?=
 =?us-ascii?q?tYgULvT5rarrMEGX3/hxlliBBdydt6oUzbKO+4nbGkU4qa6bt34DdJEeHzQksu?=
 =?us-ascii?q?4x2zIaPcieFEfgJ+TrZSFpVO5LVVti4m3peRMNQJW2aFLduGC94iAPERvjKwV1?=
 =?us-ascii?q?Ov71GonPhMiryuy+4ZLebxlLiTanfb9+MAi9oBnMuMURnYZsMLs6xAHTontPde?=
 =?us-ascii?q?RWxGdoKkyWkh3h+Mq+/4Nt/jpJtf45+MFOTav1f6IjTbxFFzsmKHw65NfqtRbY?=
 =?us-ascii?q?UwSC4GYXX3gMnRpJBwjF6wz6Xov0vyDnuOdxxDWWMMvrRr0vRz+s87lkRwPpiC?=
 =?us-ascii?q?cfNj427mfXitBrjKlGpB6tvgFzz5LIbI2QMvd1Y6HTcs4ARWdZQ8hfSSJBDIO/?=
 =?us-ascii?q?YYUBAeUOMuRXr4jhqFUBthu+HQuhCfjzyjJKnHL6wbE23uojHAzAwQcuH8gOsH?=
 =?us-ascii?q?PRrNjtMKkSVuC1zK/VxjvBcvNZwizy55LSch88vPqBWrBwccrMyUY0DAzKlE+Q?=
 =?us-ascii?q?ppH+MjOTyOQNsnWU7+t6Wu61l2EnrARxryGpy8wxiYfJnpoYxk7Y+Sh92oo5ON?=
 =?us-ascii?q?O1RFBhbdK5E5ZcqzuWOop0T886Xm1kpDw2xqAYtZO0ZiQG1Y4ryh7HZ/CZboSF?=
 =?us-ascii?q?4wjvWPiPLTp7nn5pZayziwuo/US9xeDxWM+520tQoCVfiNnDrHUN2gTT6seZTv?=
 =?us-ascii?q?t9+V+s2SiA1w/N8OFIO0M0mrTBK54n3LEwkoAfsUPZHi/5gEn2jamWeVs4+uWw?=
 =?us-ascii?q?9ejrfrHrqoWBO4J6lA3yKLkil8+jDegiMwUDXXCX+eGm273i+U35Tq9Kjvozkq?=
 =?us-ascii?q?TBtJDaJMIbprO2AgNM0YYs9Qy/DzG439QchHUHK1xEeBSZgIjzIFzOPv/4Auml?=
 =?us-ascii?q?g1i2kzdk2erGPrv/DZXJNHTDl63hfbll505G1AUz1cxf545TCrwZJPLzW0zxu8?=
 =?us-ascii?q?LCAh42Lgy52OLnCNR71oMDVmODGK6ZMKXOsVCW4uIjOfWDZIgQuDzlMfgq++bu?=
 =?us-ascii?q?jWMlmV8aZaSp3YEYaHGkHvt8JEWVe3rsgsobHmcMsQozV+jqiFyEUT5OaHe+Ra?=
 =?us-ascii?q?M85jcnCI24CYfPXJyigLuE3C2jBJ1ZenhGCkyQEXfvb4iEWfAMZzyILs9ilTwE?=
 =?us-ascii?q?U76hS4g62BGqtQ/6zadnL+XO9i0Zs5LjyMZ65+nJmR4u8jx0CtyX03uRQGFsgm?=
 =?us-ascii?q?MIWzg20bhlrkxmyleD1qt4j+ZCFdNJ5fNESQM6NZ/az+xnBNH+QAPBftGVSFm4?=
 =?us-ascii?q?RtWqGy0+TtU0w9UWeUZyB82ijgzf3yqtG7IVlKaEBJou/qLY3nj+PcB9y3nd2a?=
 =?us-ascii?q?kljlkmRNZPNGK8iq5+8QjTG5DGk0GDm6m2cqQc2TbH9H2fwmqWoEFYTAlwXL3Y?=
 =?us-ascii?q?Un8FeEvZs8715kPYQL+oErQoLA1BxcmGKqtJb93piU5LRPPiONTYfmKwlH28BR?=
 =?us-ascii?q?eOxrOQcoXqf38R0znaCEgBiwoT52qJNRAiBie9pGLTFCFhGkjxY0zy6+V+qGm0?=
 =?us-ascii?q?Tkkvzw6UaU1szL61+h8ThfyBRPIfxLMEuCE9qzpqGFaxxc7ZC92FpwB5ZqVTfc?=
 =?us-ascii?q?s94Etb1WLerwF9Ip2gL6Vlhl4CcwR2v1ni1wltBoVHi8gqqHIqzAx9Ka+D1FNB?=
 =?us-ascii?q?djWY3Y3/O7HNK2ny+gyvZLDS2l3EzNmW/aIP4uwiq1r/pAGpClYi83J/3tZP13?=
 =?us-ascii?q?uT+JrLAxQSUJ7rSUk36gV1qKvcYik+4IPUy2ZhMa21sj/ExtIoC/Epyhemf9dD?=
 =?us-ascii?q?LqyEEBX+HNEdB8irMOYqgUSmbgoYPOBO8645J8Ond+ac1KG3JuphnTKmgn5B4I?=
 =?us-ascii?q?ByyU+M8yt8SujV35cK2f2Y3w2HVyvig1elqMz4hYdEZTQKFGql1SfkHJJRZrF1?=
 =?us-ascii?q?fYsTE2euItG4yc9kh5HwXH5U7lijCE0c2M+oYBadc0by3QlN2ksJu3ynni24wi?=
 =?us-ascii?q?dwkzEoqKqfwSPPz/7jdBoBJm5EWm1igU3wLoizitAQRFKoYBQxlBu5+Ub6wLBW?=
 =?us-ascii?q?paR+L2XJQUZEZTP2L3xkUqarsrqCYshP6I4nsClNUeS8Z0yaRaD5oxcAzyzjGG?=
 =?us-ascii?q?5ezighdz62opX5gwB6iGWFIXZxtnXZY8JwxRTY5NDGXv5exDkGRCp5iTnRGFe8?=
 =?us-ascii?q?OcKk/dGVl5fFr+C/WHitVpxVcSn304yAsDG36nFtAR26h/qzgMHoERAm0S/n0N?=
 =?us-ascii?q?lnTSXJowz7YoXx1aS6MORnc1JsBF/97cp6B45/npExhJEWxXgVmJGV8WAbnmf0?=
 =?us-ascii?q?NNVRwbj+Y2YVRT4X397V5xDo2E55IXKI3Y75VmidzdFnZ9m1eW4W3iM978ZXCK?=
 =?us-ascii?q?ab9rBEnC11okamogLVe/RygjAdyf424n4An+4JoBYtzjmaAr0KGElYPC/smA6S?=
 =?us-ascii?q?49GwsqpXf3qvfqaq1EVlg9+hFr6CrxpYWHb4fJciACBx4t9+MFLKzH389IXkdM?=
 =?us-ascii?q?PMYtIUsx2eiw3AgPRNKJItivoKgjJqOX76vX0g0eI6jAZh3ZenvIicMGVt/bm0?=
 =?us-ascii?q?AhpZNj3zesMS9SvhjadYnsaKwY+vGo9tFSkMXJvtVfioCi4dtez7NwaSFz0xsm?=
 =?us-ascii?q?yUFqDYHQ+b9UdqtXbPE42wOnGTJXkZy8hiRRaHKExehgAUQCs1npojGg+2w8zh?=
 =?us-ascii?q?dV9z5ioN6V7gthtM1uVoOgH9UmjFoQendCw0RIKDLBZM7QFC+kTVMdGY7u1pBC?=
 =?us-ascii?q?FV5ZmhrA2LKmyGaAVEF2AJWkqYB1/9Ormi/8XP8++dBuCmNfvBfa2OqfBCV/eP?=
 =?us-ascii?q?3Z+u0oxm/yuVNsWSJHZiCec32lFEXXB/FMTUgDEPSy0RlyLQYM+Xvha8+itrrs?=
 =?us-ascii?q?+h9PTnQh7g5YyKC7FKK9Vg5wi2gbufN+6XnCt5KShX1pIPxX/LybgTxl0Siztp?=
 =?us-ascii?q?dzmiD7sArzPNTLnLl69TDh4bbT5zNcRS46I92AlNJdDUitfv2rFkif41Dk9PVU?=
 =?us-ascii?q?b9lcGxecwKP2a9OUvdC0aWL7uGPyPEwsHtbaO4SL1dl+FUtxy2uTaGHE7vJDWD?=
 =?us-ascii?q?lz/1Vx+xNeFAlj2UPBtbuIulaBZiFXDjTM76ah28KNJ3kTw2wbguinzWL2ITLT?=
 =?us-ascii?q?h8fF1LrrCL6SNYg/N/G3FO73Z/LOmEnTqZ4PfcKpoMrfRrBSF0nfpA4Hsm07tV?=
 =?us-ascii?q?8D1ERPttlSrStNFupVSmkuiJyjV/URtOqixEhISEvUh5PaXZ959AWWvL/R4X7G?=
 =?us-ascii?q?WQDQgKqMVhCtH1p69Qzd3Px+rPL2Jg/sjT+MZUKMzVM9mKeC4jPh7oAyXZJBEI?=
 =?us-ascii?q?QT6iKSfUgEkLw9+I8XjAjJ86rNDMhZAEQ6VXHAg3Fu4QAE1/EPQEPpZ7Xzpimr?=
 =?us-ascii?q?me2p1brUGipQXcEZ0J9qvMUeifVLC2cG6U?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0ClAABihhFch0O0hNFkHAEBAQQBAQcEA?=
 =?us-ascii?q?QGBVAQBAQsBgVqCEScKg3KULIFgCCUUmTIUGBMBhz4iNwYNAQMBAQEBAQECARM?=
 =?us-ascii?q?BAQEIDQkIKSMMgjYkAYJhAQEBAQIBAQIgBBkBATcBBQkBAQoOCgICJgICA0QQB?=
 =?us-ascii?q?g0BBQIBAQEWA4I4S4F6CAWmDXB8M4J2AQEFgkOEZAiBC4oVgRwXgUA/gREnDIF?=
 =?us-ascii?q?hfoRNJ4MRgleJGyCBd4RcN5BJCZFRBhiBXIgdJocnLIoCjxGBXIF4MxoIGxU7g?=
 =?us-ascii?q?myCGwwXg0qKdB8BMYEFAQGKHgIkAoEGgR8BAQ?=
X-IPAS-Result: =?us-ascii?q?A0ClAABihhFch0O0hNFkHAEBAQQBAQcEAQGBVAQBAQsBgVq?=
 =?us-ascii?q?CEScKg3KULIFgCCUUmTIUGBMBhz4iNwYNAQMBAQEBAQECARMBAQEIDQkIKSMMg?=
 =?us-ascii?q?jYkAYJhAQEBAQIBAQIgBBkBATcBBQkBAQoOCgICJgICA0QQBg0BBQIBAQEWA4I?=
 =?us-ascii?q?4S4F6CAWmDXB8M4J2AQEFgkOEZAiBC4oVgRwXgUA/gREnDIFhfoRNJ4MRgleJG?=
 =?us-ascii?q?yCBd4RcN5BJCZFRBhiBXIgdJocnLIoCjxGBXIF4MxoIGxU7gmyCGwwXg0qKdB8?=
 =?us-ascii?q?BMYEFAQGKHgIkAoEGgR8BAQ?=
X-IronPort-AV: E=Sophos;i="5.56,346,1539673200"; 
   d="scan'208";a="66563271"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mga01b.intel.com with ESMTP; 12 Dec 2018 14:12:05 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1728415AbeLLWMD (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Wed, 12 Dec 2018 17:12:03 -0500
Received: from hqemgate14.nvidia.com ([216.228.121.143]:19933 "EHLO
        hqemgate14.nvidia.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726297AbeLLWMC (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Wed, 12 Dec 2018 17:12:02 -0500
Received: from hqpgpgate101.nvidia.com (Not Verified[216.228.121.13]) by hqemgate14.nvidia.com (using TLS: TLSv1.2, DES-CBC3-SHA)
        id <B5c1187ac0001>; Wed, 12 Dec 2018 14:11:56 -0800
Received: from hqmail.nvidia.com ([172.20.161.6])
  by hqpgpgate101.nvidia.com (PGP Universal service);
  Wed, 12 Dec 2018 14:11:59 -0800
X-PGP-Universal: processed;
        by hqpgpgate101.nvidia.com on Wed, 12 Dec 2018 14:11:59 -0800
Received: from [10.110.48.28] (10.124.1.5) by HQMAIL101.nvidia.com
 (172.20.187.10) with Microsoft SMTP Server (TLS) id 15.0.1395.4; Wed, 12 Dec
 2018 22:11:59 +0000
Subject: Re: [PATCH 1/2] mm: introduce put_user_page*(), placeholder versions
To: Jerome Glisse <jglisse@redhat.com>
CC: Dan Williams <dan.j.williams@intel.com>, Jan Kara <jack@suse.cz>,
        Matthew Wilcox <willy@infradead.org>,
        John Hubbard <john.hubbard@gmail.com>,
        Andrew Morton <akpm@linux-foundation.org>,
        Linux MM <linux-mm@kvack.org>, <tom@talpey.com>,
        Al Viro <viro@zeniv.linux.org.uk>, <benve@cisco.com>,
        Christoph Hellwig <hch@infradead.org>,
        Christopher Lameter <cl@linux.com>,
        "Dalessandro, Dennis" <dennis.dalessandro@intel.com>,
        Doug Ledford <dledford@redhat.com>,
        Jason Gunthorpe <jgg@ziepe.ca>,
        Michal Hocko <mhocko@kernel.org>,
        Mike Marciniszyn <mike.marciniszyn@intel.com>,
        <rcampbell@nvidia.com>,
        Linux Kernel Mailing List <linux-kernel@vger.kernel.org>,
        linux-fsdevel <linux-fsdevel@vger.kernel.org>
References: <59ca5c4b-fd5b-1fc6-f891-c7986d91908e@nvidia.com>
 <7b4733be-13d3-c790-ff1b-ac51b505e9a6@nvidia.com>
 <20181207191620.GD3293@redhat.com>
 <3c4d46c0-aced-f96f-1bf3-725d02f11b60@nvidia.com>
 <20181208022445.GA7024@redhat.com> <20181210102846.GC29289@quack2.suse.cz>
 <20181212150319.GA3432@redhat.com>
 <CAPcyv4go0Xzhz8rXdfscWuXDu83BO9v8WD4upDUJWb7gKzX5OQ@mail.gmail.com>
 <20181212213005.GE5037@redhat.com>
 <514cc9e1-dc4d-b979-c6bc-88ac503c098d@nvidia.com>
 <20181212220418.GH5037@redhat.com>
From: John Hubbard <jhubbard@nvidia.com>
X-Nvconfidentiality: public
Message-ID: <311cd7a7-6727-a298-964e-ad238a30bdef@nvidia.com>
Date: Wed, 12 Dec 2018 14:11:58 -0800
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101
 Thunderbird/60.3.3
MIME-Version: 1.0
In-Reply-To: <20181212220418.GH5037@redhat.com>
X-Originating-IP: [10.124.1.5]
X-ClientProxiedBy: HQMAIL101.nvidia.com (172.20.187.10) To
 HQMAIL101.nvidia.com (172.20.187.10)
Content-Type: text/plain; charset="utf-8"
Content-Language: en-US-large
Content-Transfer-Encoding: 7bit
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=nvidia.com; s=n1;
        t=1544652716; bh=z/x2a1S2T3wxysT+GCOVRIUfD8bty9W63HKs3f6kza0=;
        h=X-PGP-Universal:Subject:To:CC:References:From:X-Nvconfidentiality:
         Message-ID:Date:User-Agent:MIME-Version:In-Reply-To:
         X-Originating-IP:X-ClientProxiedBy:Content-Type:Content-Language:
         Content-Transfer-Encoding;
        b=FkYNxkjyeTT898FapeUyHufVgNbxso+yRRMlmZLdfchtFPbfyOwU8TNVEwqaWbbCY
         030V002NyRXyj7TCojOCPFWOGEJOKxKJcbyrsc3wmI0VrawV3bitqlK8Ar16OeKbVS
         itTBZjggMM0mKreOkkTLTFIvJYew3HgE+2RqL/St8Miaa/guvBuXrsV15z/+nRzTWc
         64LGlRJA7A+dPrjBWzgbePLKP+FpyMflMXoAvQOosqVsnwtz63eUx9K2EBf4eIgH9s
         8CL6/T4tcYllhkLBnzcY3CJ0ejlBwBnJOaXeqloWcOOWenX490wQUVXFAO7jrgKmmq
         NYvp5aTqXb80g==
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On 12/12/18 2:04 PM, Jerome Glisse wrote:
> On Wed, Dec 12, 2018 at 01:56:00PM -0800, John Hubbard wrote:
>> On 12/12/18 1:30 PM, Jerome Glisse wrote:
>>> On Wed, Dec 12, 2018 at 08:27:35AM -0800, Dan Williams wrote:
>>>> On Wed, Dec 12, 2018 at 7:03 AM Jerome Glisse <jglisse@redhat.com> wrote:
>>>>>
>>>>> On Mon, Dec 10, 2018 at 11:28:46AM +0100, Jan Kara wrote:
>>>>>> On Fri 07-12-18 21:24:46, Jerome Glisse wrote:
>>>>>>> Another crazy idea, why not treating GUP as another mapping of the page
>>>>>>> and caller of GUP would have to provide either a fake anon_vma struct or
>>>>>>> a fake vma struct (or both for PRIVATE mapping of a file where you can
>>>>>>> have a mix of both private and file page thus only if it is a read only
>>>>>>> GUP) that would get added to the list of existing mapping.
>>>>>>>
>>>>>>> So the flow would be:
>>>>>>>     somefunction_thatuse_gup()
>>>>>>>     {
>>>>>>>         ...
>>>>>>>         GUP(_fast)(vma, ..., fake_anon, fake_vma);
>>>>>>>         ...
>>>>>>>     }
>>>>>>>
>>>>>>>     GUP(vma, ..., fake_anon, fake_vma)
>>>>>>>     {
>>>>>>>         if (vma->flags == ANON) {
>>>>>>>             // Add the fake anon vma to the anon vma chain as a child
>>>>>>>             // of current vma
>>>>>>>         } else {
>>>>>>>             // Add the fake vma to the mapping tree
>>>>>>>         }
>>>>>>>
>>>>>>>         // The existing GUP except that now it inc mapcount and not
>>>>>>>         // refcount
>>>>>>>         GUP_old(..., &nanonymous, &nfiles);
>>>>>>>
>>>>>>>         atomic_add(&fake_anon->refcount, nanonymous);
>>>>>>>         atomic_add(&fake_vma->refcount, nfiles);
>>>>>>>
>>>>>>>         return nanonymous + nfiles;
>>>>>>>     }
>>>>>>
>>>>>> Thanks for your idea! This is actually something like I was suggesting back
>>>>>> at LSF/MM in Deer Valley. There were two downsides to this I remember
>>>>>> people pointing out:
>>>>>>
>>>>>> 1) This cannot really work with __get_user_pages_fast(). You're not allowed
>>>>>> to get necessary locks to insert new entry into the VMA tree in that
>>>>>> context. So essentially we'd loose get_user_pages_fast() functionality.
>>>>>>
>>>>>> 2) The overhead e.g. for direct IO may be noticeable. You need to allocate
>>>>>> the fake tracking VMA, get VMA interval tree lock, insert into the tree.
>>>>>> Then on IO completion you need to queue work to unpin the pages again as you
>>>>>> cannot remove the fake VMA directly from interrupt context where the IO is
>>>>>> completed.
>>>>>>
>>>>>> You are right that the cost could be amortized if gup() is called for
>>>>>> multiple consecutive pages however for small IOs there's no help...
>>>>>>
>>>>>> So this approach doesn't look like a win to me over using counter in struct
>>>>>> page and I'd rather try looking into squeezing HMM public page usage of
>>>>>> struct page so that we can fit that gup counter there as well. I know that
>>>>>> it may be easier said than done...
>>>>>
>>>>> So i want back to the drawing board and first i would like to ascertain
>>>>> that we all agree on what the objectives are:
>>>>>
>>>>>     [O1] Avoid write back from a page still being written by either a
>>>>>          device or some direct I/O or any other existing user of GUP.
>>>>>          This would avoid possible file system corruption.
>>>>>
>>>>>     [O2] Avoid crash when set_page_dirty() is call on a page that is
>>>>>          considered clean by core mm (buffer head have been remove and
>>>>>          with some file system this turns into an ugly mess).
>>>>>
>>>>>     [O3] DAX and the device block problems, ie with DAX the page map in
>>>>>          userspace is the same as the block (persistent memory) and no
>>>>>          filesystem nor block device understand page as block or pinned
>>>>>          block.
>>>>>
>>>>> For [O3] i don't think any pin count would help in anyway. I believe
>>>>> that the current long term GUP API that does not allow GUP of DAX is
>>>>> the only sane solution for now.
>>>>
>>>> No, that's not a sane solution, it's an emergency hack.
>>>>
>>>>> The real fix would be to teach file-
>>>>> system about DAX/pinned block so that a pinned block is not reuse
>>>>> by filesystem.
>>>>
>>>> We already have taught filesystems about pinned dax pages, see
>>>> dax_layout_busy_page(). As much as possible I want to eliminate the
>>>> concept of "dax pages" as a special case that gets sprinkled
>>>> throughout the mm.
>>>
>>> So thinking on O3 issues what about leveraging the recent change i
>>> did to mmu notifier. Add a event for truncate or any other file
>>> event that need to invalidate the file->page for a range of offset.
>>>
>>> Add mmu notifier listener to GUP user (except direct I/O) so that
>>> they invalidate they hardware mapping or switch the hardware mapping
>>> to use a crappy page. When such event happens what ever user do to
>>> the page through that driver is broken anyway. So it is better to
>>> be loud about it then trying to make it pass under the radar.
>>>
>>> This will put the burden on broken user and allow you to properly
>>> recycle your DAX page.
>>>
>>> Think of it as revoke through mmu notifier.
>>>
>>> So patchset would be:
>>>     enum mmu_notifier_event {
>>> +       MMU_NOTIFY_TRUNCATE,
>>>     };
>>>
>>> +   Change truncate code path to emit MMU_NOTIFY_TRUNCATE
>>>
>>
>> That part looks good.
>>
>>> Then for each user of GUP (except direct I/O or other very short
>>> term GUP):
>>
>> but, why is there a difference between how we handle long- and
>> short-term callers? Aren't we just leaving a harder-to-reproduce race
>> condition, if we ignore the short-term gup callers?
>>
>> So, how does activity (including direct IO and other short-term callers)
>> get quiesced (stopped, and guaranteed not to restart or continue), so 
>> that truncate or umount can continue on?
> 
> The fs would delay block reuse to after refcount is gone so it would
> wait for that. It is ok to do that only for short term user in case of
> direct I/O this should really not happen as it means that the application
> is doing something really stupid. So the waiting on short term user
> would be a rare event.

OK, I think that sounds like there are no race conditions left.

> 
> 
>>>     Patch 1: register mmu notifier
>>>     Patch 2: listen to MMU_NOTIFY_TRUNCATE and MMU_NOTIFY_UNMAP
>>>              when that happens update the device page table or
>>>              usage to point to a crappy page and do put_user_page
>>>              on all previously held page
>>
>> Minor point, this sequence should be done within a wrapper around existing 
>> get_user_pages(), such as get_user_pages_revokable() or something.
> 
> No we want to teach everyone to abide by the rules, if we add yet another
> GUP function prototype people will use the one where they don;t have to
> say they abide by the rules. It is time we advertise the fact that GUP
> should not be use willy nilly for anything without worrying about the
> implication it has :)

Well, the best way to do that is to provide a named function call that 
implements the rules. That also makes it easy to grep around and see which
call sites still need upgrades, and which don't.

> 
> So i would rather see a consolidation in the number of GUP prototype we
> have than yet another one.

We could eventually get rid of the older GUP prototypes, once we're done
converting. Having a new, named function call will *without question* make
the call site conversion go much easier, and the end result is also better:
the common code is in a central function, rather than being at all the call
sites.

thanks,
-- 
John Hubbard
NVIDIA
