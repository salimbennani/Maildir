Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 05 Dec 2018 14:28:14 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga006.jf.intel.com (orsmga006.jf.intel.com [10.7.209.51])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 1500D580375;
	Tue,  4 Dec 2018 17:41:09 -0800 (PST)
Received: from orsmga102-1.jf.intel.com (HELO mga09.intel.com) ([10.7.208.27])
  by orsmga006-1.jf.intel.com with ESMTP; 04 Dec 2018 17:41:09 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3ACn2XvhbEaMOODFQrnAsCfpn/LSx+4OfEezUN459i?=
 =?us-ascii?q?sYplN5qZpcm8Zh7h7PlgxGXEQZ/co6odzbaO4+a4ASQp2tWoiDg6aptCVhsI24?=
 =?us-ascii?q?09vjcLJ4q7M3D9N+PgdCcgHc5PBxdP9nC/NlVJSo6lPwWB6nK94iQPFRrhKAF7?=
 =?us-ascii?q?Ovr6GpLIj8Swyuu+54Dfbx9HiTahYr5+Ngm6oRnMvcQKnIVuLbo8xAHUqXVSYe?=
 =?us-ascii?q?RWwm1oJVOXnxni48q74YBu/SdNtf8/7sBMSar1cbg2QrxeFzQmLns65Nb3uhnZ?=
 =?us-ascii?q?TAuA/WUTX2MLmRdVGQfF7RX6XpDssivms+d2xSeXMdHqQb0yRD+v9LlgRgP2hy?=
 =?us-ascii?q?gbNj456GDXhdJ2jKJHuxKquhhzz5fJbI2JKPZye6XQds4YS2VcRMZcTzFPDJ2y?=
 =?us-ascii?q?b4UPDOQPM+hXoIb/qFsPsRSwChKsBPvtxzJTmn/737c33/g9HQzI3gEtGc8Fvn?=
 =?us-ascii?q?TOrNXyMacfSeS7zK7IzTXFcvhY2yr955PTfR87u/GDQK97fM3TyUkvCgPKlU6f?=
 =?us-ascii?q?ppb/PzyIzekNtXab7+t9WuKukWErsR1+oj+qxso1jITCm4wbylfB9SpjwYY1I8?=
 =?us-ascii?q?W1SE1lbt6+FptfqSWaO5FxQsM4TGFkoCI6y7MAuZKheigF1ognyhjCYPKEa4iF?=
 =?us-ascii?q?+gzvWPqVLDtimX5odqyziwyv/UWj1uHwTMi53VRSoiZYjtXArG0B2wLc58SdV/?=
 =?us-ascii?q?dx5Ees1S6R2wzO6+xIO144mbTYJpI737I8iIcfvV7eEiL5lkj7irKdeF8+9eiy?=
 =?us-ascii?q?8evnZ63rpp+COI9wjQHzKroumsOhDuQiKAQOXHaU+f661LL9+U31WrJKjuc5kq?=
 =?us-ascii?q?XBsZDaI9oUprKhDgNLzoou7wyzAyqo3dgGh3ULMVFIdA6dg4XoOVzCOPX4Au2+?=
 =?us-ascii?q?g1Sonjdr3ffGPrj5D5XJL3jDlqrhfLlk505f1gUz19Zf6IxQCr0YJ/LyVEnxu8?=
 =?us-ascii?q?LCDhIiLQy0zPjoCM9n2oMdR22PGKmZP73WsVOS4eIvOeaMaJcPuDnhM/gl++Lu?=
 =?us-ascii?q?jXghlF8ZfKmp3oUYZGq3H/R7OEiZZXvsgtEcEWYFpAY+TerqiEGcXj5XfXq9Q6?=
 =?us-ascii?q?U85jQjAoK8EYjDXpytgKCG3CqjHp1ZfGFGCkyWHnfvbYmEW+oMZziUIs9uiTEE?=
 =?us-ascii?q?UbmhS4k81RChrgP6yrxnLvbK9S0cr57syN915+jLnxEo6TN0F9id032KT2xsnG?=
 =?us-ascii?q?MIQCE50Lp8oUx6zFeD1694judcFdxS4fNJTwg7OYTdz+x8F9D9RAbBcs2VR1ah?=
 =?us-ascii?q?R9WsGSsxQc4pw98Sf0Z9HM2vgQrY3yqqBL8VlKaHBIYu/aLexHXxI8d9y3Db1K?=
 =?us-ascii?q?gulVUmQ81PNXG4ia577QTcG4nJk0CBnaawaascxDLN9HuEzWeWv0FXSghwXb/B?=
 =?us-ascii?q?XXAefETWq9v561jGT7+vD7QnLwRAxdSDKqtMdt3mk1FGSO3/N9TZZmK7g32wCg?=
 =?us-ascii?q?qQxrOQcIrqfH0Q0zjHB0gajQ8f53aGOhI4Bie6vW3eCjtiFVbsY0Pp9elzs3e7?=
 =?us-ascii?q?Tk4yzwGXYExtzbu1+hgJhfOCT/MfxK4LuCAkqz9sBlayw8rWC8acpwpmZKhTfM?=
 =?us-ascii?q?ky4ElZ2m7DtwxyJJqgL7t8iV4YaAl3u0Lu1xNqCoRPi8QqrXUqzBZsJqKcyl9O?=
 =?us-ascii?q?azSY3ZXoML3NNmby5Ayva7LR2lzGzNaW+6IP6PMkq1XjvAClDFYi/29g09lP13?=
 =?us-ascii?q?uc55PKDAUJXJL1U0Y38QV6pr7Abik84YPUyWNjMa2uvjDe3NIpAfMvygy8cNdH?=
 =?us-ascii?q?LKOECAjyHtUACMiqNuMrlEambxIeMO9J6a47Itmpd/2F2K6sIuZthzamjWVB4I?=
 =?us-ascii?q?Bg3UOA7St8SujU35kbx/GUxBeIVzD5jF25qMD4hZhEZS0OHmq40SXlBJNRardu?=
 =?us-ascii?q?cooRCGauI9e4xtNxh5P2X35Y9VijB04J2cOzeBqSaUD90hNU1UgNvXOnni64xS?=
 =?us-ascii?q?Rukz41tqqfwDDOw+P6eRodO25EWHNtjU3xLoizldwaWlanbxIolBe8+Un13axb?=
 =?us-ascii?q?q7lhIGnJRkdIeTP7L2VjUqu2q7qDbNRD6JIusSVLTuu8ZUqWRaL6oxsfyynjBX?=
 =?us-ascii?q?dRxCgndzG2vZX0hxx7iGOHIHdztnbZYtxwxRHE6dzYRP5R2CcGRSZihTnWAFi8?=
 =?us-ascii?q?I8em/dGOm5jfteC+UnqrVodPfinz0YOAqCy76HVoARKlhfCzmd7nEQ8g3S/gzd?=
 =?us-ascii?q?ZqVibIrBf6Yob10aS3KuZnfkhuBF/h5Mt2AIB+ko0shJ4O3XgWnIma/X0CkW3r?=
 =?us-ascii?q?K9VUxbr+bGYRRT4M29PU4BLq2Ep5InKLxoL5UG6Qws9gZ9m8f2MX1Tgx78FMCK?=
 =?us-ascii?q?eI8rNEmTF5rUa/rQLUefJ9hCsSyeMy6H4GhOEEoAktwT+cArwIHUlYIDbjlxKH?=
 =?us-ascii?q?792lqKVXZWCvcaW/1UZknNChCq2CrR9YWHrjZpgiGipw5N1lMF3QyH3z9p3keN?=
 =?us-ascii?q?7IYNIWtx2UkA3Pg/JbKZ0vjfoKmTRoOXjmsnI70e47ggdj3ZW7vIiBNmVs87i1?=
 =?us-ascii?q?Ah9eNj3pecwT/ivhgrpZnsaTx4qvBIluGi0XXJv0SvKlCC4StfPiNwqUETwzsH?=
 =?us-ascii?q?GbGafEHQ+E6UdrtHbPE5GtN3GKK3gV19RiRB+BJENBhAAYRik1npk8Fgqy3szu?=
 =?us-ascii?q?bF955iwN5l7/shZD0fhnNx77UmfCvguobio0RYObLBpX6AFC+kjUPdaf7uJ1Ay?=
 =?us-ascii?q?FX4JmhoBaRJWycYgRCFXsJVVCcB1D/Irmu4sHN8+qCCeq7MfTOeq+CqfBEW/eL?=
 =?us-ascii?q?2JKv0ZZm/zCWOcWUJXRiC/w72kxeXXF2AcjZmjMPSzAJmCLJdcKUuBC8+ihvpM?=
 =?us-ascii?q?Cl7PvrQB7v5ZeIC7ZKMdRv/A22gb6eN+GKgiZ1My1Y1pQKxX/H07Uf2F8Siydz?=
 =?us-ascii?q?dziiC7gAtCjNTL7Ola9TFRIUdyRzNM5Q5aImwgZNIdLbis/y1rNgjv46FVZFVV?=
 =?us-ascii?q?/imsGof8AKIHu9NFTIBEmVLrSGOCbLztrzYaO9T71QkepVuwexuTadD0/sIDCD?=
 =?us-ascii?q?myP1WBCoNOFGlDubMwBGuIGhbhZtDnDuQ8j7ahKgKt94kz03zacwhnPLLmMcNT?=
 =?us-ascii?q?l8c0VQrryf9y9Yg/N/G3Bf4XphN+WLhyGZ7+zAIJYMrfRrGjh0l/5d4Hki17tV?=
 =?us-ascii?q?7SRESOZplyfIstFuo0+pku+UxzpjURpOrCtLhY2RsUVjP6XZ6odPWXLe8B0R6m?=
 =?us-ascii?q?WQDgwAp8F5Bd33p6BQ1t/PmbrzKThY9dLU+MocB87MJ8OGMHouKxzpGDHPAQsB?=
 =?us-ascii?q?TD6rM3zfhkNHnPGT8H2VsoY1qpz2lJUSTb9bUQ99KvRPL0VmEZQ4KY9wRjQ+jf?=
 =?us-ascii?q?bPltYQ+VK6rR/MVINbtJWBXfWXV7GnfDSejb8CZwYCy7riIawcM4T63wppbVws?=
 =?us-ascii?q?z6rQHE+Ff9dNpGVBcwU+qV9BuCx8TnA50Uv/Ziur/n4fFPfylRkz3FgtKd8x/S?=
 =?us-ascii?q?vhtg9kbmHBozE9xQxowY3o?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0BfAADOKwdch0O0hNFcCBsBAQEBAwEBA?=
 =?us-ascii?q?QcDAQEBgWWCaoECJwqDb5QngWAIJRSIfpAqFBgNBgGHTCI4EgEDAQEBAQEBAgE?=
 =?us-ascii?q?TAQEBCA0JCCkjDII2JAGCYQEBAQEDAQIgHQEBNwEFCQEBCg4HAwICJgICAx4mE?=
 =?us-ascii?q?AYBDAEFAgEBAYMcAYFpAxUFCqQqcIEvgnYBAQWBMAELAQdAP4JEDYIUAwWBC4c?=
 =?us-ascii?q?pgk6BHBeBQD+BEScMgl+CV0cBAwKBOIMpgleJIQ+BcoRYNo9sLgmHA4cQgygGG?=
 =?us-ascii?q?IFbhREFgnqHOyyIWoEDg2WBDIl8gV2BdjMaCBsVgycfgXwMF4NKhFk7hWAfATE?=
 =?us-ascii?q?BgQQBAYoDgR8BAQ?=
X-IPAS-Result: =?us-ascii?q?A0BfAADOKwdch0O0hNFcCBsBAQEBAwEBAQcDAQEBgWWCaoE?=
 =?us-ascii?q?CJwqDb5QngWAIJRSIfpAqFBgNBgGHTCI4EgEDAQEBAQEBAgETAQEBCA0JCCkjD?=
 =?us-ascii?q?II2JAGCYQEBAQEDAQIgHQEBNwEFCQEBCg4HAwICJgICAx4mEAYBDAEFAgEBAYM?=
 =?us-ascii?q?cAYFpAxUFCqQqcIEvgnYBAQWBMAELAQdAP4JEDYIUAwWBC4cpgk6BHBeBQD+BE?=
 =?us-ascii?q?ScMgl+CV0cBAwKBOIMpgleJIQ+BcoRYNo9sLgmHA4cQgygGGIFbhREFgnqHOyy?=
 =?us-ascii?q?IWoEDg2WBDIl8gV2BdjMaCBsVgycfgXwMF4NKhFk7hWAfATEBgQQBAYoDgR8BA?=
 =?us-ascii?q?Q?=
X-IronPort-AV: E=Sophos;i="5.56,316,1539673200"; 
   d="scan'208";a="56122591"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 04 Dec 2018 17:41:06 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726703AbeLEBlE (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Tue, 4 Dec 2018 20:41:04 -0500
Received: from hqemgate14.nvidia.com ([216.228.121.143]:10155 "EHLO
        hqemgate14.nvidia.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1725834AbeLEBlD (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Tue, 4 Dec 2018 20:41:03 -0500
Received: from hqpgpgate101.nvidia.com (Not Verified[216.228.121.13]) by hqemgate14.nvidia.com (using TLS: TLSv1.2, DES-CBC3-SHA)
        id <B5c072cab0000>; Tue, 04 Dec 2018 17:40:59 -0800
Received: from hqmail.nvidia.com ([172.20.161.6])
  by hqpgpgate101.nvidia.com (PGP Universal service);
  Tue, 04 Dec 2018 17:41:00 -0800
X-PGP-Universal: processed;
        by hqpgpgate101.nvidia.com on Tue, 04 Dec 2018 17:41:00 -0800
Received: from [10.110.48.28] (10.124.1.5) by HQMAIL101.nvidia.com
 (172.20.187.10) with Microsoft SMTP Server (TLS) id 15.0.1395.4; Wed, 5 Dec
 2018 01:40:59 +0000
Subject: Re: [PATCH 1/2] mm: introduce put_user_page*(), placeholder versions
To: Mike Rapoport <rppt@linux.ibm.com>, <john.hubbard@gmail.com>
CC: Andrew Morton <akpm@linux-foundation.org>, <linux-mm@kvack.org>,
        Jan Kara <jack@suse.cz>, Tom Talpey <tom@talpey.com>,
        Al Viro <viro@zeniv.linux.org.uk>,
        Christian Benvenuti <benve@cisco.com>,
        Christoph Hellwig <hch@infradead.org>,
        Christopher Lameter <cl@linux.com>,
        Dan Williams <dan.j.williams@intel.com>,
        Dennis Dalessandro <dennis.dalessandro@intel.com>,
        Doug Ledford <dledford@redhat.com>,
        Jason Gunthorpe <jgg@ziepe.ca>,
        Jerome Glisse <jglisse@redhat.com>,
        Matthew Wilcox <willy@infradead.org>,
        Michal Hocko <mhocko@kernel.org>,
        Mike Marciniszyn <mike.marciniszyn@intel.com>,
        Ralph Campbell <rcampbell@nvidia.com>,
        LKML <linux-kernel@vger.kernel.org>,
        <linux-fsdevel@vger.kernel.org>
References: <20181204001720.26138-1-jhubbard@nvidia.com>
 <20181204001720.26138-2-jhubbard@nvidia.com>
 <20181204075323.GI26700@rapoport-lnx>
X-Nvconfidentiality: public
From: John Hubbard <jhubbard@nvidia.com>
Message-ID: <a34aca95-7372-4fd2-b72a-004731fc10ed@nvidia.com>
Date: Tue, 4 Dec 2018 17:40:59 -0800
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101
 Thunderbird/60.3.2
MIME-Version: 1.0
In-Reply-To: <20181204075323.GI26700@rapoport-lnx>
X-Originating-IP: [10.124.1.5]
X-ClientProxiedBy: HQMAIL104.nvidia.com (172.18.146.11) To
 HQMAIL101.nvidia.com (172.20.187.10)
Content-Type: text/plain; charset="utf-8"
Content-Language: en-US-large
Content-Transfer-Encoding: 7bit
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=nvidia.com; s=n1;
        t=1543974059; bh=y6Xwvnu8XjxkVeXMZSuda+QBJrSVAAziiUVKiHLR/ic=;
        h=X-PGP-Universal:Subject:To:CC:References:X-Nvconfidentiality:From:
         Message-ID:Date:User-Agent:MIME-Version:In-Reply-To:
         X-Originating-IP:X-ClientProxiedBy:Content-Type:Content-Language:
         Content-Transfer-Encoding;
        b=GVqVKte8DiuPq7IeLR54HWjnRUS5cCq1yzIn431+XmKQKmCroHplf1TdtkQBsjAMi
         kgh6bGqXjqn/SICa7sdSelqfN0cbZ8Z7OoHkA1Te5wtgKXHnSi7n3GXsfFjB70gZjH
         OtRMADgY01hTgIASu4rZhLXCiX1sP+vwD7hUP470GxrTklAdcEOroJDPbnoIg0NhDp
         Cf/g/PAltYpHkzXO2l3NoWwIYzlnUIQ/SObdswJyPk0buMdCqROKFWdONa7IPlW1ph
         VSi27Npd/VcmMYcBGMPhbsjqaSUXwV/j96bgZ5z8qd2bJiFw/WxU06J0ihFDFExmCP
         63qWeP4JaEtqQ==
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On 12/3/18 11:53 PM, Mike Rapoport wrote:
> Hi John,
> 
> Thanks for having documentation as a part of the patch. Some kernel-doc
> nits below.
> 
> On Mon, Dec 03, 2018 at 04:17:19PM -0800, john.hubbard@gmail.com wrote:
>> From: John Hubbard <jhubbard@nvidia.com>
>>
>> Introduces put_user_page(), which simply calls put_page().
>> This provides a way to update all get_user_pages*() callers,
>> so that they call put_user_page(), instead of put_page().
>>
>> Also introduces put_user_pages(), and a few dirty/locked variations,
>> as a replacement for release_pages(), and also as a replacement
>> for open-coded loops that release multiple pages.
>> These may be used for subsequent performance improvements,
>> via batching of pages to be released.
>>
>> This is the first step of fixing the problem described in [1]. The steps
>> are:
>>
>> 1) (This patch): provide put_user_page*() routines, intended to be used
>>    for releasing pages that were pinned via get_user_pages*().
>>
>> 2) Convert all of the call sites for get_user_pages*(), to
>>    invoke put_user_page*(), instead of put_page(). This involves dozens of
>>    call sites, and will take some time.
>>
>> 3) After (2) is complete, use get_user_pages*() and put_user_page*() to
>>    implement tracking of these pages. This tracking will be separate from
>>    the existing struct page refcounting.
>>
>> 4) Use the tracking and identification of these pages, to implement
>>    special handling (especially in writeback paths) when the pages are
>>    backed by a filesystem. Again, [1] provides details as to why that is
>>    desirable.
>>
>> [1] https://lwn.net/Articles/753027/ : "The Trouble with get_user_pages()"
>>
>> Reviewed-by: Jan Kara <jack@suse.cz>
>>
>> Cc: Matthew Wilcox <willy@infradead.org>
>> Cc: Michal Hocko <mhocko@kernel.org>
>> Cc: Christopher Lameter <cl@linux.com>
>> Cc: Jason Gunthorpe <jgg@ziepe.ca>
>> Cc: Dan Williams <dan.j.williams@intel.com>
>> Cc: Jan Kara <jack@suse.cz>
>> Cc: Al Viro <viro@zeniv.linux.org.uk>
>> Cc: Jerome Glisse <jglisse@redhat.com>
>> Cc: Christoph Hellwig <hch@infradead.org>
>> Cc: Ralph Campbell <rcampbell@nvidia.com>
>> Signed-off-by: John Hubbard <jhubbard@nvidia.com>
>> ---
>>  include/linux/mm.h | 20 ++++++++++++
>>  mm/swap.c          | 80 ++++++++++++++++++++++++++++++++++++++++++++++
>>  2 files changed, 100 insertions(+)
>>
>> diff --git a/include/linux/mm.h b/include/linux/mm.h
>> index 5411de93a363..09fbb2c81aba 100644
>> --- a/include/linux/mm.h
>> +++ b/include/linux/mm.h
>> @@ -963,6 +963,26 @@ static inline void put_page(struct page *page)
>>  		__put_page(page);
>>  }
>>
>> +/*
>> + * put_user_page() - release a page that had previously been acquired via
>> + * a call to one of the get_user_pages*() functions.
> 
> Please add @page parameter description, otherwise kernel-doc is unhappy

Hi Mike,

Sorry I missed these kerneldoc points from your earlier review! I'll fix it
up now and it will show up in the next posting.

> 
>> + *
>> + * Pages that were pinned via get_user_pages*() must be released via
>> + * either put_user_page(), or one of the put_user_pages*() routines
>> + * below. This is so that eventually, pages that are pinned via
>> + * get_user_pages*() can be separately tracked and uniquely handled. In
>> + * particular, interactions with RDMA and filesystems need special
>> + * handling.
>> + */
>> +static inline void put_user_page(struct page *page)
>> +{
>> +	put_page(page);
>> +}
>> +
>> +void put_user_pages_dirty(struct page **pages, unsigned long npages);
>> +void put_user_pages_dirty_lock(struct page **pages, unsigned long npages);
>> +void put_user_pages(struct page **pages, unsigned long npages);
>> +
>>  #if defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)
>>  #define SECTION_IN_PAGE_FLAGS
>>  #endif
>> diff --git a/mm/swap.c b/mm/swap.c
>> index aa483719922e..bb8c32595e5f 100644
>> --- a/mm/swap.c
>> +++ b/mm/swap.c
>> @@ -133,6 +133,86 @@ void put_pages_list(struct list_head *pages)
>>  }
>>  EXPORT_SYMBOL(put_pages_list);
>>
>> +typedef int (*set_dirty_func)(struct page *page);
>> +
>> +static void __put_user_pages_dirty(struct page **pages,
>> +				   unsigned long npages,
>> +				   set_dirty_func sdf)
>> +{
>> +	unsigned long index;
>> +
>> +	for (index = 0; index < npages; index++) {
>> +		struct page *page = compound_head(pages[index]);
>> +
>> +		if (!PageDirty(page))
>> +			sdf(page);
>> +
>> +		put_user_page(page);
>> +	}
>> +}
>> +
>> +/*
>> + * put_user_pages_dirty() - for each page in the @pages array, make
>> + * that page (or its head page, if a compound page) dirty, if it was
>> + * previously listed as clean. Then, release the page using
>> + * put_user_page().
>> + *
>> + * Please see the put_user_page() documentation for details.
>> + *
>> + * set_page_dirty(), which does not lock the page, is used here.
>> + * Therefore, it is the caller's responsibility to ensure that this is
>> + * safe. If not, then put_user_pages_dirty_lock() should be called instead.
>> + *
>> + * @pages:  array of pages to be marked dirty and released.
>> + * @npages: number of pages in the @pages array.
> 
> Please put the parameters description next to the brief function
> description, as described in [1]
> 
> [1] https://www.kernel.org/doc/html/latest/doc-guide/kernel-doc.html#function-documentation
> 

OK. 

> 
>> + *
>> + */
>> +void put_user_pages_dirty(struct page **pages, unsigned long npages)
>> +{
>> +	__put_user_pages_dirty(pages, npages, set_page_dirty);
>> +}
>> +EXPORT_SYMBOL(put_user_pages_dirty);
>> +
>> +/*
>> + * put_user_pages_dirty_lock() - for each page in the @pages array, make
>> + * that page (or its head page, if a compound page) dirty, if it was
>> + * previously listed as clean. Then, release the page using
>> + * put_user_page().
>> + *
>> + * Please see the put_user_page() documentation for details.
>> + *
>> + * This is just like put_user_pages_dirty(), except that it invokes
>> + * set_page_dirty_lock(), instead of set_page_dirty().
>> + *
>> + * @pages:  array of pages to be marked dirty and released.
>> + * @npages: number of pages in the @pages array.
> 
> Ditto

OK.

> 
>> + *
>> + */
>> +void put_user_pages_dirty_lock(struct page **pages, unsigned long npages)
>> +{
>> +	__put_user_pages_dirty(pages, npages, set_page_dirty_lock);
>> +}
>> +EXPORT_SYMBOL(put_user_pages_dirty_lock);
>> +
>> +/*
>> + * put_user_pages() - for each page in the @pages array, release the page
>> + * using put_user_page().
>> + *
>> + * Please see the put_user_page() documentation for details.
>> + *
>> + * @pages:  array of pages to be marked dirty and released.
>> + * @npages: number of pages in the @pages array.
>> + *
> 
> And here as well :)

OK.


thanks,
-- 
John Hubbard
NVIDIA
 
