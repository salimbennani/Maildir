Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by i7-8700 with POP3-SSL;
  13 Dec 2018 08:52:23 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga006.fm.intel.com (fmsmga006.fm.intel.com [10.253.24.20])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 6550258079D;
	Wed, 12 Dec 2018 07:55:58 -0800 (PST)
Received: from orsmga101.jf.intel.com ([10.7.208.22])
  by fmsmga006-1.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 12 Dec 2018 07:55:57 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AhQc9VBPCT/rLrrC8fvMl6mtUPXoX/o7sNwtQ0KIM?=
 =?us-ascii?q?zox0KPn9pMbcNUDSrc9gkEXOFd2Cra4c26yO6+jJYi8p2d65qncMcZhBBVcuqP?=
 =?us-ascii?q?49uEgeOvODElDxN/XwbiY3T4xoXV5h+GynYwAOQJ6tL1LdrWev4jEMBx7xKRR6?=
 =?us-ascii?q?JvjvGo7Vks+7y/2+94fcbglUhzexe69+IAmrpgjNq8cahpdvJLwswRXTuHtIfO?=
 =?us-ascii?q?pWxWJsJV2Nmhv3+9m98p1+/SlOovwt78FPX7n0cKQ+VrxYES8pM3sp683xtBnM?=
 =?us-ascii?q?VhWA630BWWgLiBVIAgzF7BbnXpfttybxq+Rw1DWGMcDwULs5Qiqp4bt1RxD0iS?=
 =?us-ascii?q?cHLz85/3/Risxsl6JQvRatqwViz4LIfI2ZMfxzdb7fc9wHX2pMRshfWSxfDI2h?=
 =?us-ascii?q?dYsPAeUOMvpFoIb/qVQArgC+BRGuCe701j9EmmX70bEm3+k7Dw3L2hErEdIUsH?=
 =?us-ascii?q?TTqdX4LKkeX/qvw6nVyTXIcvxY1S3g44bPbh8vpO+DXbR2ccXPyUgjGR7Og1KI?=
 =?us-ascii?q?qYP/IjOV0uENvHSY7+d7UeKvimgnqxx+ozS1x8cjkYzJipgJxVDD8CV02YA4Ls?=
 =?us-ascii?q?C2Rk58ZN6rCppQtyeCOotyQ8MiRXxouSkiyr0CpJ67eTMFx4o9xx7Ed/OHdI6I?=
 =?us-ascii?q?4hz5WOaWOzd4i3Roc6+8iRaq6UWs1PHwW82u3FpXoCdJjMPAum0O2hDP8MSKS/?=
 =?us-ascii?q?lw8l+l1DqV1Q3f9vtILEMqmabBJZMswbg9nYcJv0vZBC/5gkD2gbeWdko6/uio?=
 =?us-ascii?q?7PzqYqvpppCCLY94kAL+Pbo0msy5H+s4NhICX2+B+eSzzLHj/Ev5T6tWjvAujK?=
 =?us-ascii?q?XVrJTXKd4GqqO3HQNZyJsv5hWjAzu80dkVn2ELLFdfdxKGi4jpNUvOIPf9Dfqn?=
 =?us-ascii?q?h1SskTFrx+3JP7H4AZXCMGLDkLH/crZ58kJczwQyzdZB6JJOEbwBPv3zVVHrtN?=
 =?us-ascii?q?DCDR82LRa0w+D5B9V5zI8eXniPAqCBPKPIrVCI/v4vI/WLZIINvDb9Kvsl6OD0?=
 =?us-ascii?q?gX42hF8QZq2p3ZoRaHClEfVqOUSZYXzwgtgfFWcGpBYxTOvviFeaSz5ce26yX7?=
 =?us-ascii?q?4g5jE8EI+mDZ3MRoGxgLOb2ye3BJ1WZn1cBVCKHnflbIGEW/YKaCKPLc5tiD0E?=
 =?us-ascii?q?Vb69S4A/0RGirhP1y71iLuDM4C0XqYrj1MRp5+3UjRwy9zt0ANqH32GOSGF0mG?=
 =?us-ascii?q?UIRzgt0aB7oEx9zEqD0Kdij/xZE9xT++1GUgMgOZHAyOx6Dsj4WhjdcdeRVFam?=
 =?us-ascii?q?XtKmDCkrQdIqw98OZEV9F8+4jh/Z3SqnGLsVl72NBJwp/aPQxXnxJ8Bhy3nY0K?=
 =?us-ascii?q?ktlUUpQsxKNWe+nK5w6xDTB5LVk0Wej6uqdr4T3CjX+GeHzGqBpkdYUAFrXKXB?=
 =?us-ascii?q?XHAfYFbWrNvj6kPDSb+uFaooMg9bxcGeLatKb8XjjU9aS/f7JNTef2Wxln+tCh?=
 =?us-ascii?q?mS2LODcpDme2UH0yXbE0gLjQYT8XGCNQg9Ayehp3nTDDhvFVLpfkPt/vNyqHK9?=
 =?us-ascii?q?Tk8o0Q6Ka1dt2Kay+h4QnfacUe8c3qoYuCc9rDV5BEuy0MjIC9WevQZhfL9TYd?=
 =?us-ascii?q?Um4FhZ02LUrAh9Pp2mL6B/iV8SaQV3v0Xy1xppDoVMi9QlrHQvzABqM6KXzEtB?=
 =?us-ascii?q?dy+E3ZD3IrDXNmjy/BWoa67K2lHf0Mya+rsV5PQ/sVXjuACpFkwt83h819lV0n?=
 =?us-ascii?q?2c5ojFDQYIUJLxVFo3+AZ+p73AfiY94IbU32V2Maaoqj/Cx84pBOw9xxm6e9dQ?=
 =?us-ascii?q?LqyFGxHyEsEAHMeuNfEllEKvbhIHO+BS6rU5P8end/uAxa6qM/xsnDOgjWRb/o?=
 =?us-ascii?q?991liA+DZ7Su7Nx5wF2e2X3hObVzfgi1esqsD2mZ1eaT4OBGa+yCjkC5RXZq19?=
 =?us-ascii?q?ZosLDWauI8uqxtRxnZLtWnhY9EK9CFMCwsOmZR2Sb1nl1w1KyUsXuWCnmTe/zz?=
 =?us-ascii?q?FsiTEpr7aQ0zbUw+v/cxoLIGhLS3d4jVftOIS7k8oVXEy1YAc3jhul4kD6yrNf?=
 =?us-ascii?q?pKR+KWnTXEhJczL3L2FkTqu/qL6Cb9RT55MvtCVdSP68bkyCSr7hvxsa1DvuHm?=
 =?us-ascii?q?tfxDwhdzCmoI75nwF8iG+GKHZzrXzZedx/xBvF5dzcQ+JR0SQCRCVilTbXAV28?=
 =?us-ascii?q?NcGz/dqIj5fDrvy+V2W5W51Raybr14CAtCi85WFwGh2whfOzmt7mEQg8zyD70c?=
 =?us-ascii?q?JnVSHJrBb6f4nq2L62MeNhfkl0GlD879B2FZ15kos1nJsQw2QVho2J/Xoblmf+?=
 =?us-ascii?q?KdVa1rj5bHYXRz4LwtjV7RPh2E1iKHKJ2o34Wm+cwstne9m1fGcW1jgh4MBNDa?=
 =?us-ascii?q?ee9KZEkjdtolqksQLRZuBwnjQHxvsv8nIag/wJtxArziWSGb0SGUhYPSrxlxWH?=
 =?us-ascii?q?9dy+raNXZHqxfri0zkZxgdehDLSaqAFGRHn5YosiHTN37shnNVLM0X7z5Zv+dN?=
 =?us-ascii?q?jec90TrQGUnAnaj+dONp0xjPUKiDFjOWL8u30l1uE6gQZv3ZG8oIiINWFt8Lil?=
 =?us-ascii?q?DR5fMz3/f9kT9S31jaZCgsaW2JiiHolmGjUORpfpTOinEDQPtfT8LAaOESYxqn?=
 =?us-ascii?q?OaGbrZAA+e511qr3PJE5C3KX6XIGMVwsllRBmYPEZfmhwbXC0mnp4lEQCn3Nfu?=
 =?us-ascii?q?f11+5jAV+170sB9Myv9zOhn7U2ffohqoazguRJieKhpW8h9N50PPPcOC6eJzGj?=
 =?us-ascii?q?lS/oe9owyVNmybewNIAHkLWkOeAlDsIKKu6cPc8+SCBeq+MvjOYaiIqeNETPeF?=
 =?us-ascii?q?3pav0opg/zaROcSDJHhiD/sn2kVdWXB1AdjWmzIKSyYPjSLCc9abpAug+i1wts?=
 =?us-ascii?q?2+8PPrWB/25YuSEbRSN89j+wuxgaeFOO6dnyJ5KTde1pMRyn7E0rkf3FgOiy5w?=
 =?us-ascii?q?czmhC6gPtSnITKjIgK9YEwYbaz9vNMtP96883hdCOc/YitP21750lP01C01CVV?=
 =?us-ascii?q?zuhM6pf9EKI3qmOVPDBUaLMqmGJDLRz8H2Z6O8VaNfjOFOuxKsvjabFlfpPi6f?=
 =?us-ascii?q?mDnxSxCvLeZMgTmBMxxDo469aAhiCGj5QNLiax22K9t3jTwwwb0piXLGL28cMT?=
 =?us-ascii?q?5gc0xTqr2c9z9XgvJ6G2ZZ9HpqMfGEmzqF7+nfMpsWrfprAiFum+1G+ng117tV?=
 =?us-ascii?q?4z9CRPFunCvSr9huo0ypk+WVyzpnVgZOpShPhI6RoUpiPqDZ/IFaWXnY5BIN8X?=
 =?us-ascii?q?mQCxMSqtplENLvvaVQytnJlK3rKTZC88jb/c0TB8XPLMKHMXwhMQfmGTLODQsF?=
 =?us-ascii?q?SyKrOn/bh0BHjP6S8XiV/dAGrc3Gn4MTTPd1WVo6XqcQDV14GpoMLZF4djwhmL?=
 =?us-ascii?q?+fysUP4CztlhTJQNRmucXfX++WBP7sADKYi6RUIRoCxPXzKoFAGJf83hkoUlBn?=
 =?us-ascii?q?mMznXQL1VvNApCtsf0V89F1E7Xx7RW4b30PjdxPr4XUWU/Wzm0hl2UNFfe0x+W?=
 =?us-ascii?q?K0sB8MLV3QqX51yRFplA=3D=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AEAAA+LxFch0O0hNFkGwEBAQEDAQEBB?=
 =?us-ascii?q?wMBAQGBUQYBAQELAYFagQ+BAieMFF+LM4INFGiRW4R8FIFfEgEBGA0GAYc+IjQ?=
 =?us-ascii?q?JDQEDAQEBAQEBAgETAQEBCA0JCCkjDII2JAGCYQEBAQEBAQEBAg8VEz8FAQkBA?=
 =?us-ascii?q?QoYCSUDDAUYMRMFHYI0SwGBeQgDAgqbHYlYAQEBgWozhC0BgQOEdwWMPBEGgUA?=
 =?us-ascii?q?/gRABgl01gx4BAoEtARIBhXoCiT2Bc4UTkEkHAgKHCYpBCxiBXIUailAsjXGKe?=
 =?us-ascii?q?wIEBgUCEwGBRoEdcXAVggiBH4InF4hehWBRgQUBASGKHYI+AQE?=
X-IPAS-Result: =?us-ascii?q?A0AEAAA+LxFch0O0hNFkGwEBAQEDAQEBBwMBAQGBUQYBAQE?=
 =?us-ascii?q?LAYFagQ+BAieMFF+LM4INFGiRW4R8FIFfEgEBGA0GAYc+IjQJDQEDAQEBAQEBA?=
 =?us-ascii?q?gETAQEBCA0JCCkjDII2JAGCYQEBAQEBAQEBAg8VEz8FAQkBAQoYCSUDDAUYMRM?=
 =?us-ascii?q?FHYI0SwGBeQgDAgqbHYlYAQEBgWozhC0BgQOEdwWMPBEGgUA/gRABgl01gx4BA?=
 =?us-ascii?q?oEtARIBhXoCiT2Bc4UTkEkHAgKHCYpBCxiBXIUailAsjXGKewIEBgUCEwGBRoE?=
 =?us-ascii?q?dcXAVggiBH4InF4hehWBRgQUBASGKHYI+AQE?=
X-IronPort-AV: E=Sophos;i="5.56,344,1539673200"; 
   d="scan'208";a="44671809"
X-Amp-Result: UNSCANNABLE
X-Amp-File-Uploaded: False
Unscannable: 2
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 12 Dec 2018 07:55:55 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1727863AbeLLPzx (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Wed, 12 Dec 2018 10:55:53 -0500
Received: from mx0a-001b2d01.pphosted.com ([148.163.156.1]:60280 "EHLO
        mx0a-001b2d01.pphosted.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S1726358AbeLLPzx (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Wed, 12 Dec 2018 10:55:53 -0500
Received: from pps.filterd (m0098394.ppops.net [127.0.0.1])
        by mx0a-001b2d01.pphosted.com (8.16.0.22/8.16.0.22) with SMTP id wBCFsEwa101921
        for <linux-kernel@vger.kernel.org>; Wed, 12 Dec 2018 10:55:51 -0500
Received: from e12.ny.us.ibm.com (e12.ny.us.ibm.com [129.33.205.202])
        by mx0a-001b2d01.pphosted.com with ESMTP id 2pb4pv2781-1
        (version=TLSv1.2 cipher=AES256-GCM-SHA384 bits=256 verify=NOT)
        for <linux-kernel@vger.kernel.org>; Wed, 12 Dec 2018 10:55:51 -0500
Received: from localhost
        by e12.ny.us.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
        for <linux-kernel@vger.kernel.org> from <paulmck@linux.vnet.ibm.com>;
        Wed, 12 Dec 2018 15:55:49 -0000
Received: from b01cxnp22033.gho.pok.ibm.com (9.57.198.23)
        by e12.ny.us.ibm.com (146.89.104.199) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
        (version=TLSv1/SSLv3 cipher=AES256-GCM-SHA384 bits=256/256)
        Wed, 12 Dec 2018 15:55:47 -0000
Received: from b01ledav003.gho.pok.ibm.com (b01ledav003.gho.pok.ibm.com [9.57.199.108])
        by b01cxnp22033.gho.pok.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id wBCFtkK121561410
        (version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256 verify=FAIL);
        Wed, 12 Dec 2018 15:55:46 GMT
Received: from b01ledav003.gho.pok.ibm.com (unknown [127.0.0.1])
        by IMSVA (Postfix) with ESMTP id 072CBB205F;
        Wed, 12 Dec 2018 15:55:46 +0000 (GMT)
Received: from b01ledav003.gho.pok.ibm.com (unknown [127.0.0.1])
        by IMSVA (Postfix) with ESMTP id C9E58B2064;
        Wed, 12 Dec 2018 15:55:45 +0000 (GMT)
Received: from paulmck-ThinkPad-W541 (unknown [9.70.82.38])
        by b01ledav003.gho.pok.ibm.com (Postfix) with ESMTP;
        Wed, 12 Dec 2018 15:55:45 +0000 (GMT)
Received: by paulmck-ThinkPad-W541 (Postfix, from userid 1000)
        id 9282916C17E5; Wed, 12 Dec 2018 07:55:46 -0800 (PST)
Date: Wed, 12 Dec 2018 07:55:46 -0800
From: "Paul E. McKenney" <paulmck@linux.ibm.com>
To: Boqun Feng <boqun.feng@gmail.com>
Cc: Joel Fernandes <joel@joelfernandes.org>,
        Sebastian Andrzej Siewior <bigeasy@linutronix.de>,
        linux-kernel@vger.kernel.org,
        Lai Jiangshan <jiangshanlai@gmail.com>,
        Josh Triplett <josh@joshtriplett.org>,
        Steven Rostedt <rostedt@goodmis.org>,
        Mathieu Desnoyers <mathieu.desnoyers@efficios.com>,
        tglx@linutronix.de, tj@kernel.org
Subject: Re: [PATCH] srcu: Remove srcu_queue_delayed_work_on()
Reply-To: paulmck@linux.ibm.com
References: <20181211111238.13474-1-bigeasy@linutronix.de>
 <20181212014016.GA97542@google.com>
 <20181212140519.GA10937@tardis>
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: <20181212140519.GA10937@tardis>
User-Agent: Mutt/1.5.21 (2010-09-15)
X-TM-AS-GCONF: 00
x-cbid: 18121215-0060-0000-0000-000002E2E942
X-IBM-SpamModules-Scores: 
X-IBM-SpamModules-Versions: BY=3.00010214; HX=3.00000242; KW=3.00000007;
 PH=3.00000004; SC=3.00000270; SDB=6.01130746; UDB=6.00587591; IPR=6.00910879;
 MB=3.00024669; MTD=3.00000008; XFM=3.00000015; UTC=2018-12-12 15:55:49
X-IBM-AV-DETECTION: SAVI=unused REMOTE=unused XFE=unused
x-cbparentid: 18121215-0061-0000-0000-000047833198
Message-Id: <20181212155546.GY4170@linux.ibm.com>
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10434:,, definitions=2018-12-12_04:,,
 signatures=0
X-Proofpoint-Spam-Details: rule=outbound_notspam policy=outbound score=0 priorityscore=1501
 malwarescore=0 suspectscore=0 phishscore=0 bulkscore=0 spamscore=0
 clxscore=1015 lowpriorityscore=0 mlxscore=0 impostorscore=0
 mlxlogscore=999 adultscore=0 classifier=spam adjust=0 reason=mlx
 scancount=1 engine=8.0.1-1810050000 definitions=main-1812120138
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On Wed, Dec 12, 2018 at 10:05:19PM +0800, Boqun Feng wrote:
> Hi Joel,
> 
> On Tue, Dec 11, 2018 at 05:40:16PM -0800, Joel Fernandes wrote:
> > On Tue, Dec 11, 2018 at 12:12:38PM +0100, Sebastian Andrzej Siewior wrote:
> > > srcu_queue_delayed_work_on() disables preemption (and therefore CPU
> > > hotplug in RCU's case) and then checks based on its own accounting if a
> > > CPU is online. If the CPU is online it uses queue_delayed_work_on()
> > > otherwise it fallbacks to queue_delayed_work().
> > > The problem here is that queue_work() on -RT does not work with disabled
> > > preemption.
> > > 
> > > queue_work_on() works also on an offlined CPU. queue_delayed_work_on()
> > > has the problem that it is possible to program a timer on an offlined
> > > CPU. This timer will fire once the CPU is online again. But until then,
> > > the timer remains programmed and nothing will happen.
> > > Add a local timer which will fire (as requested per delay) on the local
> > > CPU and then enqueue the work on the specific CPU.
> > > 
> > > RCUtorture testing with SRCU-P for 24h showed no problems.
> > > 
> > > Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
> > > ---
> > >  include/linux/srcutree.h |  3 ++-
> > >  kernel/rcu/srcutree.c    | 57 ++++++++++++++++++----------------------
> > >  kernel/rcu/tree.c        |  4 ---
> > >  kernel/rcu/tree.h        |  8 ------
> > >  4 files changed, 27 insertions(+), 45 deletions(-)
> > > 
> > > diff --git a/include/linux/srcutree.h b/include/linux/srcutree.h
> > > index 6f292bd3e7db7..0faa978c98807 100644
> > > --- a/include/linux/srcutree.h
> > > +++ b/include/linux/srcutree.h
> > > @@ -45,7 +45,8 @@ struct srcu_data {
> > >  	unsigned long srcu_gp_seq_needed;	/* Furthest future GP needed. */
> > >  	unsigned long srcu_gp_seq_needed_exp;	/* Furthest future exp GP. */
> > >  	bool srcu_cblist_invoking;		/* Invoking these CBs? */
> > > -	struct delayed_work work;		/* Context for CB invoking. */
> > > +	struct timer_list delay_work;		/* Delay for CB invoking */
> > > +	struct work_struct work;		/* Context for CB invoking. */
> > >  	struct rcu_head srcu_barrier_head;	/* For srcu_barrier() use. */
> > >  	struct srcu_node *mynode;		/* Leaf srcu_node. */
> > >  	unsigned long grpmask;			/* Mask for leaf srcu_node */
> > > diff --git a/kernel/rcu/srcutree.c b/kernel/rcu/srcutree.c
> > > index 3600d88d8956b..7f041f2435df9 100644
> > > --- a/kernel/rcu/srcutree.c
> > > +++ b/kernel/rcu/srcutree.c
> > > @@ -58,6 +58,7 @@ static bool __read_mostly srcu_init_done;
> > >  static void srcu_invoke_callbacks(struct work_struct *work);
> > >  static void srcu_reschedule(struct srcu_struct *ssp, unsigned long delay);
> > >  static void process_srcu(struct work_struct *work);
> > > +static void srcu_delay_timer(struct timer_list *t);
> > >  
> > >  /* Wrappers for lock acquisition and release, see raw_spin_lock_rcu_node(). */
> > >  #define spin_lock_rcu_node(p)					\
> > > @@ -156,7 +157,8 @@ static void init_srcu_struct_nodes(struct srcu_struct *ssp, bool is_static)
> > >  			snp->grphi = cpu;
> > >  		}
> > >  		sdp->cpu = cpu;
> > > -		INIT_DELAYED_WORK(&sdp->work, srcu_invoke_callbacks);
> > > +		INIT_WORK(&sdp->work, srcu_invoke_callbacks);
> > > +		timer_setup(&sdp->delay_work, srcu_delay_timer, 0);
> > >  		sdp->ssp = ssp;
> > >  		sdp->grpmask = 1 << (cpu - sdp->mynode->grplo);
> > >  		if (is_static)
> > > @@ -386,13 +388,19 @@ void _cleanup_srcu_struct(struct srcu_struct *ssp, bool quiesced)
> > >  	} else {
> > >  		flush_delayed_work(&ssp->work);
> > >  	}
> > > -	for_each_possible_cpu(cpu)
> > > +	for_each_possible_cpu(cpu) {
> > > +		struct srcu_data *sdp = per_cpu_ptr(ssp->sda, cpu);
> > > +
> > >  		if (quiesced) {
> > > -			if (WARN_ON(delayed_work_pending(&per_cpu_ptr(ssp->sda, cpu)->work)))
> > > +			if (WARN_ON(timer_pending(&sdp->delay_work)))
> > > +				return; /* Just leak it! */
> > > +			if (WARN_ON(work_pending(&sdp->work)))
> > >  				return; /* Just leak it! */
> > >  		} else {
> > > -			flush_delayed_work(&per_cpu_ptr(ssp->sda, cpu)->work);
> > > +			del_timer_sync(&sdp->delay_work);
> > > +			flush_work(&sdp->work);
> > >  		}
> > > +	}
> > >  	if (WARN_ON(rcu_seq_state(READ_ONCE(ssp->srcu_gp_seq)) != SRCU_STATE_IDLE) ||
> > >  	    WARN_ON(srcu_readers_active(ssp))) {
> > >  		pr_info("%s: Active srcu_struct %p state: %d\n",
> > > @@ -463,39 +471,23 @@ static void srcu_gp_start(struct srcu_struct *ssp)
> > >  	WARN_ON_ONCE(state != SRCU_STATE_SCAN1);
> > >  }
> > >  
> > > -/*
> > > - * Track online CPUs to guide callback workqueue placement.
> > > - */
> > > -DEFINE_PER_CPU(bool, srcu_online);
> > >  
> > > -void srcu_online_cpu(unsigned int cpu)
> > > +static void srcu_delay_timer(struct timer_list *t)
> > >  {
> > > -	WRITE_ONCE(per_cpu(srcu_online, cpu), true);
> > > +	struct srcu_data *sdp = container_of(t, struct srcu_data, delay_work);
> > > +
> > > +	queue_work_on(sdp->cpu, rcu_gp_wq, &sdp->work);
> > >  }
> > >  
> > > -void srcu_offline_cpu(unsigned int cpu)
> > > -{
> > > -	WRITE_ONCE(per_cpu(srcu_online, cpu), false);
> > > -}
> > > -
> > > -/*
> > > - * Place the workqueue handler on the specified CPU if online, otherwise
> > > - * just run it whereever.  This is useful for placing workqueue handlers
> > > - * that are to invoke the specified CPU's callbacks.
> > > - */
> > > -static bool srcu_queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
> > > -				       struct delayed_work *dwork,
> > > +static void srcu_queue_delayed_work_on(struct srcu_data *sdp,
> > >  				       unsigned long delay)
> > >  {
> > > -	bool ret;
> > > +	if (!delay) {
> > > +		queue_work_on(sdp->cpu, rcu_gp_wq, &sdp->work);
> > > +		return;
> > > +	}
> > >  
> > > -	preempt_disable();
> > > -	if (READ_ONCE(per_cpu(srcu_online, cpu)))
> > > -		ret = queue_delayed_work_on(cpu, wq, dwork, delay);
> > > -	else
> > > -		ret = queue_delayed_work(wq, dwork, delay);
> > > -	preempt_enable();
> > 
> > The deleted code looks like 'cpu' could be offlined.
> > 
> > Question for my clarification: According to sync_rcu_exp_select_cpus, we have
> > to disable preemption before calling queue_work_on to ensure the CPU is online.
> > 
> > Also same is said in Boqun's presentation on the topic:
> > https://linuxplumbersconf.org/event/2/contributions/158/attachments/68/79/workqueue_and_cpu_hotplug.pdf
> > 
> > Calling queue_work_on on an offline CPU sounds like could be problematic. So
> > in your patch, don't you still need to disable preemption around
> > queue_work_on, or the very least check if said CPU is online before calling
> > queue_work_on?
> > 
> 
> I should be the one who answers this ;-)
> 
> So I found something after my LPC topic, that is queue_work_on() may
> work well even if racing with a cpu offline. The reason is that in the
> cpu offline hook for workqueue only switches the percpu pool into an
> unbound one, so as long as the related cpu has been online once, we are
> fine here. 
> 
> Please note this is only my own analysis, and I'm the one who told
> Sebastian and Paul about this.
> 
> I should send an email to check with workqueue maintainers about this,
> but I didn't find the time. Sorry about this.. But let's do this right
> now, while we at it.
> 
> (Cc TJ)
> 
> So Jiangshan and TJ, what's your opion on this one? If we call a
> queue_work_on() at a place where that target cpu may be offlined, I
> think we have the guarantee that the work will be eventually executed
> even if the cpu is never online again, right? In other words, if a cpu
> has been online once, queue_work_on() on it will be free from racing
> with cpu hotplug.
> 
> Am I right about this, or did I miss something subtle?

Well, what I was relying on was Thomas Gleixner's assertion that it should
work and would be fixed if it didn't.  ;-)

							Thanx, Paul

> Regards,
> Boqun
> 
> > thanks,
> > 
> >  - Joel
> > 


