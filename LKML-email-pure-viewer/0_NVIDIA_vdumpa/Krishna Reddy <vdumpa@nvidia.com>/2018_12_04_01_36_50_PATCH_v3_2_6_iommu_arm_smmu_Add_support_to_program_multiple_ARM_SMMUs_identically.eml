Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 04 Dec 2018 09:35:14 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga008.jf.intel.com (orsmga008.jf.intel.com [10.7.209.65])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 8F7DD580375;
	Mon,  3 Dec 2018 17:37:04 -0800 (PST)
Received: from fmsmga103.fm.intel.com ([10.1.193.90])
  by orsmga008-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 03 Dec 2018 17:37:04 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3A9E6qVx/0ZU10K/9uRHKM819IXTAuvvDOBiVQ1KB9?=
 =?us-ascii?q?1+4UIJqq85mqBkHD//Il1AaPAd2Lraocw8Pt8InYEVQa5piAtH1QOLdtbDQizf?=
 =?us-ascii?q?ssogo7HcSeAlf6JvO5JwYzHcBFSUM3tyrjaRsdF8nxfUDdrWOv5jAOBBr/KRB1?=
 =?us-ascii?q?JuPoEYLOksi7ze+/94HQbglSmDaxfa55IQmrownWqsQYm5ZpJLwryhvOrHtIeu?=
 =?us-ascii?q?BWyn1tKFmOgRvy5dq+8YB6/ShItP0v68BPUaPhf6QlVrNYFygpM3o05MLwqxbO?=
 =?us-ascii?q?SxaE62YGXWUXlhpIBBXF7A3/U5zsvCb2qvZx1S+HNsDwULs6Wymt771zRRHoli?=
 =?us-ascii?q?kJKjA3/mLQhMNygqJUrw6upwdnw4PWe4yVKOZyc7nBcd4AWWZNQsBcXDFBDIOm?=
 =?us-ascii?q?aIsPCvIMM+NCr4n8vFsOrR2+ChOxD+3z1DBHm2L53K0n2OkmDwHJwREgH9cWsH?=
 =?us-ascii?q?vOt9j1MrkSUeGow6nJ1zrPde9Z2TD46IXRdB0qvPKCXapofMbP1UUiExnJgkie?=
 =?us-ascii?q?pID7JT+Zy+cAv3SB4+dhV++jk3ArpxxwrzS12MsglIrEipgIxlza6Cl12ps5KN?=
 =?us-ascii?q?K2RUN9fNWqCoFftzuAOItzWs4iQ39nuCI9yrAep567czYFyI49yx7cdfOHaY6I?=
 =?us-ascii?q?7QznVOqLJjd4nn1ldKq+hxa070eg1vXxWteo3FtOtCZJj9fBum4X2xDO6cWLVu?=
 =?us-ascii?q?Fx80aj1DqX0gDc8OBEIUQ6larBLJ4hx6Y9lp4SsUTFAy/3l1z6jKyIeUU+/Oin?=
 =?us-ascii?q?9eDnbqzhpp6SMY97lBv+P78wmsywH+s4KBICX2uF9uSm0r3s40n5TK9Njv0sna?=
 =?us-ascii?q?nVqIraKtgDpq6lHw9V1Z4u6xK+DzelztsUh3YGLE9edRKDjojpPUzOIf/iAfe+?=
 =?us-ascii?q?hVSsjClkx/TcMrL9BZXNK2DJkK39crZl905c1A0zwMhf551OC7EBPOj8WkjruN?=
 =?us-ascii?q?zYEx82KQq0w+n8BdV514MeX3+PA6CDPKPTt1+I+vwgI+2WaIAJvzb9LuAv5+Ty?=
 =?us-ascii?q?gn8hhV8dYa6p0IMVaHC/APtqOUaZYX3qgtcHFmcHpQ4+TO3siF2fXj9ffXeyX6?=
 =?us-ascii?q?Qg5j4lDIKqF5vMRoeogLaZxie0AoVWZnxaClCLCXrnbZ6EW/cLaCKROMNhiCYL?=
 =?us-ascii?q?Vbq6Ro8l1BGushL6yrV9IurV/C0YqYzs1Nxv6+LPkhEy8CR+D96B3GGVU2F0gm?=
 =?us-ascii?q?QISic13K9lp0xx0FOD0alijPxeGtxe/PdJUgY8NZ7BwO12EdHyWgTdftiXTFaq?=
 =?us-ascii?q?WMmpATY0Ttgp2d8Bf159G8m+jhDExyelHqUal6KVC5Au8qLTxX7xJ91jxHbA06?=
 =?us-ascii?q?Uhi1omQs5LNWC9gq5/9g7TB5PGkkmDlqaqc7gc0zDJ9GuZ0WWOu0RYWhZqUarZ?=
 =?us-ascii?q?RXAfelfWrdPh60zfVbCuF6ooPhFBycGYLKtKccPmjVNdSffnOdTeZX+xmmiqCR?=
 =?us-ascii?q?aJwLOMcJTle2EH0CrBD0gElhgZ/WyaOggmGiehv2XeASRyFVLuZkPs9vV+qHO7?=
 =?us-ascii?q?T0Mu0wGGdUph16Cx+h4Ug/ycROge3rYFuCcntjV1E0yx39PQC9qcuQVheL9Qbs?=
 =?us-ascii?q?864FdCzWjZrRByPoS8L6B+gV4Tax54v0fw2BR4FIpBkdImo2g3zAVvM62Y1lBB?=
 =?us-ascii?q?dzSG3Z3rPr3XK2/y/A2gaqLM21He1sqW9bkL6Pgit1rjuwSpHFI483p7y9lVz2?=
 =?us-ascii?q?ec5pLSAQsSTJL9SFo4+wJ7prHaeCY94YzU2GZoMam1tD/Cxt0oCPEkyhamY9dQ?=
 =?us-ascii?q?LqeEGBXuHM0dAsikMPYqlESxbhIYIOBS87Y5P9mnd/SawqGrJvtvnDW8gWRB/o?=
 =?us-ascii?q?99yEOM+zNgReHS25YK3u+X3hGAVzf6l1qhtsH3mYZZZTAdBGa/yC7kBJJPaa12?=
 =?us-ascii?q?Z4oEFWCuI8ivzNVkm5HtQ2JY9EKkB14e2M6peACeblzn0Q1LyEQXp2eqmS+5zz?=
 =?us-ascii?q?xyjjEoobCT3C3Iw+T+ahUHPnRHS3VljVfpOYK0lcwVXFC0bwg1kxuo/UT6yLJB?=
 =?us-ascii?q?pKR8LGnTRl1EfzPsIGFhUauwt7yCbNBJ6J4ztSVXUeK8YU2VS7Lnohsa1T/jEH?=
 =?us-ascii?q?VaxDwhaz6qvZD5lQRgiG2BNHZzsGbZecZoyBbf/tPcQuRd3jgHRCl+kjTXAlm8?=
 =?us-ascii?q?P9+0/dSbjZvDs+a+V36/WZ1XayXk0YSAtC6j721wHRK/h+yzmsHgEQUi0i/0zd?=
 =?us-ascii?q?hqWT/IrBbheIbr0au6PPljfklpAl/89sV7Fpt/kos2mJEfx3waio+J8noAlGf5?=
 =?us-ascii?q?Kc9b1r7mbHoRWT4LxMbY4A3k2E1gNH2F3Yz4WWuGwst9eda6eHgZ1Twn78BFE6?=
 =?us-ascii?q?qU6L1EnS1oolu3tw7RYP59nisDxvsq8nIVn+YJuA81xCWHHr8SBVVYPTDrlxmQ?=
 =?us-ascii?q?89C+q6BXaHyucLeq0kp+gMuhDKqDogxHXHb5e5EiHTJ/78llMVLM1mHz5Z/geN?=
 =?us-ascii?q?XKcd0TsRiUmQ/aj+dJMJIxiuYKhS1/NG3mp3IlzOo7jQF00ZG+oYiKMGFt/KO/?=
 =?us-ascii?q?Ah5FOTz5fcIT+jfxjalAmsaaxZygHpJkGj8TRpvnUeqoEC4OtfTgLwuOEiczqn?=
 =?us-ascii?q?KFFrrEBwOf9F1qr3HSE5CoKnGXImMWzc55SRmZJUxfhh0UXTogkp44EACq2NLu?=
 =?us-ascii?q?cENj6j8N4V74rwNGyvh0OBnnTmffuACoZy80SZiYMRpW9xxC6F3TMcCE9eJzGC?=
 =?us-ascii?q?dY/pK6oQyJK2ybYRlIDG4TVkyFAVDjIqeh5d3a/+eEAeq+KuPEYa+SpuxGS/eI?=
 =?us-ascii?q?2ZWv35Nm/zmSN8WPPXpiD/wh1kpAR3B5HMvZlCsVSywKjCLAdMqbpBa6+i1qoc?=
 =?us-ascii?q?Gz6vXrWATz5YSRD7teK8lg+xezga2bLe6fmD55KSpE1pML3XLH0qUf00UIiy5w?=
 =?us-ascii?q?dzihC7AAtS/WQaLUm69XCQMbaixpOMtJ6aI8whdCOcrBhtzp0b54i+Y/C01ZWl?=
 =?us-ascii?q?z5hsGpecsKLnmhNFPGAUaHLqiJKSfXzMH3f6y8T6ZdjOFVtx22pDaaHFXvPjWF?=
 =?us-ascii?q?lzn1SR+vNftAgz2cPBxboIu9aApiCXD/TNL6bR22KN92gicwwb0xh3PKM3YTMD?=
 =?us-ascii?q?l8c0xXqL2Q4jhVgvF+G2xH83pkIvOImyef7+nENJkWteFnDThzl+JfszwGzO4B?=
 =?us-ascii?q?8yBAQLp5lSLZqdhGqkuj1OKIz2wjGE5UrTxKgsSOoF96ObTx/4NFH33D+URJpT?=
 =?us-ascii?q?GSDA4Nj4tvG8HioOZX0NeZx4zpLzIX0MjV5ssaDoDuJc6LITJ1Lxb0HSbdJA0b?=
 =?us-ascii?q?TDKqPCfUgEkLw6LazWGcspVv8suko5EJULIODFE=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0ARAABt2QVch0O0hNFNFhwBAQEEAQEHB?=
 =?us-ascii?q?AEBgVEHAQELAYNrJ4wRjA9SBoFJiQuOKRSBWxgYEwGICSI0CQ0BAwEBAQEBAQI?=
 =?us-ascii?q?BEwEBAQoLCQgpIwyCNiKCZQMDAQIkGQEBMgUBBQkBATUbA1QGAQ0FBYMcgXUNB?=
 =?us-ascii?q?aRQgWwzgnYBAQWHLgiHbYMTgRwXeIEHgRGCZIRsLIVyiTKBb4ULkBYJkTwCFpE?=
 =?us-ascii?q?miQSPaYFGgg0zGggbFYMnghsMFxKDOIp0HwExgQUBAYsBAQE?=
X-IPAS-Result: =?us-ascii?q?A0ARAABt2QVch0O0hNFNFhwBAQEEAQEHBAEBgVEHAQELAYN?=
 =?us-ascii?q?rJ4wRjA9SBoFJiQuOKRSBWxgYEwGICSI0CQ0BAwEBAQEBAQIBEwEBAQoLCQgpI?=
 =?us-ascii?q?wyCNiKCZQMDAQIkGQEBMgUBBQkBATUbA1QGAQ0FBYMcgXUNBaRQgWwzgnYBAQW?=
 =?us-ascii?q?HLgiHbYMTgRwXeIEHgRGCZIRsLIVyiTKBb4ULkBYJkTwCFpEmiQSPaYFGgg0zG?=
 =?us-ascii?q?ggbFYMnghsMFxKDOIp0HwExgQUBAYsBAQE?=
X-IronPort-AV: E=Sophos;i="5.56,312,1539673200"; 
   d="scan'208";a="54377558"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 03 Dec 2018 17:37:02 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726084AbeLDBhA (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Mon, 3 Dec 2018 20:37:00 -0500
Received: from hqemgate14.nvidia.com ([216.228.121.143]:17625 "EHLO
        hqemgate14.nvidia.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726017AbeLDBg6 (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 3 Dec 2018 20:36:58 -0500
Received: from hqpgpgate102.nvidia.com (Not Verified[216.228.121.13]) by hqemgate14.nvidia.com (using TLS: TLSv1.2, DES-CBC3-SHA)
        id <B5c05da380000>; Mon, 03 Dec 2018 17:36:56 -0800
Received: from hqmail.nvidia.com ([172.20.161.6])
  by hqpgpgate102.nvidia.com (PGP Universal service);
  Mon, 03 Dec 2018 17:36:56 -0800
X-PGP-Universal: processed;
        by hqpgpgate102.nvidia.com on Mon, 03 Dec 2018 17:36:56 -0800
Received: from HQMAIL109.nvidia.com (172.20.187.15) by HQMAIL107.nvidia.com
 (172.20.187.13) with Microsoft SMTP Server (TLS) id 15.0.1395.4; Tue, 4 Dec
 2018 01:36:56 +0000
Received: from HQMAIL102.nvidia.com (172.18.146.10) by HQMAIL109.nvidia.com
 (172.20.187.15) with Microsoft SMTP Server (TLS) id 15.0.1395.4; Tue, 4 Dec
 2018 01:36:56 +0000
Received: from hqnvemgw02.nvidia.com (172.16.227.111) by HQMAIL102.nvidia.com
 (172.18.146.10) with Microsoft SMTP Server (TLS) id 15.0.1395.4 via Frontend
 Transport; Tue, 4 Dec 2018 01:36:56 +0000
Received: from vdumpa-ubuntu.nvidia.com (Not Verified[172.17.173.140]) by hqnvemgw02.nvidia.com with Trustwave SEG (v7,5,8,10121)
        id <B5c05da380000>; Mon, 03 Dec 2018 17:36:56 -0800
From: Krishna Reddy <vdumpa@nvidia.com>
To: <will.deacon@arm.com>, <robin.murphy@arm.com>, <joro@8bytes.org>
CC: <linux-arm-kernel@lists.infradead.org>,
        <iommu@lists.linux-foundation.org>, <linux-kernel@vger.kernel.org>,
        <linux-tegra@vger.kernel.org>, <treding@nvidia.com>,
        <yhsu@nvidia.com>, <snikam@nvidia.com>, <praithatha@nvidia.com>,
        <talho@nvidia.com>, <avanbrunt@nvidia.com>, <thomasz@nvidia.com>,
        <olof@lixom.net>, <jtukkinen@nvidia.com>, <mperttunen@nvidia.com>,
        Krishna Reddy <vdumpa@nvidia.com>
Subject: [PATCH v3 2/6] iommu/arm-smmu: Add support to program multiple ARM SMMU's identically
Date: Mon, 3 Dec 2018 17:36:50 -0800
Message-ID: <1543887414-18209-3-git-send-email-vdumpa@nvidia.com>
X-Mailer: git-send-email 2.1.4
In-Reply-To: <1543887414-18209-1-git-send-email-vdumpa@nvidia.com>
References: <1543887414-18209-1-git-send-email-vdumpa@nvidia.com>
X-NVConfidentiality: public
MIME-Version: 1.0
Content-Type: text/plain
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=nvidia.com; s=n1;
        t=1543887416; bh=s8JdRXlCfFf/VUjvXGNSRSzU2Q30F9CtI7YcJoh1QzI=;
        h=X-PGP-Universal:From:To:CC:Subject:Date:Message-ID:X-Mailer:
         In-Reply-To:References:X-NVConfidentiality:MIME-Version:
         Content-Type;
        b=ErFn7UtmclIt02KWXZu69bBLvpQ0cpBH4GnIbyYcQGWPmdHIt/VnQQjjEsXjJP1fq
         Bsb9Tn2Z5ZdceBZOlZYqPmIosUmCs1Tq8383DF2ZKzdjzlsnaaIs3TNztF6fp1Lnnl
         eq5wbfCZQdtq8ieRyBR+ksVC1nfXt0SsVe/m8A0PvDe3FZ1d/G9qLpezA3a3tj1U55
         f91b/ji3SaagDyaOCSjtfHkXJvcSr2pbfE2GVTdbdT2cE2oqPpMvOZl+pYfbO3cBwy
         IFfXWzxgQ43RnAKr65VGmSE98HCqOQqHuAxcvSk0bfRUYlTy16iQEzkzGxdRqQyN9c
         iG8Zh5JI9L8Ew==
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Add support to program multiple ARM SMMU's identically as one SMMU device.
Tegra194 uses Two ARM SMMU's as one SMMU device and both ARM SMMU's need
to be programmed identically.

Signed-off-by: Krishna Reddy <vdumpa@nvidia.com>
---
 drivers/iommu/lib-arm-smmu.c | 191 ++++++++++++++++++++++++++++++++-----------
 1 file changed, 144 insertions(+), 47 deletions(-)

diff --git a/drivers/iommu/lib-arm-smmu.c b/drivers/iommu/lib-arm-smmu.c
index 6aba5db..7036763 100644
--- a/drivers/iommu/lib-arm-smmu.c
+++ b/drivers/iommu/lib-arm-smmu.c
@@ -69,9 +69,9 @@
  * therefore this actually makes more sense than it might first appear.
  */
 #ifdef CONFIG_64BIT
-#define smmu_write_atomic_lq		writeq_relaxed
+#define smmu_write_atomic_lq		writeq_all_relaxed
 #else
-#define smmu_write_atomic_lq		writel_relaxed
+#define smmu_write_atomic_lq		writel_all_relaxed
 #endif
 
 /* Translation context bank */
@@ -135,6 +135,48 @@ struct arm_smmu_domain {
 	struct iommu_domain		domain;
 };
 
+#define to_smmu_intance(smmu, inst, addr) \
+	(addr - smmu->bases[0] + smmu->bases[inst])
+
+static void writel_all(struct arm_smmu_device *smmu,
+		       u32 value, void __iomem *addr)
+{
+	int i;
+
+	writel(value, addr);
+	for (i = 1; i < smmu->num_smmus; i++) {
+		void __iomem *reg_addr = to_smmu_intance(smmu, i, addr);
+
+		writel(value, reg_addr);
+	}
+}
+
+static void writel_all_relaxed(struct arm_smmu_device *smmu,
+			       u32 value, void __iomem *addr)
+{
+	int i;
+
+	writel_relaxed(value, addr);
+	for (i = 1; i < smmu->num_smmus; i++) {
+		void __iomem *reg_addr = to_smmu_intance(smmu, i, addr);
+
+		writel_relaxed(value, reg_addr);
+	}
+}
+
+static void writeq_all_relaxed(struct arm_smmu_device *smmu,
+			       u64 value, void __iomem *addr)
+{
+	int i;
+
+	writeq_relaxed(value, addr);
+	for (i = 1; i < smmu->num_smmus; i++) {
+		void __iomem *reg_addr = to_smmu_intance(smmu, i, addr);
+
+		writeq_relaxed(value, reg_addr);
+	}
+}
+
 static struct arm_smmu_domain *to_smmu_domain(struct iommu_domain *dom)
 {
 	return container_of(dom, struct arm_smmu_domain, domain);
@@ -179,25 +221,37 @@ static void __arm_smmu_tlb_sync(struct arm_smmu_device *smmu,
 
 static void arm_smmu_tlb_sync_global(struct arm_smmu_device *smmu)
 {
-	void __iomem *base = ARM_SMMU_GR0(smmu);
+	int i;
 	unsigned long flags;
 
 	spin_lock_irqsave(&smmu->global_sync_lock, flags);
-	__arm_smmu_tlb_sync(smmu, base + ARM_SMMU_GR0_sTLBGSYNC,
-			    base + ARM_SMMU_GR0_sTLBGSTATUS);
+	for (i = 0; i < smmu->num_smmus; i++) {
+		void __iomem *base = ARM_SMMU_GR0(smmu);
+
+		if (i > 0)
+			base = to_smmu_intance(smmu, i, base);
+		__arm_smmu_tlb_sync(smmu, base + ARM_SMMU_GR0_sTLBGSYNC,
+				    base + ARM_SMMU_GR0_sTLBGSTATUS);
+	}
 	spin_unlock_irqrestore(&smmu->global_sync_lock, flags);
 }
 
 static void arm_smmu_tlb_sync_context(void *cookie)
 {
+	int i;
 	struct arm_smmu_domain *smmu_domain = cookie;
 	struct arm_smmu_device *smmu = smmu_domain->smmu;
-	void __iomem *base = ARM_SMMU_CB(smmu, smmu_domain->cfg.cbndx);
 	unsigned long flags;
 
 	spin_lock_irqsave(&smmu_domain->cb_lock, flags);
-	__arm_smmu_tlb_sync(smmu, base + ARM_SMMU_CB_TLBSYNC,
-			    base + ARM_SMMU_CB_TLBSTATUS);
+	for (i = 0; i < smmu->num_smmus; i++) {
+		void __iomem *base = ARM_SMMU_CB(smmu, smmu_domain->cfg.cbndx);
+
+		if (i > 0)
+			base = to_smmu_intance(smmu, i, base);
+		__arm_smmu_tlb_sync(smmu, base + ARM_SMMU_CB_TLBSYNC,
+				    base + ARM_SMMU_CB_TLBSTATUS);
+	}
 	spin_unlock_irqrestore(&smmu_domain->cb_lock, flags);
 }
 
@@ -212,13 +266,14 @@ static void arm_smmu_tlb_inv_context_s1(void *cookie)
 {
 	struct arm_smmu_domain *smmu_domain = cookie;
 	struct arm_smmu_cfg *cfg = &smmu_domain->cfg;
-	void __iomem *base = ARM_SMMU_CB(smmu_domain->smmu, cfg->cbndx);
+	struct arm_smmu_device *smmu = smmu_domain->smmu;
+	void __iomem *base = ARM_SMMU_CB(smmu, cfg->cbndx);
 
 	/*
 	 * NOTE: this is not a relaxed write; it needs to guarantee that PTEs
 	 * cleared by the current CPU are visible to the SMMU before the TLBI.
 	 */
-	writel(cfg->asid, base + ARM_SMMU_CB_S1_TLBIASID);
+	writel_all(smmu, cfg->asid, base + ARM_SMMU_CB_S1_TLBIASID);
 	arm_smmu_tlb_sync_context(cookie);
 }
 
@@ -229,7 +284,7 @@ static void arm_smmu_tlb_inv_context_s2(void *cookie)
 	void __iomem *base = ARM_SMMU_GR0(smmu);
 
 	/* NOTE: see above */
-	writel(smmu_domain->cfg.vmid, base + ARM_SMMU_GR0_TLBIVMID);
+	writel_all(smmu, smmu_domain->cfg.vmid, base + ARM_SMMU_GR0_TLBIVMID);
 	arm_smmu_tlb_sync_global(smmu);
 }
 
@@ -237,11 +292,12 @@ static void arm_smmu_tlb_inv_range_nosync(unsigned long iova, size_t size,
 				size_t granule, bool leaf, void *cookie)
 {
 	struct arm_smmu_domain *smmu_domain = cookie;
+	struct arm_smmu_device *smmu = smmu_domain->smmu;
 	struct arm_smmu_cfg *cfg = &smmu_domain->cfg;
 	bool stage1 = cfg->cbar != CBAR_TYPE_S2_TRANS;
-	void __iomem *reg = ARM_SMMU_CB(smmu_domain->smmu, cfg->cbndx);
+	void __iomem *reg = ARM_SMMU_CB(smmu, cfg->cbndx);
 
-	if (smmu_domain->smmu->features & ARM_SMMU_FEAT_COHERENT_WALK)
+	if (smmu->features & ARM_SMMU_FEAT_COHERENT_WALK)
 		wmb();
 
 	if (stage1) {
@@ -251,7 +307,7 @@ static void arm_smmu_tlb_inv_range_nosync(unsigned long iova, size_t size,
 			iova &= ~12UL;
 			iova |= cfg->asid;
 			do {
-				writel_relaxed(iova, reg);
+				writel_all_relaxed(smmu, iova, reg);
 				iova += granule;
 			} while (size -= granule);
 		} else {
@@ -267,7 +323,7 @@ static void arm_smmu_tlb_inv_range_nosync(unsigned long iova, size_t size,
 			      ARM_SMMU_CB_S2_TLBIIPAS2;
 		iova >>= 12;
 		do {
-			smmu_write_atomic_lq(iova, reg);
+			smmu_write_atomic_lq(smmu, iova, reg);
 			iova += granule >> 12;
 		} while (size -= granule);
 	}
@@ -283,12 +339,13 @@ static void arm_smmu_tlb_inv_vmid_nosync(unsigned long iova, size_t size,
 				size_t granule, bool leaf, void *cookie)
 {
 	struct arm_smmu_domain *smmu_domain = cookie;
-	void __iomem *base = ARM_SMMU_GR0(smmu_domain->smmu);
+	struct arm_smmu_device *smmu = smmu_domain->smmu;
+	void __iomem *base = ARM_SMMU_GR0(smmu);
 
-	if (smmu_domain->smmu->features & ARM_SMMU_FEAT_COHERENT_WALK)
+	if (smmu->features & ARM_SMMU_FEAT_COHERENT_WALK)
 		wmb();
 
-	writel_relaxed(smmu_domain->cfg.vmid, base + ARM_SMMU_GR0_TLBIVMID);
+	writel_all_relaxed(smmu, smmu_domain->cfg.vmid, base + ARM_SMMU_GR0_TLBIVMID);
 }
 
 static const struct iommu_gather_ops arm_smmu_s1_tlb_ops = {
@@ -309,7 +366,8 @@ static const struct iommu_gather_ops arm_smmu_s2_tlb_ops_v1 = {
 	.tlb_sync	= arm_smmu_tlb_sync_vmid,
 };
 
-irqreturn_t arm_smmu_context_fault(int irq, void *dev)
+static irqreturn_t __arm_smmu_context_fault(int irq, void *dev,
+					    void __iomem *cb_base)
 {
 	u32 fsr, fsynr;
 	unsigned long iova;
@@ -317,9 +375,7 @@ irqreturn_t arm_smmu_context_fault(int irq, void *dev)
 	struct arm_smmu_domain *smmu_domain = to_smmu_domain(domain);
 	struct arm_smmu_cfg *cfg = &smmu_domain->cfg;
 	struct arm_smmu_device *smmu = smmu_domain->smmu;
-	void __iomem *cb_base;
 
-	cb_base = ARM_SMMU_CB(smmu, cfg->cbndx);
 	fsr = readl_relaxed(cb_base + ARM_SMMU_CB_FSR);
 
 	if (!(fsr & FSR_FAULT))
@@ -336,11 +392,33 @@ irqreturn_t arm_smmu_context_fault(int irq, void *dev)
 	return IRQ_HANDLED;
 }
 
-irqreturn_t arm_smmu_global_fault(int irq, void *dev)
+static irqreturn_t arm_smmu_context_fault(int irq, void *dev)
+{
+	int i;
+	irqreturn_t irq_ret = IRQ_NONE;
+	struct iommu_domain *domain = dev;
+	struct arm_smmu_domain *smmu_domain = to_smmu_domain(domain);
+	struct arm_smmu_cfg *cfg = &smmu_domain->cfg;
+	struct arm_smmu_device *smmu = smmu_domain->smmu;
+
+	for (i = 0; i < smmu->num_smmus; i++) {
+		void __iomem *cb_base = ARM_SMMU_CB(smmu, cfg->cbndx);
+
+		if (i > 0)
+			cb_base = to_smmu_intance(smmu, i, cb_base);
+		irq_ret = __arm_smmu_context_fault(irq, dev, cb_base);
+		if (irq_ret == IRQ_HANDLED)
+			break;
+	}
+
+	return irq_ret;
+}
+
+static irqreturn_t __arm_smmu_global_fault(int irq, void *dev,
+					   void __iomem *gr0_base)
 {
-	u32 gfsr, gfsynr0, gfsynr1, gfsynr2;
 	struct arm_smmu_device *smmu = dev;
-	void __iomem *gr0_base = ARM_SMMU_GR0_NS(smmu);
+	u32 gfsr, gfsynr0, gfsynr1, gfsynr2;
 
 	gfsr = readl_relaxed(gr0_base + ARM_SMMU_GR0_sGFSR);
 	gfsynr0 = readl_relaxed(gr0_base + ARM_SMMU_GR0_sGFSYNR0);
@@ -360,6 +438,25 @@ irqreturn_t arm_smmu_global_fault(int irq, void *dev)
 	return IRQ_HANDLED;
 }
 
+irqreturn_t arm_smmu_global_fault(int irq, void *dev)
+{
+	int i;
+	irqreturn_t irq_ret = IRQ_NONE;
+	struct arm_smmu_device *smmu = dev;
+
+	for (i = 0; i < smmu->num_smmus; i++) {
+		void __iomem *gr0_base = ARM_SMMU_GR0_NS(smmu);
+
+		if (i > 0)
+			gr0_base = to_smmu_intance(smmu, i, gr0_base);
+		irq_ret = __arm_smmu_global_fault(irq, dev, gr0_base);
+		if (irq_ret == IRQ_HANDLED)
+			break;
+	}
+
+	return irq_ret;
+}
+
 static void arm_smmu_init_context_bank(struct arm_smmu_domain *smmu_domain,
 				       struct io_pgtable_cfg *pgtbl_cfg)
 {
@@ -423,7 +520,7 @@ static void arm_smmu_write_context_bank(struct arm_smmu_device *smmu, int idx)
 
 	/* Unassigned context banks only need disabling */
 	if (!cfg) {
-		writel_relaxed(0, cb_base + ARM_SMMU_CB_SCTLR);
+		writel_all_relaxed(smmu, 0, cb_base + ARM_SMMU_CB_SCTLR);
 		return;
 	}
 
@@ -440,7 +537,7 @@ static void arm_smmu_write_context_bank(struct arm_smmu_device *smmu, int idx)
 		if (smmu->features & ARM_SMMU_FEAT_VMID16)
 			reg |= cfg->vmid << CBA2R_VMID_SHIFT;
 
-		writel_relaxed(reg, gr1_base + ARM_SMMU_GR1_CBA2R(idx));
+		writel_all_relaxed(smmu, reg, gr1_base + ARM_SMMU_GR1_CBA2R(idx));
 	}
 
 	/* CBAR */
@@ -459,7 +556,7 @@ static void arm_smmu_write_context_bank(struct arm_smmu_device *smmu, int idx)
 		/* 8-bit VMIDs live in CBAR */
 		reg |= cfg->vmid << CBAR_VMID_SHIFT;
 	}
-	writel_relaxed(reg, gr1_base + ARM_SMMU_GR1_CBAR(idx));
+	writel_all_relaxed(smmu, reg, gr1_base + ARM_SMMU_GR1_CBAR(idx));
 
 	/*
 	 * TTBCR
@@ -467,14 +564,14 @@ static void arm_smmu_write_context_bank(struct arm_smmu_device *smmu, int idx)
 	 * access behaviour of some fields (in particular, ASID[15:8]).
 	 */
 	if (stage1 && smmu->version > ARM_SMMU_V1)
-		writel_relaxed(cb->tcr[1], cb_base + ARM_SMMU_CB_TTBCR2);
-	writel_relaxed(cb->tcr[0], cb_base + ARM_SMMU_CB_TTBCR);
+		writel_all_relaxed(smmu, cb->tcr[1], cb_base + ARM_SMMU_CB_TTBCR2);
+	writel_all_relaxed(smmu, cb->tcr[0], cb_base + ARM_SMMU_CB_TTBCR);
 
 	/* TTBRs */
 	if (cfg->fmt == ARM_SMMU_CTX_FMT_AARCH32_S) {
-		writel_relaxed(cfg->asid, cb_base + ARM_SMMU_CB_CONTEXTIDR);
-		writel_relaxed(cb->ttbr[0], cb_base + ARM_SMMU_CB_TTBR0);
-		writel_relaxed(cb->ttbr[1], cb_base + ARM_SMMU_CB_TTBR1);
+		writel_all_relaxed(smmu, cfg->asid, cb_base + ARM_SMMU_CB_CONTEXTIDR);
+		writel_all_relaxed(smmu, cb->ttbr[0], cb_base + ARM_SMMU_CB_TTBR0);
+		writel_all_relaxed(smmu, cb->ttbr[1], cb_base + ARM_SMMU_CB_TTBR1);
 	} else {
 		writeq_relaxed(cb->ttbr[0], cb_base + ARM_SMMU_CB_TTBR0);
 		if (stage1)
@@ -484,8 +581,8 @@ static void arm_smmu_write_context_bank(struct arm_smmu_device *smmu, int idx)
 
 	/* MAIRs (stage-1 only) */
 	if (stage1) {
-		writel_relaxed(cb->mair[0], cb_base + ARM_SMMU_CB_S1_MAIR0);
-		writel_relaxed(cb->mair[1], cb_base + ARM_SMMU_CB_S1_MAIR1);
+		writel_all_relaxed(smmu, cb->mair[0], cb_base + ARM_SMMU_CB_S1_MAIR0);
+		writel_all_relaxed(smmu, cb->mair[1], cb_base + ARM_SMMU_CB_S1_MAIR1);
 	}
 
 	/* SCTLR */
@@ -495,7 +592,7 @@ static void arm_smmu_write_context_bank(struct arm_smmu_device *smmu, int idx)
 	if (IS_ENABLED(CONFIG_CPU_BIG_ENDIAN))
 		reg |= SCTLR_E;
 
-	writel_relaxed(reg, cb_base + ARM_SMMU_CB_SCTLR);
+	writel_all_relaxed(smmu, reg, cb_base + ARM_SMMU_CB_SCTLR);
 }
 
 static int arm_smmu_init_domain_context(struct iommu_domain *domain,
@@ -763,7 +860,7 @@ static void arm_smmu_write_smr(struct arm_smmu_device *smmu, int idx)
 
 	if (!(smmu->features & ARM_SMMU_FEAT_EXIDS) && smr->valid)
 		reg |= SMR_VALID;
-	writel_relaxed(reg, ARM_SMMU_GR0(smmu) + ARM_SMMU_GR0_SMR(idx));
+	writel_all_relaxed(smmu, reg, ARM_SMMU_GR0(smmu) + ARM_SMMU_GR0_SMR(idx));
 }
 
 static void arm_smmu_write_s2cr(struct arm_smmu_device *smmu, int idx)
@@ -776,7 +873,7 @@ static void arm_smmu_write_s2cr(struct arm_smmu_device *smmu, int idx)
 	if (smmu->features & ARM_SMMU_FEAT_EXIDS && smmu->smrs &&
 	    smmu->smrs[idx].valid)
 		reg |= S2CR_EXIDVALID;
-	writel_relaxed(reg, ARM_SMMU_GR0(smmu) + ARM_SMMU_GR0_S2CR(idx));
+	writel_all_relaxed(smmu, reg, ARM_SMMU_GR0(smmu) + ARM_SMMU_GR0_S2CR(idx));
 }
 
 static void arm_smmu_write_sme(struct arm_smmu_device *smmu, int idx)
@@ -1071,9 +1168,9 @@ static phys_addr_t arm_smmu_iova_to_phys_hard(struct iommu_domain *domain,
 	/* ATS1 registers can only be written atomically */
 	va = iova & ~0xfffUL;
 	if (smmu->version == ARM_SMMU_V2)
-		smmu_write_atomic_lq(va, cb_base + ARM_SMMU_CB_ATS1PR);
+		smmu_write_atomic_lq(smmu, va, cb_base + ARM_SMMU_CB_ATS1PR);
 	else /* Register is only 32-bit in v1 */
-		writel_relaxed(va, cb_base + ARM_SMMU_CB_ATS1PR);
+		writel_all_relaxed(smmu, va, cb_base + ARM_SMMU_CB_ATS1PR);
 
 	if (readl_poll_timeout_atomic(cb_base + ARM_SMMU_CB_ATSR, tmp,
 				      !(tmp & ATSR_ACTIVE), 5, 50)) {
@@ -1346,7 +1443,7 @@ void arm_smmu_device_reset(struct arm_smmu_device *smmu)
 
 	/* clear global FSR */
 	reg = readl_relaxed(ARM_SMMU_GR0_NS(smmu) + ARM_SMMU_GR0_sGFSR);
-	writel(reg, ARM_SMMU_GR0_NS(smmu) + ARM_SMMU_GR0_sGFSR);
+	writel_all(smmu, reg, ARM_SMMU_GR0_NS(smmu) + ARM_SMMU_GR0_sGFSR);
 
 	/*
 	 * Reset stream mapping groups: Initial values mark all SMRn as
@@ -1371,7 +1468,7 @@ void arm_smmu_device_reset(struct arm_smmu_device *smmu)
 		 * TLB entries for reduced latency.
 		 */
 		reg |= ARM_MMU500_ACR_SMTNMB_TLBEN | ARM_MMU500_ACR_S2CRB_TLBEN;
-		writel_relaxed(reg, gr0_base + ARM_SMMU_GR0_sACR);
+		writel_all_relaxed(smmu, reg, gr0_base + ARM_SMMU_GR0_sACR);
 	}
 
 	/* Make sure all context banks are disabled and clear CB_FSR  */
@@ -1379,7 +1476,7 @@ void arm_smmu_device_reset(struct arm_smmu_device *smmu)
 		void __iomem *cb_base = ARM_SMMU_CB(smmu, i);
 
 		arm_smmu_write_context_bank(smmu, i);
-		writel_relaxed(FSR_FAULT, cb_base + ARM_SMMU_CB_FSR);
+		writel_all_relaxed(smmu, FSR_FAULT, cb_base + ARM_SMMU_CB_FSR);
 		/*
 		 * Disable MMU-500's not-particularly-beneficial next-page
 		 * prefetcher for the sake of errata #841119 and #826419.
@@ -1387,13 +1484,13 @@ void arm_smmu_device_reset(struct arm_smmu_device *smmu)
 		if (smmu->model == ARM_MMU500) {
 			reg = readl_relaxed(cb_base + ARM_SMMU_CB_ACTLR);
 			reg &= ~ARM_MMU500_ACTLR_CPRE;
-			writel_relaxed(reg, cb_base + ARM_SMMU_CB_ACTLR);
+			writel_all_relaxed(smmu, reg, cb_base + ARM_SMMU_CB_ACTLR);
 		}
 	}
 
 	/* Invalidate the TLB, just in case */
-	writel_relaxed(0, gr0_base + ARM_SMMU_GR0_TLBIALLH);
-	writel_relaxed(0, gr0_base + ARM_SMMU_GR0_TLBIALLNSNH);
+	writel_all_relaxed(smmu, 0, gr0_base + ARM_SMMU_GR0_TLBIALLH);
+	writel_all_relaxed(smmu, 0, gr0_base + ARM_SMMU_GR0_TLBIALLNSNH);
 
 	reg = readl_relaxed(ARM_SMMU_GR0_NS(smmu) + ARM_SMMU_GR0_sCR0);
 
@@ -1424,7 +1521,7 @@ void arm_smmu_device_reset(struct arm_smmu_device *smmu)
 
 	/* Push the button */
 	arm_smmu_tlb_sync_global(smmu);
-	writel(reg, ARM_SMMU_GR0_NS(smmu) + ARM_SMMU_GR0_sCR0);
+	writel_all(smmu, reg, ARM_SMMU_GR0_NS(smmu) + ARM_SMMU_GR0_sCR0);
 }
 
 static int arm_smmu_id_size_to_bits(int size)
@@ -1666,6 +1763,6 @@ int arm_smmu_device_remove(struct platform_device *pdev)
 		dev_err(&pdev->dev, "removing device with active domains!\n");
 
 	/* Turn the thing off */
-	writel(sCR0_CLIENTPD, ARM_SMMU_GR0_NS(smmu) + ARM_SMMU_GR0_sCR0);
+	writel_all(smmu, sCR0_CLIENTPD, ARM_SMMU_GR0_NS(smmu) + ARM_SMMU_GR0_sCR0);
 	return 0;
 }
-- 
2.1.4

