Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from pop3.zju.edu.cn (61.164.42.155:110) by
  likexu-MOBL1.ccr.corp.intel.com with POP3; 08 Nov 2018 10:29:06 -0000
Received: from icoremail.net (unknown [209.85.214.177])
	by mail-app2 (Coremail) with SMTP id by_KCgD3_48S_uNbGdxeAQ--.60430S3;
	Thu, 08 Nov 2018 17:12:50 +0800 (CST)
Received: from mail-pl1-f177.google.com (unknown [209.85.214.177])
	by mx2.icoremail.net (Coremail) with SMTP id AQAAfwB3X0sP_uNblvcbAA--.6739S3;
	Thu, 08 Nov 2018 17:12:47 +0800 (CST)
Received: by mail-pl1-f177.google.com with SMTP id f12-v6so8074005plo.1
        for <xuliker@zju.edu.cn>; Thu, 08 Nov 2018 01:12:47 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-original-authentication-results:x-gm-message-state:delivered-to
         :from:to:cc:subject:date:message-id:sender:precedence:list-id;
        bh=7AUOV/xWq579Hqb63FoSPiZRxaQqi4eQi6moBY7EJ9Y=;
        b=EjHbGefTu85cfxqKTLaMLVFL4m2kM/SZb7OKbrPMQGVjGaTAbE7rLk/zJomRKmhcUc
         Yrw7QirwO8Bkocka7QPLdf6tufuubaAng90toECQzXqphoq8Dj1d1EWjKx9XyyoEx5wK
         H2/HWHpy7arGf9EL3Q+qG8jDLB9mpF2HK1mfzH0A8kHjmPVtAsjXq7p/mg0O18Ibgbmt
         KV9UUTn2JzEyf0XOUlNvdkTz+zYf2YGGqclof9uCN0yhk2Ld6ts/CUwn3E7BN357PXBy
         UtuHqPXWwkSTqAPztsTnCDjLzJ6FZHCLtQpVICBGfqm91VAFGh6EYO1yh5gMXAU2bt+t
         dIJA==
X-Original-Authentication-Results: mx.google.com;       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org
X-Gm-Message-State: AGRZ1gLDyigg3mhq/uW8hs6GnZErvD+hWU4Wwc6RMrEhPzOYk3jPPBUp
	jyyF0HgQ/i7MUlEhMFtjgiuqsy4XIXrVApMGdGlNfryHy+mItj1R/w==
X-Received: by 2002:a17:902:a9:: with SMTP id a38-v6mr3801500pla.7.1541668367131;
        Thu, 08 Nov 2018 01:12:47 -0800 (PST)
X-Forwarded-To: xuliker@zju.edu.cn
X-Forwarded-For: liker.xu@gmail.com xuliker@zju.edu.cn
Delivered-To: liker.xu@gmail.com
Received: by 2002:a17:90a:c304:0:0:0:0 with SMTP id g4-v6csp275582pjt;
        Thu, 8 Nov 2018 01:12:46 -0800 (PST)
X-Google-Smtp-Source: AJdET5eS8TxXiDice9CeWq1vqB5mHWMEcPQZ2v/JHHg9yOIGso2vQhORMKC4YdfUbE2FTSfi9My3
X-Received: by 2002:a63:3c44:: with SMTP id i4mr3130287pgn.286.1541668366219;
        Thu, 08 Nov 2018 01:12:46 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1541668366; cv=none;
        d=google.com; s=arc-20160816;
        b=yjvv+vZ1cd6dPdDsc/fI4lsupRY4verp4rQ3UMgn9SeG+9QvnxfjPt239vYiL8nyAH
         Qk2AeFP90yQIhUPTHvuW6oG8+uQvLt3l13bER6kjL0+oGLwa+RxHDNfkA84WcmfArmhB
         irFsg3PHPFQYUnY+lDBC9IJguhNFB2iY9c9+RzZ+aXNbXmAPxgko2IQMA+4zn8Cbs+zd
         8fuZNGtAfuFg8JtS4Vk5EHUX+MF5KMzM4Xso48ma6KnGtnlY/Tirp4Kl8AZ6i2JlFZd+
         8iG9RnidWd+6glsfW4BNI1hs3F2z/HoNvbJt5p1UcKW8cB1nszL5QwsuT670F0WBYDC2
         f+8Q==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=list-id:precedence:sender:message-id:date:subject:cc:to:from;
        bh=7AUOV/xWq579Hqb63FoSPiZRxaQqi4eQi6moBY7EJ9Y=;
        b=NlP1OLvK2s81jUZkSkXjKI5lNvsaZxvGbfv9/48T+GnJYwtHy8SzYmTR6odQs2apLW
         CFUeTS3f7WEghKFyACFsrcMjs4v6urAZBOIOjZjHcwNbsThptafBVJg0/RHMP1JLdDTz
         HEN2fGpymg05y8fOslOK7O6AcUUYrzHW6Z9oK6qV2Z0XWDEnuE7BErSdeJ5Bq8ho0keK
         /JUN7776wlmz1ZCQnvSsms7q1KdBjF8IbkqELnwV7n+St1dbYKkJvh7/7xdd54NgTfXP
         pOr9yGO+FoBvP80JRzfQDuIabbHLxHBCcNQOc5hXLlXiIGants4ZkG0rmZ0r7ZKQz7Ca
         JJNA==
ARC-Authentication-Results: i=1; mx.google.com;
       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org
Received: from vger.kernel.org (vger.kernel.org. [209.132.180.67])
        by mx.google.com with ESMTP id w2si2886544pgs.264.2018.11.08.01.12.30;
        Thu, 08 Nov 2018 01:12:46 -0800 (PST)
Received-SPF: pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) client-ip=209.132.180.67;
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726892AbeKHSqz (ORCPT <rfc822;lumpython@gmail.com> + 99 others);
        Thu, 8 Nov 2018 13:46:55 -0500
Received: from outbound-smtp16.blacknight.com ([46.22.139.233]:51108 "EHLO
        outbound-smtp16.blacknight.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S1726145AbeKHSqy (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Thu, 8 Nov 2018 13:46:54 -0500
Received: from mail.blacknight.com (pemlinmail01.blacknight.ie [81.17.254.10])
        by outbound-smtp16.blacknight.com (Postfix) with ESMTPS id 675FB1C2E66
        for <linux-kernel@vger.kernel.org>; Thu,  8 Nov 2018 09:12:19 +0000 (GMT)
Received: (qmail 23747 invoked from network); 8 Nov 2018 09:12:19 -0000
Received: from unknown (HELO stampy.163woodhaven.lan) (mgorman@techsingularity.net@[37.228.229.69])
  by 81.17.254.9 with ESMTPA; 8 Nov 2018 09:12:19 -0000
From: Mel Gorman <mgorman@techsingularity.net>
To: Linux-MM <linux-mm@kvack.org>
Cc: Andrew Morton <akpm@linux-foundation.org>,
        Vlastimil Babka <vbabka@suse.cz>,
        David Rientjes <rientjes@google.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        Zi Yan <zi.yan@cs.rutgers.edu>,
        LKML <linux-kernel@vger.kernel.org>,
        Mel Gorman <mgorman@techsingularity.net>
Subject: [PATCH 0/4] Fragmentation avoidance improvements v3
Date: Thu,  8 Nov 2018 09:12:14 +0000
Message-Id: <20181108091218.32715-1-mgorman@techsingularity.net>
X-Mailer: git-send-email 2.16.4
Sender: liker.xu+caf_=xuliker=zju.edu.cn@gmail.com
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-CM-TRANSID: AQAAfwB3X0sP_uNblvcbAA--.6739S3
Authentication-Results: mail-app2; spf=pass smtp.mail=liker.xu+caf_=xu
	liker=zju.edu.cn@gmail.com;
X-Coremail-Antispam: 1UD129KBjvJXoW3AF1DXrW3ury3Jw13JFWfAFb_yoW7Ar13pF
	WrKr15t3Z5Jw1xu3sxZw4xW34fGa18GFWUXr1agry0y3Z8CrnYkFWftF1UtFyUArs7A3Wj
	vrWUtas8CrZrZa7anT9S1TB71UUUUUUqnTZGkaVYY2UrUUUU0bIjqfuFe4nvWSU8nxnvy2
	9KBjDU0xBIdaVrnRJUUUmjb7Iv0xC_Kw4lb4IE77IF4wAFF20E14v26r1j6r4UM7CIcVAF
	z4kK6r1j6r18M28lY4IEw2IIxxk0rwA2F7IY1VAKz4vEj48ve4kI8wA2z4x0Y4vE2Ix0cI
	8IcVAFwI0_Ar0_tr1l84ACjcxK6xIIjxv20xvEc7CjxVAFwI0_Gr0_Cr1l84ACjcxK6I8E
	87Iv67AKxVW0oVCq3wA2z4x0Y4vEx4A2jsIEc7CjxVAFwI0_GcCE3s1le2I262IYc4CY6c
	8Ij28IcVAaY2xG8wAqx4xG64xvF2IEw4CE5I8CrVC2j2WlYx0E2Ix0cI8IcVAFwI0_Jr0_
	Jr4lYx0Ex4A2jsIE14v26r4j6F4UMcvjeVCFs4IE7xkEbVWUJVW8JwACjcxG0xvY0x0EwI
	xGrwCjxxvEa2IrMxkF7I0Ew4C26cxK6c8Ij28IcwCY1Ik267vv6cIUMxkFs20EY4vE44CY
	bxCE4x80FwCY02Avz4vEIxC_GFWlc2IjII80xcxEwVAKI48JMxvI42IY6xIIjxv20xvE14
	v26r4j6ryUMxvI42IY6xIIjxv20xvEc7CjxVAFwI0_Gr0_Cr1lcIIF0xvEx4A2jsIE14v2
	6F4UJVW0owCYIxAIcVC2z280aVCY1x0267AKxVWxJr0_GcWl42xK82IYc2Ij64vIr41l42
	xK82IY64kExVAvwVAq07x20xyl4x8a6x804xWl4I8I3I0E4IkC6x0Yz7v_Jr0_Gr1lx2Iq
	xVAqx4xG67AKxVWUJVWUGwC20s026x8GjcxK67AKxVWUGVWUWwC2zVAF1VAY17CE14v26r
	1q6r43MIIYrxkI7VAKI48JMIIF0xvE42xK8VAvwI8IcIk0rVW3JVWrJrUvcSsGvfC2Kfnx
	nUUI43ZEXa7IUooa0JUUUUU==

Sorry to send out a v3 so quickly. I dropped patch 5 as I'm not very happy
with the approach or that it is without side-effects. I have some ideas
on how it could be better achieved which can be done without delaying the
other 4 patches.  I've also updated patch 4 to reduce the stall timeout as
longer-lived tests showed it achieves similar reductions in fragmentation
latency without a much lower impact on fault latencies.

Changelog since v2
o Drop patch 5 as it was borderline
o Decrease timeout when stalling on fragmentation events

Changelog since v1
o Rebase to v4.20-rc1 for the THP __GFP_THISNODE patch in particular
o Add tracepoint to record fragmentation stall durations
o Add vmstat event to record that a fragmentation stall occurred
o Stalls now alter watermark boosting
o Stalls occur only when the allocation is about to fail

It has been noted before that fragmentation avoidance (aka
anti-fragmentation) is not perfect. Given sufficient time or an adverse
workload, memory gets fragmented and the long-term success of high-order
allocations degrades. This series defines an adverse workload, a definition
of external fragmentation events (including serious) ones and a series
that reduces the level of those fragmentation events.

The details of the workload and the consequences are described in more
detail in the changelogs. However, from patch 1, this is a high-level
summary of the adverse workload. The exact details are found in the
mmtests implementation.

The broad details of the workload are as follows;

1. Create an XFS filesystem (not specified in the configuration but done
   as part of the testing for this patch)
2. Start 4 fio threads that write a number of 64K files inefficiently.
   Inefficiently means that files are created on first access and not
   created in advance (fio parameterr create_on_open=1) and fallocate
   is not used (fallocate=none). With multiple IO issuers this creates
   a mix of slab and page cache allocations over time. The total size
   of the files is 150% physical memory so that the slabs and page cache
   pages get mixed
3. Warm up a number of fio read-only threads accessing the same files
   created in step 2. This part runs for the same length of time it
   took to create the files. It'll fault back in old data and further
   interleave slab and page cache allocations. As it's now low on
   memory due to step 2, fragmentation occurs as pageblocks get
   stolen.
4. While step 3 is still running, start a process that tries to allocate
   75% of memory as huge pages with a number of threads. The number of
   threads is based on a (NR_CPUS_SOCKET - NR_FIO_THREADS)/4 to avoid THP
   threads contending with fio, any other threads or forcing cross-NUMA
   scheduling. Note that the test has not been used on a machine with less
   than 8 cores. The benchmark records whether huge pages were allocated
   and what the fault latency was in microseconds
5. Measure the number of events potentially causing external fragmentation,
   the fault latency and the huge page allocation success rate.
6. Cleanup

Overall the series reduces external fragmentation causing events by over 95%
on 1 and 2 socket machines, which in turn impacts high-order allocation
success rates over the long term. There are differences in latencies and
high-order allocation success rates. Latencies are a mixed bag as they
are vulnerable to exact system state and whether allocations succeeded so
they are treated as a secondary metric.

Patch 1 uses lower zones if they are populated and have free memory
	instead of fragmenting a higher zone. It's special cased to
	handle a Normal->DMA32 fallback with the reasons explained
	in the changelog.

Patch 2+3 boosts watermarks temporarily when an external fragmentation
	event occurs. kswapd wakes to reclaim a small amount of old memory
	and then wakes kcompactd on completion to recover the system
	slightly. This introduces some overhead in the slowpath. The level
	of boosting can be tuned or disabled depending on the tolerance
	for fragmentation vs allocation latency.

Patch 4 is more heavy handed. In the event of a movable allocation
	request that can stall, it'll wake kswapd as in patch 3.  However,
	if the expected fragmentation event is serious then the request
	will stall briefly on pfmemalloc_wait until kswapd completes
	light reclaim work and retry the allocation without stalling.
	This can avoid the fragmentation event entirely in some cases.
	The definition of a serious fragmentation event can be tuned
	or disabled.

The bulk of the improvement in fragmentation avoidance is from patches
1-3 (94-97% reduction in fragmentation events for an adverse workload on
both a 1-socket and 2-socket machine). The primary benefit of patch 4 is
the increase in THP success rates and the fact it reduces fragmentation
events to almost negligible levels with the option of eliminating them.

 Documentation/sysctl/vm.txt   |  42 ++++++++
 include/linux/mm.h            |   2 +
 include/linux/mmzone.h        |  14 ++-
 include/linux/vm_event_item.h |   1 +
 include/trace/events/kmem.h   |  21 ++++
 kernel/sysctl.c               |  18 ++++
 mm/compaction.c               |   2 +-
 mm/internal.h                 |  14 ++-
 mm/page_alloc.c               | 239 ++++++++++++++++++++++++++++++++++++++----
 mm/vmscan.c                   | 123 ++++++++++++++++++++--
 mm/vmstat.c                   |   1 +
 11 files changed, 437 insertions(+), 40 deletions(-)

-- 
2.16.4
