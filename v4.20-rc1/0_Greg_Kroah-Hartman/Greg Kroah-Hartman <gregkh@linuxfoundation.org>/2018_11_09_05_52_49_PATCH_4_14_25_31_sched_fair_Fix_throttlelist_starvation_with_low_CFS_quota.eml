Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from pop3.zju.edu.cn (124.160.105.205:110) by
  likexu-MOBL1.ccr.corp.intel.com with POP3; 09 Nov 2018 00:38:47 -0000
Received: from icoremail.net (unknown [209.85.215.176])
	by mail-app2 (Coremail) with SMTP id by_KCgCn3yQ7tORbdO1iAQ--.30430S3;
	Fri, 09 Nov 2018 06:10:04 +0800 (CST)
Received: from mail-pg1-f176.google.com (unknown [209.85.215.176])
	by mx2.icoremail.net (Coremail) with SMTP id AQAAfwB3X0s4tORbv+AeAA--.2155S3;
	Fri, 09 Nov 2018 06:10:00 +0800 (CST)
Received: by mail-pg1-f176.google.com with SMTP id n10-v6so9432102pgv.10
        for <xuliker@zju.edu.cn>; Thu, 08 Nov 2018 14:10:00 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:delivered-to:dkim-signature:from:to:cc:subject
         :date:message-id:in-reply-to:references:user-agent:mime-version
         :content-transfer-encoding:sender:precedence:list-id;
        bh=L5nIPPWFgpfvDbIrWK7/owogzlORNhloNO7C2qIUwyY=;
        b=EZZzTRed4q2x2NHwO6sVQVXV5AMUekmlY7tvNBHWQQorp4aTFOxpZ8juvgeynakIyS
         Dz3MzebeDDgV21jgGjA+V6+snLhcA9ukCtejyMvsCJ7BRaw1yPl2DP1a8FbIkKBErRvG
         E3MaVF6RB3yHGAmMpUSaYqNVhG8Q2WOmBDe/j+/ma9drc/8HmyHd26u7g6R0+63X62y9
         3AtuzqrZHqaO8ermWMCwkSSbhZ7rr4zBeBrYUsTl0YYUQeOT+ukviAF6OZnBKKn/NlW6
         LFWhUjd72SYS6X8IpKSiVF0oOPSLl5N4g8vw3kxAQEev/H2Ii1u//zKDyY30VgCCReIi
         7VKA==
X-Gm-Message-State: AGRZ1gI5Subx///UJy4xYYE8b+HUm6w0r5f2kRdRxLFBreECwbyfPkNR
	WZ/j6Supw94t2TwYu5J+wuCeWV1odBbM0U0Ii0iuYo2Sfk/GhMHvnw==
X-Received: by 2002:a63:5816:: with SMTP id m22-v6mr5179825pgb.332.1541715000501;
        Thu, 08 Nov 2018 14:10:00 -0800 (PST)
X-Forwarded-To: xuliker@zju.edu.cn
X-Forwarded-For: liker.xu@gmail.com xuliker@zju.edu.cn
Delivered-To: liker.xu@gmail.com
Received: by 2002:a17:90a:c304:0:0:0:0 with SMTP id g4-v6csp790210pjt;
        Thu, 8 Nov 2018 14:09:59 -0800 (PST)
X-Google-Smtp-Source: AJdET5eaDZpNDlgj2d0fNLC1xaWqYWhIGyTjpl4BDY8q8lMWPEE8Zg4PoHrOD8QE+4ryCLEEa4x6
X-Received: by 2002:a65:55ca:: with SMTP id k10mr5202628pgs.448.1541714999528;
        Thu, 08 Nov 2018 14:09:59 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1541714999; cv=none;
        d=google.com; s=arc-20160816;
        b=Jsw/DEt/aimP/dgb1qSmOgZrtRnxcGpI3LByYX1Z90zfXnTuQOYq5MIVyJyI7hD0Gv
         Vq7tGaTMpdh9Vq2CkUVW3jxnf/NpvIghHEmiqGeGwWqIPZ3HWH/WpNqQPzf2EGIegpY9
         xkfUMFBvFVOtWh8R0iJgR/9jgYe/RBpfe/jQI9AsqowwwkFTvXNqQiuBnUssfC/Er4QF
         sKB5/vVWnYE0Piv/EpnS7QHg5N3AnNZih1MlNQuhayUiNCPJ/frGcvWcWBK5ycClqzmm
         K8BHhlYMD5MmXXGYHVkbNDjO7aej2RFFSrTZruvaT8zq/iydP7sIKyn/e39G0RN4VZ+3
         NWsA==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=list-id:precedence:sender:content-transfer-encoding:mime-version
         :user-agent:references:in-reply-to:message-id:date:subject:cc:to
         :from:dkim-signature;
        bh=L5nIPPWFgpfvDbIrWK7/owogzlORNhloNO7C2qIUwyY=;
        b=RTjuMPghJbE5I9RjLugnxhlo6agiwxXAwP9GXoUcSTxCJWPm7TUSVFwRYZvxNDkEno
         PRheo/R2TnlNY9tgtVQEkox2VtDlnkSeVlZQE2YJUyo8LfuJZcm0gM2fXkhsiBqYZpJQ
         RZub4ohv9/EBNrbN8g/7O7rmvfwYenzCg3e7Nx/nfQd+kkp9/74EErqXSy0snW/MlxyM
         lkZ+0Nk/OsX2iJB8ZpAEcWiMKce8PJiof5d1pCwaOF5vjhZ33rB8GiObty/nVgubwuGU
         aixDGg8T/dN+OAv8mxjZLcTWyiYy4Ij8GVkAwPVHHubdUV5zuSQPrZy2eZwZ46I9xv6f
         822g==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass header.i=@kernel.org header.s=default header.b=j2HGWMyp;
       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org
Received: from vger.kernel.org (vger.kernel.org. [209.132.180.67])
        by mx.google.com with ESMTP id a61-v6si5610750plc.35.2018.11.08.14.09.44;
        Thu, 08 Nov 2018 14:09:59 -0800 (PST)
Received-SPF: pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) client-ip=209.132.180.67;
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1732736AbeKIHqT (ORCPT <rfc822;tu.k.phung@gmail.com>
        + 99 others); Fri, 9 Nov 2018 02:46:19 -0500
Received: from mail.kernel.org ([198.145.29.99]:40166 "EHLO mail.kernel.org"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1730006AbeKIHqS (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Fri, 9 Nov 2018 02:46:18 -0500
Received: from localhost (unknown [208.72.13.198])
        (using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
        (No client certificate requested)
        by mail.kernel.org (Postfix) with ESMTPSA id 418E22147D;
        Thu,  8 Nov 2018 22:08:46 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=kernel.org;
        s=default; t=1541714926;
        bh=Za0Mmo5jbqdnP5wskUCA5KgvwCryULBtYE4pV6V1IBE=;
        h=From:To:Cc:Subject:Date:In-Reply-To:References:From;
        b=j2HGWMyp+9RlWSSt0oWH0m7NJ/0z7ZxejBmkHfBky7lqct1dtkmGYD6Mi8fNz9/LF
         0JCnyFyS/TquXOVDgvmCFbPZ/YkvrlTEjl3ri89hmZxvIy6eEiPW1mLMKnJyofKAa0
         Q+Z/pExQtgCxC1Q0rQrK8hpivmn1s7V2ssMbr3jk=
From: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
To: linux-kernel@vger.kernel.org
Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>,
        stable@vger.kernel.org, Phil Auld <pauld@redhat.com>,
        Ben Segall <bsegall@google.com>,
        Linus Torvalds <torvalds@linux-foundation.org>,
        Peter Zijlstra <peterz@infradead.org>,
        Thomas Gleixner <tglx@linutronix.de>,
        Ingo Molnar <mingo@kernel.org>
Subject: [PATCH 4.14 25/31] sched/fair: Fix throttle_list starvation with low CFS quota
Date: Thu,  8 Nov 2018 13:52:49 -0800
Message-Id: <20181108215131.751474646@linuxfoundation.org>
X-Mailer: git-send-email 2.19.1
In-Reply-To: <20181108215129.641434673@linuxfoundation.org>
References: <20181108215129.641434673@linuxfoundation.org>
User-Agent: quilt/0.65
X-stable: review
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
Sender: liker.xu+caf_=xuliker=zju.edu.cn@gmail.com
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-CM-TRANSID: AQAAfwB3X0s4tORbv+AeAA--.2155S3
Authentication-Results: mail-app2; spf=pass smtp.mail=liker.xu+caf_=xu
	liker=zju.edu.cn@gmail.com;
X-Coremail-Antispam: 1UD129KBjvJXoWxKryxZryUZw1fAFWxtF4fKrg_yoWxXr4UpF
	ZrWw43Kw18X392qwn8ArsxWayrW3yfGry7AryDG3WrZw45u343tF1fJr4IvF1YyFWFkF13
	tr18WrW3GFWYv3DanT9S1TB71UUUUUDqnTZGkaVYY2UrUUUU0bIjqfuFe4nvWSU8nxnvy2
	9KBjDU0xBIdaVrnRJUUUPSb7Iv0xC_Kw4lb4IE77IF4wAFF20E14v26r1j6r4UM7CIcVAF
	z4kK6r1j6r18M28lY4IEw2IIxxk0rwA2F7IY1VAKz4vEj48ve4kI8wA2z4x0Y4vE2Ix0cI
	8IcVAFwI0_tr0E3s1l84ACjcxK6xIIjxv20xvEc7CjxVAFwI0_Gr1j6F4UJwA2z4x0Y4vE
	x4A2jsIE14v26rxl6s0DM28EF7xvwVC2z280aVCY1x0267AKxVW0oVCq3wAS0I0E0xvYzx
	vE52x082IY62kv0487Mc02F40EFcxC0VAKzVAqx4xG6I80ewAv7VC0I7IYx2IY67AKxVWU
	AVWUtwAv7VC2z280aVAFwI0_Gr0_Cr1lOx8S6xCaFVCjc4AY6r1j6r4UM4x0Y48IcxkI7V
	AKI48JMx02cVCv0xWlc7CjxVAKzI0EY4vE52x082I5MxkFs20EY4vE44CYbxCE4x80FwCY
	02Avz4vEIxC_ZwCY0x0Ix7I2Y4AK64vIr41lcIIF0xvE2Ix0cI8IcVAFwI0_tr0E3s1lcI
	IF0xvE2Ix0cI8IcVCY1x0267AKxVW8Jr0_Cr1UMxvI42IY6I8E87Iv67AKxVWxJr0_GcWl
	cIIF0xvEx4A2jsIEc7CjxVAFwI0_Cr1j6rxdMxAIw28IcxkI7VAKI48JMxAIw28IcVAKzI
	0EY4vE52x082I5MxCjnVCjjxCrMxC20s026xCaFVCjc4AY6r1j6r4UMI8I3I0E5I8CrVAF
	wI0_Jr0_Jr4lx2IqxVCjr7xvwVAFwI0_JrI_JrWlx4CE17CEb7AF67AKxVWUtVW8ZwCIc4
	0Y0x0EwIxGrwCI42IY6xAIw20EY4v20xvaj40_Jr0_JF7vcSsGvfC2KfnxnUUI43ZEXa7I
	UOAUU5UUUUU==

4.14-stable review patch.  If anyone has any objections, please let me know.

------------------

From: Phil Auld <pauld@redhat.com>

commit baa9be4ffb55876923dc9716abc0a448e510ba30 upstream.

With a very low cpu.cfs_quota_us setting, such as the minimum of 1000,
distribute_cfs_runtime may not empty the throttled_list before it runs
out of runtime to distribute. In that case, due to the change from
c06f04c7048 to put throttled entries at the head of the list, later entries
on the list will starve.  Essentially, the same X processes will get pulled
off the list, given CPU time and then, when expired, get put back on the
head of the list where distribute_cfs_runtime will give runtime to the same
set of processes leaving the rest.

Fix the issue by setting a bit in struct cfs_bandwidth when
distribute_cfs_runtime is running, so that the code in throttle_cfs_rq can
decide to put the throttled entry on the tail or the head of the list.  The
bit is set/cleared by the callers of distribute_cfs_runtime while they hold
cfs_bandwidth->lock.

This is easy to reproduce with a handful of CPU consumers. I use 'crash' on
the live system. In some cases you can simply look at the throttled list and
see the later entries are not changing:

  crash> list cfs_rq.throttled_list -H 0xffff90b54f6ade40 -s cfs_rq.runtime_remaining | paste - - | awk '{print $1"  "$4}' | pr -t -n3
    1     ffff90b56cb2d200  -976050
    2     ffff90b56cb2cc00  -484925
    3     ffff90b56cb2bc00  -658814
    4     ffff90b56cb2ba00  -275365
    5     ffff90b166a45600  -135138
    6     ffff90b56cb2da00  -282505
    7     ffff90b56cb2e000  -148065
    8     ffff90b56cb2fa00  -872591
    9     ffff90b56cb2c000  -84687
   10     ffff90b56cb2f000  -87237
   11     ffff90b166a40a00  -164582

  crash> list cfs_rq.throttled_list -H 0xffff90b54f6ade40 -s cfs_rq.runtime_remaining | paste - - | awk '{print $1"  "$4}' | pr -t -n3
    1     ffff90b56cb2d200  -994147
    2     ffff90b56cb2cc00  -306051
    3     ffff90b56cb2bc00  -961321
    4     ffff90b56cb2ba00  -24490
    5     ffff90b166a45600  -135138
    6     ffff90b56cb2da00  -282505
    7     ffff90b56cb2e000  -148065
    8     ffff90b56cb2fa00  -872591
    9     ffff90b56cb2c000  -84687
   10     ffff90b56cb2f000  -87237
   11     ffff90b166a40a00  -164582

Sometimes it is easier to see by finding a process getting starved and looking
at the sched_info:

  crash> task ffff8eb765994500 sched_info
  PID: 7800   TASK: ffff8eb765994500  CPU: 16  COMMAND: "cputest"
    sched_info = {
      pcount = 8,
      run_delay = 697094208,
      last_arrival = 240260125039,
      last_queued = 240260327513
    },
  crash> task ffff8eb765994500 sched_info
  PID: 7800   TASK: ffff8eb765994500  CPU: 16  COMMAND: "cputest"
    sched_info = {
      pcount = 8,
      run_delay = 697094208,
      last_arrival = 240260125039,
      last_queued = 240260327513
    },

Signed-off-by: Phil Auld <pauld@redhat.com>
Reviewed-by: Ben Segall <bsegall@google.com>
Cc: Linus Torvalds <torvalds@linux-foundation.org>
Cc: Peter Zijlstra <peterz@infradead.org>
Cc: Thomas Gleixner <tglx@linutronix.de>
Cc: stable@vger.kernel.org
Fixes: c06f04c70489 ("sched: Fix potential near-infinite distribute_cfs_runtime() loop")
Link: http://lkml.kernel.org/r/20181008143639.GA4019@pauld.bos.csb
Signed-off-by: Ingo Molnar <mingo@kernel.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

---
 kernel/sched/fair.c  |   22 +++++++++++++++++++---
 kernel/sched/sched.h |    2 ++
 2 files changed, 21 insertions(+), 3 deletions(-)

--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -4299,9 +4299,13 @@ static void throttle_cfs_rq(struct cfs_r
 
 	/*
 	 * Add to the _head_ of the list, so that an already-started
-	 * distribute_cfs_runtime will not see us
+	 * distribute_cfs_runtime will not see us. If disribute_cfs_runtime is
+	 * not running add to the tail so that later runqueues don't get starved.
 	 */
-	list_add_rcu(&cfs_rq->throttled_list, &cfs_b->throttled_cfs_rq);
+	if (cfs_b->distribute_running)
+		list_add_rcu(&cfs_rq->throttled_list, &cfs_b->throttled_cfs_rq);
+	else
+		list_add_tail_rcu(&cfs_rq->throttled_list, &cfs_b->throttled_cfs_rq);
 
 	/*
 	 * If we're the first throttled task, make sure the bandwidth
@@ -4445,14 +4449,16 @@ static int do_sched_cfs_period_timer(str
 	 * in us over-using our runtime if it is all used during this loop, but
 	 * only by limited amounts in that extreme case.
 	 */
-	while (throttled && cfs_b->runtime > 0) {
+	while (throttled && cfs_b->runtime > 0 && !cfs_b->distribute_running) {
 		runtime = cfs_b->runtime;
+		cfs_b->distribute_running = 1;
 		raw_spin_unlock(&cfs_b->lock);
 		/* we can't nest cfs_b->lock while distributing bandwidth */
 		runtime = distribute_cfs_runtime(cfs_b, runtime,
 						 runtime_expires);
 		raw_spin_lock(&cfs_b->lock);
 
+		cfs_b->distribute_running = 0;
 		throttled = !list_empty(&cfs_b->throttled_cfs_rq);
 
 		cfs_b->runtime -= min(runtime, cfs_b->runtime);
@@ -4563,6 +4569,11 @@ static void do_sched_cfs_slack_timer(str
 
 	/* confirm we're still not at a refresh boundary */
 	raw_spin_lock(&cfs_b->lock);
+	if (cfs_b->distribute_running) {
+		raw_spin_unlock(&cfs_b->lock);
+		return;
+	}
+
 	if (runtime_refresh_within(cfs_b, min_bandwidth_expiration)) {
 		raw_spin_unlock(&cfs_b->lock);
 		return;
@@ -4572,6 +4583,9 @@ static void do_sched_cfs_slack_timer(str
 		runtime = cfs_b->runtime;
 
 	expires = cfs_b->runtime_expires;
+	if (runtime)
+		cfs_b->distribute_running = 1;
+
 	raw_spin_unlock(&cfs_b->lock);
 
 	if (!runtime)
@@ -4582,6 +4596,7 @@ static void do_sched_cfs_slack_timer(str
 	raw_spin_lock(&cfs_b->lock);
 	if (expires == cfs_b->runtime_expires)
 		cfs_b->runtime -= min(runtime, cfs_b->runtime);
+	cfs_b->distribute_running = 0;
 	raw_spin_unlock(&cfs_b->lock);
 }
 
@@ -4690,6 +4705,7 @@ void init_cfs_bandwidth(struct cfs_bandw
 	cfs_b->period_timer.function = sched_cfs_period_timer;
 	hrtimer_init(&cfs_b->slack_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	cfs_b->slack_timer.function = sched_cfs_slack_timer;
+	cfs_b->distribute_running = 0;
 }
 
 static void init_cfs_rq_runtime(struct cfs_rq *cfs_rq)
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -288,6 +288,8 @@ struct cfs_bandwidth {
 	/* statistics */
 	int nr_periods, nr_throttled;
 	u64 throttled_time;
+
+	bool distribute_running;
 #endif
 };
 

