Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from pop3.zju.edu.cn (124.160.105.205:110) by
  likexu-MOBL1.ccr.corp.intel.com with POP3; 09 Nov 2018 00:38:33 -0000
Received: from icoremail.net (unknown [209.85.210.173])
	by mail-app2 (Coremail) with SMTP id by_KCgDHH+6LsuRbXORiAQ--.30059S3;
	Fri, 09 Nov 2018 06:02:52 +0800 (CST)
Received: from mail-pf1-f173.google.com (unknown [209.85.210.173])
	by mx2.icoremail.net (Coremail) with SMTP id AQAAfwAX7zqGsuRbCdseAA--.8202S3;
	Fri, 09 Nov 2018 06:02:46 +0800 (CST)
Received: by mail-pf1-f173.google.com with SMTP id v9-v6so7623108pff.2
        for <xuliker@zju.edu.cn>; Thu, 08 Nov 2018 14:02:46 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:delivered-to:dkim-signature:from:to:cc:subject
         :date:message-id:in-reply-to:references:user-agent:mime-version
         :content-transfer-encoding:sender:precedence:list-id;
        bh=v7xPTY7k1TS3Rim3kOl2bEG8rMs1tOFIS8Ict+5VmNs=;
        b=YHKHoNzePxXc0sKYr/3XzkhwXhbnTCybtajTMBw8xb3+3ZRX0bKh9/7mJk/w1sBRK0
         DIx5eIJ+LGrvPGMFrLJQ98jtn5WmO/0dFMEETT1vNGuetgYYki3a94GUg6VCAWqT6poq
         oRpuc60bPiqlfzGRRDDI6hnro/idTphwr/H02Pv5ZB9cVbRWSaAjZae19bcpm/Xa7wSt
         7pS5u9ogIY6Ry+G2sGk/aS7hBRegqU6nYySwSdt1jxkT/eDQP3TLt2AHH9W4grP0GZQN
         YeWHx2GER/wqr8yGHvjiJ4TdWzgu3j6MxrniDfM11WGIiNiRmKiDWQZy2GUEqEqFob3+
         jyDQ==
X-Gm-Message-State: AGRZ1gJ4LscDRKGO6MaxIBx3Smo7ED69KpspkUBVZAOco0ObIAkjYsMZ
	I8RjREW1Dzs9yHMhzvo9KVfDpAECdd4CSaPDRFAZtMXcbl73r14UFQ==
X-Received: by 2002:a62:3687:: with SMTP id d129-v6mr6382763pfa.56.1541714566300;
        Thu, 08 Nov 2018 14:02:46 -0800 (PST)
X-Forwarded-To: xuliker@zju.edu.cn
X-Forwarded-For: liker.xu@gmail.com xuliker@zju.edu.cn
Delivered-To: liker.xu@gmail.com
Received: by 2002:a17:90a:c304:0:0:0:0 with SMTP id g4-v6csp782184pjt;
        Thu, 8 Nov 2018 14:02:45 -0800 (PST)
X-Google-Smtp-Source: AJdET5f5h3wxdJbqcqoqMm3cNXKsperHIywihqD80neqRc+9XHCc8B+HSRNGSotltRsT/Pee7A85
X-Received: by 2002:a65:55ca:: with SMTP id k10mr5173731pgs.448.1541714565392;
        Thu, 08 Nov 2018 14:02:45 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1541714565; cv=none;
        d=google.com; s=arc-20160816;
        b=wT2UaK4ATVWRrLS06xZlcCGSJqurx6CABIJ8EkffREyHXD39y6/fOZDb7fzmVNxvdc
         F7cI6vQxTnqm+NJBBqIwcKbuwZxKPo89gfTo0PrQNhJhT8THPVvs7d8Mym2PstR0BZBB
         ibU61LUphAnPNgmuE0jDSN/tzxyxVZvzH8+TjroU5/310MX4O6pdS5lVcG6cvCVM3xQ1
         qnhgFbuGUbZ513jy5pCnY3KDKBzINmBHhQsbcGjb9seveLIOYQylOUCt70e4UvYivJ1j
         QiSocyJiRcNScuQdfUFfa5kks7B2Ju2HRODx8T1s5ImcUVJH8SJ0vxc7pxt1u8Qu19ZB
         Ym6w==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=list-id:precedence:sender:content-transfer-encoding:mime-version
         :user-agent:references:in-reply-to:message-id:date:subject:cc:to
         :from:dkim-signature;
        bh=v7xPTY7k1TS3Rim3kOl2bEG8rMs1tOFIS8Ict+5VmNs=;
        b=EF2e2WBSgnxhpLpXW/cgyUAS6Y6fiyEsYWqFTV1/ur5TlRx3HcAjAib0TpSUimyyno
         Eoz02IR3a5RvFp5inHtMUxCoUBeUt7vpbT0T82qd3XhlLHfcdwfbAHUbjErPMmj+qPlE
         K0IW3mrYpSnibnemWePwVQF7nJ2NRUO+DSs7+TPsYbZuwA/qL0EMfJ4r8fJWthrvPQig
         SHFXw0Nt5Odu9Cai+anoIVMoMs2lbYKlrdWdhxDQIImgwGsyxObPm6Z4WAteuuo0GrXZ
         OnquhR8GMKK7Ba8rv5RQwheLTJ6WE/c2sGJlzPwZKuOKfjAHpKw2AGEzY+Y02TBS0uFw
         6jIg==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass header.i=@kernel.org header.s=default header.b=v11Mwng5;
       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org
Received: from vger.kernel.org (vger.kernel.org. [209.132.180.67])
        by mx.google.com with ESMTP id m75-v6si5087331pga.481.2018.11.08.14.02.29;
        Thu, 08 Nov 2018 14:02:45 -0800 (PST)
Received-SPF: pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) client-ip=209.132.180.67;
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1730936AbeKIHjR (ORCPT <rfc822;tu.k.phung@gmail.com>
        + 99 others); Fri, 9 Nov 2018 02:39:17 -0500
Received: from mail.kernel.org ([198.145.29.99]:58884 "EHLO mail.kernel.org"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1728072AbeKIHjQ (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Fri, 9 Nov 2018 02:39:16 -0500
Received: from localhost (unknown [208.72.13.198])
        (using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
        (No client certificate requested)
        by mail.kernel.org (Postfix) with ESMTPSA id 56B6820892;
        Thu,  8 Nov 2018 22:01:47 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=kernel.org;
        s=default; t=1541714507;
        bh=i8KdzySMB7Xm1pMbBc+3gCTrcDG4hTEkxX5cUMn4Uk4=;
        h=From:To:Cc:Subject:Date:In-Reply-To:References:From;
        b=v11Mwng5jPjycH05qJccOGPXYzfvoEaQFAzb06sHwcN9ysFPL1/z6L3xtyNJs5d7N
         H7yg5uhvfIfcgEtkBP+yZrKI3suAPQIXUBfjTbLCQkgJbsuP/YdwW2aARZoIgxW03Q
         BtE7iYijCYHNJD8EtXc3wYuSHu4KfljyRgIQ9fqk=
From: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
To: linux-kernel@vger.kernel.org
Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>,
        stable@vger.kernel.org, Phil Auld <pauld@redhat.com>,
        Ben Segall <bsegall@google.com>,
        Linus Torvalds <torvalds@linux-foundation.org>,
        Peter Zijlstra <peterz@infradead.org>,
        Thomas Gleixner <tglx@linutronix.de>,
        Ingo Molnar <mingo@kernel.org>
Subject: [PATCH 4.4 112/114] sched/fair: Fix throttle_list starvation with low CFS quota
Date: Thu,  8 Nov 2018 13:52:07 -0800
Message-Id: <20181108215110.870968850@linuxfoundation.org>
X-Mailer: git-send-email 2.19.1
In-Reply-To: <20181108215059.051093652@linuxfoundation.org>
References: <20181108215059.051093652@linuxfoundation.org>
User-Agent: quilt/0.65
X-stable: review
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
Sender: liker.xu+caf_=xuliker=zju.edu.cn@gmail.com
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-CM-TRANSID: AQAAfwAX7zqGsuRbCdseAA--.8202S3
Authentication-Results: mail-app2; spf=pass smtp.mail=liker.xu+caf_=xu
	liker=zju.edu.cn@gmail.com;
X-Coremail-Antispam: 1UD129KBjvJXoWxKryxZryUZw1fAFWxtF4fKrg_yoWxXr1kpF
	ZrW3y3Gw18Xws2qwnxArsxWayrW3yfJry7AryDG3WrZw45u343tF13Jr4IvF1YyFWFkF13
	tr18WrW3GFWYv3DanT9S1TB71UUUUUUqnTZGkaVYY2UrUUUU0bIjqfuFe4nvWSU8nxnvy2
	9KBjDU0xBIdaVrnRJUUUPIb7Iv0xC_Kw4lb4IE77IF4wAFF20E14v26r1j6r4UM7CIcVAF
	z4kK6r1j6r18M28lY4IEw2IIxxk0rwA2F7IY1VAKz4vEj48ve4kI8wA2z4x0Y4vE2Ix0cI
	8IcVAFwI0_tr0E3s1l84ACjcxK6xIIjxv20xvEc7CjxVAFwI0_Cr0_Gr1UM28EF7xvwVC2
	z280aVAFwI0_GcCE3s1l84ACjcxK6I8E87Iv6xkF7I0E14v26rxl6s0DM2AIxVAIcxkEcV
	Aq07x20xvEncxIr21l5I8CrVACY4xI64kE6c02F40Ex7xfMcIj6xIIjxv20xvE14v26r1Y
	6r17McIj6I8E87Iv67AKxVW8JVWxJwAm72CE4IkC6x0Yz7v_Jr0_Gr1lF7xvr2IYc2Ij64
	vIr41l7I0Y6sxI4wCY1x0264kExVAvwVAq07x20xylc7Ca8VAvwVCFzxkY4VCF77xAMxkI
	ecxEwVCI4VW3MxkI7II2jI8vz4vEwIxGrwCYIxAIcVC0I7IYx2IY67AKxVWDJVCq3wCYIx
	AIcVC0I7IYx2IY6xkF7I0E14v26F4j6r4UJwCYIxAIcVC2z280aVAFwI0_Cr1j6rxdMxvI
	42IY6I8E87Iv6xkF7I0E14v26F4UJVW0owCF04k20xvY0x0EwIxGrwCF04k20xvEw4C26c
	xK6c8Ij28IcwCF72vE77IF4wCFx2IqxVCFs4IE7xkEbVWUJVW8JwC20s026c02F40E14v2
	6r1j6r18MI8I3I0E7480Y4vE14v26r106r1rMI8E67AF67kF1VAFwI0_Jw0_GFylIxkGc2
	Ij64vIr41lIxAIcVCF04k26cxKx2IYs7xG6r1I6r4UYxBIdaVFxhVjvjDU0xZFpf9x07bd
	lksUUUUU=

4.4-stable review patch.  If anyone has any objections, please let me know.

------------------

From: Phil Auld <pauld@redhat.com>

commit baa9be4ffb55876923dc9716abc0a448e510ba30 upstream.

With a very low cpu.cfs_quota_us setting, such as the minimum of 1000,
distribute_cfs_runtime may not empty the throttled_list before it runs
out of runtime to distribute. In that case, due to the change from
c06f04c7048 to put throttled entries at the head of the list, later entries
on the list will starve.  Essentially, the same X processes will get pulled
off the list, given CPU time and then, when expired, get put back on the
head of the list where distribute_cfs_runtime will give runtime to the same
set of processes leaving the rest.

Fix the issue by setting a bit in struct cfs_bandwidth when
distribute_cfs_runtime is running, so that the code in throttle_cfs_rq can
decide to put the throttled entry on the tail or the head of the list.  The
bit is set/cleared by the callers of distribute_cfs_runtime while they hold
cfs_bandwidth->lock.

This is easy to reproduce with a handful of CPU consumers. I use 'crash' on
the live system. In some cases you can simply look at the throttled list and
see the later entries are not changing:

  crash> list cfs_rq.throttled_list -H 0xffff90b54f6ade40 -s cfs_rq.runtime_remaining | paste - - | awk '{print $1"  "$4}' | pr -t -n3
    1     ffff90b56cb2d200  -976050
    2     ffff90b56cb2cc00  -484925
    3     ffff90b56cb2bc00  -658814
    4     ffff90b56cb2ba00  -275365
    5     ffff90b166a45600  -135138
    6     ffff90b56cb2da00  -282505
    7     ffff90b56cb2e000  -148065
    8     ffff90b56cb2fa00  -872591
    9     ffff90b56cb2c000  -84687
   10     ffff90b56cb2f000  -87237
   11     ffff90b166a40a00  -164582

  crash> list cfs_rq.throttled_list -H 0xffff90b54f6ade40 -s cfs_rq.runtime_remaining | paste - - | awk '{print $1"  "$4}' | pr -t -n3
    1     ffff90b56cb2d200  -994147
    2     ffff90b56cb2cc00  -306051
    3     ffff90b56cb2bc00  -961321
    4     ffff90b56cb2ba00  -24490
    5     ffff90b166a45600  -135138
    6     ffff90b56cb2da00  -282505
    7     ffff90b56cb2e000  -148065
    8     ffff90b56cb2fa00  -872591
    9     ffff90b56cb2c000  -84687
   10     ffff90b56cb2f000  -87237
   11     ffff90b166a40a00  -164582

Sometimes it is easier to see by finding a process getting starved and looking
at the sched_info:

  crash> task ffff8eb765994500 sched_info
  PID: 7800   TASK: ffff8eb765994500  CPU: 16  COMMAND: "cputest"
    sched_info = {
      pcount = 8,
      run_delay = 697094208,
      last_arrival = 240260125039,
      last_queued = 240260327513
    },
  crash> task ffff8eb765994500 sched_info
  PID: 7800   TASK: ffff8eb765994500  CPU: 16  COMMAND: "cputest"
    sched_info = {
      pcount = 8,
      run_delay = 697094208,
      last_arrival = 240260125039,
      last_queued = 240260327513
    },

Signed-off-by: Phil Auld <pauld@redhat.com>
Reviewed-by: Ben Segall <bsegall@google.com>
Cc: Linus Torvalds <torvalds@linux-foundation.org>
Cc: Peter Zijlstra <peterz@infradead.org>
Cc: Thomas Gleixner <tglx@linutronix.de>
Cc: stable@vger.kernel.org
Fixes: c06f04c70489 ("sched: Fix potential near-infinite distribute_cfs_runtime() loop")
Link: http://lkml.kernel.org/r/20181008143639.GA4019@pauld.bos.csb
Signed-off-by: Ingo Molnar <mingo@kernel.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

---
 kernel/sched/fair.c  |   22 +++++++++++++++++++---
 kernel/sched/sched.h |    2 ++
 2 files changed, 21 insertions(+), 3 deletions(-)

--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -3624,9 +3624,13 @@ static void throttle_cfs_rq(struct cfs_r
 
 	/*
 	 * Add to the _head_ of the list, so that an already-started
-	 * distribute_cfs_runtime will not see us
+	 * distribute_cfs_runtime will not see us. If disribute_cfs_runtime is
+	 * not running add to the tail so that later runqueues don't get starved.
 	 */
-	list_add_rcu(&cfs_rq->throttled_list, &cfs_b->throttled_cfs_rq);
+	if (cfs_b->distribute_running)
+		list_add_rcu(&cfs_rq->throttled_list, &cfs_b->throttled_cfs_rq);
+	else
+		list_add_tail_rcu(&cfs_rq->throttled_list, &cfs_b->throttled_cfs_rq);
 
 	/*
 	 * If we're the first throttled task, make sure the bandwidth
@@ -3769,14 +3773,16 @@ static int do_sched_cfs_period_timer(str
 	 * in us over-using our runtime if it is all used during this loop, but
 	 * only by limited amounts in that extreme case.
 	 */
-	while (throttled && cfs_b->runtime > 0) {
+	while (throttled && cfs_b->runtime > 0 && !cfs_b->distribute_running) {
 		runtime = cfs_b->runtime;
+		cfs_b->distribute_running = 1;
 		raw_spin_unlock(&cfs_b->lock);
 		/* we can't nest cfs_b->lock while distributing bandwidth */
 		runtime = distribute_cfs_runtime(cfs_b, runtime,
 						 runtime_expires);
 		raw_spin_lock(&cfs_b->lock);
 
+		cfs_b->distribute_running = 0;
 		throttled = !list_empty(&cfs_b->throttled_cfs_rq);
 
 		cfs_b->runtime -= min(runtime, cfs_b->runtime);
@@ -3887,6 +3893,11 @@ static void do_sched_cfs_slack_timer(str
 
 	/* confirm we're still not at a refresh boundary */
 	raw_spin_lock(&cfs_b->lock);
+	if (cfs_b->distribute_running) {
+		raw_spin_unlock(&cfs_b->lock);
+		return;
+	}
+
 	if (runtime_refresh_within(cfs_b, min_bandwidth_expiration)) {
 		raw_spin_unlock(&cfs_b->lock);
 		return;
@@ -3896,6 +3907,9 @@ static void do_sched_cfs_slack_timer(str
 		runtime = cfs_b->runtime;
 
 	expires = cfs_b->runtime_expires;
+	if (runtime)
+		cfs_b->distribute_running = 1;
+
 	raw_spin_unlock(&cfs_b->lock);
 
 	if (!runtime)
@@ -3906,6 +3920,7 @@ static void do_sched_cfs_slack_timer(str
 	raw_spin_lock(&cfs_b->lock);
 	if (expires == cfs_b->runtime_expires)
 		cfs_b->runtime -= min(runtime, cfs_b->runtime);
+	cfs_b->distribute_running = 0;
 	raw_spin_unlock(&cfs_b->lock);
 }
 
@@ -4017,6 +4032,7 @@ void init_cfs_bandwidth(struct cfs_bandw
 	cfs_b->period_timer.function = sched_cfs_period_timer;
 	hrtimer_init(&cfs_b->slack_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	cfs_b->slack_timer.function = sched_cfs_slack_timer;
+	cfs_b->distribute_running = 0;
 }
 
 static void init_cfs_rq_runtime(struct cfs_rq *cfs_rq)
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -233,6 +233,8 @@ struct cfs_bandwidth {
 	/* statistics */
 	int nr_periods, nr_throttled;
 	u64 throttled_time;
+
+	bool distribute_running;
 #endif
 };
 

