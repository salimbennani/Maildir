Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from pop3.zju.edu.cn (124.160.105.205:110) by
  likexu-MOBL1.ccr.corp.intel.com with POP3; 09 Nov 2018 00:39:39 -0000
Received: from icoremail.net (unknown [209.85.215.175])
	by mail-app2 (Coremail) with SMTP id by_KCgD3__u+uuRb9A1jAQ--.30248S3;
	Fri, 09 Nov 2018 06:37:50 +0800 (CST)
Received: from mail-pg1-f175.google.com (unknown [209.85.215.175])
	by mx2.icoremail.net (Coremail) with SMTP id AQAAfwCXbDi7uuRbw_seAA--.2795S3;
	Fri, 09 Nov 2018 06:37:47 +0800 (CST)
Received: by mail-pg1-f175.google.com with SMTP id k1-v6so9488338pgq.1
        for <xuliker@zju.edu.cn>; Thu, 08 Nov 2018 14:37:47 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:delivered-to:dkim-signature:from:to:cc:subject
         :date:message-id:in-reply-to:references:user-agent:mime-version
         :content-transfer-encoding:sender:precedence:list-id;
        bh=uc60FHquEdDiBBE/inZh8cLxGtESOSUEixataf4YS3c=;
        b=D45oV0LCo/Blro7D8Ghn4QBOI/d8XV+yaBZS11D99FQq52N14mcGFIIAKkhLcUbNEl
         5tduRa5c8acfLTbN4F7UtJrU8SNVF5S2grbByfSnynMY9z8Zl5JfFCHS440s3xFTHFAM
         mZGdb8/ezsPM0dFqfzl1rU99+9YRWVlxGDdNcR6yXkAzzGMvSbQMiYc+RzZZEg+9/QNo
         hp0nYVLGRWy8JfjjyNbZQnjs28Y5BDO5/xMLj0CkaEUwHBxBguueuRB8n6PtctZE1E0q
         6VzrjPAEOfFq24LA9Si6JKdFx7vztY5cUoZA7aF5Ly0zB1FGlUXucH+fYYG5490CoAaf
         v7Wg==
X-Gm-Message-State: AGRZ1gIEYdfl92DbPAziwCZG9PzGOAawLO4TLTsC7Ql5m99e7EzTAVNg
	QpuuAe6OpXj5NyctyS0Uj3O52CW1nIDCDiT19Ic+/c3ngdZveoOyww==
X-Received: by 2002:a63:460a:: with SMTP id t10-v6mr5363682pga.197.1541716667204;
        Thu, 08 Nov 2018 14:37:47 -0800 (PST)
X-Forwarded-To: xuliker@zju.edu.cn
X-Forwarded-For: liker.xu@gmail.com xuliker@zju.edu.cn
Delivered-To: liker.xu@gmail.com
Received: by 2002:a17:90a:c304:0:0:0:0 with SMTP id g4-v6csp813316pjt;
        Thu, 8 Nov 2018 14:37:46 -0800 (PST)
X-Google-Smtp-Source: AJdET5csdxkGPjlwolzsDWPZ0yqlCAwQAhmwnyzyUPZnu5LiEoEhCFqxlgI/asQuzvNt5WRlgT1w
X-Received: by 2002:a17:902:15c5:: with SMTP id a5-v6mr6338120plh.136.1541716666321;
        Thu, 08 Nov 2018 14:37:46 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1541716666; cv=none;
        d=google.com; s=arc-20160816;
        b=V5hxmVGAIwxoAmX8254x1czyx0k+iDSPhJsdwiCvFG/+aYwJVqOYuncYrLleOCs4Ow
         4XsYyeFdGFsvPe3cJZVRvcu17NeuxQy1VzA9oWClEmIrh8393eLYfL2hkH2DZrpgNXo3
         XmpqP+7iwA6knnVmvbKiFykeC2VXCB+Dbo9fCqS86RFAT0onMmL4dGPvYCw0X9ilpsvd
         W7F4dJrFnFLzQnBTvt0/UVPx6U2ZrpfamXzU1nXeouNlkeMyj8Lmi30+rABakFcaA2Eh
         bck1MJgBmymiyhQJ+WThvymYk7L5qZRk4vrfNr0KWzUL4CF/Z4vUEu/23gRlfMt3WP6n
         b7RA==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=list-id:precedence:sender:content-transfer-encoding:mime-version
         :user-agent:references:in-reply-to:message-id:date:subject:cc:to
         :from:dkim-signature;
        bh=uc60FHquEdDiBBE/inZh8cLxGtESOSUEixataf4YS3c=;
        b=o9xbfMogfErehQNtQdHu5tnfVhLBrmDt7jxp0tjQLttal05udU1w9kMtxgzQ/Ec2UM
         Gii4f1KBrlh2gGVmWfpPeE+Rn5RMOTYj5Ht0tL3OpdwRhMC/ITmOKTNBEmc/aJki1O2c
         ccOrXC+sG3adW+L96wWGOwyioCsX0PFHBpyW7YysHJwjeqUF6vprG205qlM9MtH6LpbK
         Jfl8Y0gx564yWWxHcQ1SPpQmx/oUfYrDrT8hw+WeVs/EC4Q1lFgyjmc62m84lIpoFqZL
         sEtLAz9ui/1vmGwJvA2kKMrtdSMSyESLLrY/nWHNLDfi4HwJ6qfddpBisbyForw4Wp5k
         u6eg==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass header.i=@kernel.org header.s=default header.b=0JNGsumu;
       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org
Received: from vger.kernel.org (vger.kernel.org. [209.132.180.67])
        by mx.google.com with ESMTP id w65-v6si6197015pfd.55.2018.11.08.14.37.31;
        Thu, 08 Nov 2018 14:37:46 -0800 (PST)
Received-SPF: pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) client-ip=209.132.180.67;
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1729730AbeKIHgB (ORCPT <rfc822;tu.k.phung@gmail.com>
        + 99 others); Fri, 9 Nov 2018 02:36:01 -0500
Received: from mail.kernel.org ([198.145.29.99]:53072 "EHLO mail.kernel.org"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1727508AbeKIHgA (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Fri, 9 Nov 2018 02:36:00 -0500
Received: from localhost (unknown [208.72.13.198])
        (using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
        (No client certificate requested)
        by mail.kernel.org (Postfix) with ESMTPSA id 806F120818;
        Thu,  8 Nov 2018 21:58:31 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=kernel.org;
        s=default; t=1541714311;
        bh=pU3bcGPafiFiE6CAYEQF5OPrUeLAkKXEcaePp/4J+fM=;
        h=From:To:Cc:Subject:Date:In-Reply-To:References:From;
        b=0JNGsumu9gvjNOAEkoANTWw9oIDXm8nCzGI1fH79gLTMMCOpdQNCKEplnWXogRgg0
         7WEAkvkv/RyfjY6OR21zO+wIQ9ayHj7Xeo9mN3GMheoohMf9zSE5x3WoV2gK2FXB9t
         RmHJygspjdYPIv1fQ4Pa0QsvM/qh1FR1sWR50edw=
From: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
To: linux-kernel@vger.kernel.org
Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>,
        stable@vger.kernel.org, Phil Auld <pauld@redhat.com>,
        Ben Segall <bsegall@google.com>,
        Linus Torvalds <torvalds@linux-foundation.org>,
        Peter Zijlstra <peterz@infradead.org>,
        Thomas Gleixner <tglx@linutronix.de>,
        Ingo Molnar <mingo@kernel.org>
Subject: [PATCH 3.18 144/144] sched/fair: Fix throttle_list starvation with low CFS quota
Date: Thu,  8 Nov 2018 13:51:55 -0800
Message-Id: <20181108215108.639467582@linuxfoundation.org>
X-Mailer: git-send-email 2.19.1
In-Reply-To: <20181108215054.826084593@linuxfoundation.org>
References: <20181108215054.826084593@linuxfoundation.org>
User-Agent: quilt/0.65
X-stable: review
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
Sender: liker.xu+caf_=xuliker=zju.edu.cn@gmail.com
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-CM-TRANSID: AQAAfwCXbDi7uuRbw_seAA--.2795S3
Authentication-Results: mail-app2; spf=pass smtp.mail=liker.xu+caf_=xu
	liker=zju.edu.cn@gmail.com;
X-Coremail-Antispam: 1UD129KBjvJXoWxKryxZryUZw1fAFWxtF4fKrg_yoWxXFWfpF
	ZrW3y3Gw18Xws2qrn8ArsxWayrW3yfJry7CryDG3WrZw45u343tF13Jr4IvF1YyFWFkF13
	tr18WrW3GrWYv3DanT9S1TB71UUUUjUqnTZGkaVYY2UrUUUU0bIjqfuFe4nvWSU8nxnvy2
	9KBjDU0xBIdaVrnRJUUUm2b7Iv0xC_Kw4lb4IE77IF4wAFF20E14v26r1j6r4UM7CIcVAF
	z4kK6r1j6r18M28lY4IEw2IIxxk0rwA2F7IY1VAKz4vEj48ve4kI8wA2z4x0Y4vE2Ix0cI
	8IcVAFwI0_tr0E3s1l84ACjcxK6xIIjxv20xvEc7CjxVAFwI0_Gr1j6F4UJwA2z4x0Y4vE
	x4A2jsIE14v26rxl6s0DM28EF7xvwVC2z280aVCY1x0267AKxVW0oVCq3wAaw2AFwI0_Jr
	v_JF1le2I262IYc4CY6c8Ij28IcVAaY2xG8wAqx4xG64xvF2IEw4CE5I8CrVC2j2WlYx0E
	2Ix0cI8IcVAFwI0_Jrv_JF1lYx0Ex4A2jsIE14v26r4j6F4UMcvjeVCFs4IE7xkEbVWUJV
	W8JwACjcxG0xvY0x0EwIxGrwCjxxvEa2IrMxkF7I0Ew4C26cxK6c8Ij28IcwCY1Ik26cxK
	6xAEc7vF6xCjj44lc2xSY4AK6IIF6Fylc2IjII80xcxEwVAKI48JMxvI42IY6xIIjxv20x
	vE14v26w1j6s0DMxvI42IY6xIIjxv20xvEc7CjxVAFwI0_Gr1j6F4UJwCYIxAIcVC2z280
	aVAFwI0_Cr1j6rxdMxvI42IY6I8E87Iv6xkF7I0E14v26F4UJVW0owCF04k20xvY0x0EwI
	xGrwCF04k20xvEw4C26cxK6c8Ij28IcwCF72vE77IF4wCFx2IqxVCFs4IE7xkEbVWUJVW8
	JwC20s026c02F40E14v26r1j6r18MI8I3I0E7480Y4vE14v26r106r1rMI8E67AF67kF1V
	AFwI0_Jw0_GFylIxkGc2Ij64vIr41lIxAIcVCF04k26cxKx2IYs7xG6r1j6r1xYxBIdaVF
	xhVjvjDU0xZFpf9x07bLHqcUUUUU=

3.18-stable review patch.  If anyone has any objections, please let me know.

------------------

From: Phil Auld <pauld@redhat.com>

commit baa9be4ffb55876923dc9716abc0a448e510ba30 upstream.

With a very low cpu.cfs_quota_us setting, such as the minimum of 1000,
distribute_cfs_runtime may not empty the throttled_list before it runs
out of runtime to distribute. In that case, due to the change from
c06f04c7048 to put throttled entries at the head of the list, later entries
on the list will starve.  Essentially, the same X processes will get pulled
off the list, given CPU time and then, when expired, get put back on the
head of the list where distribute_cfs_runtime will give runtime to the same
set of processes leaving the rest.

Fix the issue by setting a bit in struct cfs_bandwidth when
distribute_cfs_runtime is running, so that the code in throttle_cfs_rq can
decide to put the throttled entry on the tail or the head of the list.  The
bit is set/cleared by the callers of distribute_cfs_runtime while they hold
cfs_bandwidth->lock.

This is easy to reproduce with a handful of CPU consumers. I use 'crash' on
the live system. In some cases you can simply look at the throttled list and
see the later entries are not changing:

  crash> list cfs_rq.throttled_list -H 0xffff90b54f6ade40 -s cfs_rq.runtime_remaining | paste - - | awk '{print $1"  "$4}' | pr -t -n3
    1     ffff90b56cb2d200  -976050
    2     ffff90b56cb2cc00  -484925
    3     ffff90b56cb2bc00  -658814
    4     ffff90b56cb2ba00  -275365
    5     ffff90b166a45600  -135138
    6     ffff90b56cb2da00  -282505
    7     ffff90b56cb2e000  -148065
    8     ffff90b56cb2fa00  -872591
    9     ffff90b56cb2c000  -84687
   10     ffff90b56cb2f000  -87237
   11     ffff90b166a40a00  -164582

  crash> list cfs_rq.throttled_list -H 0xffff90b54f6ade40 -s cfs_rq.runtime_remaining | paste - - | awk '{print $1"  "$4}' | pr -t -n3
    1     ffff90b56cb2d200  -994147
    2     ffff90b56cb2cc00  -306051
    3     ffff90b56cb2bc00  -961321
    4     ffff90b56cb2ba00  -24490
    5     ffff90b166a45600  -135138
    6     ffff90b56cb2da00  -282505
    7     ffff90b56cb2e000  -148065
    8     ffff90b56cb2fa00  -872591
    9     ffff90b56cb2c000  -84687
   10     ffff90b56cb2f000  -87237
   11     ffff90b166a40a00  -164582

Sometimes it is easier to see by finding a process getting starved and looking
at the sched_info:

  crash> task ffff8eb765994500 sched_info
  PID: 7800   TASK: ffff8eb765994500  CPU: 16  COMMAND: "cputest"
    sched_info = {
      pcount = 8,
      run_delay = 697094208,
      last_arrival = 240260125039,
      last_queued = 240260327513
    },
  crash> task ffff8eb765994500 sched_info
  PID: 7800   TASK: ffff8eb765994500  CPU: 16  COMMAND: "cputest"
    sched_info = {
      pcount = 8,
      run_delay = 697094208,
      last_arrival = 240260125039,
      last_queued = 240260327513
    },

Signed-off-by: Phil Auld <pauld@redhat.com>
Reviewed-by: Ben Segall <bsegall@google.com>
Cc: Linus Torvalds <torvalds@linux-foundation.org>
Cc: Peter Zijlstra <peterz@infradead.org>
Cc: Thomas Gleixner <tglx@linutronix.de>
Cc: stable@vger.kernel.org
Fixes: c06f04c70489 ("sched: Fix potential near-infinite distribute_cfs_runtime() loop")
Link: http://lkml.kernel.org/r/20181008143639.GA4019@pauld.bos.csb
Signed-off-by: Ingo Molnar <mingo@kernel.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

---
 kernel/sched/fair.c  |   23 ++++++++++++++++++++---
 kernel/sched/sched.h |    2 ++
 2 files changed, 22 insertions(+), 3 deletions(-)

--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -3416,9 +3416,14 @@ static void throttle_cfs_rq(struct cfs_r
 	raw_spin_lock(&cfs_b->lock);
 	/*
 	 * Add to the _head_ of the list, so that an already-started
-	 * distribute_cfs_runtime will not see us
+	 * distribute_cfs_runtime will not see us. If disribute_cfs_runtime is
+	 * not running add to the tail so that later runqueues don't get starved.
 	 */
-	list_add_rcu(&cfs_rq->throttled_list, &cfs_b->throttled_cfs_rq);
+	if (cfs_b->distribute_running)
+		list_add_rcu(&cfs_rq->throttled_list, &cfs_b->throttled_cfs_rq);
+	else
+		list_add_tail_rcu(&cfs_rq->throttled_list, &cfs_b->throttled_cfs_rq);
+
 	if (!cfs_b->timer_active)
 		__start_cfs_bandwidth(cfs_b, false);
 	raw_spin_unlock(&cfs_b->lock);
@@ -3562,14 +3567,16 @@ static int do_sched_cfs_period_timer(str
 	 * in us over-using our runtime if it is all used during this loop, but
 	 * only by limited amounts in that extreme case.
 	 */
-	while (throttled && cfs_b->runtime > 0) {
+	while (throttled && cfs_b->runtime > 0 && !cfs_b->distribute_running) {
 		runtime = cfs_b->runtime;
+		cfs_b->distribute_running = 1;
 		raw_spin_unlock(&cfs_b->lock);
 		/* we can't nest cfs_b->lock while distributing bandwidth */
 		runtime = distribute_cfs_runtime(cfs_b, runtime,
 						 runtime_expires);
 		raw_spin_lock(&cfs_b->lock);
 
+		cfs_b->distribute_running = 0;
 		throttled = !list_empty(&cfs_b->throttled_cfs_rq);
 
 		cfs_b->runtime -= min(runtime, cfs_b->runtime);
@@ -3680,6 +3687,11 @@ static void do_sched_cfs_slack_timer(str
 
 	/* confirm we're still not at a refresh boundary */
 	raw_spin_lock(&cfs_b->lock);
+	if (cfs_b->distribute_running) {
+		raw_spin_unlock(&cfs_b->lock);
+		return;
+	}
+
 	if (runtime_refresh_within(cfs_b, min_bandwidth_expiration)) {
 		raw_spin_unlock(&cfs_b->lock);
 		return;
@@ -3689,6 +3701,9 @@ static void do_sched_cfs_slack_timer(str
 		runtime = cfs_b->runtime;
 
 	expires = cfs_b->runtime_expires;
+	if (runtime)
+		cfs_b->distribute_running = 1;
+
 	raw_spin_unlock(&cfs_b->lock);
 
 	if (!runtime)
@@ -3699,6 +3714,7 @@ static void do_sched_cfs_slack_timer(str
 	raw_spin_lock(&cfs_b->lock);
 	if (expires == cfs_b->runtime_expires)
 		cfs_b->runtime -= min(runtime, cfs_b->runtime);
+	cfs_b->distribute_running = 0;
 	raw_spin_unlock(&cfs_b->lock);
 }
 
@@ -3790,6 +3806,7 @@ void init_cfs_bandwidth(struct cfs_bandw
 	cfs_b->period_timer.function = sched_cfs_period_timer;
 	hrtimer_init(&cfs_b->slack_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	cfs_b->slack_timer.function = sched_cfs_slack_timer;
+	cfs_b->distribute_running = 0;
 }
 
 static void init_cfs_rq_runtime(struct cfs_rq *cfs_rq)
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -202,6 +202,8 @@ struct cfs_bandwidth {
 	/* statistics */
 	int nr_periods, nr_throttled;
 	u64 throttled_time;
+
+	bool distribute_running;
 #endif
 };
 

