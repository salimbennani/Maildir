Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 07 Dec 2018 08:49:19 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga002.jf.intel.com (orsmga002.jf.intel.com [10.7.209.21])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 12A42580443;
	Thu,  6 Dec 2018 07:37:40 -0800 (PST)
Received: from fmsmga103.fm.intel.com ([10.1.193.90])
  by orsmga002-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 06 Dec 2018 07:37:39 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3A+BijARGLL6SOqs1asOqmBZ1GYnF86YWxBRYc798d?=
 =?us-ascii?q?s5kLTJ75os+4bnLW6fgltlLVR4KTs6sC17KG9fi4EUU7or+5+EgYd5JNUxJXwe?=
 =?us-ascii?q?43pCcHRPC/NEvgMfTxZDY7FskRHHVs/nW8LFQHUJ2mPw6arXK99yMdFQviPgRp?=
 =?us-ascii?q?OOv1BpTSj8Oq3Oyu5pHfeQpFiCa+bL9oMBm6sRjau9ULj4dlNqs/0AbCrGFSe+?=
 =?us-ascii?q?RRy2NoJFaTkAj568yt4pNt8Dletuw4+cJYXqr0Y6o3TbpDDDQ7KG81/9HktQPC?=
 =?us-ascii?q?TQSU+HQRVHgdnwdSDAjE6BH6WYrxsjf/u+Fg1iSWIdH6QLYpUjm58axlVAHnhz?=
 =?us-ascii?q?sGNz4h8WHYlMpwjL5AoBm8oxBz2pPYbJ2JOPZ7eK7WYNEUSndbXstJVyJPHJ6y?=
 =?us-ascii?q?YYUMAeQGP+lYoYbyqVQVrRumBwShH//jxzxSi3Pqx6A2z/gtHAfb1wIgBdIOt3?=
 =?us-ascii?q?HUoc3rOacSS+C1zbLIzSnEb/NO2Tf29YrGfQ4lofGIU7NwcMXRwlQoGgPFklqQ?=
 =?us-ascii?q?qZfoPzeO1uQRqWSU8vRvWPuphmU6qA9xuiCiytkwhoTNnI4Z117J+TtjzIooJt?=
 =?us-ascii?q?C0VFR3bN+mHZdIuSyXNJF6Tt48T2xpoio216AKtYChcCQXxpkqxBjSYOGdfYeS?=
 =?us-ascii?q?+BLsTuORLC94hH17fLK/gA6/8VavyuLiTMm4ylVKoTRfktnKqH8N0wbf6s+dSv?=
 =?us-ascii?q?ty5kuh2DCP2B7P6uxcP0w4ia7WJ4Q8zrM+iJYfq1nPEyzqlEnsjaKbdF0o+u2y?=
 =?us-ascii?q?5OTmZrXmqIWcN4hxigzmNqQum8q/Afk3MwQXXGiU5/681Lv98k39WblKifM3kq?=
 =?us-ascii?q?/Hv5DAPskbuKG5DBFP0oo56BawES2m0NIGknkDNl5FfwiHj4fxN1HUPP/4Feu/?=
 =?us-ascii?q?g0irkDpzw/DGP77hDYvXInnMjbfsZrJ9609ayAouwtFT/ZNUCrcdIP3tXk/9rs?=
 =?us-ascii?q?DXDhg8Mwas2eboFM191p8CWWKIGqKZMLndsV6U5u01JOmAfo8VuCvnJPgj6P7j?=
 =?us-ascii?q?lns5mV4bfam00pobcnG4HvJ6I0qHZXrgmMsOEWAPvgAmVuzllEWCUSJPZ3a1R6?=
 =?us-ascii?q?885DY7B5y8AYfAQYCthrqB3CCgE51SZ2BGDE2MEHjye4WFXfcMdDydIst7njMY?=
 =?us-ascii?q?UrihTpcr1Quyuw/i17pnMu3U9zUCupL41Nh14O7TmQso9TNuDcSQyGWNT2Bynm?=
 =?us-ascii?q?MVSD46xqF/oUphylid1ah0meBXFdtW5/lRSAc1KYbcz/BmC9D1Qg/Bfc2GSFC8?=
 =?us-ascii?q?TtWiADE+VNQxw9AVbkZ5GtWiiA3D3iWwD78UkbyLGII78qbG03ftIMZ9zm7M1L?=
 =?us-ascii?q?M9gFk+XstPKWqmi7Zi+AfJGY7GjV+Vl6aweqQaxy7C6mGDwW2KvEFbVQ5wVb7I?=
 =?us-ascii?q?XXQeZkvQsNT46VnOT76oCbQ7LARBzdSOJbdNat3slV9GXuvsOMzCY2KtnGe9HQ?=
 =?us-ascii?q?uHyamSbIX0YWkd3D/SCE4fkw8N+3aLLgw+Biano2LDAz1iD1PvY0Xw8eZgrHO3?=
 =?us-ascii?q?VFM7zwaPb0d5zbq65gYVheCAS/MUxr8EpCYhqzBzHFah39LXC8CMpxZ7cKVbe9?=
 =?us-ascii?q?M95FZH1WTWtwFmOpygLqZihkMRcghtvkPu0Ql3BZtEkcQwsHwqyw9yI7qC0Fxd?=
 =?us-ascii?q?bzOYwYzwOrrPJ2bo+BCgdaHX1U/e0dqM4agP9ek3pE/lvAGqEUoi7W5q091U03?=
 =?us-ascii?q?ua+5XLAxAeUZP3UkYr6Rd6o6vWbTU654PRzXdsK7W7sife29I1A+so0hahf8pF?=
 =?us-ascii?q?PKyYDgPzEs0aCNKoKOwlgFWpahMEPOZP9K87Jc+mdv2G2LK1M+Zkhj6pkWNH4I?=
 =?us-ascii?q?Vl2EKW6yV8UvLI34oCw/yAxAuHVivzg027ss/qnoBIfzcSEXSlySjlHYJeerd9?=
 =?us-ascii?q?fYIWBmiwOc23wdN+ioXpW35Z8l6jGlwH1NWoeRqUc1zywwlQ2V4LrnygnCuy1y?=
 =?us-ascii?q?Z0nC0xrqqDwCzOxPzvdRoGOmJRRGhul0zsIZWyj90BWEiobg4plAaq5Ergxqhb?=
 =?us-ascii?q?orh/IHfXQUtSYyf2KGRiWLOqtrWee85P9I8osSJPXeS+e1+aUL39oxgd0y/5BG?=
 =?us-ascii?q?tR3jM7dzKrupX/gRN6jnmQLHJyrHrfZMFxyg3T5N3aRf5NwDUGQDN0hiXQBli5?=
 =?us-ascii?q?J9Op58mbl4/fsuCiUGKsTp1SfjPszY+atiu75GtqDAa7n/CynN3nDAc73TX619?=
 =?us-ascii?q?lsSSXHshL8bpP32KS9NOJtZlNoC0Pk68pmBoF+lZM9hJIK1ngbnJmV/WcHnn31?=
 =?us-ascii?q?MdVUwq/+aHsNRTgWw9/a+gTl2UtjLm6XyIL9THmS3sxhZ9yiaGMMxi0999xKCL?=
 =?us-ascii?q?uT7LFcmCt1o1m4ohjLbflzgDgd0ucu52AAg+4SpgUt1CqdD6sWHUlZOyzsihuJ?=
 =?us-ascii?q?48q/rKVReGagb7yw2FBiktCmCbGIuhtcV2rhepc+AS9w6d1yME7L0H328I3lec?=
 =?us-ascii?q?PfbdQOth2PiBfAjvNYKJYwlvoMmCpmNnjxvXwjy+4nkxNu2Yu2s5SAK2Vo5Ki5?=
 =?us-ascii?q?GAJXNiXpZ8MP/THglb1RkdyR34CrA5VtADELXIbzQPKsETISs+nnNgmUHD09rH?=
 =?us-ascii?q?ebBaTQHQuF5Eh6qHLPFoihN2uLK3kB0dViWB6dKVRdgA8OWTU1gIU5Ghq2xMD7?=
 =?us-ascii?q?c0d5+zMR5kP+qhRW0eJlLB3/UmbZpAe1ZTY4UpmfLBxK7g5c40fZK9CR7uV2Hy?=
 =?us-ascii?q?tA5J2usBSNKnCHZwRPFWwIWkuEB036Prmz/9bA9fKUBvG5L/vIbrWDsuheV/aO?=
 =?us-ascii?q?xZKy3Ypq5TeMNsOTPnZ8C/03wFZMXXd8G87BgTUAVzQXlz7Rb86cvBq8+jN4rs?=
 =?us-ascii?q?G88PToWQLj/4iPC6FVMdVg5R+2m7qDN/WLiSZ9KDZY0I4MxHDSxLge2l4Slz9h?=
 =?us-ascii?q?dz23Hbscsi7NSbramrVLAB4DdyNzKMxI4rom0QlQIsHbkM36175igv4xCldITl?=
 =?us-ascii?q?jhmsCvZcwXLGCxLlLHBECXNLuYIT3H2d34YaS5SbdIluVbqwWwuSqHE0/kJjmD?=
 =?us-ascii?q?izjpVxW1Pe5Qli2UIBxet5+7cht2DWjjTdTmagC0MdNtjD02x6E0iW3ONWIGLT?=
 =?us-ascii?q?d8dEZNpKWK7SxEmvV/B3BB7n19IOiEgSmZ6ezYKpcQsfRzAyV0l/hV4HI1y7ZO?=
 =?us-ascii?q?6CFERfp1mDbdr9J0olGmlPWPxSRjUBZUtjlLg4eL7g1ePvDY8ZxFRF7A/QkR9i?=
 =?us-ascii?q?OUChIXt55rDcDpt6lMy9/J0qXpJ3MK19PS4NBUO8/ONsuDPGFpZQbuAiDdCAce?=
 =?us-ascii?q?ZTqqM3zPwkJajfee/2GUqZ58rYLjzt5GaLJATFEvXt8TDEtsFcYOaL1tWS4pi/?=
 =?us-ascii?q?bPhtQP+HekhAPcSMVTotbMUffEUtv1LzPMprleZgAUxqv4ZaQeMonk2kMqPllx?=
 =?us-ascii?q?nKzOAFaWUd0b8X4pVRM9vEgYqCs2dWY0wU+wL1r1uHI=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AhAABdQQlch0O0hNFkHAEBAQQBAQcEA?=
 =?us-ascii?q?QGBUwUBAQsBg2snmCFQAQEGgUmJDI4ugWwzEwGHVyI2Bw0BAwEBAQEBAQIBEwE?=
 =?us-ascii?q?BAQgNCQgpIwyCNiQBgmIDAwECJAsBDQEBNwEFCQEBHDQDRg4GARIFgxyBdQ0DA?=
 =?us-ascii?q?qUFgWwzgnYBAQWHIwiHcIQvF3iBB45/iROHIpAsCYEVkCYjiXKHQwGJDpFLB4F?=
 =?us-ascii?q?/MxoIHBSDJ4IbDBeIXoVgHzKBAgMBASETigYBAQ?=
X-IPAS-Result: =?us-ascii?q?A0AhAABdQQlch0O0hNFkHAEBAQQBAQcEAQGBUwUBAQsBg2s?=
 =?us-ascii?q?nmCFQAQEGgUmJDI4ugWwzEwGHVyI2Bw0BAwEBAQEBAQIBEwEBAQgNCQgpIwyCN?=
 =?us-ascii?q?iQBgmIDAwECJAsBDQEBNwEFCQEBHDQDRg4GARIFgxyBdQ0DAqUFgWwzgnYBAQW?=
 =?us-ascii?q?HIwiHcIQvF3iBB45/iROHIpAsCYEVkCYjiXKHQwGJDpFLB4F/MxoIHBSDJ4IbD?=
 =?us-ascii?q?BeIXoVgHzKBAgMBASETigYBAQ?=
X-IronPort-AV: E=Sophos;i="5.56,322,1539673200"; 
   d="scan'208";a="54820780"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 06 Dec 2018 07:37:38 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726184AbeLFPhf (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Thu, 6 Dec 2018 10:37:35 -0500
Received: from bombadil.infradead.org ([198.137.202.133]:35530 "EHLO
        bombadil.infradead.org" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726032AbeLFPhe (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Thu, 6 Dec 2018 10:37:34 -0500
DKIM-Signature: v=1; a=rsa-sha256; q=dns/txt; c=relaxed/relaxed;
        d=infradead.org; s=bombadil.20170209; h=Content-Transfer-Encoding:
        MIME-Version:References:In-Reply-To:Message-Id:Date:Subject:Cc:To:From:Sender
        :Reply-To:Content-Type:Content-ID:Content-Description:Resent-Date:Resent-From
        :Resent-Sender:Resent-To:Resent-Cc:Resent-Message-ID:List-Id:List-Help:
        List-Unsubscribe:List-Subscribe:List-Post:List-Owner:List-Archive;
        bh=r2tCfy0gv7xX8wChvI6rLRILuCXpaIpp+3rmaZAl7QI=; b=cyPAP4gz+4vJOlYc75qFExBksH
        13Nvyhff2v3TvR4ZBnk/LOBTNY+vmKOTdqo3gAEMy4Qr4q1R5IXmbYxH0Ry227AySkJKFIgiBSRTh
        3s9jp7MtoKJKwHAqkuFshwcy/G/PWl1OC9upwOWdGK5MADVJqVhxypEarWiGEcAfywTWOISXzT1Py
        8H/xnl4vAldkl5yIl6E3ehWHXmrtdGGsEIsq+NT8tyLkDfq9skF6otUSmdB/9LwqCoySdgn0rX2GE
        yEBlZrIuOjllZUxNFfwGt8HfBM74nYcFPCGA0y7r7Pxl5jRFoDXUc69ggE7dWLG2DdMkeFfGp6us4
        uIFzdOcQ==;
Received: from [38.126.112.138] (helo=localhost)
        by bombadil.infradead.org with esmtpsa (Exim 4.90_1 #2 (Red Hat Linux))
        id 1gUviQ-0000nY-Hk; Thu, 06 Dec 2018 15:37:26 +0000
From: Christoph Hellwig <hch@lst.de>
To: iommu@lists.linux-foundation.org,
        Linus Torvalds <torvalds@linux-foundation.org>,
        Jesper Dangaard Brouer <brouer@redhat.com>
Cc: Tariq Toukan <tariqt@mellanox.com>,
        Ilias Apalodimas <ilias.apalodimas@linaro.org>,
        =?UTF-8?q?Toke=20H=C3=B8iland-J=C3=B8rgensen?= <toke@toke.dk>,
        Robin Murphy <robin.murphy@arm.com>,
        linux-kernel@vger.kernel.org
Subject: [PATCH] dma-mapping: bypass indirect calls for dma-direct
Date: Thu,  6 Dec 2018 07:37:20 -0800
Message-Id: <20181206153720.10702-2-hch@lst.de>
X-Mailer: git-send-email 2.19.1
In-Reply-To: <20181206153720.10702-1-hch@lst.de>
References: <20181206153720.10702-1-hch@lst.de>
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
X-SRS-Rewrite: SMTP reverse-path rewritten from <hch@infradead.org> by bombadil.infradead.org. See http://www.infradead.org/rpr.html
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Avoid expensive indirect calls in the fast path DMA mapping
operations by directly calling the dma_direct_* ops if we are using
the directly mapped DMA operations.

Signed-off-by: Christoph Hellwig <hch@lst.de>
---
 include/linux/dma-direct.h  |  17 -----
 include/linux/dma-mapping.h | 138 +++++++++++++++++++++++++++++++-----
 kernel/dma/direct.c         |  13 ++--
 3 files changed, 127 insertions(+), 41 deletions(-)

diff --git a/include/linux/dma-direct.h b/include/linux/dma-direct.h
index 3b0a3ea3876d..b7338702592a 100644
--- a/include/linux/dma-direct.h
+++ b/include/linux/dma-direct.h
@@ -60,22 +60,5 @@ void dma_direct_free_pages(struct device *dev, size_t size, void *cpu_addr,
 struct page *__dma_direct_alloc_pages(struct device *dev, size_t size,
 		dma_addr_t *dma_handle, gfp_t gfp, unsigned long attrs);
 void __dma_direct_free_pages(struct device *dev, size_t size, struct page *page);
-dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
-		unsigned long offset, size_t size, enum dma_data_direction dir,
-		unsigned long attrs);
-void dma_direct_unmap_page(struct device *dev, dma_addr_t addr,
-		size_t size, enum dma_data_direction dir, unsigned long attrs);
-int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,
-		enum dma_data_direction dir, unsigned long attrs);
-void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,
-		int nents, enum dma_data_direction dir, unsigned long attrs);
-void dma_direct_sync_single_for_device(struct device *dev,
-		dma_addr_t addr, size_t size, enum dma_data_direction dir);
-void dma_direct_sync_sg_for_device(struct device *dev,
-		struct scatterlist *sgl, int nents, enum dma_data_direction dir);
-void dma_direct_sync_single_for_cpu(struct device *dev,
-		dma_addr_t addr, size_t size, enum dma_data_direction dir);
-void dma_direct_sync_sg_for_cpu(struct device *dev,
-		struct scatterlist *sgl, int nents, enum dma_data_direction dir);
 int dma_direct_supported(struct device *dev, u64 mask);
 #endif /* _LINUX_DMA_DIRECT_H */
diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 8916499d2805..648ca7da3bb3 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -221,6 +221,69 @@ static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
 }
 #endif
 
+static inline bool dma_is_direct(const struct dma_map_ops *ops)
+{
+	return IS_ENABLED(CONFIG_DMA_DIRECT_OPS) && ops == &dma_direct_ops;
+}
+
+/*
+ * All the dma_direct_* declarations are here just for the indirect call bypass,
+ * and must not be used directly drivers!
+ */
+dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
+		unsigned long offset, size_t size, enum dma_data_direction dir,
+		unsigned long attrs);
+int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,
+		enum dma_data_direction dir, unsigned long attrs);
+
+#if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \
+    defined(CONFIG_SWIOTLB)
+void dma_direct_sync_single_for_device(struct device *dev,
+		dma_addr_t addr, size_t size, enum dma_data_direction dir);
+void dma_direct_sync_sg_for_device(struct device *dev,
+		struct scatterlist *sgl, int nents, enum dma_data_direction dir);
+#else
+static inline void dma_direct_sync_single_for_device(struct device *dev,
+		dma_addr_t addr, size_t size, enum dma_data_direction dir)
+{
+}
+static inline void dma_direct_sync_sg_for_device(struct device *dev,
+		struct scatterlist *sgl, int nents, enum dma_data_direction dir)
+{
+}
+#endif
+
+#if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \
+    defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU_ALL) || \
+    defined(CONFIG_SWIOTLB)
+void dma_direct_unmap_page(struct device *dev, dma_addr_t addr,
+		size_t size, enum dma_data_direction dir, unsigned long attrs);
+void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,
+		int nents, enum dma_data_direction dir, unsigned long attrs);
+void dma_direct_sync_single_for_cpu(struct device *dev,
+		dma_addr_t addr, size_t size, enum dma_data_direction dir);
+void dma_direct_sync_sg_for_cpu(struct device *dev,
+		struct scatterlist *sgl, int nents, enum dma_data_direction dir);
+#else
+static inline void dma_direct_unmap_page(struct device *dev, dma_addr_t addr,
+		size_t size, enum dma_data_direction dir, unsigned long attrs)
+{
+}
+static inline void dma_direct_unmap_sg(struct device *dev,
+		struct scatterlist *sgl, int nents, enum dma_data_direction dir,
+		unsigned long attrs)
+{
+}
+static inline void dma_direct_sync_single_for_cpu(struct device *dev,
+		dma_addr_t addr, size_t size, enum dma_data_direction dir)
+{
+}
+static inline void dma_direct_sync_sg_for_cpu(struct device *dev,
+		struct scatterlist *sgl, int nents, enum dma_data_direction dir)
+{
+}
+#endif
+
 static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,
 					      size_t size,
 					      enum dma_data_direction dir,
@@ -231,9 +294,12 @@ static inline dma_addr_t dma_map_single_attrs(struct device *dev, void *ptr,
 
 	BUG_ON(!valid_dma_direction(dir));
 	debug_dma_map_single(dev, ptr, size);
-	addr = ops->map_page(dev, virt_to_page(ptr),
-			     offset_in_page(ptr), size,
-			     dir, attrs);
+	if (dma_is_direct(ops))
+		addr = dma_direct_map_page(dev, virt_to_page(ptr),
+				offset_in_page(ptr), size, dir, attrs);
+	else
+		addr = ops->map_page(dev, virt_to_page(ptr),
+				offset_in_page(ptr), size, dir, attrs);
 	debug_dma_map_page(dev, virt_to_page(ptr),
 			   offset_in_page(ptr), size,
 			   dir, addr, true);
@@ -248,8 +314,12 @@ static inline void dma_unmap_single_attrs(struct device *dev, dma_addr_t addr,
 	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!valid_dma_direction(dir));
-	if (ops->unmap_page)
-		ops->unmap_page(dev, addr, size, dir, attrs);
+	if (ops->unmap_page) {
+		if (dma_is_direct(ops))
+			dma_direct_unmap_page(dev, addr, size, dir, attrs);
+		else
+			ops->unmap_page(dev, addr, size, dir, attrs);
+	}
 	debug_dma_unmap_page(dev, addr, size, dir, true);
 }
 
@@ -265,7 +335,10 @@ static inline int dma_map_sg_attrs(struct device *dev, struct scatterlist *sg,
 	int ents;
 
 	BUG_ON(!valid_dma_direction(dir));
-	ents = ops->map_sg(dev, sg, nents, dir, attrs);
+	if (dma_is_direct(ops))
+		ents = dma_direct_map_sg(dev, sg, nents, dir, attrs);
+	else
+		ents = ops->map_sg(dev, sg, nents, dir, attrs);
 	BUG_ON(ents < 0);
 	debug_dma_map_sg(dev, sg, nents, ents, dir);
 
@@ -280,8 +353,12 @@ static inline void dma_unmap_sg_attrs(struct device *dev, struct scatterlist *sg
 
 	BUG_ON(!valid_dma_direction(dir));
 	debug_dma_unmap_sg(dev, sg, nents, dir);
-	if (ops->unmap_sg)
-		ops->unmap_sg(dev, sg, nents, dir, attrs);
+	if (ops->unmap_sg) {
+		if (dma_is_direct(ops))
+			dma_direct_unmap_sg(dev, sg, nents, dir, attrs);
+		else
+			ops->unmap_sg(dev, sg, nents, dir, attrs);
+	}
 }
 
 static inline dma_addr_t dma_map_page_attrs(struct device *dev,
@@ -294,7 +371,10 @@ static inline dma_addr_t dma_map_page_attrs(struct device *dev,
 	dma_addr_t addr;
 
 	BUG_ON(!valid_dma_direction(dir));
-	addr = ops->map_page(dev, page, offset, size, dir, attrs);
+	if (dma_is_direct(ops))
+		addr = dma_direct_map_page(dev, page, offset, size, dir, attrs);
+	else
+		addr = ops->map_page(dev, page, offset, size, dir, attrs);
 	debug_dma_map_page(dev, page, offset, size, dir, addr, false);
 
 	return addr;
@@ -308,8 +388,12 @@ static inline void dma_unmap_page_attrs(struct device *dev,
 	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!valid_dma_direction(dir));
-	if (ops->unmap_page)
-		ops->unmap_page(dev, addr, size, dir, attrs);
+	if (ops->unmap_page) {
+		if (dma_is_direct(ops))
+			dma_direct_unmap_page(dev, addr, size, dir, attrs);
+		else
+			ops->unmap_page(dev, addr, size, dir, attrs);
+	}
 	debug_dma_unmap_page(dev, addr, size, dir, false);
 }
 
@@ -355,8 +439,12 @@ static inline void dma_sync_single_for_cpu(struct device *dev, dma_addr_t addr,
 	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!valid_dma_direction(dir));
-	if (ops->sync_single_for_cpu)
-		ops->sync_single_for_cpu(dev, addr, size, dir);
+	if (ops->sync_single_for_cpu) {
+		if (dma_is_direct(ops))
+			dma_direct_sync_single_for_cpu(dev, addr, size, dir);
+		else
+			ops->sync_single_for_cpu(dev, addr, size, dir);
+	}
 	debug_dma_sync_single_for_cpu(dev, addr, size, dir);
 }
 
@@ -374,8 +462,12 @@ static inline void dma_sync_single_for_device(struct device *dev,
 	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!valid_dma_direction(dir));
-	if (ops->sync_single_for_device)
-		ops->sync_single_for_device(dev, addr, size, dir);
+	if (ops->sync_single_for_device) {
+		if (dma_is_direct(ops))
+			dma_direct_sync_single_for_device(dev, addr, size, dir);
+		else
+			ops->sync_single_for_device(dev, addr, size, dir);
+	}
 	debug_dma_sync_single_for_device(dev, addr, size, dir);
 }
 
@@ -393,8 +485,12 @@ dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg,
 	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!valid_dma_direction(dir));
-	if (ops->sync_sg_for_cpu)
-		ops->sync_sg_for_cpu(dev, sg, nelems, dir);
+	if (ops->sync_sg_for_cpu) {
+		if (dma_is_direct(ops))
+			dma_direct_sync_sg_for_cpu(dev, sg, nelems, dir);
+		else
+			ops->sync_sg_for_cpu(dev, sg, nelems, dir);
+	}
 	debug_dma_sync_sg_for_cpu(dev, sg, nelems, dir);
 }
 
@@ -405,8 +501,12 @@ dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
 	const struct dma_map_ops *ops = get_dma_ops(dev);
 
 	BUG_ON(!valid_dma_direction(dir));
-	if (ops->sync_sg_for_device)
-		ops->sync_sg_for_device(dev, sg, nelems, dir);
+	if (ops->sync_sg_for_device) {
+		if (dma_is_direct(ops))
+			dma_direct_sync_sg_for_device(dev, sg, nelems, dir);
+		else
+			ops->sync_sg_for_device(dev, sg, nelems, dir);
+	}
 	debug_dma_sync_sg_for_device(dev, sg, nelems, dir);
 
 }
diff --git a/kernel/dma/direct.c b/kernel/dma/direct.c
index 372c1955454a..cd535e7c67d7 100644
--- a/kernel/dma/direct.c
+++ b/kernel/dma/direct.c
@@ -223,6 +223,7 @@ void dma_direct_sync_single_for_device(struct device *dev,
 	if (!dev_is_dma_coherent(dev))
 		arch_sync_dma_for_device(dev, paddr, size, dir);
 }
+EXPORT_SYMBOL(dma_direct_sync_single_for_device);
 
 void dma_direct_sync_sg_for_device(struct device *dev,
 		struct scatterlist *sgl, int nents, enum dma_data_direction dir)
@@ -240,6 +241,7 @@ void dma_direct_sync_sg_for_device(struct device *dev,
 					dir);
 	}
 }
+EXPORT_SYMBOL(dma_direct_sync_sg_for_device);
 #endif
 
 #if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \
@@ -258,6 +260,7 @@ void dma_direct_sync_single_for_cpu(struct device *dev,
 	if (is_swiotlb_buffer(paddr))
 		swiotlb_tbl_sync_single(dev, paddr, size, dir, SYNC_FOR_CPU);
 }
+EXPORT_SYMBOL(dma_direct_sync_single_for_cpu);
 
 void dma_direct_sync_sg_for_cpu(struct device *dev,
 		struct scatterlist *sgl, int nents, enum dma_data_direction dir)
@@ -277,6 +280,7 @@ void dma_direct_sync_sg_for_cpu(struct device *dev,
 	if (!dev_is_dma_coherent(dev))
 		arch_sync_dma_for_cpu_all(dev);
 }
+EXPORT_SYMBOL(dma_direct_sync_sg_for_cpu);
 
 void dma_direct_unmap_page(struct device *dev, dma_addr_t addr,
 		size_t size, enum dma_data_direction dir, unsigned long attrs)
@@ -289,6 +293,7 @@ void dma_direct_unmap_page(struct device *dev, dma_addr_t addr,
 	if (is_swiotlb_buffer(phys))
 		swiotlb_tbl_unmap_single(dev, phys, size, dir, attrs);
 }
+EXPORT_SYMBOL(dma_direct_unmap_page);
 
 void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,
 		int nents, enum dma_data_direction dir, unsigned long attrs)
@@ -300,11 +305,7 @@ void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,
 		dma_direct_unmap_page(dev, sg->dma_address, sg_dma_len(sg), dir,
 			     attrs);
 }
-#else
-void dma_direct_unmap_sg(struct device *dev, struct scatterlist *sgl,
-		int nents, enum dma_data_direction dir, unsigned long attrs)
-{
-}
+EXPORT_SYMBOL(dma_direct_unmap_sg);
 #endif
 
 static inline bool dma_direct_possible(struct device *dev, dma_addr_t dma_addr,
@@ -331,6 +332,7 @@ dma_addr_t dma_direct_map_page(struct device *dev, struct page *page,
 		arch_sync_dma_for_device(dev, phys, size, dir);
 	return dma_addr;
 }
+EXPORT_SYMBOL(dma_direct_map_page);
 
 int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,
 		enum dma_data_direction dir, unsigned long attrs)
@@ -352,6 +354,7 @@ int dma_direct_map_sg(struct device *dev, struct scatterlist *sgl, int nents,
 	dma_direct_unmap_sg(dev, sgl, i, dir, attrs | DMA_ATTR_SKIP_CPU_SYNC);
 	return 0;
 }
+EXPORT_SYMBOL(dma_direct_map_sg);
 
 /*
  * Because 32-bit DMA masks are so common we expect every architecture to be
-- 
2.19.1

