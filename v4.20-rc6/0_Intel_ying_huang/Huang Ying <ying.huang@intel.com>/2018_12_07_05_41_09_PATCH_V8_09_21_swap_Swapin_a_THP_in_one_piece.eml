Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 07 Dec 2018 16:18:01 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga007.fm.intel.com (fmsmga007.fm.intel.com [10.253.24.52])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 1331D580375;
	Thu,  6 Dec 2018 21:41:58 -0800 (PST)
Received: from fmsmga101.fm.intel.com ([10.1.193.65])
  by fmsmga007-1.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 06 Dec 2018 21:41:57 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AnGluxhbsDE551g1P6+m1bS//LSx+4OfEezUN459i?=
 =?us-ascii?q?sYplN5qZpci/ZB7h7PlgxGXEQZ/co6odzbaO4+a4ASQp2tWoiDg6aptCVhsI24?=
 =?us-ascii?q?09vjcLJ4q7M3D9N+PgdCcgHc5PBxdP9nC/NlVJSo6lPwWB6nK94iQPFRrhKAF7?=
 =?us-ascii?q?Ovr6GpLIj8Swyuu+54Dfbx9HiTahYr5+Ngm6oRnMvcQKnIVuLbo8xAHUqXVSYe?=
 =?us-ascii?q?RWwm1oJVOXnxni48q74YBu/SdNtf8/7sBMSar1cbg2QrxeFzQmLns65Nb3uhnZ?=
 =?us-ascii?q?TAuA/WUTX2MLmRdVGQfF7RX6XpDssivms+d2xSeXMdHqQb0yRD+v9LlgRgP2hy?=
 =?us-ascii?q?gbNj456GDXhdJ2jKJHuxKquhhzz5fJbI2JKPZye6XQds4YS2VcRMZcTyxPDJ2h?=
 =?us-ascii?q?YYUBDOQPOuRXr4fmp1sWrxazHhWgCP/1xzNUnHL6wbE23uI8Gg/GxgwgGNcOvW?=
 =?us-ascii?q?zWotXoLqgSV++1x7TKwjXCafNW1ir25Y/IcxAgp/GMUqh8ccrMyUY1EQPFgU6d?=
 =?us-ascii?q?qY3jPzOJyOsNt3KX4PZnVeKqkmMqrRx6rDu3xso0lIXFmoYYxkrZ+Sh33oo5P8?=
 =?us-ascii?q?C0RU1nbdK+EZZdtTmWO5ZyT888WW1luCY3xqcGtJKlZiQHy5cqyhjCYPKdaYeI?=
 =?us-ascii?q?+AjsVOOJLDd4mn1lfLW/ig6s8Uiv1OL8TNO40FVUoSpflNnDqHQN2wbU6sidRf?=
 =?us-ascii?q?tx5kah2TCR2ADP8uxIP1w4mK7BJ5I8zLM8iIAfvVnAEyPqgkn7ja2bel0h+uey?=
 =?us-ascii?q?6uTnZrvmpoWbN49xkgz+NqUumsqiAeU3KwQOXHaU+f661LL9+U31WbJKj/Mwkq?=
 =?us-ascii?q?bHqpDXPtobpqGnDA9PyIoj6AiwDy2g0NsGmXkLNlVFeAiIj4TxIVHBPOj4Deuj?=
 =?us-ascii?q?g1SriDprw/HGPr7/DZnXIXnDjazsfbJ8605a1QoywspT55NSCrEdPv3zXlX9u8?=
 =?us-ascii?q?DfDh88Kwa02froCM1h1oMCXmKCGq+ZP7nTsV+U/O0vJPOAZI8IuDnnLfgl6OXj?=
 =?us-ascii?q?jXs4mV8bYKmo0oEbaHG+HvR6PUqZZWDgjcsGEWcPpgA+VvDliEWeUT5PYHa/R6?=
 =?us-ascii?q?A85jYlB468DofDQYatgLqG3CqgGp1WZ2ZGCk2DEHvydoWEXesMZzyWIsN7jjME?=
 =?us-ascii?q?Ur2hQZc71R6yrA/616ZnLu3M9yICrpLj1N915+7JmREo7zN0Dd+Q02WMT2Fyg2?=
 =?us-ascii?q?MJSCU63KF5oUxh1FiD1bJ0jOBfFdxW//lJSBs1NYbAz+xmDND/QgHBcc2PSFq8?=
 =?us-ascii?q?RtWmACs+TtQ+w9IVZ0Z9GtOijg3M3iawAr8VkaCLC4Iw8q7Gw3fxIMN9wW7c1K?=
 =?us-ascii?q?Y9l1kmXtdPNWq+i6Fi7QfTGZDGn1+Zl6mwc6QcxzDC9GGEwWqKv0FYVQpwXL7B?=
 =?us-ascii?q?XXAeYEvWsNv46lnDT7+oFbQoLA9BxdSeJatNb93jlU9GS+v7ONTCf2KxnH+9Cg?=
 =?us-ascii?q?uSybOScoXmYWUd0z/bCEgfjQ8T+22LNQw/BienvmLfAyZiFVPpY0Pw7+Z+rGm3?=
 =?us-ascii?q?QVMzzwGPd0dhzaa6+gYJhfyATPMexq4EuCYkqzVzAFa939LXB8CcpwZ7e6Vce9?=
 =?us-ascii?q?c94FZB1WLWrAF9Op2gL6Z/hl8RaQh3vkXu1wlpBYVEi8QlsHQqzA9qI6KCzFxB?=
 =?us-ascii?q?by+Y3Yz3OrDPMGby+A6gaqHI1VDeytqZ4boP5+kipFXlvwGpEVQi/m5j09lU1X?=
 =?us-ascii?q?uc+5rLABATUZL3TkY46Rx6q6vGbSk64oPezWdsPrWssj/ex9IpA/Moyxa9f9tF?=
 =?us-ascii?q?LKyIDg7zE80ACMioJ+wngFypbhMCPOBP+684JcKmd/2a2KG1OOZshi6pjWNC4I?=
 =?us-ascii?q?plyEKD6zJ8SvLU35YC2/yYxAqHWCvmg1e7r8/3nppIZTcMEWqlyCjoHZJeabd2?=
 =?us-ascii?q?fYkWF2iuOcq3xtNlip7pWn5Y8kOjBlwc1M+ofxqSc0Ly3QlK2UsLpnynnDOyzy?=
 =?us-ascii?q?ZonDExsqqfwCvOzvzidRoGIGJKRHNujUzxIYiylN0aWEmobw40lBqq/0r6xq5b?=
 =?us-ascii?q?pLhhIGnXW0tHYy/2L2R6WKuqqrWCe9JP6I8vsShPUuSze0qaSr3+oxsdyS/jBH?=
 =?us-ascii?q?FRxDM4dzGrvJX2gRp6hXmZLHZyqnrZZM5xyQ3e5NzaWf5ewD4GSDNkhjnQA1i2?=
 =?us-ascii?q?J8Op8smMl5ffruC+UHqsV51Jfins14+Atiq75WtxDB27nvCznMDnEAcg3S/60d?=
 =?us-ascii?q?lqSTvHrBLmbob30KS6NPptflN0C1/k98p6BoZ+n5MzhJ4K2HgWnJWV/Wcdnmf1?=
 =?us-ascii?q?PtVWwqb+bHsLRT4WzN/Z+gnl2Et/Ln2Xw4L1TGmSwsxkZ9OieGMZxjo979xWCK?=
 =?us-ascii?q?eT9LFEmCp1olmiog7Lb/lygCwdyecw534AmO4GphAtziqGD7AWHElYOzHslhuS?=
 =?us-ascii?q?49C/qqVXeHigcbyq2EVimtChCamIohtAV3bhZpciAShw491jMFLL1X3/8IDld8?=
 =?us-ascii?q?PWbdIOrR2UiBbAgvNRKJIwkPoKmCVmNXj8vX0j1+40kxhu0Yums4iALmVn5Li5?=
 =?us-ascii?q?DQJANj3pe8MT/Snggr1EkcaR24CvA49tGjEWXJbzSfKoETQStenoNgqUET08rG?=
 =?us-ascii?q?ubFqTbHQOF9EhmqHfPGYixN36LPHkZ0cliRB6FKUxdmg8UWSs1noUjGgC23sDh?=
 =?us-ascii?q?c1p55jMM5l7+sBRM0fllNx3+UmfZuQepZS04SJmZLBpK8A5C413ZPtCZ7uJ2By?=
 =?us-ascii?q?tY5IGurBSRKmyHYARFFWEIWkuZB1D6I7mh/9/A//KDBuq5KfvObq6DqehfV/eO?=
 =?us-ascii?q?2JKu3ZFq/zeKNsWTIHZiC+c31VZEXXB8A87ZgSkASzQLlyLRaM6WvBe89TN2rs?=
 =?us-ascii?q?C88/TrWRjj5YiVC7tVPtVg5Qq2gbqYN+OLgCZ5KDBY1o4DxHPSybgf2kIShD9q?=
 =?us-ascii?q?dzW3DbsAsivNRrrKmqBLFx4bdz9zNMxQ4q0mwwlCItTbhc3117Jiiv41CkxIVV?=
 =?us-ascii?q?jgmsGveMwLLHuxNFLBBEaXKruGISfHzN3wYaO5UbdQlvlbtwWsuTaHFE/uJiiD?=
 =?us-ascii?q?lzjsVxy1Le5Alj2UPBxAt4G7aRttDWnjTNT7ahy0KtN3jDs2waEqiXPOL2ITLT?=
 =?us-ascii?q?98c0ZVpL2K8SxYmul/G3BG7nd9LeiLgSCZ7+zZKpYQqfRqAyR0mPhc4HQ10LZV?=
 =?us-ascii?q?6CBERPpomCrdtNJuolemkvWRxTpjShZBtjFLhIfY9XllbIzf+oNNRj7h+wgR6m?=
 =?us-ascii?q?PYXx0Jv95NDtzpprAVxN/Skq76NDZF9ZTT58RKVObOL8fSEGcoNxWhKT/SA0NR?=
 =?us-ascii?q?XD+tOiffmkVbn9mT8GGYqt4xrZ26y8lGcaNSSFFgTqBSMU9iBtFXZc4vBj4=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0ADAAB9Bwpch0O0hNFbCBsBAQEBAwEBA?=
 =?us-ascii?q?QcDAQEBgVEGAQEBCwGDayeME44xlzuBJANPERgTAYdXIjQJDQEDAQEBAQEBAgE?=
 =?us-ascii?q?TAQEBCA0JCCkvgjYkAYJiAwMBAiRSBgkBARg4A1QGEwWDHIICBaZ2M4owh3CEL?=
 =?us-ascii?q?4IWgRGCZIUKhgACiSyBeYUPUY9dBwKRPwsYX32IPIcdAZkagUZsgSEzGiODPII?=
 =?us-ascii?q?nF44qMgEBMQGBBAEBij8BAQ?=
X-IPAS-Result: =?us-ascii?q?A0ADAAB9Bwpch0O0hNFbCBsBAQEBAwEBAQcDAQEBgVEGAQE?=
 =?us-ascii?q?BCwGDayeME44xlzuBJANPERgTAYdXIjQJDQEDAQEBAQEBAgETAQEBCA0JCCkvg?=
 =?us-ascii?q?jYkAYJiAwMBAiRSBgkBARg4A1QGEwWDHIICBaZ2M4owh3CEL4IWgRGCZIUKhgA?=
 =?us-ascii?q?CiSyBeYUPUY9dBwKRPwsYX32IPIcdAZkagUZsgSEzGiODPIInF44qMgEBMQGBB?=
 =?us-ascii?q?AEBij8BAQ?=
X-IronPort-AV: E=Sophos;i="5.56,324,1539673200"; 
   d="scan'208";a="65746497"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mga01b.intel.com with ESMTP; 06 Dec 2018 21:41:56 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726168AbeLGFlz (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Fri, 7 Dec 2018 00:41:55 -0500
Received: from mga01.intel.com ([192.55.52.88]:55177 "EHLO mga01.intel.com"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1726153AbeLGFlx (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Fri, 7 Dec 2018 00:41:53 -0500
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from fmsmga007.fm.intel.com ([10.253.24.52])
  by fmsmga101.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 06 Dec 2018 21:41:53 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.56,324,1539673200"; 
   d="scan'208";a="105567259"
Received: from yhuang-mobile.sh.intel.com ([10.239.196.133])
  by fmsmga007.fm.intel.com with ESMTP; 06 Dec 2018 21:41:50 -0800
From: Huang Ying <ying.huang@intel.com>
To: Andrew Morton <akpm@linux-foundation.org>
Cc: linux-mm@kvack.org, linux-kernel@vger.kernel.org,
        Huang Ying <ying.huang@intel.com>,
        "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        Michal Hocko <mhocko@kernel.org>,
        Johannes Weiner <hannes@cmpxchg.org>,
        Shaohua Li <shli@kernel.org>, Hugh Dickins <hughd@google.com>,
        Minchan Kim <minchan@kernel.org>,
        Rik van Riel <riel@redhat.com>,
        Dave Hansen <dave.hansen@linux.intel.com>,
        Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>,
        Zi Yan <zi.yan@cs.rutgers.edu>,
        Daniel Jordan <daniel.m.jordan@oracle.com>
Subject: [PATCH -V8 09/21] swap: Swapin a THP in one piece
Date: Fri,  7 Dec 2018 13:41:09 +0800
Message-Id: <20181207054122.27822-10-ying.huang@intel.com>
X-Mailer: git-send-email 2.18.1
In-Reply-To: <20181207054122.27822-1-ying.huang@intel.com>
References: <20181207054122.27822-1-ying.huang@intel.com>
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

With this patch, when page fault handler find a PMD swap mapping, it
will swap in a THP in one piece.  This avoids the overhead of
splitting/collapsing before/after the THP swapping.  And improves the
swap performance greatly for reduced page fault count etc.

do_huge_pmd_swap_page() is added in the patch to implement this.  It
is similar to do_swap_page() for normal page swapin.

If failing to allocate a THP, the huge swap cluster and the PMD swap
mapping will be split to fallback to normal page swapin.

If the huge swap cluster has been split already, the PMD swap mapping
will be split to fallback to normal page swapin.

Signed-off-by: "Huang, Ying" <ying.huang@intel.com>
Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
Cc: Andrea Arcangeli <aarcange@redhat.com>
Cc: Michal Hocko <mhocko@kernel.org>
Cc: Johannes Weiner <hannes@cmpxchg.org>
Cc: Shaohua Li <shli@kernel.org>
Cc: Hugh Dickins <hughd@google.com>
Cc: Minchan Kim <minchan@kernel.org>
Cc: Rik van Riel <riel@redhat.com>
Cc: Dave Hansen <dave.hansen@linux.intel.com>
Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Cc: Zi Yan <zi.yan@cs.rutgers.edu>
Cc: Daniel Jordan <daniel.m.jordan@oracle.com>
---
 include/linux/huge_mm.h |   9 +++
 mm/huge_memory.c        | 174 ++++++++++++++++++++++++++++++++++++++++
 mm/memory.c             |  16 ++--
 3 files changed, 193 insertions(+), 6 deletions(-)

diff --git a/include/linux/huge_mm.h b/include/linux/huge_mm.h
index f4dbd0662438..909321c772b5 100644
--- a/include/linux/huge_mm.h
+++ b/include/linux/huge_mm.h
@@ -373,4 +373,13 @@ static inline gfp_t alloc_hugepage_direct_gfpmask(struct vm_area_struct *vma,
 }
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 
+#ifdef CONFIG_THP_SWAP
+extern int do_huge_pmd_swap_page(struct vm_fault *vmf, pmd_t orig_pmd);
+#else /* CONFIG_THP_SWAP */
+static inline int do_huge_pmd_swap_page(struct vm_fault *vmf, pmd_t orig_pmd)
+{
+	return 0;
+}
+#endif /* CONFIG_THP_SWAP */
+
 #endif /* _LINUX_HUGE_MM_H */
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 2004d8ae4390..3bb2df7f5f84 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -34,6 +34,8 @@
 #include <linux/shmem_fs.h>
 #include <linux/oom.h>
 #include <linux/numa.h>
+#include <linux/delayacct.h>
+#include <linux/swap.h>
 
 #include <asm/tlb.h>
 #include <asm/pgalloc.h>
@@ -1667,6 +1669,178 @@ static void __split_huge_swap_pmd(struct vm_area_struct *vma,
 	pmd_populate(mm, pmd, pgtable);
 }
 
+#ifdef CONFIG_THP_SWAP
+static int split_huge_swap_pmd(struct vm_area_struct *vma, pmd_t *pmd,
+			       unsigned long address, pmd_t orig_pmd)
+{
+	struct mm_struct *mm = vma->vm_mm;
+	spinlock_t *ptl;
+	int ret = 0;
+
+	ptl = pmd_lock(mm, pmd);
+	if (pmd_same(*pmd, orig_pmd))
+		__split_huge_swap_pmd(vma, address, pmd);
+	else
+		ret = -ENOENT;
+	spin_unlock(ptl);
+
+	return ret;
+}
+
+int do_huge_pmd_swap_page(struct vm_fault *vmf, pmd_t orig_pmd)
+{
+	struct page *page;
+	struct mem_cgroup *memcg;
+	struct vm_area_struct *vma = vmf->vma;
+	unsigned long haddr = vmf->address & HPAGE_PMD_MASK;
+	swp_entry_t entry;
+	pmd_t pmd;
+	int i, locked, exclusive = 0, ret = 0;
+
+	entry = pmd_to_swp_entry(orig_pmd);
+	VM_BUG_ON(non_swap_entry(entry));
+	delayacct_set_flag(DELAYACCT_PF_SWAPIN);
+retry:
+	page = lookup_swap_cache(entry, NULL, vmf->address);
+	if (!page) {
+		page = read_swap_cache_async(entry, GFP_HIGHUSER_MOVABLE, vma,
+					     haddr, false);
+		if (!page) {
+			/*
+			 * Back out if somebody else faulted in this pmd
+			 * while we released the pmd lock.
+			 */
+			if (likely(pmd_same(*vmf->pmd, orig_pmd))) {
+				/*
+				 * Failed to allocate huge page, split huge swap
+				 * cluster, and fallback to swapin normal page
+				 */
+				ret = split_swap_cluster(entry, 0);
+				/* Somebody else swapin the swap entry, retry */
+				if (ret == -EEXIST) {
+					ret = 0;
+					goto retry;
+				/* swapoff occurs under us */
+				} else if (ret == -EINVAL)
+					ret = 0;
+				else
+					goto fallback;
+			}
+			delayacct_clear_flag(DELAYACCT_PF_SWAPIN);
+			goto out;
+		}
+
+		/* Had to read the page from swap area: Major fault */
+		ret = VM_FAULT_MAJOR;
+		count_vm_event(PGMAJFAULT);
+		count_memcg_event_mm(vma->vm_mm, PGMAJFAULT);
+	} else if (!PageTransCompound(page))
+		goto fallback;
+
+	locked = lock_page_or_retry(page, vma->vm_mm, vmf->flags);
+
+	delayacct_clear_flag(DELAYACCT_PF_SWAPIN);
+	if (!locked) {
+		ret |= VM_FAULT_RETRY;
+		goto out_release;
+	}
+
+	/*
+	 * Make sure try_to_free_swap or reuse_swap_page or swapoff did not
+	 * release the swapcache from under us.  The page pin, and pmd_same
+	 * test below, are not enough to exclude that.  Even if it is still
+	 * swapcache, we need to check that the page's swap has not changed.
+	 */
+	if (unlikely(!PageSwapCache(page) || page_private(page) != entry.val))
+		goto out_page;
+
+	if (mem_cgroup_try_charge_delay(page, vma->vm_mm, GFP_KERNEL,
+					&memcg, true)) {
+		ret = VM_FAULT_OOM;
+		goto out_page;
+	}
+
+	/*
+	 * Back out if somebody else already faulted in this pmd.
+	 */
+	vmf->ptl = pmd_lockptr(vma->vm_mm, vmf->pmd);
+	spin_lock(vmf->ptl);
+	if (unlikely(!pmd_same(*vmf->pmd, orig_pmd)))
+		goto out_nomap;
+
+	if (unlikely(!PageUptodate(page))) {
+		ret = VM_FAULT_SIGBUS;
+		goto out_nomap;
+	}
+
+	/*
+	 * The page isn't present yet, go ahead with the fault.
+	 *
+	 * Be careful about the sequence of operations here.
+	 * To get its accounting right, reuse_swap_page() must be called
+	 * while the page is counted on swap but not yet in mapcount i.e.
+	 * before page_add_anon_rmap() and swap_free(); try_to_free_swap()
+	 * must be called after the swap_free(), or it will never succeed.
+	 */
+
+	add_mm_counter(vma->vm_mm, MM_ANONPAGES, HPAGE_PMD_NR);
+	add_mm_counter(vma->vm_mm, MM_SWAPENTS, -HPAGE_PMD_NR);
+	pmd = mk_huge_pmd(page, vma->vm_page_prot);
+	if ((vmf->flags & FAULT_FLAG_WRITE) && reuse_swap_page(page, NULL)) {
+		pmd = maybe_pmd_mkwrite(pmd_mkdirty(pmd), vma);
+		vmf->flags &= ~FAULT_FLAG_WRITE;
+		ret |= VM_FAULT_WRITE;
+		exclusive = RMAP_EXCLUSIVE;
+	}
+	for (i = 0; i < HPAGE_PMD_NR; i++)
+		flush_icache_page(vma, page + i);
+	if (pmd_swp_soft_dirty(orig_pmd))
+		pmd = pmd_mksoft_dirty(pmd);
+	do_page_add_anon_rmap(page, vma, haddr,
+			      exclusive | RMAP_COMPOUND);
+	mem_cgroup_commit_charge(page, memcg, true, true);
+	activate_page(page);
+	set_pmd_at(vma->vm_mm, haddr, vmf->pmd, pmd);
+
+	swap_free(entry, HPAGE_PMD_NR);
+	if (mem_cgroup_swap_full(page) ||
+	    (vma->vm_flags & VM_LOCKED) || PageMlocked(page))
+		try_to_free_swap(page);
+	unlock_page(page);
+
+	if (vmf->flags & FAULT_FLAG_WRITE) {
+		spin_unlock(vmf->ptl);
+		ret |= do_huge_pmd_wp_page(vmf, pmd);
+		if (ret & VM_FAULT_ERROR)
+			ret &= VM_FAULT_ERROR;
+		goto out;
+	}
+
+	/* No need to invalidate - it was non-present before */
+	update_mmu_cache_pmd(vma, vmf->address, vmf->pmd);
+	spin_unlock(vmf->ptl);
+out:
+	return ret;
+out_nomap:
+	mem_cgroup_cancel_charge(page, memcg, true);
+	spin_unlock(vmf->ptl);
+out_page:
+	unlock_page(page);
+out_release:
+	put_page(page);
+	return ret;
+fallback:
+	delayacct_clear_flag(DELAYACCT_PF_SWAPIN);
+	if (!split_huge_swap_pmd(vmf->vma, vmf->pmd, vmf->address, orig_pmd))
+		ret = VM_FAULT_FALLBACK;
+	else
+		ret = 0;
+	if (page)
+		put_page(page);
+	return ret;
+}
+#endif
+
 /*
  * Return true if we do MADV_FREE successfully on entire pmd page.
  * Otherwise, return false.
diff --git a/mm/memory.c b/mm/memory.c
index 35973cc5425b..39b9e9ab412f 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -3865,13 +3865,17 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 
 		barrier();
 		if (unlikely(is_swap_pmd(orig_pmd))) {
-			VM_BUG_ON(thp_migration_supported() &&
-					  !is_pmd_migration_entry(orig_pmd));
-			if (is_pmd_migration_entry(orig_pmd))
+			if (thp_migration_supported() &&
+			    is_pmd_migration_entry(orig_pmd)) {
 				pmd_migration_entry_wait(mm, vmf.pmd);
-			return 0;
-		}
-		if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {
+				return 0;
+			} else if (IS_ENABLED(CONFIG_THP_SWAP)) {
+				ret = do_huge_pmd_swap_page(&vmf, orig_pmd);
+				if (!(ret & VM_FAULT_FALLBACK))
+					return ret;
+			} else
+				VM_BUG_ON(1);
+		} else if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {
 			if (pmd_protnone(orig_pmd) && vma_is_accessible(vma))
 				return do_huge_pmd_numa_page(&vmf, orig_pmd);
 
-- 
2.18.1

