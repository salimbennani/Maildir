Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 10 Dec 2018 18:12:54 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga007.fm.intel.com (fmsmga007.fm.intel.com [10.253.24.52])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 918855800CB;
	Sun,  9 Dec 2018 18:02:56 -0800 (PST)
Received: from fmsmga104.fm.intel.com ([10.1.193.100])
  by fmsmga007-1.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 09 Dec 2018 18:02:56 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AFLEakRffUk1lz9gCWPpWmIyElGMj4u6mDksu8pMi?=
 =?us-ascii?q?zoh2WeGdxc6+bByN2/xhgRfzUJnB7Loc0qyK6/CmATRIyK3CmUhKSIZLWR4BhJ?=
 =?us-ascii?q?detC0bK+nBN3fGKuX3ZTcxBsVIWQwt1Xi6NU9IBJS2PAWK8TW94jEIBxrwKxd+?=
 =?us-ascii?q?KPjrFY7OlcS30P2594HObwlSizexfbB/IA+qoQnNq8IbnZZsJqEtxxXTv3BGYf?=
 =?us-ascii?q?5WxWRmJVKSmxbz+MK994N9/ipTpvws6ddOXb31cKokQ7NYCi8mM30u683wqRbD?=
 =?us-ascii?q?VwqP6WACXWgQjxFFHhLK7BD+Xpf2ryv6qu9w0zSUMMHqUbw5Xymp4rx1QxH0li?=
 =?us-ascii?q?gIKz858HnWisNuiqJbvAmhrAF7z4LNfY2ZKOZycqbbcNgHR2ROQ9xRWjRBDI2i?=
 =?us-ascii?q?coUBAekPM+FaoInzvFsOtRmzCBKwCO/z0DJEmmX70bEm3+knDArI3BYgH9ULsH?=
 =?us-ascii?q?nMqtv1KboZXP2vw6nPyTXDcu5d1DDm54fSdRAhpeyMUah0ccrM0kQvEwLFjlON?=
 =?us-ascii?q?qY3qJT+V1+INs3SF4OpkT+6gl2knqwRorzWp28wiiZHJi5oLxlzY8Sh12ps5KN?=
 =?us-ascii?q?OmREJhfNKpE4dcuzubOoZ0Ws8uXm9ltSkgxrEbt5O2czIGxIkpyhPecfCKfZWE?=
 =?us-ascii?q?7xT+X+iLOzh4nmhqeLenihay70egzur8W9Gq0FZFsCVFiMPAtnMT2BzJ7MiIVP?=
 =?us-ascii?q?998l2m2TaV2ADf8uBEIUYqmqrHM5Mt3KI8m54JvUjeECL6hl/6gLKVe0k44OSl?=
 =?us-ascii?q?6ubqbq3jppCGNo90jg/+Mr4pmsy6Gek4NgkOX26G+eWzzbHj/lP2QK9MjvIolq?=
 =?us-ascii?q?nVqZfaJModpqGnGQ9YyZgj6xmhADe8ytgYnmcILEhDeB2Zi4jlIVbOIOr3Dfun?=
 =?us-ascii?q?mVSjjC9rx+zaPr3mGpjNKnnDkLT/crpn5E9c1RE+zdRe55JSF7EAL+j/Wk73tN?=
 =?us-ascii?q?zEEBA5Nxa4zPrgCNV4zokeQ36AAreFMKPOtl+F/uIvLPONZI8Jojn9LOIp5/7z?=
 =?us-ascii?q?jXAjn18dcrKk3Z8WaHC+A/RnLF+VYXvqgtcdD2gKuhAyQ/DtiF2HSTRTfWq9X7?=
 =?us-ascii?q?og5jEnD4KrFYXDRoezj7Cb3ye7GZtWZmZBCl2XFXfodoOEW+oDaS6II89hlCAE?=
 =?us-ascii?q?WqalS4M7yR6uswr6waJ9LuXI4i0YqY7j1N9t6uLJjhEy9Tt0D8eH32GXVW50nH?=
 =?us-ascii?q?gFRzs33KB5vEx8xU2P0al+g/xEC9NT4+lFXRs9NZ7Z1+Z6Ecz9WhrdfteVT1ar?=
 =?us-ascii?q?WtamDis3Tt4rx98OYlxyG9Otjh3Y2yqqArkVl6GEBZAu86Lc2WTxKNh5y3rcyK?=
 =?us-ascii?q?YhiFwmSNNVNWK6nq5/6xTTB4nRnkqEjamqa7oT0DTN9GiZy2qOp19XUAh3XaXB?=
 =?us-ascii?q?XnAfY0/WoM/95kPDSb+uFLsmPhFAyc6ENqtFdNnpgU9aS/fkPdTUe3ixlHuoBR?=
 =?us-ascii?q?aU2rOMa5LndHgH0yXDFkcIiQAT8myANQglGCihpXnTDDhvFVLpfkPt/vNyqHK9?=
 =?us-ascii?q?Tk8o0Q6Ka1dt2Kay+h4QnfacUe8c3qoYuCc9rDV5BEq939PTC9qHuwphfKVdbc?=
 =?us-ascii?q?kh4Ftd0mLZrQh9Pp2mL6BtnVMedwV3v0Xz1xR4EIlAkM4qrG80wwp2M66XzFRB?=
 =?us-ascii?q?dzaA15DqJrLXMnXy/Ayoa6POwF7e1MiZ+6gR5/U4sVnspxypGVc4/HVh0NlV1G?=
 =?us-ascii?q?Wc647ODAoTV5LxT0k2+wJ7p7Hcfiky+YfU2WdwPqmztz/Iw8gpC/c9yha8Y9df?=
 =?us-ascii?q?N7uJFQ/vE8EAG8eiMu0rm1izYRICM+Bf76o0P8Kgd/ub16+nJudgnDS6jWtZ5I?=
 =?us-ascii?q?BxyF6D9y15SuTQxZYK3+mY3hebVzf7lFqhqMH3lpxeajEIA2W/zjLoBIhPaa1o?=
 =?us-ascii?q?fIYHEHuhLNezxtpjmZHtQXlY9Fi4ClMC2c+pfweSblPn0Q1R00QXvWKomS+iwz?=
 =?us-ascii?q?NolDEpq7KV3DbSzOT6aBoHJmlLSXFijFftO4S1j8oWXFO1bwgvjxal4Uf6x65G?=
 =?us-ascii?q?pKVwNWXTQEFIfzToIGFmSKe/qr2CY8tX4pMyrSpXSPi8YUydSrPloRsVyTnjH2?=
 =?us-ascii?q?hdxDA8bT2qoY/2nx95iGKcKnZ8snzZdNp0xRfe4tzcWPFQ0iAHRCl+lTnYGFy8?=
 =?us-ascii?q?M8O18tWTkpfJqvq+WH65Vp1PbSnrypuNtSuh6m1tGxG/nfGzmtv8HAg+0C/70c?=
 =?us-ascii?q?RqVCrSoBb9ZInry7q1MeZ9ckZ0A1/87tJwGptinYsomJEQxX8ai42W/XUdkGf/?=
 =?us-ascii?q?K9Vb2b/kY3oLSj4G2NrV4Anj2E1+IXOF3YP5VnOBwsR/Y9m2eH8Z2iU478pSEq?=
 =?us-ascii?q?eb8KREnTdpolq/tQ/RZPl9njQHxvc05n8VnfoJuBYzwSWHGb8dA1NYPSP3mhSM?=
 =?us-ascii?q?7tC+qrhXZWm1fbix0kp+gc6uDLWYrg5AX3b5f48oHTVs4cVnLFLMzHrz55n4eN?=
 =?us-ascii?q?nXaNIfrByVnA3Gj+hIM5IxjfsKiDFjOWL8u30l1uE6gQZv3ZG8oIiINWFt8Lil?=
 =?us-ascii?q?DR5fMz3/f9kT9S31jaZCgsaW2JiiEY57FTUMWJvoUOikEDYPtfn8MwaOETs8qm?=
 =?us-ascii?q?qUGLbFHA+f7ltmoGzLE5yxK36XI3wZx814RBaBPExfnBwUXDIik547DA+qwdLu?=
 =?us-ascii?q?cFxj6jAN/FL4qQZMyuF1Nxn5SGjfvxylajM1SJiZMRpX4RtO50bTMcyC8O1zGz?=
 =?us-ascii?q?tU8YGmrAyIMmabfRhHDXkVWkyYAFDuJrmv5d7d8+iBG+qxNf3Oba+VqexZUfeI?=
 =?us-ascii?q?yomi0o9n/zaKK8WOMWNuD/w92kpfQ395H97VlCkISywSjyjNddKUpA+g+i1rqc?=
 =?us-ascii?q?Cy6PTrVxzu5YuMCLtSMM9j+xOsgaeEOO6fmjx5KStD1p4XwX/Iyb4f3EMdii10?=
 =?us-ascii?q?djmtF6gAujDJTK7KhqBXCBsbYTtpNMRU96I8whVNOcnDh9zvy753leQ1BElFVF?=
 =?us-ascii?q?D7nsGpZNcHI2W8NFPBGUaKO66KJTzNw8Hrf6y8Tadcg/lTtx21oTybCVPsPiyf?=
 =?us-ascii?q?lzn1UBCiKf1MjCCePBBEpI6xaApiCWj9Q9LgcRC7NN53jTsrwbw7nH/KNGgcMS?=
 =?us-ascii?q?RifENJtLGf8SRYgvBnEWxb8nVlNfWEmzqe7+TAKpcZq/1rAiF1l+Jc+Hs7yrtV?=
 =?us-ascii?q?4ztCRPx6gybSqt9uo1e7kuiA0DZnURxOqipVi4KPp0ltJaLZ9pwTEUrDqTAK4H?=
 =?us-ascii?q?+fQy4NqtdoDpW7uadKzfCfnrPvJSwE9Mjbq49UK8HKKc7PD30mPRruBSXZRF8J?=
 =?us-ascii?q?Uju0b0negUJHmf2f/3HTqYI1/MvCgp0LH5tGHHY8EfcHQhBvFdsGPdFzQxsgkL?=
 =?us-ascii?q?iGnIgJ42azqF/aQ8AM7cOPbe6bHfi6cGXRtrJDfRZdhOqgdYk=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0ANAAAZyQ1ch0O0hNFkHAEBAQQBAQcEA?=
 =?us-ascii?q?QGBUQcBAQsBg2snjBOMDYIhlz2BbBsYEwGBS4YSIjQJDQEDAQEBAQEBAgETAQE?=
 =?us-ascii?q?BCgsJCCkjDII2JAGCYgMDAQIkUgYJAQEYOANUBgESBYMcggIFpkIzih+HcoQvg?=
 =?us-ascii?q?haBEY1uAok5hwCPZ1UHApFECxhffY9hiSKBBY8JgUaCDk0jXQ2CUoInF4c1hnU?=
 =?us-ascii?q?yAQExAQGBAwEBjGQBAQ?=
X-IPAS-Result: =?us-ascii?q?A0ANAAAZyQ1ch0O0hNFkHAEBAQQBAQcEAQGBUQcBAQsBg2s?=
 =?us-ascii?q?njBOMDYIhlz2BbBsYEwGBS4YSIjQJDQEDAQEBAQEBAgETAQEBCgsJCCkjDII2J?=
 =?us-ascii?q?AGCYgMDAQIkUgYJAQEYOANUBgESBYMcggIFpkIzih+HcoQvghaBEY1uAok5hwC?=
 =?us-ascii?q?PZ1UHApFECxhffY9hiSKBBY8JgUaCDk0jXQ2CUoInF4c1hnUyAQExAQGBAwEBj?=
 =?us-ascii?q?GQBAQ?=
X-IronPort-AV: E=Sophos;i="5.56,336,1539673200"; 
   d="scan'208";a="54484822"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 09 Dec 2018 18:02:55 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726476AbeLJCCw (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Sun, 9 Dec 2018 21:02:52 -0500
Received: from mga06.intel.com ([134.134.136.31]:8030 "EHLO mga06.intel.com"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1726348AbeLJCCt (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Sun, 9 Dec 2018 21:02:49 -0500
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from fmsmga006.fm.intel.com ([10.253.24.20])
  by orsmga104.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 09 Dec 2018 18:02:49 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.56,336,1539673200"; 
   d="scan'208";a="300767168"
Received: from allen-box.sh.intel.com ([10.239.161.122])
  by fmsmga006.fm.intel.com with ESMTP; 09 Dec 2018 18:02:46 -0800
From: Lu Baolu <baolu.lu@linux.intel.com>
To: Joerg Roedel <joro@8bytes.org>,
        David Woodhouse <dwmw2@infradead.org>
Cc: ashok.raj@intel.com, sanjay.k.kumar@intel.com,
        jacob.jun.pan@intel.com, kevin.tian@intel.com, yi.l.liu@intel.com,
        yi.y.sun@intel.com, peterx@redhat.com,
        Jean-Philippe Brucker <jean-philippe.brucker@arm.com>,
        iommu@lists.linux-foundation.org, linux-kernel@vger.kernel.org,
        Lu Baolu <baolu.lu@linux.intel.com>,
        Jacob Pan <jacob.jun.pan@linux.intel.com>
Subject: [PATCH v6 04/12] iommu/vt-d: Add 256-bit invalidation descriptor support
Date: Mon, 10 Dec 2018 09:58:58 +0800
Message-Id: <20181210015906.27929-5-baolu.lu@linux.intel.com>
X-Mailer: git-send-email 2.17.1
In-Reply-To: <20181210015906.27929-1-baolu.lu@linux.intel.com>
References: <20181210015906.27929-1-baolu.lu@linux.intel.com>
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

Intel vt-d spec rev3.0 requires software to use 256-bit
descriptors in invalidation queue. As the spec reads in
section 6.5.2:

Remapping hardware supporting Scalable Mode Translations
(ECAP_REG.SMTS=1) allow software to additionally program
the width of the descriptors (128-bits or 256-bits) that
will be written into the Queue. Software should setup the
Invalidation Queue for 256-bit descriptors before progra-
mming remapping hardware for scalable-mode translation as
128-bit descriptors are treated as invalid descriptors
(see Table 21 in Section 6.5.2.10) in scalable-mode.

This patch adds 256-bit invalidation descriptor support
if the hardware presents scalable mode capability.

Cc: Ashok Raj <ashok.raj@intel.com>
Cc: Jacob Pan <jacob.jun.pan@linux.intel.com>
Cc: Kevin Tian <kevin.tian@intel.com>
Signed-off-by: Sanjay Kumar <sanjay.k.kumar@intel.com>
Signed-off-by: Liu Yi L <yi.l.liu@intel.com>
Signed-off-by: Lu Baolu <baolu.lu@linux.intel.com>
---
 drivers/iommu/dmar.c                | 91 +++++++++++++++++++----------
 drivers/iommu/intel-svm.c           | 76 +++++++++++++++---------
 drivers/iommu/intel_irq_remapping.c |  6 +-
 include/linux/intel-iommu.h         |  9 ++-
 4 files changed, 121 insertions(+), 61 deletions(-)

diff --git a/drivers/iommu/dmar.c b/drivers/iommu/dmar.c
index d9c748b6f9e4..9511f9aeb77c 100644
--- a/drivers/iommu/dmar.c
+++ b/drivers/iommu/dmar.c
@@ -1160,6 +1160,7 @@ static int qi_check_fault(struct intel_iommu *iommu, int index)
 	int head, tail;
 	struct q_inval *qi = iommu->qi;
 	int wait_index = (index + 1) % QI_LENGTH;
+	int shift = qi_shift(iommu);
 
 	if (qi->desc_status[wait_index] == QI_ABORT)
 		return -EAGAIN;
@@ -1173,13 +1174,19 @@ static int qi_check_fault(struct intel_iommu *iommu, int index)
 	 */
 	if (fault & DMA_FSTS_IQE) {
 		head = readl(iommu->reg + DMAR_IQH_REG);
-		if ((head >> DMAR_IQ_SHIFT) == index) {
-			pr_err("VT-d detected invalid descriptor: "
-				"low=%llx, high=%llx\n",
-				(unsigned long long)qi->desc[index].low,
-				(unsigned long long)qi->desc[index].high);
-			memcpy(&qi->desc[index], &qi->desc[wait_index],
-					sizeof(struct qi_desc));
+		if ((head >> shift) == index) {
+			struct qi_desc *desc = qi->desc + head;
+
+			/*
+			 * desc->qw2 and desc->qw3 are either reserved or
+			 * used by software as private data. We won't print
+			 * out these two qw's for security consideration.
+			 */
+			pr_err("VT-d detected invalid descriptor: qw0 = %llx, qw1 = %llx\n",
+			       (unsigned long long)desc->qw0,
+			       (unsigned long long)desc->qw1);
+			memcpy(desc, qi->desc + (wait_index << shift),
+			       1 << shift);
 			writel(DMA_FSTS_IQE, iommu->reg + DMAR_FSTS_REG);
 			return -EINVAL;
 		}
@@ -1191,10 +1198,10 @@ static int qi_check_fault(struct intel_iommu *iommu, int index)
 	 */
 	if (fault & DMA_FSTS_ITE) {
 		head = readl(iommu->reg + DMAR_IQH_REG);
-		head = ((head >> DMAR_IQ_SHIFT) - 1 + QI_LENGTH) % QI_LENGTH;
+		head = ((head >> shift) - 1 + QI_LENGTH) % QI_LENGTH;
 		head |= 1;
 		tail = readl(iommu->reg + DMAR_IQT_REG);
-		tail = ((tail >> DMAR_IQ_SHIFT) - 1 + QI_LENGTH) % QI_LENGTH;
+		tail = ((tail >> shift) - 1 + QI_LENGTH) % QI_LENGTH;
 
 		writel(DMA_FSTS_ITE, iommu->reg + DMAR_FSTS_REG);
 
@@ -1222,15 +1229,14 @@ int qi_submit_sync(struct qi_desc *desc, struct intel_iommu *iommu)
 {
 	int rc;
 	struct q_inval *qi = iommu->qi;
-	struct qi_desc *hw, wait_desc;
+	int offset, shift, length;
+	struct qi_desc wait_desc;
 	int wait_index, index;
 	unsigned long flags;
 
 	if (!qi)
 		return 0;
 
-	hw = qi->desc;
-
 restart:
 	rc = 0;
 
@@ -1243,16 +1249,21 @@ int qi_submit_sync(struct qi_desc *desc, struct intel_iommu *iommu)
 
 	index = qi->free_head;
 	wait_index = (index + 1) % QI_LENGTH;
+	shift = qi_shift(iommu);
+	length = 1 << shift;
 
 	qi->desc_status[index] = qi->desc_status[wait_index] = QI_IN_USE;
 
-	hw[index] = *desc;
-
-	wait_desc.low = QI_IWD_STATUS_DATA(QI_DONE) |
+	offset = index << shift;
+	memcpy(qi->desc + offset, desc, length);
+	wait_desc.qw0 = QI_IWD_STATUS_DATA(QI_DONE) |
 			QI_IWD_STATUS_WRITE | QI_IWD_TYPE;
-	wait_desc.high = virt_to_phys(&qi->desc_status[wait_index]);
+	wait_desc.qw1 = virt_to_phys(&qi->desc_status[wait_index]);
+	wait_desc.qw2 = 0;
+	wait_desc.qw3 = 0;
 
-	hw[wait_index] = wait_desc;
+	offset = wait_index << shift;
+	memcpy(qi->desc + offset, &wait_desc, length);
 
 	qi->free_head = (qi->free_head + 2) % QI_LENGTH;
 	qi->free_cnt -= 2;
@@ -1261,7 +1272,7 @@ int qi_submit_sync(struct qi_desc *desc, struct intel_iommu *iommu)
 	 * update the HW tail register indicating the presence of
 	 * new descriptors.
 	 */
-	writel(qi->free_head << DMAR_IQ_SHIFT, iommu->reg + DMAR_IQT_REG);
+	writel(qi->free_head << shift, iommu->reg + DMAR_IQT_REG);
 
 	while (qi->desc_status[wait_index] != QI_DONE) {
 		/*
@@ -1298,8 +1309,10 @@ void qi_global_iec(struct intel_iommu *iommu)
 {
 	struct qi_desc desc;
 
-	desc.low = QI_IEC_TYPE;
-	desc.high = 0;
+	desc.qw0 = QI_IEC_TYPE;
+	desc.qw1 = 0;
+	desc.qw2 = 0;
+	desc.qw3 = 0;
 
 	/* should never fail */
 	qi_submit_sync(&desc, iommu);
@@ -1310,9 +1323,11 @@ void qi_flush_context(struct intel_iommu *iommu, u16 did, u16 sid, u8 fm,
 {
 	struct qi_desc desc;
 
-	desc.low = QI_CC_FM(fm) | QI_CC_SID(sid) | QI_CC_DID(did)
+	desc.qw0 = QI_CC_FM(fm) | QI_CC_SID(sid) | QI_CC_DID(did)
 			| QI_CC_GRAN(type) | QI_CC_TYPE;
-	desc.high = 0;
+	desc.qw1 = 0;
+	desc.qw2 = 0;
+	desc.qw3 = 0;
 
 	qi_submit_sync(&desc, iommu);
 }
@@ -1331,10 +1346,12 @@ void qi_flush_iotlb(struct intel_iommu *iommu, u16 did, u64 addr,
 	if (cap_read_drain(iommu->cap))
 		dr = 1;
 
-	desc.low = QI_IOTLB_DID(did) | QI_IOTLB_DR(dr) | QI_IOTLB_DW(dw)
+	desc.qw0 = QI_IOTLB_DID(did) | QI_IOTLB_DR(dr) | QI_IOTLB_DW(dw)
 		| QI_IOTLB_GRAN(type) | QI_IOTLB_TYPE;
-	desc.high = QI_IOTLB_ADDR(addr) | QI_IOTLB_IH(ih)
+	desc.qw1 = QI_IOTLB_ADDR(addr) | QI_IOTLB_IH(ih)
 		| QI_IOTLB_AM(size_order);
+	desc.qw2 = 0;
+	desc.qw3 = 0;
 
 	qi_submit_sync(&desc, iommu);
 }
@@ -1347,15 +1364,17 @@ void qi_flush_dev_iotlb(struct intel_iommu *iommu, u16 sid, u16 pfsid,
 	if (mask) {
 		WARN_ON_ONCE(addr & ((1ULL << (VTD_PAGE_SHIFT + mask)) - 1));
 		addr |= (1ULL << (VTD_PAGE_SHIFT + mask - 1)) - 1;
-		desc.high = QI_DEV_IOTLB_ADDR(addr) | QI_DEV_IOTLB_SIZE;
+		desc.qw1 = QI_DEV_IOTLB_ADDR(addr) | QI_DEV_IOTLB_SIZE;
 	} else
-		desc.high = QI_DEV_IOTLB_ADDR(addr);
+		desc.qw1 = QI_DEV_IOTLB_ADDR(addr);
 
 	if (qdep >= QI_DEV_IOTLB_MAX_INVS)
 		qdep = 0;
 
-	desc.low = QI_DEV_IOTLB_SID(sid) | QI_DEV_IOTLB_QDEP(qdep) |
+	desc.qw0 = QI_DEV_IOTLB_SID(sid) | QI_DEV_IOTLB_QDEP(qdep) |
 		   QI_DIOTLB_TYPE | QI_DEV_IOTLB_PFSID(pfsid);
+	desc.qw2 = 0;
+	desc.qw3 = 0;
 
 	qi_submit_sync(&desc, iommu);
 }
@@ -1403,16 +1422,24 @@ static void __dmar_enable_qi(struct intel_iommu *iommu)
 	u32 sts;
 	unsigned long flags;
 	struct q_inval *qi = iommu->qi;
+	u64 val = virt_to_phys(qi->desc);
 
 	qi->free_head = qi->free_tail = 0;
 	qi->free_cnt = QI_LENGTH;
 
+	/*
+	 * Set DW=1 and QS=1 in IQA_REG when Scalable Mode capability
+	 * is present.
+	 */
+	if (ecap_smts(iommu->ecap))
+		val |= (1 << 11) | 1;
+
 	raw_spin_lock_irqsave(&iommu->register_lock, flags);
 
 	/* write zero to the tail reg */
 	writel(0, iommu->reg + DMAR_IQT_REG);
 
-	dmar_writeq(iommu->reg + DMAR_IQA_REG, virt_to_phys(qi->desc));
+	dmar_writeq(iommu->reg + DMAR_IQA_REG, val);
 
 	iommu->gcmd |= DMA_GCMD_QIE;
 	writel(iommu->gcmd, iommu->reg + DMAR_GCMD_REG);
@@ -1448,8 +1475,12 @@ int dmar_enable_qi(struct intel_iommu *iommu)
 
 	qi = iommu->qi;
 
-
-	desc_page = alloc_pages_node(iommu->node, GFP_ATOMIC | __GFP_ZERO, 0);
+	/*
+	 * Need two pages to accommodate 256 descriptors of 256 bits each
+	 * if the remapping hardware supports scalable mode translation.
+	 */
+	desc_page = alloc_pages_node(iommu->node, GFP_ATOMIC | __GFP_ZERO,
+				     !!ecap_smts(iommu->ecap));
 	if (!desc_page) {
 		kfree(qi);
 		iommu->qi = NULL;
diff --git a/drivers/iommu/intel-svm.c b/drivers/iommu/intel-svm.c
index 9014b07ab975..9b6771a89207 100644
--- a/drivers/iommu/intel-svm.c
+++ b/drivers/iommu/intel-svm.c
@@ -161,27 +161,40 @@ static void intel_flush_svm_range_dev (struct intel_svm *svm, struct intel_svm_d
 		 * because that's the only option the hardware gives us. Despite
 		 * the fact that they are actually only accessible through one. */
 		if (gl)
-			desc.low = QI_EIOTLB_PASID(svm->pasid) | QI_EIOTLB_DID(sdev->did) |
-				QI_EIOTLB_GRAN(QI_GRAN_ALL_ALL) | QI_EIOTLB_TYPE;
+			desc.qw0 = QI_EIOTLB_PASID(svm->pasid) |
+					QI_EIOTLB_DID(sdev->did) |
+					QI_EIOTLB_GRAN(QI_GRAN_ALL_ALL) |
+					QI_EIOTLB_TYPE;
 		else
-			desc.low = QI_EIOTLB_PASID(svm->pasid) | QI_EIOTLB_DID(sdev->did) |
-				QI_EIOTLB_GRAN(QI_GRAN_NONG_PASID) | QI_EIOTLB_TYPE;
-		desc.high = 0;
+			desc.qw0 = QI_EIOTLB_PASID(svm->pasid) |
+					QI_EIOTLB_DID(sdev->did) |
+					QI_EIOTLB_GRAN(QI_GRAN_NONG_PASID) |
+					QI_EIOTLB_TYPE;
+		desc.qw1 = 0;
 	} else {
 		int mask = ilog2(__roundup_pow_of_two(pages));
 
-		desc.low = QI_EIOTLB_PASID(svm->pasid) | QI_EIOTLB_DID(sdev->did) |
-			QI_EIOTLB_GRAN(QI_GRAN_PSI_PASID) | QI_EIOTLB_TYPE;
-		desc.high = QI_EIOTLB_ADDR(address) | QI_EIOTLB_GL(gl) |
-			QI_EIOTLB_IH(ih) | QI_EIOTLB_AM(mask);
+		desc.qw0 = QI_EIOTLB_PASID(svm->pasid) |
+				QI_EIOTLB_DID(sdev->did) |
+				QI_EIOTLB_GRAN(QI_GRAN_PSI_PASID) |
+				QI_EIOTLB_TYPE;
+		desc.qw1 = QI_EIOTLB_ADDR(address) |
+				QI_EIOTLB_GL(gl) |
+				QI_EIOTLB_IH(ih) |
+				QI_EIOTLB_AM(mask);
 	}
+	desc.qw2 = 0;
+	desc.qw3 = 0;
 	qi_submit_sync(&desc, svm->iommu);
 
 	if (sdev->dev_iotlb) {
-		desc.low = QI_DEV_EIOTLB_PASID(svm->pasid) | QI_DEV_EIOTLB_SID(sdev->sid) |
-			QI_DEV_EIOTLB_QDEP(sdev->qdep) | QI_DEIOTLB_TYPE;
+		desc.qw0 = QI_DEV_EIOTLB_PASID(svm->pasid) |
+				QI_DEV_EIOTLB_SID(sdev->sid) |
+				QI_DEV_EIOTLB_QDEP(sdev->qdep) |
+				QI_DEIOTLB_TYPE;
 		if (pages == -1) {
-			desc.high = QI_DEV_EIOTLB_ADDR(-1ULL >> 1) | QI_DEV_EIOTLB_SIZE;
+			desc.qw1 = QI_DEV_EIOTLB_ADDR(-1ULL >> 1) |
+					QI_DEV_EIOTLB_SIZE;
 		} else if (pages > 1) {
 			/* The least significant zero bit indicates the size. So,
 			 * for example, an "address" value of 0x12345f000 will
@@ -189,10 +202,13 @@ static void intel_flush_svm_range_dev (struct intel_svm *svm, struct intel_svm_d
 			unsigned long last = address + ((unsigned long)(pages - 1) << VTD_PAGE_SHIFT);
 			unsigned long mask = __rounddown_pow_of_two(address ^ last);
 
-			desc.high = QI_DEV_EIOTLB_ADDR((address & ~mask) | (mask - 1)) | QI_DEV_EIOTLB_SIZE;
+			desc.qw1 = QI_DEV_EIOTLB_ADDR((address & ~mask) |
+					(mask - 1)) | QI_DEV_EIOTLB_SIZE;
 		} else {
-			desc.high = QI_DEV_EIOTLB_ADDR(address);
+			desc.qw1 = QI_DEV_EIOTLB_ADDR(address);
 		}
+		desc.qw2 = 0;
+		desc.qw3 = 0;
 		qi_submit_sync(&desc, svm->iommu);
 	}
 }
@@ -237,8 +253,11 @@ static void intel_flush_pasid_dev(struct intel_svm *svm, struct intel_svm_dev *s
 {
 	struct qi_desc desc;
 
-	desc.high = 0;
-	desc.low = QI_PC_TYPE | QI_PC_DID(sdev->did) | QI_PC_PASID_SEL | QI_PC_PASID(pasid);
+	desc.qw0 = QI_PC_TYPE | QI_PC_DID(sdev->did) |
+			QI_PC_PASID_SEL | QI_PC_PASID(pasid);
+	desc.qw1 = 0;
+	desc.qw2 = 0;
+	desc.qw3 = 0;
 
 	qi_submit_sync(&desc, svm->iommu);
 }
@@ -667,24 +686,27 @@ static irqreturn_t prq_event_thread(int irq, void *d)
 	no_pasid:
 		if (req->lpig) {
 			/* Page Group Response */
-			resp.low = QI_PGRP_PASID(req->pasid) |
+			resp.qw0 = QI_PGRP_PASID(req->pasid) |
 				QI_PGRP_DID((req->bus << 8) | req->devfn) |
 				QI_PGRP_PASID_P(req->pasid_present) |
 				QI_PGRP_RESP_TYPE;
-			resp.high = QI_PGRP_IDX(req->prg_index) |
-				QI_PGRP_PRIV(req->private) | QI_PGRP_RESP_CODE(result);
-
-			qi_submit_sync(&resp, iommu);
+			resp.qw1 = QI_PGRP_IDX(req->prg_index) |
+				QI_PGRP_PRIV(req->private) |
+				QI_PGRP_RESP_CODE(result);
 		} else if (req->srr) {
 			/* Page Stream Response */
-			resp.low = QI_PSTRM_IDX(req->prg_index) |
-				QI_PSTRM_PRIV(req->private) | QI_PSTRM_BUS(req->bus) |
-				QI_PSTRM_PASID(req->pasid) | QI_PSTRM_RESP_TYPE;
-			resp.high = QI_PSTRM_ADDR(address) | QI_PSTRM_DEVFN(req->devfn) |
+			resp.qw0 = QI_PSTRM_IDX(req->prg_index) |
+				QI_PSTRM_PRIV(req->private) |
+				QI_PSTRM_BUS(req->bus) |
+				QI_PSTRM_PASID(req->pasid) |
+				QI_PSTRM_RESP_TYPE;
+			resp.qw1 = QI_PSTRM_ADDR(address) |
+				QI_PSTRM_DEVFN(req->devfn) |
 				QI_PSTRM_RESP_CODE(result);
-
-			qi_submit_sync(&resp, iommu);
 		}
+		resp.qw2 = 0;
+		resp.qw3 = 0;
+		qi_submit_sync(&resp, iommu);
 
 		head = (head + sizeof(*req)) & PRQ_RING_MASK;
 	}
diff --git a/drivers/iommu/intel_irq_remapping.c b/drivers/iommu/intel_irq_remapping.c
index c2d6c11431de..24d45b07f425 100644
--- a/drivers/iommu/intel_irq_remapping.c
+++ b/drivers/iommu/intel_irq_remapping.c
@@ -145,9 +145,11 @@ static int qi_flush_iec(struct intel_iommu *iommu, int index, int mask)
 {
 	struct qi_desc desc;
 
-	desc.low = QI_IEC_IIDEX(index) | QI_IEC_TYPE | QI_IEC_IM(mask)
+	desc.qw0 = QI_IEC_IIDEX(index) | QI_IEC_TYPE | QI_IEC_IM(mask)
 		   | QI_IEC_SELECTIVE;
-	desc.high = 0;
+	desc.qw1 = 0;
+	desc.qw2 = 0;
+	desc.qw3 = 0;
 
 	return qi_submit_sync(&desc, iommu);
 }
diff --git a/include/linux/intel-iommu.h b/include/linux/intel-iommu.h
index b4da61385ebf..08ff588a4df7 100644
--- a/include/linux/intel-iommu.h
+++ b/include/linux/intel-iommu.h
@@ -401,13 +401,18 @@ enum {
 #define QI_GRAN_NONG_PASID		2
 #define QI_GRAN_PSI_PASID		3
 
+#define qi_shift(iommu)		(DMAR_IQ_SHIFT + !!ecap_smts((iommu)->ecap))
+
 struct qi_desc {
-	u64 low, high;
+	u64 qw0;
+	u64 qw1;
+	u64 qw2;
+	u64 qw3;
 };
 
 struct q_inval {
 	raw_spinlock_t  q_lock;
-	struct qi_desc  *desc;          /* invalidation queue */
+	void		*desc;          /* invalidation queue */
 	int             *desc_status;   /* desc status */
 	int             free_head;      /* first free entry */
 	int             free_tail;      /* last free entry */
-- 
2.17.1

