Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 04 Dec 2018 08:54:12 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga004.jf.intel.com (orsmga004.jf.intel.com [10.7.209.38])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 799CF58014B;
	Mon,  3 Dec 2018 07:46:57 -0800 (PST)
Received: from fmsmga103.fm.intel.com ([10.1.193.90])
  by orsmga004-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 03 Dec 2018 07:46:57 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3A0A3mEBJHSVFBsVacoNmcpTZWNBhigK39O0sv0rFi?=
 =?us-ascii?q?tYgUL/75rarrMEGX3/hxlliBBdydt6oUzbKO+4nbGkU4qa6bt34DdJEeHzQksu?=
 =?us-ascii?q?4x2zIaPcieFEfgJ+TrZSFpVO5LVVti4m3peRMNQJW2aFLduGC94iAPERvjKwV1?=
 =?us-ascii?q?Ov71GonPhMiryuy+4ZLebxlLiTanfb9+MAi9oBnMuMURnYZsMLs6xAHTontPde?=
 =?us-ascii?q?RWxGdoKkyWkh3h+Mq+/4Nt/jpJtf45+MFOTav1f6IjTbxFFzsmKHw65NfqtRbY?=
 =?us-ascii?q?UwSC4GYXX3gMnRpJBwjF6wz6Xov0vyDnuOdxxDWWMMvrRr0vRz+s87lkRwPpiC?=
 =?us-ascii?q?cfNj427mfXitBrjKlGpB6tvgFzz5LIbI2QMvd1Y6HTcs4ARWdZQ8hfSSJBDIO/?=
 =?us-ascii?q?YYUBAeUOMuRXoJXyqVsVtRuzBxKhBP/txzJSmnP6waM33uYnHArb3AIgBdUOsH?=
 =?us-ascii?q?HModjpMqcSSuC1zLTNzTrZafNdxDLz6IjSfRAnvP6MQKh/cc7MwkQoDQzFiE6Q?=
 =?us-ascii?q?qYz4PzOQzOsNtXKX7+lgVe21jW4othxxrSKrxso3kIbJnIcVxkrY+iV+xYY4PN?=
 =?us-ascii?q?u1Q1N4b968CJZcqT2WOo9sTs8/TWxkpjw2xqAFtJKnZiQHyZYqywbBZ/CacYWE?=
 =?us-ascii?q?+A/vWemNLTtimX5pZK+ziwyw/ES8zOD3S9O630xQriVfl9nBrnAN2ALX6siAUv?=
 =?us-ascii?q?Z94Eih1iiV1wzJ6eFLP1o0lazFJJ4l2LIwkYATsUvbEi/3nkX5krOWe1069uS0?=
 =?us-ascii?q?7+nreKjqq5GCO4Nulw3zMbgilta+DOk6KgQOWnKU+eW41L3t5035R7BKg+Uykq?=
 =?us-ascii?q?nYtpDaOMsaqre6AwBLyIYj7QiwDzO/3NQfk3gHKkxKeAicgoj3NFHBPur4Ae28?=
 =?us-ascii?q?g1uyijdrwe7JPrn7DpXKNHjDn6/tfaxh5E5E1Aoz0ddf6opQCrEAI/L8RFX9td?=
 =?us-ascii?q?PFDhIiNwy0wuDnCMhy148EWGKPBLOZP73WsVOS+u0vJOyMbpcPuDnhM/gl++Lu?=
 =?us-ascii?q?jXghlF8dZ6ap3IcXZ2q/Hvh8I0WZfGDjgtEOEWoRugo+TerqiECNUDJJZnayWb?=
 =?us-ascii?q?486S8/CI68EYjDQYWtiqSb3CinBp1WenxGCleUHHfqcIWLRe0AaCGVIs9nlDwE?=
 =?us-ascii?q?UqOsS4sg1RGoqQ/7xKBrLuvS+i0Eq53j0MJ56PHUlRE37TZ0FdiS03mRT2FomW?=
 =?us-ascii?q?MFXyU53Lt/oUx6yVePy7J4jOZaFdFI4/NJUwE6NYPTzuBgCtDyXB7BccmNSFq8?=
 =?us-ascii?q?XtqmBjQxRMorw9ASe0Z9B8mijhfb0iqpGbAVkaaHBJg18q3G2XjxKN1wy3LH1K?=
 =?us-ascii?q?knklknTdFDNWyghq5j6QfTA5TFnFmel6avba4cxjLC9H+fzWqSu0FVSBN/Xr/b?=
 =?us-ascii?q?XX8BfEfWrc725kXZT7CwD7QrNQ9Byc2HKqtOcdDpiVRGRPH+ONXReW6xmmGwBQ?=
 =?us-ascii?q?qWybOIdoblZ2Id3CDFAkgejw8T5WqGNRQ5Biq5vm3RFiJuGkz1b0Ps6+Z+rmi7?=
 =?us-ascii?q?QVEyzwyRa01h1ry1+gMahPCGSvMT2K4EtzklqzluAFm92NfWAcKapwV9ZKVcfc?=
 =?us-ascii?q?894FBf2GLFtgx9O5ugL7xihl8eaQh3o1ni1xJtCoVEkMgqqnwqwRF2KaKZ1lNB?=
 =?us-ascii?q?ajyZ0YrxOr3RNmn94hSvZ7TK1VHZ1dac4r0P5+ggq1X/oAGpEVIv/G9j09ZL3H?=
 =?us-ascii?q?qT+JXLABAJXpLsT0k47R56p7LdYikj/I7U0XxsMa+psj7Nwd4pBe0lygq+cNdb?=
 =?us-ascii?q?Kq+LCAjyE8gCDci0NOMqg0Spbg4DPO1K9K80ItmqeOec1K+qPOZvhjSmjWtc7Y?=
 =?us-ascii?q?B500KM8Td8S+HS05YExfGYwhWIVzPmgFi9tcD3nJhOZSsOEWqn1SjkGIlRa7Vo?=
 =?us-ascii?q?fYYKFWihOde3ych5h5L3XX5X6kSjB1If1MC1YxWSa0Hy0hNK1UQQp3yqgi+4zz?=
 =?us-ascii?q?1ykzE0oauTxi3Ow+L+dBUZPm5HXnVtjVDpIYKsldAVQFCobxQ1lBui/Uv7x6lb?=
 =?us-ascii?q?qL5/LmXJWkdIYi72InpmUquxsLqCfsFO5IkpsSVRTOSzf1SaRqThrBsd1iPpB3?=
 =?us-ascii?q?FeyywjdzG2ppX5mAR3iGCHI3Zpr3rZesZwyQ3E5NPGRv5R3TsGRC9mhjnRHVW8?=
 =?us-ascii?q?O9ip/dOJl5bMqOy+VmShVoFNfinv14+PqCy75WhyCx2lg/+zgsHnERQ90SLj19?=
 =?us-ascii?q?hlTyLIoAz+Yonq0aS3KuZnfkhuBF/h5Mt2AIB+ko0shJ4O3XgWnIma/X0CkW3r?=
 =?us-ascii?q?K9VUxbr+bGYRRT4M29PV4xLq2Ex5InKJ2oL2THOdwsR6atm+Y2MW3D897s9QBK?=
 =?us-ascii?q?eV6rxEgTV6ol6ioQ3NZvh9my8XyeEy534Cn+EJpA0twz2YArATHklXJzbglhqW?=
 =?us-ascii?q?4NClsKVYenyvfqOu20pkktCsF7WCogBaWHbkdZYuBy5w7sNjMF3S1H3/8J3reN?=
 =?us-ascii?q?7VbdgLrB2bjw/Aj/RJKJI2jvcKmS1nOWfnsXwk0eE7iwFu3YqhvIiGMGht+KO5?=
 =?us-ascii?q?AhhFNjz6fc8T+zftjbpAkcaSxYygApJhGjATVpvyUf2oCC4StejgNwuWCzIzsH?=
 =?us-ascii?q?CbGb7CHQOF7EdmsmnCE5SqN3GROXkYws9uRBibJExDng8UWC82kYI+FgCv3Mbh?=
 =?us-ascii?q?alt25igN5l7krRtB0uBoOAP6UmjBvwekcCs0RIKcLBpL7QFC+kHVPtaF7u9oGy?=
 =?us-ascii?q?FY/5uhrBGCK2CBZgRIC30JVVKAB1z5Irau4tzA+fCCBuWiN/vOfamOqetGWvaI?=
 =?us-ascii?q?wpKvz5Jm/yuWOcWJJHViFPo72kxMXX1iH8TZmjMPSzEYli7Xbs6bogu89TNzrs?=
 =?us-ascii?q?yl7PvrXwfv75OVC7ROKdVv5wy2gaCbOu6Qmil5KDVY1pANxXPSy7gfxlkSiy5w?=
 =?us-ascii?q?eDm3DLQArjXATKbRmq9REh4aZDl/NMpO76IgwAZNPdTXhc/y1r59lvQ1EUtKVU?=
 =?us-ascii?q?T9msG1YswHO3uyNE7cC0aRKruHJSfHw8X2Ya6nTb1QjeNUtwC/uDqBEk/jOCiD?=
 =?us-ascii?q?mCftVxy1Le5MiySbNgREuI6hahZtFXTjTNX+Zx2nMd93iCc6zqEuinzWNW4TLz?=
 =?us-ascii?q?58flhJrr2R6yNYn/p+F3ZA7npjMemLhSKZ4/PEJZYRtPthGj50mP5C4HQm17tV?=
 =?us-ascii?q?6zlJRfxvlyvUq95uolemnfGOyzpnShVOrDlLiZmPvUVjP6XZ65ZBVWzF/BIL8W?=
 =?us-ascii?q?WfFRAKq8F5Bd3ovqADguTIwZruLjZeu/vb58IDDsycfNqaNns9dxPmAjfLBQ8t?=
 =?us-ascii?q?RCSuc2rYghoZ2PKM83SPq7A+q4Pwg9wFUflQU1goE/kbAwJoEMFRDo1wW2YNl6?=
 =?us-ascii?q?WbkcsB4zKdrBDXSNhTutiTX/SJAejqITDfjrlNYRcVyLfQJJ4WcIb83hoxORFB?=
 =?us-ascii?q?gI3WFh+IDph2qSp7Y1px+R0V/Q=3D=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AOAADmTgVch0O0hNFiDg4BAQEEAQEHB?=
 =?us-ascii?q?AEBgVEHAQELAYNrFBODeYgYjAqBYAglFJc0gXYpEwGICSI0CQ0BAwEBAQEBAQI?=
 =?us-ascii?q?BEwEBAQoLCQgpL4I2JAGCYQEBAQECAQECIFEFBQEJAQEKGAICJgICA1QGAQoCB?=
 =?us-ascii?q?gIBAQGDHIF6CAQBpUOBL4VAhF+BC4sRgVc/gREnDIIxLogFglcCiR6GVjWQFwc?=
 =?us-ascii?q?Cgh8EjxMeiWuHO4kEkS+CDU0jgzyCJxcSjVBDNwEBMYEFAQGKVwEB?=
X-IPAS-Result: =?us-ascii?q?A0AOAADmTgVch0O0hNFiDg4BAQEEAQEHBAEBgVEHAQELAYN?=
 =?us-ascii?q?rFBODeYgYjAqBYAglFJc0gXYpEwGICSI0CQ0BAwEBAQEBAQIBEwEBAQoLCQgpL?=
 =?us-ascii?q?4I2JAGCYQEBAQECAQECIFEFBQEJAQEKGAICJgICA1QGAQoCBgIBAQGDHIF6CAQ?=
 =?us-ascii?q?BpUOBL4VAhF+BC4sRgVc/gREnDIIxLogFglcCiR6GVjWQFwcCgh8EjxMeiWuHO?=
 =?us-ascii?q?4kEkS+CDU0jgzyCJxcSjVBDNwEBMYEFAQGKVwEB?=
X-IronPort-AV: E=Sophos;i="5.56,310,1539673200"; 
   d="scan'208";a="54293340"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 03 Dec 2018 07:46:55 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726753AbeLCPqx (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Mon, 3 Dec 2018 10:46:53 -0500
Received: from foss.arm.com ([217.140.101.70]:40210 "EHLO foss.arm.com"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1726571AbeLCPqx (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 3 Dec 2018 10:46:53 -0500
Received: from usa-sjc-imap-foss1.foss.arm.com (unknown [10.72.51.249])
        by usa-sjc-mx-foss1.foss.arm.com (Postfix) with ESMTP id C15E41682;
        Mon,  3 Dec 2018 07:46:50 -0800 (PST)
Received: from [10.1.37.145] (p8cg001049571a15.cambridge.arm.com [10.1.37.145])
        by usa-sjc-imap-foss1.foss.arm.com (Postfix) with ESMTPSA id C2D243F5AF;
        Mon,  3 Dec 2018 07:46:48 -0800 (PST)
Subject: Re: [PATCH v9 8/8] KVM: arm64: Add support for creating PUD hugepages
 at stage 2
To: Punit Agrawal <punit.agrawal@arm.com>, kvmarm@lists.cs.columbia.edu
Cc: suzuki.poulose@arm.com, marc.zyngier@arm.com,
        Catalin Marinas <catalin.marinas@arm.com>, will.deacon@arm.com,
        linux-kernel@vger.kernel.org,
        Christoffer Dall <christoffer.dall@arm.com>,
        punitagrawal@gmail.com, Russell King <linux@armlinux.org.uk>,
        linux-arm-kernel@lists.infradead.org
References: <20181031175745.18650-1-punit.agrawal@arm.com>
 <20181031175745.18650-9-punit.agrawal@arm.com>
From: Anshuman Khandual <anshuman.khandual@arm.com>
Message-ID: <3c37af59-6936-20fb-b3a3-eaa10f3ada53@arm.com>
Date: Mon, 3 Dec 2018 21:16:54 +0530
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko/20100101
 Thunderbird/52.9.1
MIME-Version: 1.0
In-Reply-To: <20181031175745.18650-9-punit.agrawal@arm.com>
Content-Type: text/plain; charset=utf-8
Content-Language: en-US
Content-Transfer-Encoding: 7bit
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org



On 10/31/2018 11:27 PM, Punit Agrawal wrote:
> KVM only supports PMD hugepages at stage 2. Now that the various page
> handling routines are updated, extend the stage 2 fault handling to
> map in PUD hugepages.
> 
> Addition of PUD hugepage support enables additional page sizes (e.g.,
> 1G with 4K granule) which can be useful on cores that support mapping
> larger block sizes in the TLB entries.
> 
> Signed-off-by: Punit Agrawal <punit.agrawal@arm.com>
> Reviewed-by: Suzuki Poulose <suzuki.poulose@arm.com>
> Cc: Christoffer Dall <christoffer.dall@arm.com>
> Cc: Marc Zyngier <marc.zyngier@arm.com>
> Cc: Russell King <linux@armlinux.org.uk>
> Cc: Catalin Marinas <catalin.marinas@arm.com>
> Cc: Will Deacon <will.deacon@arm.com>
> ---
>  arch/arm/include/asm/kvm_mmu.h         |  20 +++++
>  arch/arm/include/asm/stage2_pgtable.h  |   5 ++
>  arch/arm64/include/asm/kvm_mmu.h       |  16 ++++
>  arch/arm64/include/asm/pgtable-hwdef.h |   2 +
>  arch/arm64/include/asm/pgtable.h       |   2 +
>  virt/kvm/arm/mmu.c                     | 104 +++++++++++++++++++++++--
>  6 files changed, 143 insertions(+), 6 deletions(-)
> 
> diff --git a/arch/arm/include/asm/kvm_mmu.h b/arch/arm/include/asm/kvm_mmu.h
> index e62f0913ce7d..6336319a0d5b 100644
> --- a/arch/arm/include/asm/kvm_mmu.h
> +++ b/arch/arm/include/asm/kvm_mmu.h
> @@ -84,11 +84,14 @@ void kvm_clear_hyp_idmap(void);
>  
>  #define kvm_pfn_pte(pfn, prot)	pfn_pte(pfn, prot)
>  #define kvm_pfn_pmd(pfn, prot)	pfn_pmd(pfn, prot)
> +#define kvm_pfn_pud(pfn, prot)	(__pud(0))

This makes sense. Was not enabled before and getting added here.

>  
>  #define kvm_pud_pfn(pud)	({ BUG(); 0; })
>  
>  
>  #define kvm_pmd_mkhuge(pmd)	pmd_mkhuge(pmd)
> +/* No support for pud hugepages */
> +#define kvm_pud_mkhuge(pud)	( {BUG(); pud; })
>  
>  /*
>   * The following kvm_*pud*() functions are provided strictly to allow
> @@ -105,6 +108,23 @@ static inline bool kvm_s2pud_readonly(pud_t *pud)
>  	return false;
>  }
>  
> +static inline void kvm_set_pud(pud_t *pud, pud_t new_pud)
> +{
> +	BUG();
> +}
> +
> +static inline pud_t kvm_s2pud_mkwrite(pud_t pud)
> +{
> +	BUG();
> +	return pud;
> +}
> +
> +static inline pud_t kvm_s2pud_mkexec(pud_t pud)
> +{
> +	BUG();
> +	return pud;
> +}
> +

But the previous patches have been adding some of these helpers
as well. Needs consolidation.

>  static inline bool kvm_s2pud_exec(pud_t *pud)
>  {
>  	BUG();
> diff --git a/arch/arm/include/asm/stage2_pgtable.h b/arch/arm/include/asm/stage2_pgtable.h
> index f6a7ea805232..f9017167a8d1 100644
> --- a/arch/arm/include/asm/stage2_pgtable.h
> +++ b/arch/arm/include/asm/stage2_pgtable.h
> @@ -68,4 +68,9 @@ stage2_pmd_addr_end(struct kvm *kvm, phys_addr_t addr, phys_addr_t end)
>  #define stage2_pmd_table_empty(kvm, pmdp)	kvm_page_empty(pmdp)
>  #define stage2_pud_table_empty(kvm, pudp)	false
>  
> +static inline bool kvm_stage2_has_pud(struct kvm *kvm)
> +{
> +	return false;
> +}
>

Makes sense. Disabled on arm32.

 +
>  #endif	/* __ARM_S2_PGTABLE_H_ */
> diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h
> index 9f941f70775c..8af4b1befa42 100644
> --- a/arch/arm64/include/asm/kvm_mmu.h
> +++ b/arch/arm64/include/asm/kvm_mmu.h
> @@ -184,12 +184,16 @@ void kvm_clear_hyp_idmap(void);
>  #define kvm_mk_pgd(pudp)					\
>  	__pgd(__phys_to_pgd_val(__pa(pudp)) | PUD_TYPE_TABLE)
>  
> +#define kvm_set_pud(pudp, pud)		set_pud(pudp, pud)
> +
>  #define kvm_pfn_pte(pfn, prot)		pfn_pte(pfn, prot)
>  #define kvm_pfn_pmd(pfn, prot)		pfn_pmd(pfn, prot)
> +#define kvm_pfn_pud(pfn, prot)		pfn_pud(pfn, prot)
>  
>  #define kvm_pud_pfn(pud)		pud_pfn(pud)
>  
>  #define kvm_pmd_mkhuge(pmd)		pmd_mkhuge(pmd)
> +#define kvm_pud_mkhuge(pud)		pud_mkhuge(pud)
>  
>  static inline pte_t kvm_s2pte_mkwrite(pte_t pte)
>  {
> @@ -203,6 +207,12 @@ static inline pmd_t kvm_s2pmd_mkwrite(pmd_t pmd)
>  	return pmd;
>  }
>  
> +static inline pud_t kvm_s2pud_mkwrite(pud_t pud)
> +{
> +	pud_val(pud) |= PUD_S2_RDWR;
> +	return pud;
> +}
> +
>  static inline pte_t kvm_s2pte_mkexec(pte_t pte)
>  {
>  	pte_val(pte) &= ~PTE_S2_XN;
> @@ -215,6 +225,12 @@ static inline pmd_t kvm_s2pmd_mkexec(pmd_t pmd)
>  	return pmd;
>  }
>  
> +static inline pud_t kvm_s2pud_mkexec(pud_t pud)
> +{
> +	pud_val(pud) &= ~PUD_S2_XN;
> +	return pud;
> +}
> +

Should not these be consolidated in previous patches ?

>  static inline void kvm_set_s2pte_readonly(pte_t *ptep)
>  {
>  	pteval_t old_pteval, pteval;
> diff --git a/arch/arm64/include/asm/pgtable-hwdef.h b/arch/arm64/include/asm/pgtable-hwdef.h
> index 336e24cddc87..6f1c187f1c86 100644
> --- a/arch/arm64/include/asm/pgtable-hwdef.h
> +++ b/arch/arm64/include/asm/pgtable-hwdef.h
> @@ -193,6 +193,8 @@
>  #define PMD_S2_RDWR		(_AT(pmdval_t, 3) << 6)   /* HAP[2:1] */
>  #define PMD_S2_XN		(_AT(pmdval_t, 2) << 53)  /* XN[1:0] */
>  
> +#define PUD_S2_RDONLY		(_AT(pudval_t, 1) << 6)   /* HAP[2:1] */
> +#define PUD_S2_RDWR		(_AT(pudval_t, 3) << 6)   /* HAP[2:1] */
>  #define PUD_S2_XN		(_AT(pudval_t, 2) << 53)  /* XN[1:0] */

All PUD related HW page table definitions should have been added
at once in one of the preparatory patches.

>  
>  /*
> diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
> index bb0f3f17a7a9..576128635f3c 100644
> --- a/arch/arm64/include/asm/pgtable.h
> +++ b/arch/arm64/include/asm/pgtable.h
> @@ -390,6 +390,8 @@ static inline int pmd_protnone(pmd_t pmd)
>  #define pud_mkyoung(pud)	pte_pud(pte_mkyoung(pud_pte(pud)))
>  #define pud_write(pud)		pte_write(pud_pte(pud))
>  
> +#define pud_mkhuge(pud)		(__pud(pud_val(pud) & ~PUD_TABLE_BIT))
> +

All new pud_* definitions should have come together at once as well. 

>  #define __pud_to_phys(pud)	__pte_to_phys(pud_pte(pud))
>  #define __phys_to_pud_val(phys)	__phys_to_pte_val(phys)
>  #define pud_pfn(pud)		((__pud_to_phys(pud) & PUD_MASK) >> PAGE_SHIFT)



> diff --git a/virt/kvm/arm/mmu.c b/virt/kvm/arm/mmu.c
> index 3893ea6a50bf..2dcff38868d4 100644
> --- a/virt/kvm/arm/mmu.c
> +++ b/virt/kvm/arm/mmu.c
> @@ -115,6 +115,25 @@ static void stage2_dissolve_pmd(struct kvm *kvm, phys_addr_t addr, pmd_t *pmd)
>  	put_page(virt_to_page(pmd));
>  }
>  
> +/**
> + * stage2_dissolve_pud() - clear and flush huge PUD entry
> + * @kvm:	pointer to kvm structure.
> + * @addr:	IPA
> + * @pud:	pud pointer for IPA
> + *
> + * Function clears a PUD entry, flushes addr 1st and 2nd stage TLBs. Marks all
> + * pages in the range dirty.
> + */
> +static void stage2_dissolve_pud(struct kvm *kvm, phys_addr_t addr, pud_t *pudp)
> +{
> +	if (!stage2_pud_huge(kvm, *pudp))
> +		return;
> +
> +	stage2_pud_clear(kvm, pudp);
> +	kvm_tlb_flush_vmid_ipa(kvm, addr);
> +	put_page(virt_to_page(pudp));
> +}
> +
>  static int mmu_topup_memory_cache(struct kvm_mmu_memory_cache *cache,
>  				  int min, int max)
>  {
> @@ -1022,7 +1041,7 @@ static pmd_t *stage2_get_pmd(struct kvm *kvm, struct kvm_mmu_memory_cache *cache
>  	pmd_t *pmd;
>  
>  	pud = stage2_get_pud(kvm, cache, addr);
> -	if (!pud)
> +	if (!pud || stage2_pud_huge(kvm, *pud))
>  		return NULL;
>  
>  	if (stage2_pud_none(kvm, *pud)) {
> @@ -1083,6 +1102,36 @@ static int stage2_set_pmd_huge(struct kvm *kvm, struct kvm_mmu_memory_cache
>  	return 0;
>  }
>  
> +static int stage2_set_pud_huge(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,
> +			       phys_addr_t addr, const pud_t *new_pudp)
> +{
> +	pud_t *pudp, old_pud;
> +
> +	pudp = stage2_get_pud(kvm, cache, addr);
> +	VM_BUG_ON(!pudp);
> +
> +	old_pud = *pudp;
> +
> +	/*
> +	 * A large number of vcpus faulting on the same stage 2 entry,
> +	 * can lead to a refault due to the
> +	 * stage2_pud_clear()/tlb_flush(). Skip updating the page
> +	 * tables if there is no change.
> +	 */
> +	if (pud_val(old_pud) == pud_val(*new_pudp))
> +		return 0;
> +
> +	if (stage2_pud_present(kvm, old_pud)) {
> +		stage2_pud_clear(kvm, pudp);
> +		kvm_tlb_flush_vmid_ipa(kvm, addr);
> +	} else {
> +		get_page(virt_to_page(pudp));
> +	}
> +
> +	kvm_set_pud(pudp, *new_pudp);
> +	return 0;
> +}
> +
>  /*
>   * stage2_get_leaf_entry - walk the stage2 VM page tables and return
>   * true if a valid and present leaf-entry is found. A pointer to the
> @@ -1149,6 +1198,7 @@ static int stage2_set_pte(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,
>  			  phys_addr_t addr, const pte_t *new_pte,
>  			  unsigned long flags)
>  {
> +	pud_t *pud;
>  	pmd_t *pmd;
>  	pte_t *pte, old_pte;
>  	bool iomap = flags & KVM_S2PTE_FLAG_IS_IOMAP;
> @@ -1157,7 +1207,31 @@ static int stage2_set_pte(struct kvm *kvm, struct kvm_mmu_memory_cache *cache,
>  	VM_BUG_ON(logging_active && !cache);
>  
>  	/* Create stage-2 page table mapping - Levels 0 and 1 */
> -	pmd = stage2_get_pmd(kvm, cache, addr);
> +	pud = stage2_get_pud(kvm, cache, addr);
> +	if (!pud) {
> +		/*
> +		 * Ignore calls from kvm_set_spte_hva for unallocated
> +		 * address ranges.
> +		 */
> +		return 0;
> +	}
> +
> +	/*
> +	 * While dirty page logging - dissolve huge PUD, then continue
> +	 * on to allocate page.
> +	 */
> +	if (logging_active)
> +		stage2_dissolve_pud(kvm, addr, pud);
> +
> +	if (stage2_pud_none(kvm, *pud)) {
> +		if (!cache)
> +			return 0; /* ignore calls from kvm_set_spte_hva */
> +		pmd = mmu_memory_cache_alloc(cache);
> +		stage2_pud_populate(kvm, pud, pmd);
> +		get_page(virt_to_page(pud));
> +	}
> +
> +	pmd = stage2_pmd_offset(kvm, pud, addr);
>  	if (!pmd) {
>  		/*
>  		 * Ignore calls from kvm_set_spte_hva for unallocated
> @@ -1557,12 +1631,19 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
>  	}
>  
>  	vma_pagesize = vma_kernel_pagesize(vma);
> -	if (vma_pagesize == PMD_SIZE && !logging_active) {
> -		gfn = (fault_ipa & PMD_MASK) >> PAGE_SHIFT;
> +	/*
> +	 * PUD level may not exist for a VM but PMD is guaranteed to
> +	 * exist.
> +	 */
> +	if ((vma_pagesize == PMD_SIZE ||
> +	     (vma_pagesize == PUD_SIZE && kvm_stage2_has_pud(kvm))) &&
> +	    !logging_active) {

This can be better and wrapped in a helper. Probably arm64 definition
for the helper should check on (PUD_SIZE || PMD_SIZE) but for arm32
it should just check on PMD_SIZE alone. But this is optional.

> +		gfn = (fault_ipa & huge_page_mask(hstate_vma(vma))) >> PAGE_SHIFT;
>  	} else {
>  		/*
>  		 * Fallback to PTE if it's not one of the Stage 2
> -		 * supported hugepage sizes
> +		 * supported hugepage sizes or the corresponding level
> +		 * doesn't exist
>  		 */
>  		vma_pagesize = PAGE_SIZE;
>  
> @@ -1661,7 +1742,18 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
>  	needs_exec = exec_fault ||
>  		(fault_status == FSC_PERM && stage2_is_exec(kvm, fault_ipa));
>  
> -	if (vma_pagesize == PMD_SIZE) {
> +	if (vma_pagesize == PUD_SIZE) {
> +		pud_t new_pud = kvm_pfn_pud(pfn, mem_type);
> +
> +		new_pud = kvm_pud_mkhuge(new_pud);
> +		if (writable)
> +			new_pud = kvm_s2pud_mkwrite(new_pud);
> +
> +		if (needs_exec)
> +			new_pud = kvm_s2pud_mkexec(new_pud);
> +
> +		ret = stage2_set_pud_huge(kvm, memcache, fault_ipa, &new_pud);
> +	} else if (vma_pagesize == PMD_SIZE) {

Ran some memory intensive tests on guests with these patches in place but
will continue experimenting and looking for more details in here.
