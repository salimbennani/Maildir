Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 07 Dec 2018 08:41:16 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga001.fm.intel.com (fmsmga001.fm.intel.com [10.253.24.23])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id DAFFB5804C1;
	Wed,  5 Dec 2018 20:59:10 -0800 (PST)
Received: from fmsmga105.fm.intel.com ([10.1.193.10])
  by fmsmga001-1.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 05 Dec 2018 20:59:10 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AR5sgsRzoyVdDiE/XCy+O+j09IxM/srCxBDY+r6Qd?=
 =?us-ascii?q?0ewQL/ad9pjvdHbS+e9qxAeQG9mDu7Qc06L/iOPJYSQ4+5GPsXQPItRndiQuro?=
 =?us-ascii?q?EopTEmG9OPEkbhLfTnPGQQFcVGU0J5rTngaRAGUMnxaEfPrXKs8DUcBgvwNRZv?=
 =?us-ascii?q?JuTyB4Xek9m72/q99pHPYAhEniaxba9vJxiqsAvdsdUbj5F/Iagr0BvJpXVIe+?=
 =?us-ascii?q?VSxWx2IF+Yggjx6MSt8pN96ipco/0u+dJOXqX8ZKQ4UKdXDC86PGAv5c3krgfM?=
 =?us-ascii?q?QA2S7XYBSGoWkx5IAw/Y7BHmW5r6ryX3uvZh1CScIMb7Vq4/Vyi84Kh3SR/okC?=
 =?us-ascii?q?YHOCA/8GHLkcx7kaZXrAu8qxBj34LYZYeYP+d8cKzAZ9MXXWRPUMZPWSJcAY28?=
 =?us-ascii?q?YYQAAPYcMuhXrYbyqUAOrQO8CAS3GOPiySVFimPq0aAg0eksFxzN0gw6H9IJtX?=
 =?us-ascii?q?TZtNb1O7kIUeCz0qbIySjIb+9I1jfm9IjDbwohofaXUrJ3bcra1E4iFxnCjlWW?=
 =?us-ascii?q?pozpJSma2v4RvGib8eVgU/mii28hqwFtvDevwt0ghZXOhoIQ013J8zhyzogyJd?=
 =?us-ascii?q?29UkF7YNikHYNLtyCaLIt2Xt0tQ2RytCY7zL0KoZu7czYPyJQg3R7ea+aLfJSR?=
 =?us-ascii?q?7h76TeqeOy90hHd7d7K6nxay9kygyuzgWci0ylpFsjNJnsDVun8XzxDc8NaHSv?=
 =?us-ascii?q?Rn8keg3jaDzxzf5vxeLUAzj6rbJIYtwqUtlpoWq0jDHyj2lF3tjK6WbUUk5van?=
 =?us-ascii?q?6+H9brr6vZ+QL5R0iw/kPaQ2gMC/GuM4PhALX2eB9uWwzrzj/Ur/Tb5XjfM2ir?=
 =?us-ascii?q?HUvI7GKckfvKK1HgFY3pg55xqiDDqqzM4UkHgFIV5dZR6Ki5blN0vOLf34F/uy?=
 =?us-ascii?q?g0qgnC12y/zYIrHsBIjGIGLZn7f7Z7l97lZRyAotwtBb4JJZEq8BIP3tVU/rrt?=
 =?us-ascii?q?DYDQE2Mxayw+n5DNVxzIQeWXiAAqOBMaPSt0GH5v43LuWSeIMYvCzxJ+Ur6vLw?=
 =?us-ascii?q?l3M1hFwQcbWz0ZYWan20BvFmLF+YYXrojNcBC2AKvg8mQezuiV2CVyNTZnmrU6?=
 =?us-ascii?q?I/+D47EoSmApnHRoy0h7yA0iG7EYNMZm1dFFCMHmnnd5+eV/cPdi2SOMlhnSIA?=
 =?us-ascii?q?VbS7TI8hzx6uuBfgy7V7NurU5jEYtZX72dhx5u3Tlg89+SZ7DsSAyGyNS2B0nm?=
 =?us-ascii?q?UVRz45xqx/oEp9ykud3qh8mfBXCdtT5/ZRWAcgKZHc1/B6C8z1Wg/ZfteGUlem?=
 =?us-ascii?q?Qsm8DjE2VN4xw8IObFx7G9WtlR3D2yuqA7kIl72EHpA086Tc32TvKMZ50XrJyK?=
 =?us-ascii?q?4hj1w+SMtVKWKmnrJ/9xTUB4PRkUWZkLileb4f3C7K8meDy22OsVpcUA5xV6XF?=
 =?us-ascii?q?QH8ealHXrdT/+kPNUbuuBa47PQtGzM6IMrFKZcHxjVVaWPfjP8zTbHiqm2ewAh?=
 =?us-ascii?q?aIxamAbJDwdGUfxyjdDEkEkwYO/XeJLwQ+ByGho37AAzxqD17gf0Ts8exmonOh?=
 =?us-ascii?q?UkA01x2Kb1Fm17et+R4an/qcR+kX3rIFoighrTp0EU2539LXDdqAugVgcL9dYd?=
 =?us-ascii?q?M7/FdIy2bZuxZhMZynKqBonkQefBhvv0PyyxV3DZ1NntUwo3M00gV+M6KY30lH?=
 =?us-ascii?q?dzODw5/wPLrbKm3x/BCqb67bwVXe0NeQ+qcS5/U0sVTjvAe1FkU893VrycVa03?=
 =?us-ascii?q?yZ5p/SFgodTYrxUlor9xh9v7zVfzMy553K2nF2Mam7qDnC28k3C+sj0Ruge9Zf?=
 =?us-ascii?q?MKWZFA79CcEaBs6uKPA0lFitdB4LIOdS9KssNcO8a/SGwLKrPPpnnD++kWRH4Y?=
 =?us-ascii?q?V90kWQ9yp8Su/ExYoFz+uf3gudUzf8jVGhss/slIBAZDESGHe/yCf+CI5QYK1y?=
 =?us-ascii?q?YZgECWO0L8KrwdV+gobnW2RE+167G1MGxMipdAKIYFz5wwJR2loYoHy9lSSj0j?=
 =?us-ascii?q?x7jismrqyc3CzJ3eTvbx4HOm9NRGl/glbgO4m0j9YGXEe2awgljgeq5UH/x6JD?=
 =?us-ascii?q?vqRwM3HTQVtUfyjxN2xjUrawuqCebMJV6ZIosT9YUOKzYV2BTr79oh0a0z7sHm?=
 =?us-ascii?q?dExTA7cS2qtYv9nxBglG2dK3NzpmLDec5s3Rff+MDcRflJ0zocRSl3lSPYBkKh?=
 =?us-ascii?q?MNmu59mUkYnMsvq/V264SpJcay3rwp6euyuh4m1qBwayn/Symt3hDAg73jX319?=
 =?us-ascii?q?hsVSXUshn8ZpPn2Li9MeJiZkNoHkPz69JmGoFilYs9nJER2XkAipSV53YHinrz?=
 =?us-ascii?q?MdNA1KL6bXoNQyMLwtHP7Ajk3k1jMmyGx4bjWnqBxcthYsGwYnkK1SIl88BKFK?=
 =?us-ascii?q?CU4aRGnSRvuFq4rgHRYf9nkjcG0/Qu630ag+APuAUzyCWQGbQSHUhePSzxmBWE?=
 =?us-ascii?q?9dG+rKNLZGmxdbi8zlZxndekDLuauAFTRG75eos+HS939sh+MEjD0Hrp5oHmed?=
 =?us-ascii?q?nQa8kethmVkxfGkuhUJ4g9lvsMhSp7J239uWcpxPI8jRxrxZu6ppSIK31x/KKl?=
 =?us-ascii?q?BR5VLj71aNkS+j33jaZeg9yZ34agHph6HjULXZ3oTe+nET4ItPTnMRqOHyM4qn?=
 =?us-ascii?q?uBBbXfGgqf4l98r33TC5CrK22XJH4BwNVnRRmdJ1ZQjBoaXTokhZ45Ch6lxNb6?=
 =?us-ascii?q?f0hn/DAR6UX1qh9NyuJuKhn+XX3TpAauajcoVpefKABa4R1F50fQKcae9P58Hz?=
 =?us-ascii?q?lE/p29qwyAMnCbZwNNDW0TW02LHVbjPqS15dnb8uiVHe6+L/rIYbWTpu1STfaI?=
 =?us-ascii?q?xZSz0oR4+zaALNmAPn5nD/cjwEpMQWh5G9jFmzUIUyEYjDjCb8mfpBeh4CF3qt?=
 =?us-ascii?q?2//e/vWALu44uPFrRTPc9u+xCwnaeMKeqQiDxlJjZf05MG3WXIx6QH3F4OlyFu?=
 =?us-ascii?q?cCGgEbYaui7IUq3QgbVbDwIBZyNwKctI7L883g9XNs7Akd710r94jvgoC1ZKT1?=
 =?us-ascii?q?Dhm8epZdAULGG5Ll/IGEGLNLGeLz3R3873eb+8SaFXjOhMtxywviubE1b+PjuZ?=
 =?us-ascii?q?kTnlTQuvMeZKjCGUJxFevIC9chBwCWnsVt7maxu7MMNpgj0y27E7mnTKNWsEOz?=
 =?us-ascii?q?hmb0xNtqGQ7T9fgvhnG2xO8HtlLeqHmyaY9eXYKYwWsfxkAitqjeJa/W86xqBR?=
 =?us-ascii?q?7CFFQvx1hSTTosRvo1GgjumA1D5nXABSpTZMgYKBpV9iNrnB9plcRXbE+woA4n?=
 =?us-ascii?q?+NCxsRvdtqFN3ut7pUytjOj6/zLDZC89TJ/coTHcTUKcSHMGY/PhrtAjLbEAwF?=
 =?us-ascii?q?TTuzP2HFm0NdiO2S9mGSrpUitpjsmZ8ORqVHW1AvCvwaClpqHNoZLZdzXzMkl6?=
 =?us-ascii?q?ObjcES6Xq/qhnRWNtVvpTdWv2OBvXvLS6TjaNYaBsQ3bP4MYMTO5X72kN4bFl6?=
 =?us-ascii?q?mYXKG0zIUtFMoi1udAk0oEpW/Xh6T20z3V/lawy37H8SE/60ggA5ig9kbesx8z?=
 =?us-ascii?q?fs5gR/Gl2fiCIukUV5o9TohyyTfSW5eL2xUIdKCSfvn0c2NZz/TkB+agjkzmJ+?=
 =?us-ascii?q?MzKRRL9LibZkPXxmkxOUo5xFHuIUGbdDZhBWzLecZ/gu+VVatiijg0RA4L2WWt?=
 =?us-ascii?q?NZiAI2fMv0/Dp70AV5YYtwfPSIKQ=3D=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0ACAAAIrAhch0O0hNFkGQEBAQEBAQEBA?=
 =?us-ascii?q?QEBAQcBAQEBAQGBUQQBAQEBAQsBgVqBD3ASJ4N6iBlfiy4pJwEBBoE1FI4KI4k?=
 =?us-ascii?q?MgSQDSBgYCwgBh1MiNAkNAQMBAQEBAQECARMBAQEIDQkIKSMMgjYkAYJiAQIDA?=
 =?us-ascii?q?QEBIAQLATsLBgkBAQoYAgIFIQICAwwFEwE1EwWDHAGBdA0FCqVpfDOKL4ELixN?=
 =?us-ascii?q?6FYEHgRGDEoMFGQEEhGKCVwKJNIZ9kCgHAocDhiGEEyMKAoIcjwiNdYsNgUZfD?=
 =?us-ascii?q?YEhcFCCbAmCHhd/AQmBR4NQgj6BXYNvMgEBMYEFAQGKXAEB?=
X-IPAS-Result: =?us-ascii?q?A0ACAAAIrAhch0O0hNFkGQEBAQEBAQEBAQEBAQcBAQEBAQG?=
 =?us-ascii?q?BUQQBAQEBAQsBgVqBD3ASJ4N6iBlfiy4pJwEBBoE1FI4KI4kMgSQDSBgYCwgBh?=
 =?us-ascii?q?1MiNAkNAQMBAQEBAQECARMBAQEIDQkIKSMMgjYkAYJiAQIDAQEBIAQLATsLBgk?=
 =?us-ascii?q?BAQoYAgIFIQICAwwFEwE1EwWDHAGBdA0FCqVpfDOKL4ELixN6FYEHgRGDEoMFG?=
 =?us-ascii?q?QEEhGKCVwKJNIZ9kCgHAocDhiGEEyMKAoIcjwiNdYsNgUZfDYEhcFCCbAmCHhd?=
 =?us-ascii?q?/AQmBR4NQgj6BXYNvMgEBMYEFAQGKXAEB?=
X-IronPort-AV: E=Sophos;i="5.56,321,1539673200"; 
   d="scan'208";a="140986650"
X-Amp-Result: UNKNOWN
X-Amp-Original-Verdict: FILE UNKNOWN
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 05 Dec 2018 20:59:08 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1728951AbeLFE7F (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Wed, 5 Dec 2018 23:59:05 -0500
Received: from mga04.intel.com ([192.55.52.120]:47109 "EHLO mga04.intel.com"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1728238AbeLFE7F (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Wed, 5 Dec 2018 23:59:05 -0500
X-Amp-Result: UNKNOWN
X-Amp-Original-Verdict: FILE UNKNOWN
X-Amp-File-Uploaded: False
Received: from fmsmga003.fm.intel.com ([10.253.24.29])
  by fmsmga104.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 05 Dec 2018 20:59:04 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.56,321,1539673200"; 
   d="scan'208";a="116431140"
Received: from shao2-debian.sh.intel.com (HELO localhost) ([10.239.13.6])
  by FMSMGA003.fm.intel.com with ESMTP; 05 Dec 2018 20:59:02 -0800
Date: Thu, 6 Dec 2018 12:59:17 +0800
From: kernel test robot <rong.a.chen@intel.com>
To: David Rientjes <rientjes@google.com>
Cc: Linus Torvalds <torvalds@linux-foundation.org>,
        Andrea Arcangeli <aarcange@redhat.com>, s.priebe@profihost.ag,
        lkp@01.org,
        Linux List Kernel Mailing <linux-kernel@vger.kernel.org>,
        mhocko@kernel.org, alex.williamson@redhat.com,
        zi.yan@cs.rutgers.edu, kirill@shutemov.name,
        Andrew Morton <akpm@linux-foundation.org>,
        mgorman@techsingularity.net, Vlastimil Babka <vbabka@suse.cz>
Subject: Re: [LKP] [patch v2 for-4.20] mm, thp: restore node-local hugepage
 allocations
Message-ID: <20181206045917.GG23332@shao2-debian>
References: <alpine.DEB.2.21.1812051445390.35948@chino.kir.corp.google.com>
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Disposition: inline
Content-Transfer-Encoding: 8bit
In-Reply-To: <alpine.DEB.2.21.1812051445390.35948@chino.kir.corp.google.com>
User-Agent: Mutt/1.10.0 (2018-05-17)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On Wed, Dec 05, 2018 at 02:46:50PM -0800, David Rientjes wrote:
> This is a full revert of ac5b2c18911f ("mm: thp: relax __GFP_THISNODE for
> MADV_HUGEPAGE mappings") and a partial revert of 89c83fb539f9 ("mm, thp:
> consolidate THP gfp handling into alloc_hugepage_direct_gfpmask").
> 
> By not setting __GFP_THISNODE, applications can allocate remote hugepages
> when the local node is fragmented or low on memory when either the thp
> defrag setting is "always" or the vma has been madvised with
> MADV_HUGEPAGE.
> 
> Remote access to hugepages often has much higher latency than local pages
> of the native page size.  On Haswell, ac5b2c18911f was shown to have a
> 13.9% access regression after this commit for binaries that remap their
> text segment to be backed by transparent hugepages.
> 
> The intent of ac5b2c18911f is to address an issue where a local node is
> low on memory or fragmented such that a hugepage cannot be allocated.  In
> every scenario where this was described as a fix, there is abundant and
> unfragmented remote memory available to allocate from, even with a greater
> access latency.
> 
> If remote memory is also low or fragmented, not setting __GFP_THISNODE was
> also measured on Haswell to have a 40% regression in allocation latency.
> 
> Restore __GFP_THISNODE for thp allocations.
> 
> Fixes: ac5b2c18911f ("mm: thp: relax __GFP_THISNODE for MADV_HUGEPAGE mappings")
> Fixes: 89c83fb539f9 ("mm, thp: consolidate THP gfp handling into alloc_hugepage_direct_gfpmask")
> Signed-off-by: David Rientjes <rientjes@google.com>
> ---
>  include/linux/mempolicy.h |  2 --
>  mm/huge_memory.c          | 42 +++++++++++++++------------------------
>  mm/mempolicy.c            |  2 +-
>  3 files changed, 17 insertions(+), 29 deletions(-)
> 
> diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
> --- a/include/linux/mempolicy.h
> +++ b/include/linux/mempolicy.h
> @@ -139,8 +139,6 @@ struct mempolicy *mpol_shared_policy_lookup(struct shared_policy *sp,
>  struct mempolicy *get_task_policy(struct task_struct *p);
>  struct mempolicy *__get_vma_policy(struct vm_area_struct *vma,
>  		unsigned long addr);
> -struct mempolicy *get_vma_policy(struct vm_area_struct *vma,
> -						unsigned long addr);
>  bool vma_policy_mof(struct vm_area_struct *vma);
>  
>  extern void numa_default_policy(void);
> diff --git a/mm/huge_memory.c b/mm/huge_memory.c
> --- a/mm/huge_memory.c
> +++ b/mm/huge_memory.c
> @@ -632,37 +632,27 @@ static vm_fault_t __do_huge_pmd_anonymous_page(struct vm_fault *vmf,
>  static inline gfp_t alloc_hugepage_direct_gfpmask(struct vm_area_struct *vma, unsigned long addr)
>  {
>  	const bool vma_madvised = !!(vma->vm_flags & VM_HUGEPAGE);
> -	gfp_t this_node = 0;
> -
> -#ifdef CONFIG_NUMA
> -	struct mempolicy *pol;
> -	/*
> -	 * __GFP_THISNODE is used only when __GFP_DIRECT_RECLAIM is not
> -	 * specified, to express a general desire to stay on the current
> -	 * node for optimistic allocation attempts. If the defrag mode
> -	 * and/or madvise hint requires the direct reclaim then we prefer
> -	 * to fallback to other node rather than node reclaim because that
> -	 * can lead to excessive reclaim even though there is free memory
> -	 * on other nodes. We expect that NUMA preferences are specified
> -	 * by memory policies.
> -	 */
> -	pol = get_vma_policy(vma, addr);
> -	if (pol->mode != MPOL_BIND)
> -		this_node = __GFP_THISNODE;
> -	mpol_cond_put(pol);
> -#endif
> +	const gfp_t gfp_mask = GFP_TRANSHUGE_LIGHT | __GFP_THISNODE;
>  
> +	/* Always do synchronous compaction */
>  	if (test_bit(TRANSPARENT_HUGEPAGE_DEFRAG_DIRECT_FLAG, &transparent_hugepage_flags))
> -		return GFP_TRANSHUGE | (vma_madvised ? 0 : __GFP_NORETRY);
> +		return GFP_TRANSHUGE | __GFP_THISNODE |
> +		       (vma_madvised ? 0 : __GFP_NORETRY);
> +
> +	/* Kick kcompactd and fail quickly */
>  	if (test_bit(TRANSPARENT_HUGEPAGE_DEFRAG_KSWAPD_FLAG, &transparent_hugepage_flags))
> -		return GFP_TRANSHUGE_LIGHT | __GFP_KSWAPD_RECLAIM | this_node;
> +		return gfp_mask | __GFP_KSWAPD_RECLAIM;
> +
> +	/* Synchronous compaction if madvised, otherwise kick kcompactd */
>  	if (test_bit(TRANSPARENT_HUGEPAGE_DEFRAG_KSWAPD_OR_MADV_FLAG, &transparent_hugepage_flags))
> -		return GFP_TRANSHUGE_LIGHT | (vma_madvised ? __GFP_DIRECT_RECLAIM :
> -							     __GFP_KSWAPD_RECLAIM | this_node);
> +		return gfp_mask | (vma_madvised ? __GFP_DIRECT_RECLAIM :
> +						  __GFP_KSWAPD_RECLAIM);
> +
> +	/* Only do synchronous compaction if madvised */
>  	if (test_bit(TRANSPARENT_HUGEPAGE_DEFRAG_REQ_MADV_FLAG, &transparent_hugepage_flags))
> -		return GFP_TRANSHUGE_LIGHT | (vma_madvised ? __GFP_DIRECT_RECLAIM :
> -							     this_node);
> -	return GFP_TRANSHUGE_LIGHT | this_node;
> +		return gfp_mask | (vma_madvised ? __GFP_DIRECT_RECLAIM : 0);
> +
> +	return gfp_mask;
>  }
>  
>  /* Caller must hold page table lock. */
> diff --git a/mm/mempolicy.c b/mm/mempolicy.c
> --- a/mm/mempolicy.c
> +++ b/mm/mempolicy.c
> @@ -1662,7 +1662,7 @@ struct mempolicy *__get_vma_policy(struct vm_area_struct *vma,
>   * freeing by another task.  It is the caller's responsibility to free the
>   * extra reference for shared policies.
>   */
> -struct mempolicy *get_vma_policy(struct vm_area_struct *vma,
> +static struct mempolicy *get_vma_policy(struct vm_area_struct *vma,
>  						unsigned long addr)
>  {
>  	struct mempolicy *pol = __get_vma_policy(vma, addr);
> _______________________________________________
> LKP mailing list
> LKP@lists.01.org
> https://lists.01.org/mailman/listinfo/lkp

Hi,

FYI, we noticed no regression of vm-scalability.throughput between the two commits.

tests: 1
testcase/path_params/tbox_group/run: vm-scalability/300-always-always-32-1-swap-w-seq-ucode=0x3d-performance/lkp-hsw-ep4

commit: 
  94e297c50b ("include/linux/notifier.h: SRCU: fix ctags")
  2f0799a0ff ("mm, thp: restore node-local hugepage allocations")

94e297c50b529f5d  2f0799a0ffc033bf3cc82d5032  
----------------  --------------------------  
         %stddev      change         %stddev
             \          |                \  
    425945 ±  9%        36%     578453 ± 18%  vm-scalability.time.minor_page_faults
       322                         313        vm-scalability.time.user_time
    905482 ±  5%        20%    1087102 ±  9%  perf-stat.page-faults
    901474 ±  5%        20%    1081707 ±  9%  perf-stat.minor-faults
    425945 ±  9%        36%     578453 ± 18%  time.minor_page_faults
       322                         313        time.user_time
      6792 ±  5%       -10%       6097        vmstat.system.cs
   1625177 ±  4%       -11%    1446045        vmstat.swap.so
   1625189 ±  4%       -11%    1446055        vmstat.io.bo
    171516 ±  5%       -24%     131111 ± 24%  vmstat.system.in
    157193 ± 15%        46%     230011 ± 20%  proc-vmstat.pgactivate
    559397 ±  8%        45%     809533 ± 12%  proc-vmstat.pgscan_direct
    207042 ±  8%        43%     295574 ± 10%  proc-vmstat.pgsteal_direct
   5852284 ± 12%        42%    8294341 ± 20%  proc-vmstat.slabs_scanned
    211428 ±  6%        39%     293175 ±  6%  proc-vmstat.pgrefill
    135763 ±  9%        39%     188155 ±  3%  proc-vmstat.nr_vmscan_write
    220023 ±  5%        38%     303404 ±  6%  proc-vmstat.nr_written
    219051 ±  5%        38%     301549 ±  6%  proc-vmstat.pgrotated
      1018 ± 10%        36%       1383 ±  4%  proc-vmstat.nr_zone_write_pending
      1017 ± 10%        36%       1379 ±  4%  proc-vmstat.nr_writeback
    919059 ±  4%        20%    1102507 ±  9%  proc-vmstat.pgfault
   1108142 ±  9%        16%    1285859 ±  5%  proc-vmstat.numa_local
   1122389 ±  9%        16%    1302063 ±  5%  proc-vmstat.numa_hit
    715724              -5%     682381 ±  4%  proc-vmstat.nr_file_pages
      5784 ±  6%        -5%       5482 ±  7%  proc-vmstat.nr_mapped
    266928             -16%     223157        proc-vmstat.nr_zone_unevictable
    266928             -16%     223157        proc-vmstat.nr_unevictable
         0            4e+05     428930 ±139%  latency_stats.avg.io_schedule.__lock_page_or_retry.do_swap_page.__handle_mm_fault.handle_mm_fault.__do_page_fault.do_page_fault.page_fault.copy_user_generic_unrolled._copy_to_user.do_syslog.kmsg_read
     11973 ± 70%      3e+05     298302 ± 83%  latency_stats.avg.io_schedule.__lock_page_or_retry.do_swap_page.__handle_mm_fault.handle_mm_fault.__do_page_fault.do_page_fault.page_fault.copy_user_enhanced_fast_string._copy_to_user.do_syslog.kmsg_read
     17650 ± 35%      2e+05     265410 ± 84%  latency_stats.avg.io_schedule.__lock_page_or_retry.do_swap_page.__handle_mm_fault.handle_mm_fault.__do_page_fault.do_page_fault.page_fault.strnlen_user.copy_strings.__do_execve_file.__x64_sys_execve
      9801 ± 42%      5e+04      61121 ±112%  latency_stats.avg.io_schedule.__lock_page.__lock_page_or_retry.do_swap_page.__handle_mm_fault.handle_mm_fault.__get_user_pages.get_user_pages_remote.__access_remote_vm.proc_pid_cmdline_read.__vfs_read.vfs_read
         0            3e+04      25998 ±141%  latency_stats.avg.rpc_wait_bit_killable.__rpc_execute.rpc_run_task.rpc_call_sync.nfs3_rpc_wrapper.nfs3_do_create.nfs3_proc_create.nfs_create.path_openat.do_filp_open.do_sys_open.do_syscall_64
         0            6e+03       6202 ±141%  latency_stats.avg.io_schedule.blk_mq_get_tag.blk_mq_get_request.blk_mq_make_request.generic_make_request.submit_bio.swap_readpage.read_swap_cache_async.swap_cluster_readahead.shmem_swapin.shmem_getpage_gfp.shmem_fault
         0            5e+03       5457 ±141%  latency_stats.avg.io_schedule.blk_mq_get_tag.blk_mq_get_request.blk_mq_make_request.generic_make_request.submit_bio.swap_readpage.swapin_readahead.do_swap_page.__handle_mm_fault.handle_mm_fault.__get_user_pages
      4585 ±173%     -5e+03          0        latency_stats.avg.io_schedule.blk_mq_get_tag.blk_mq_get_request.blk_mq_make_request.generic_make_request.submit_bio.swap_readpage.swap_cluster_readahead.shmem_swapin.shmem_getpage_gfp.shmem_fault.__do_fault
      6229 ±173%     -6e+03          0        latency_stats.avg.io_schedule.blk_mq_get_tag.blk_mq_get_request.blk_mq_make_request.generic_make_request.submit_bio.swap_readpage.read_swap_cache_async.swap_cluster_readahead.shmem_swapin.shmem_getpage_gfp.shmem_write_begin
     37391 ±173%     -4e+04          0        latency_stats.avg.io_schedule.wait_on_page_bit.shmem_getpage_gfp.shmem_fault.__do_fault.__handle_mm_fault.handle_mm_fault.__do_page_fault.do_page_fault.page_fault
     52525 ±100%     -5e+04          0        latency_stats.avg.msleep.shrink_inactive_list.shrink_node_memcg.shrink_node.do_try_to_free_pages.try_to_free_pages.__alloc_pages_slowpath.__alloc_pages_nodemask.do_huge_pmd_anonymous_page.__handle_mm_fault.handle_mm_fault.__do_page_fault
    141825 ±172%     -1e+05        590 ±126%  latency_stats.avg.io_schedule.__lock_page.shmem_getpage_gfp.shmem_file_read_iter.__vfs_read.vfs_read.ksys_read.do_syscall_64.entry_SYSCALL_64_after_hwframe
    240110 ±124%     -2e+05       2543 ±141%  latency_stats.avg.io_schedule.__lock_page_or_retry.do_swap_page.__handle_mm_fault.handle_mm_fault.__do_page_fault.do_page_fault.page_fault.copy_user_generic_unrolled._copy_to_user.perf_read.__vfs_read
    272115 ±171%     -3e+05       6043 ± 71%  latency_stats.avg.io_schedule.__lock_page_or_retry.do_swap_page.__handle_mm_fault.handle_mm_fault.__do_page_fault.do_page_fault.page_fault.copy_user_generic_unrolled.core_sys_select.kern_select.__x64_sys_select
    538363 ± 77%     -4e+05     136003 ± 68%  latency_stats.avg.io_schedule.__lock_page_or_retry.do_swap_page.__handle_mm_fault.handle_mm_fault.__do_page_fault.do_page_fault.page_fault.copy_user_enhanced_fast_string.copyout._copy_to_iter.skb_copy_datagram_iter
     23857 ± 26%      1e+06    1035782 ± 71%  latency_stats.max.io_schedule.__lock_page_or_retry.do_swap_page.__handle_mm_fault.handle_mm_fault.__do_page_fault.do_page_fault.page_fault.strnlen_user.copy_strings.__do_execve_file.__x64_sys_execve
     11973 ± 70%      8e+05     778555 ± 70%  latency_stats.max.io_schedule.__lock_page_or_retry.do_swap_page.__handle_mm_fault.handle_mm_fault.__do_page_fault.do_page_fault.page_fault.copy_user_enhanced_fast_string._copy_to_user.do_syslog.kmsg_read
         0            4e+05     428930 ±139%  latency_stats.max.io_schedule.__lock_page_or_retry.do_swap_page.__handle_mm_fault.handle_mm_fault.__do_page_fault.do_page_fault.page_fault.copy_user_generic_unrolled._copy_to_user.do_syslog.kmsg_read
         0            3e+04      25998 ±141%  latency_stats.max.rpc_wait_bit_killable.__rpc_execute.rpc_run_task.rpc_call_sync.nfs3_rpc_wrapper.nfs3_do_create.nfs3_proc_create.nfs_create.path_openat.do_filp_open.do_sys_open.do_syscall_64
         0            8e+03       7916 ±141%  latency_stats.max.io_schedule.blk_mq_get_tag.blk_mq_get_request.blk_mq_make_request.generic_make_request.submit_bio.swap_readpage.read_swap_cache_async.swap_cluster_readahead.shmem_swapin.shmem_getpage_gfp.shmem_fault
         0            7e+03       6751 ±141%  latency_stats.max.io_schedule.blk_mq_get_tag.blk_mq_get_request.blk_mq_make_request.generic_make_request.submit_bio.swap_readpage.swapin_readahead.do_swap_page.__handle_mm_fault.handle_mm_fault.__get_user_pages
      4585 ±173%     -5e+03          0        latency_stats.max.io_schedule.blk_mq_get_tag.blk_mq_get_request.blk_mq_make_request.generic_make_request.submit_bio.swap_readpage.swap_cluster_readahead.shmem_swapin.shmem_getpage_gfp.shmem_fault.__do_fault
      6229 ±173%     -6e+03          0        latency_stats.max.io_schedule.blk_mq_get_tag.blk_mq_get_request.blk_mq_make_request.generic_make_request.submit_bio.swap_readpage.read_swap_cache_async.swap_cluster_readahead.shmem_swapin.shmem_getpage_gfp.shmem_write_begin
      7425 ±117%     -7e+03         68 ±  7%  latency_stats.max.smp_call_on_cpu.lockup_detector_reconfigure.proc_watchdog_common.proc_sys_call_handler.__vfs_write.vfs_write.ksys_write.do_syscall_64.entry_SYSCALL_64_after_hwframe
     52680 ±100%     -5e+04          0        latency_stats.max.msleep.shrink_inactive_list.shrink_node_memcg.shrink_node.do_try_to_free_pages.try_to_free_pages.__alloc_pages_slowpath.__alloc_pages_nodemask.do_huge_pmd_anonymous_page.__handle_mm_fault.handle_mm_fault.__do_page_fault
    107626 ±173%     -1e+05          0        latency_stats.max.io_schedule.wait_on_page_bit.shmem_getpage_gfp.shmem_fault.__do_fault.__handle_mm_fault.handle_mm_fault.__do_page_fault.do_page_fault.page_fault
    272938 ±170%     -3e+05      16767 ± 71%  latency_stats.max.io_schedule.__lock_page_or_retry.do_swap_page.__handle_mm_fault.handle_mm_fault.__do_page_fault.do_page_fault.page_fault.copy_user_generic_unrolled.core_sys_select.kern_select.__x64_sys_select
    284952 ±170%     -3e+05        592 ±125%  latency_stats.max.io_schedule.__lock_page.shmem_getpage_gfp.shmem_file_read_iter.__vfs_read.vfs_read.ksys_read.do_syscall_64.entry_SYSCALL_64_after_hwframe
    357768 ±136%     -4e+05       2543 ±141%  latency_stats.max.io_schedule.__lock_page_or_retry.do_swap_page.__handle_mm_fault.handle_mm_fault.__do_page_fault.do_page_fault.page_fault.copy_user_generic_unrolled._copy_to_user.perf_read.__vfs_read
   1155510 ± 48%      9e+06    9751594 ±110%  latency_stats.sum.io_schedule.__lock_page.__lock_page_or_retry.do_swap_page.__handle_mm_fault.handle_mm_fault.__get_user_pages.get_user_pages_remote.__access_remote_vm.proc_pid_cmdline_read.__vfs_read.vfs_read
   4573026 ± 77%      8e+06   12847407 ±122%  latency_stats.sum.io_schedule.wait_on_page_bit.shmem_getpage_gfp.shmem_write_begin.generic_perform_write.__generic_file_write_iter.generic_file_write_iter.__vfs_write.vfs_write.ksys_write.do_syscall_64.entry_SYSCALL_64_after_hwframe
     48641 ± 48%      2e+06    2342957 ± 71%  latency_stats.sum.io_schedule.__lock_page_or_retry.do_swap_page.__handle_mm_fault.handle_mm_fault.__do_page_fault.do_page_fault.page_fault.strnlen_user.copy_strings.__do_execve_file.__x64_sys_execve
     11973 ± 70%      8e+05     788601 ± 70%  latency_stats.sum.io_schedule.__lock_page_or_retry.do_swap_page.__handle_mm_fault.handle_mm_fault.__do_page_fault.do_page_fault.page_fault.copy_user_enhanced_fast_string._copy_to_user.do_syslog.kmsg_read
         0            4e+05     428930 ±139%  latency_stats.sum.io_schedule.__lock_page_or_retry.do_swap_page.__handle_mm_fault.handle_mm_fault.__do_page_fault.do_page_fault.page_fault.copy_user_generic_unrolled._copy_to_user.do_syslog.kmsg_read
         0            4e+04      38202 ±141%  latency_stats.sum.io_schedule.blk_mq_get_tag.blk_mq_get_request.blk_mq_make_request.generic_make_request.submit_bio.swap_readpage.swapin_readahead.do_swap_page.__handle_mm_fault.handle_mm_fault.__get_user_pages
         0            3e+04      25998 ±141%  latency_stats.sum.rpc_wait_bit_killable.__rpc_execute.rpc_run_task.rpc_call_sync.nfs3_rpc_wrapper.nfs3_do_create.nfs3_proc_create.nfs_create.path_openat.do_filp_open.do_sys_open.do_syscall_64
         0            2e+04      18607 ±141%  latency_stats.sum.io_schedule.blk_mq_get_tag.blk_mq_get_request.blk_mq_make_request.generic_make_request.submit_bio.swap_readpage.read_swap_cache_async.swap_cluster_readahead.shmem_swapin.shmem_getpage_gfp.shmem_fault
      4585 ±173%     -5e+03          0        latency_stats.sum.io_schedule.blk_mq_get_tag.blk_mq_get_request.blk_mq_make_request.generic_make_request.submit_bio.swap_readpage.swap_cluster_readahead.shmem_swapin.shmem_getpage_gfp.shmem_fault.__do_fault
      8048 ± 99%     -5e+03       3141 ± 60%  latency_stats.sum.rpc_wait_bit_killable.__rpc_execute.rpc_run_task.rpc_call_sync.nfs3_rpc_wrapper.nfs3_proc_access.nfs_do_access.nfs_permission.inode_permission.link_path_walk.path_lookupat.filename_lookup
      6229 ±173%     -6e+03          0        latency_stats.sum.io_schedule.blk_mq_get_tag.blk_mq_get_request.blk_mq_make_request.generic_make_request.submit_bio.swap_readpage.read_swap_cache_async.swap_cluster_readahead.shmem_swapin.shmem_getpage_gfp.shmem_write_begin
     10774 ± 82%     -7e+03       3648 ±  5%  latency_stats.sum.smp_call_on_cpu.lockup_detector_reconfigure.proc_watchdog_common.proc_sys_call_handler.__vfs_write.vfs_write.ksys_write.do_syscall_64.entry_SYSCALL_64_after_hwframe
    112174 ±173%     -1e+05          0        latency_stats.sum.io_schedule.wait_on_page_bit.shmem_getpage_gfp.shmem_fault.__do_fault.__handle_mm_fault.handle_mm_fault.__do_page_fault.do_page_fault.page_fault
    157878 ±137%     -2e+05          0        latency_stats.sum.msleep.shrink_inactive_list.shrink_node_memcg.shrink_node.do_try_to_free_pages.try_to_free_pages.__alloc_pages_slowpath.__alloc_pages_nodemask.do_huge_pmd_anonymous_page.__handle_mm_fault.handle_mm_fault.__do_page_fault
    275908 ±168%     -3e+05      18129 ± 71%  latency_stats.sum.io_schedule.__lock_page_or_retry.do_swap_page.__handle_mm_fault.handle_mm_fault.__do_page_fault.do_page_fault.page_fault.copy_user_generic_unrolled.core_sys_select.kern_select.__x64_sys_select
    285059 ±170%     -3e+05        633 ±113%  latency_stats.sum.io_schedule.__lock_page.shmem_getpage_gfp.shmem_file_read_iter.__vfs_read.vfs_read.ksys_read.do_syscall_64.entry_SYSCALL_64_after_hwframe
    427767 ±141%     -4e+05       2543 ±141%  latency_stats.sum.io_schedule.__lock_page_or_retry.do_swap_page.__handle_mm_fault.handle_mm_fault.__do_page_fault.do_page_fault.page_fault.copy_user_generic_unrolled._copy_to_user.perf_read.__vfs_read

Best Regards,
Rong Chen
