Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 10 Dec 2018 08:56:29 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga004.jf.intel.com (orsmga004.jf.intel.com [10.7.209.38])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 7D8E75805FF;
	Sun,  9 Dec 2018 11:56:51 -0800 (PST)
Received: from orsmga101.jf.intel.com ([10.7.208.22])
  by orsmga004-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 09 Dec 2018 11:56:51 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3ABAMtaRKp3wNGk9EV49mcpTZWNBhigK39O0sv0rFi?=
 =?us-ascii?q?tYgUL/z7rarrMEGX3/hxlliBBdydt6oUzbKO+4nbGkU4qa6bt34DdJEeHzQksu?=
 =?us-ascii?q?4x2zIaPcieFEfgJ+TrZSFpVO5LVVti4m3peRMNQJW2aFLduGC94iAPERvjKwV1?=
 =?us-ascii?q?Ov71GonPhMiryuy+4ZLebxlLiTanfb9+MAi9oBnMuMURnYZsMLs6xAHTontPde?=
 =?us-ascii?q?RWxGdoKkyWkh3h+Mq+/4Nt/jpJtf45+MFOTav1f6IjTbxFFzsmKHw65NfqtRbY?=
 =?us-ascii?q?UwSC4GYXX3gMnRpJBwjF6wz6Xov0vyDnuOdxxDWWMMvrRr0vRz+s87lkRwPpiC?=
 =?us-ascii?q?cfNj427mfXitBrjKlGpB6tvgFzz5LIbI2QMvd1Y6HTcs4ARWdZUMhfVzJPDJ6m?=
 =?us-ascii?q?b4sBAOUOIftXoIvzqFUNthu+HQuhCfjzyjNUnHL6wbE23v4/HQzAwQcuH8gOsH?=
 =?us-ascii?q?PRrNjtOqscS/q6zLTMzT7eYP1awyr25o/UfR4kv/6MW7RwftTRyEUhCgjIiU2Q?=
 =?us-ascii?q?ppb4PzOR0+QCr2ub4vFkVeKujW4ntg5wriKuxsg3j4nFnJ4aylfB9Shgxos+ON?=
 =?us-ascii?q?62SFZjbNK6DJddszuWO5Z4T888WW1kpSU3xqEctZO6fiUG0JAqywTcZvCbaYSE?=
 =?us-ascii?q?/A/vWeSLLTtlmX5oe7SyjAuo/0e60O3zTMy03U5KriVbltnMsWgA1wLc6seZUP?=
 =?us-ascii?q?tx5ESh1iiV1wDV9O5EJVo4la3BK54u2rIwl5wTvlrfHiLuhkn6kKubel859uWm?=
 =?us-ascii?q?9ejreKjqq5yAO4NuiwzzMLwimsmlDuQ5NggOUXKb+eO51LD7+U35QbNKjuA5k6?=
 =?us-ascii?q?XAs5DVO94bpqinDA9Ry4oj7Bi+DzG439QChnQHMl1Fdwydj4TzOFHBPur4DfGh?=
 =?us-ascii?q?jFSoijtrwOrGPrL5DpXXMnfDiKvhfap660NEzAozzNNf6IxOBrAOPfL+QUvxtN?=
 =?us-ascii?q?3eDh8kPA242efnCNNh1owAXWKDGLOWMKTXsVWQ/OIgP/GMZJMJuDb6M/Ul5+Th?=
 =?us-ascii?q?jX4lmVAHeqmlx5sXaG2iEfRgLEWUen7sgtYHEWcXsQsyVu3qiFueUTFNY3a+Rb?=
 =?us-ascii?q?4z5jY+CIi+F4fMWpitgKCd3Ce8BpBWZGdGBU6WHXfrcIWEXfEMaCWJL89lkzwE?=
 =?us-ascii?q?U6WhSoA72RGvsg/616RoLu7O9iIEspLj0cB/5/fPmhEq6Tx0E8Od3nmXQGFvnm?=
 =?us-ascii?q?MIQDw20LploUNnyFeOyqx4g/1eFdxO6PJFSAY6NZjAz+NkD9D+QB7OftCMSFy+?=
 =?us-ascii?q?WNWpHSkxTs4tw98Je0t9GM+tjhbZ0yquAr8ajbqLBJMv/6LY3njxIdt9ynnc2K?=
 =?us-ascii?q?kgiVkmXtVANWm8iqFj8AjTApbDk1+FmKayaaQcwCnN+X+ewmWUokFXThR8UaXf?=
 =?us-ascii?q?UnAZfUvZs9L56kTGT7+tDLQnNhBMycqDKqtMd93ogk9KRPblONTCfW2xn328Cg?=
 =?us-ascii?q?qPxrOJdIDqYXkS3D3BCEgYlAAe5WuJOhIgBii/uW7eDCZhFVT0Y0zy9+lzs3e7?=
 =?us-ascii?q?Tk4yzwGXYExtzbu1+hgJhfOCT/MfxK4LuCAkqz9sBlayw8rWC8acpwpmZKhcfd?=
 =?us-ascii?q?I94FJA1WLFtwx8PoasL7x4il4ZaQR3u0Lu1xN4CohblcgqrXUqzBd9KK6C0VNB?=
 =?us-ascii?q?cS+Y0o70OrHNNmby+xWvYbbM2l7CyNaW5rsP6PMgplr5uAGmCEUj/Gtn0tVPyH?=
 =?us-ascii?q?Sc+4jFAxAUUZLyVUY36QN3p7XbYik7+oPV2mdgMaiysj/exd0pAPEpxQqnf9da?=
 =?us-ascii?q?KKmEDhP9E9UGB8iyL+wng1iobg8eMO9O7qI1PsOmeOGA2K6kJ+tgmDOmjWJa4I?=
 =?us-ascii?q?FyyE6M9ix8SvLW0JYB2f2XwgyHVzLkhle7rs/3gZxEZS0VHmen0yjkBZJeabdo?=
 =?us-ascii?q?fYkWDmeiOcu3yctkh57sQnJX6ESsB1cb18C3YxqSaFr90BZU1UQWp3ynhCS5wy?=
 =?us-ascii?q?Z1kzEvsqqQwijOz/7+exoAP25BXHNigkv0IYiok9AaW1ClbggolBe/5Uf23bNb?=
 =?us-ascii?q?pLl5L2TIRUdIfi72L3xtU6eqt7qCZdJP540ssSlNTOu8ZlWaQKbnoxQGyyPjA3?=
 =?us-ascii?q?dexDcjejGooJr5hR96iGGaLHppt3rWY8JwxRTe5NzaW/FR2CELRC15iTnRG1i9?=
 =?us-ascii?q?MMOl/dSSl5ffrO++U3itWYFUcSnu1YmArje05XV2AR2jmPC+gt3mHhI90SPh1d?=
 =?us-ascii?q?lqVCPIoQ34Yonq0aS6LO1mclNpBF/698p1BIV+npEsi5EX3HgQno+V8mYfkWfv?=
 =?us-ascii?q?LdVb3rrzbHkXSj4K2dLV4BXl11dlLnKG3I/5UnSdws18Z9i1eG8W2yQ979xUB6?=
 =?us-ascii?q?eQ9rBLgSx1ol+gpwLLffd9hisdyecp6HMChuEJuQktwT+HDrEcA0ZYJjDsmAqS?=
 =?us-ascii?q?79+lsqpXfmmvfKO01Ep/m9ChEb6DrhtdWHb/ZpctAytw4t9jP1LL1X358pvkd8?=
 =?us-ascii?q?XIbdIPqh2UlA/Nj/RSKJI0jPYLhDBoOWTgvX0+0O47jAdj3ZW7vIiBNmVs87i1?=
 =?us-ascii?q?Ah9eNj3pecwT/ivhgrpZnsaTx4qvBIluGi0XXJv0SvKlCCkduu7gNwaKDT0wsH?=
 =?us-ascii?q?ObGafEEA+b6UdmqW/PEp+xO3GWInkZ0cttRB2HKENDhwAUWS0wnoQlGQCy2Mzh?=
 =?us-ascii?q?bEB56ygT5lHirxtD1PloOwPjUmvFpweodzQ0SIWZLBVM9QFP/EPVMc2Y7uJuEC?=
 =?us-ascii?q?BU5JyhrAqRKmOFYwREF30GWkuBB1r7JLmh+cHA8/SEBuq5N/bPYbKOqfFHV/eV?=
 =?us-ascii?q?352v1JFq/y2LNsWJMXlvFPk72ktFXXBkFMXVgTQPSyoLly3Ta86Xvguz+ipyrs?=
 =?us-ascii?q?qn6vTkRBrv5ZeTC7tVKdhv+w65gaaAN+6ThSZ1MzVY1okLxX/H1rcfxkMSizp1?=
 =?us-ascii?q?ejmpELQAszPNTa3Klq9WCR4bdz18NM9S460g2QlNPNbRisnp2b5gkv41F1BFWE?=
 =?us-ascii?q?T7lcG0fsMKOX+yNVPdCEaPNbSLPjnLw8DxYaOhRrxcluRUtxusuTmFF0/vJCiM?=
 =?us-ascii?q?lz7sVxq3K+FDkDmbPABCuIG6ahttFWnjTNf8ZhKnKtN4kT02zqMyhn7RMW4cMD?=
 =?us-ascii?q?58c15Cr7GK7CNYhOl/FHJF7nZ/MeaEnCOZ5fHCKpkKqftrHjh0l+VC7XU60bRV?=
 =?us-ascii?q?6iRERP1zmCTKr99uo0upku+Ayjd8VBpOqzBLhJ+EvEl4OKXZ8IVAVmjA/B4X8W?=
 =?us-ascii?q?qQDBEK9JNZDYilm60UgvLI3uqnIjAE89/8/M0ACsySI8WCZikbPALtCQLTWRMM?=
 =?us-ascii?q?UDquPmL3g01bjeHU93eQ6JM9r8vCgp0LH/VjXUExXt5cQmFuP9EEJppmFHtwib?=
 =?us-ascii?q?eGiMsD5lK6rR/MVINbtJWBXfWXV6a8YA2FhKVJMkNbiYjzKp4eY8iigxRv?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0ABAAAecg1ch0O0hNFkGgEBAQEBAgEBA?=
 =?us-ascii?q?QEHAgEBAQGBUQUBAQEBCwGBWoIRJ4wTX4sugg0UiH6OPxSBWxYBARgTAYRAAoM?=
 =?us-ascii?q?bIjQJDQEDAQEBAQEBAgETAQEBCgsJCCkvgjYkAYJhAQEBAQIBAQIPFRM/BQEGA?=
 =?us-ascii?q?wEBChEEAQEBCSUDDAUNCyMOEwUVCIJ/Q4EnAw0IAwKbbIlYAQEBgWozhTGCRg2?=
 =?us-ascii?q?CHIwhEQaBQD+DJX6CV4F3ARIBA4NRgiYCiWqBP4URj2YnLgcCAo1hPYMkCxiBX?=
 =?us-ascii?q?COEdAWKRSyJe4R8iWYCBAYFAhMBgUaBHXFwFTsNDYEzgR+CJxd/AQeNN1EBgQQ?=
 =?us-ascii?q?BASGKAYI+AQE?=
X-IPAS-Result: =?us-ascii?q?A0ABAAAecg1ch0O0hNFkGgEBAQEBAgEBAQEHAgEBAQGBUQU?=
 =?us-ascii?q?BAQEBCwGBWoIRJ4wTX4sugg0UiH6OPxSBWxYBARgTAYRAAoMbIjQJDQEDAQEBA?=
 =?us-ascii?q?QEBAgETAQEBCgsJCCkvgjYkAYJhAQEBAQIBAQIPFRM/BQEGAwEBChEEAQEBCSU?=
 =?us-ascii?q?DDAUNCyMOEwUVCIJ/Q4EnAw0IAwKbbIlYAQEBgWozhTGCRg2CHIwhEQaBQD+DJ?=
 =?us-ascii?q?X6CV4F3ARIBA4NRgiYCiWqBP4URj2YnLgcCAo1hPYMkCxiBXCOEdAWKRSyJe4R?=
 =?us-ascii?q?8iWYCBAYFAhMBgUaBHXFwFTsNDYEzgR+CJxd/AQeNN1EBgQQBASGKAYI+AQE?=
X-IronPort-AV: E=Sophos;i="5.56,335,1539673200"; 
   d="scan'208";a="44220495"
X-Amp-Result: UNSCANNABLE
X-Amp-File-Uploaded: False
Unscannable: 2
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 09 Dec 2018 11:56:10 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726258AbeLIT4H (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Sun, 9 Dec 2018 14:56:07 -0500
Received: from mx0a-001b2d01.pphosted.com ([148.163.156.1]:57780 "EHLO
        mx0a-001b2d01.pphosted.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S1726090AbeLIT4H (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Sun, 9 Dec 2018 14:56:07 -0500
Received: from pps.filterd (m0098399.ppops.net [127.0.0.1])
        by mx0a-001b2d01.pphosted.com (8.16.0.22/8.16.0.22) with SMTP id wB9Jrss8119507
        for <linux-kernel@vger.kernel.org>; Sun, 9 Dec 2018 14:56:04 -0500
Received: from e16.ny.us.ibm.com (e16.ny.us.ibm.com [129.33.205.206])
        by mx0a-001b2d01.pphosted.com with ESMTP id 2p98m51mma-1
        (version=TLSv1.2 cipher=AES256-GCM-SHA384 bits=256 verify=NOT)
        for <linux-kernel@vger.kernel.org>; Sun, 09 Dec 2018 14:56:04 -0500
Received: from localhost
        by e16.ny.us.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
        for <linux-kernel@vger.kernel.org> from <paulmck@linux.vnet.ibm.com>;
        Sun, 9 Dec 2018 19:56:03 -0000
Received: from b01cxnp23033.gho.pok.ibm.com (9.57.198.28)
        by e16.ny.us.ibm.com (146.89.104.203) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
        (version=TLSv1/SSLv3 cipher=AES256-GCM-SHA384 bits=256/256)
        Sun, 9 Dec 2018 19:56:00 -0000
Received: from b01ledav003.gho.pok.ibm.com (b01ledav003.gho.pok.ibm.com [9.57.199.108])
        by b01cxnp23033.gho.pok.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id wB9JtxeP22610112
        (version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256 verify=FAIL);
        Sun, 9 Dec 2018 19:55:59 GMT
Received: from b01ledav003.gho.pok.ibm.com (unknown [127.0.0.1])
        by IMSVA (Postfix) with ESMTP id 6FDCCB2065;
        Sun,  9 Dec 2018 19:55:59 +0000 (GMT)
Received: from b01ledav003.gho.pok.ibm.com (unknown [127.0.0.1])
        by IMSVA (Postfix) with ESMTP id 2F17EB205F;
        Sun,  9 Dec 2018 19:55:59 +0000 (GMT)
Received: from paulmck-ThinkPad-W541 (unknown [9.80.217.118])
        by b01ledav003.gho.pok.ibm.com (Postfix) with ESMTP;
        Sun,  9 Dec 2018 19:55:58 +0000 (GMT)
Received: by paulmck-ThinkPad-W541 (Postfix, from userid 1000)
        id 4CD8216C2AC5; Sun,  9 Dec 2018 11:56:01 -0800 (PST)
Date: Sun, 9 Dec 2018 11:56:01 -0800
From: "Paul E. McKenney" <paulmck@linux.ibm.com>
To: "He, Bo" <bo.he@intel.com>
Cc: Steven Rostedt <rostedt@goodmis.org>,
        "linux-kernel@vger.kernel.org" <linux-kernel@vger.kernel.org>,
        "josh@joshtriplett.org" <josh@joshtriplett.org>,
        "mathieu.desnoyers@efficios.com" <mathieu.desnoyers@efficios.com>,
        "jiangshanlai@gmail.com" <jiangshanlai@gmail.com>,
        "Zhang, Jun" <jun.zhang@intel.com>,
        "Xiao, Jin" <jin.xiao@intel.com>,
        "Zhang, Yanmin" <yanmin.zhang@intel.com>,
        "Bai, Jie A" <jie.a.bai@intel.com>
Subject: Re: rcu_preempt caused oom
Reply-To: paulmck@linux.ibm.com
References: <CD6925E8781EFD4D8E11882D20FC406D52A14B74@SHSMSX104.ccr.corp.intel.com>
 <20181203135638.GG4170@linux.ibm.com>
 <CD6925E8781EFD4D8E11882D20FC406D52A1509A@SHSMSX104.ccr.corp.intel.com>
 <20181204194936.GD4170@linux.ibm.com>
 <CD6925E8781EFD4D8E11882D20FC406D52A16605@SHSMSX104.ccr.corp.intel.com>
 <20181205174435.GU4170@linux.ibm.com>
 <CD6925E8781EFD4D8E11882D20FC406D52A16C46@SHSMSX104.ccr.corp.intel.com>
 <20181206173808.GI4170@linux.ibm.com>
 <CD6925E8781EFD4D8E11882D20FC406D52A180C5@SHSMSX104.ccr.corp.intel.com>
 <20181207141131.GP4170@linux.ibm.com>
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: <20181207141131.GP4170@linux.ibm.com>
User-Agent: Mutt/1.5.21 (2010-09-15)
X-TM-AS-GCONF: 00
x-cbid: 18120919-0072-0000-0000-000003D6C90E
X-IBM-SpamModules-Scores: 
X-IBM-SpamModules-Versions: BY=3.00010201; HX=3.00000242; KW=3.00000007;
 PH=3.00000004; SC=3.00000270; SDB=6.01129387; UDB=6.00582089; IPR=6.00909519;
 MB=3.00024613; MTD=3.00000008; XFM=3.00000015; UTC=2018-12-09 19:56:02
X-IBM-AV-DETECTION: SAVI=unused REMOTE=unused XFE=unused
x-cbparentid: 18120919-0073-0000-0000-00004A631974
Message-Id: <20181209195601.GA7854@linux.ibm.com>
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10434:,, definitions=2018-12-09_08:,,
 signatures=0
X-Proofpoint-Spam-Details: rule=outbound_notspam policy=outbound score=0 priorityscore=1501
 malwarescore=0 suspectscore=2 phishscore=0 bulkscore=0 spamscore=0
 clxscore=1015 lowpriorityscore=0 mlxscore=0 impostorscore=0
 mlxlogscore=999 adultscore=0 classifier=spam adjust=0 reason=mlx
 scancount=1 engine=8.0.1-1810050000 definitions=main-1812090184
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

On Fri, Dec 07, 2018 at 06:11:31AM -0800, Paul E. McKenney wrote:
> On Fri, Dec 07, 2018 at 08:25:09AM +0000, He, Bo wrote:
> > Bad news,  the issue is still reproduced after 61 Hours monkey test on 1/6 boards with the CONFIG_RCU_BOOST=y, and the issue is not seen on kernel 4.14, the CONFIG_RCU_BOOST is also not enabled in our kernel 4.14 config.
> > Here enclosed is the logs.
> > 
> > > So the question becomes "Why is the grace-period kthread being awakened so many times, but not actually waking up?"  
> > maybe it's not schedule issue, I have two suspects:
> > we can see tons of grace_period with 117392312: 
> >  [219346.919405, 0] showmap-31232 [000] d..1 219346.136035: rcu_future_grace_period: rcu_preempt 117392312 117392316 0 0 3 Startleaf
> >  [219346.919417, 0] showmap-31232 [000] d..1 219346.136036: rcu_future_grace_period: rcu_preempt 117392312 117392316 0 0 3 Prestarted
> >  [219346.919429, 0] showmap-31232 [000] d..1 219346.136036: rcu_grace_period: rcu_preempt 117392312 AccWaitCB
> > 
> > "Startleaf" means start the grace period, "Prestarted" means the grace period is already started or other conditions blocked, RCU_GP_FLAG_INIT should follow the "Startedroot", then the kthread can be wakeup.
> 
> Yes, when "Startleaf" is followed by "Prestarted", that means that we
> reached an rcu_node structure that is already aware that the requested
> grace period is needed.  Breaking down the relevant "if" statement in
> rcu_start_this_gp():
> 
> 		if (ULONG_CMP_GE(rnp->gp_seq_needed, gp_seq_req) ||
> 			// A.  GP already requested at this rcu_node
> 		    rcu_seq_started(&rnp->gp_seq, gp_seq_req) ||
> 			// B.  The requested grace period already started
> 		    (rnp != rnp_start &&
> 		     rcu_seq_state(rcu_seq_current(&rnp->gp_seq)))) {
> 			// C.  Leaf rcu_node recorded request, and
> 			//     some grace period is in progress
> 
> A:	In this case, the "Startedroot" should be taken care of by some
> 	other thread, or one of B or C held earlier.
> 
> B:	This cannot be the case, because your earlier trace showed that
> 	the requested grace period had not started.
> 
> C:	This cannot be the case because both traces above are on the
> 	leaf rcu_node structure.  If it were the case, the currently
> 	running grace period would notice the need for the requested
> 	grace period when it ended, and would start the grace period
> 	at that time.
> 
> So you are saying that your trace goes back far enough to capture the
> very first "Startleaf" for this new grace period, and you don't ever see a
> "Startedroot"?  This would be OK if the later "Startedleaf" showed up at
> that point.  If you do have such a trace, could you please send it to me
> (or post it somewhere and send me the URL)?
> 
> In any case, this code has bee reworked recently, so I will take a closer
> look, which will take some time.  Please feel free to continue to do so
> as well, of course!

Hmmm...  Could you please build with CONFIG_PROVE_RCU=y and run the
original (for example, CONFIG_RCU_BOOST=n)?  I would expect this to
trigger the warning in rcu_check_gp_start_stall().  Of course, if it
does not trigger, that would be valuable information as well.

							Thanx, Paul

> > I do experiment to dump the backtrace, the rcu_quiescent_state_report is called in softirq context:
> >         <idle>-0     [000] dNs2 24471.669280: rcu_quiescent_state_report: rcu_preempt 3562401 1>0 0 0 3 0
> >           <idle>-0     [000] dNs2 24471.669293: <stack trace>
> >  => rcu_report_qs_rnp+0x1e2/0x2a0
> >  => rcu_process_callbacks+0x2f1/0x3c0
> >  => __do_softirq+0x12a/0x386
> >  => irq_exit+0xb1/0xc0
> >  => smp_apic_timer_interrupt+0xd4/0x1e0
> >  => apic_timer_interrupt+0xf/0x20
> >  => cpuidle_enter_state+0xb1/0x340
> >  => cpuidle_enter+0x17/0x20
> >  => call_cpuidle+0x23/0x40
> >  => do_idle+0x1ed/0x250
> >  => cpu_startup_entry+0x73/0x80
> >  => rest_init+0xf3/0x100
> >  => start_kernel+0x46f/0x490
> >  => x86_64_start_reservations+0x2a/0x2c
> >  => x86_64_start_kernel+0x72/0x75
> >  => secondary_startup_64+0xa4/0xb0
> > rcu_report_qs_rnp=>rcu_report_qs_rdp
> > 
> > and in the rcu_report_qs_rdp(), rcu_report_qs_rnp is follow the rcu_accelerate_cbs, we can see AccWaitCB log, but can't see rcu_quiescent_state_report, mostly it's due to condition rnp->qsmask & mask blocked.
> > 
> > static void
> > rcu_report_qs_rdp(int cpu, struct rcu_state *rsp, struct rcu_data *rdp)
> > {
> > ...
> > if ((rnp->qsmask & mask) == 0) {
> >                 raw_spin_unlock_irqrestore_rcu_node(rnp, flags);
> >         } else {
> >                 rdp->core_needs_qs = false;
> >                 needwake = rcu_accelerate_cbs(rsp, rnp, rdp); 
> >                 rcu_report_qs_rnp(mask, rsp, rnp, rnp->gp_seq, flags);	                                                                                               
> > 
> >                 if (needwake)
> >                         rcu_gp_kthread_wake(rsp);
> >         }
> > }
> 
> This is a completely different code path.  The rcu_start_this_gp()
> function is trying to start a new grace period.  In contrast, this
> rcu_report_qs_rdp() function reports a quiescent state for a currently
> running grace period.  In your earlier trace, there was no currently
> running grace period, so rcu_report_qs_rdp() exiting early is expected
> behavior.
> 
> 							Thanx, Paul
> 
> > -----Original Message-----
> > From: Paul E. McKenney <paulmck@linux.ibm.com> 
> > Sent: Friday, December 7, 2018 1:38 AM
> > To: He, Bo <bo.he@intel.com>
> > Cc: Steven Rostedt <rostedt@goodmis.org>; linux-kernel@vger.kernel.org; josh@joshtriplett.org; mathieu.desnoyers@efficios.com; jiangshanlai@gmail.com; Zhang, Jun <jun.zhang@intel.com>; Xiao, Jin <jin.xiao@intel.com>; Zhang, Yanmin <yanmin.zhang@intel.com>; Bai, Jie A <jie.a.bai@intel.com>
> > Subject: Re: rcu_preempt caused oom
> > 
> > On Thu, Dec 06, 2018 at 01:23:01PM +0000, He, Bo wrote:
> > > 1. The test is positive after set the kthread priority to SCHED_FIFO without CONFIG_RCU_BOOST,  the issue is not reproduced until now.
> > > 2. Here is previous log enable the ftrace_dump, and we can get 4 seconds ftrace. The panic log was triggered with the enclosed debug patch, replaced the wait_for_completion(&rs_array[i].completion) with wait_for_completion_timeout(&rs_array[i].completion, 3*HZ) in __wait_rcu_gp(). The logs enabled the lockdep to dump the locks, and dump all tasks backtrace.
> > 
> > Thank you for collecting this information!
> > 
> > (By the way, the usual downside of the priority increase is increased context-switch rate and thus CPU overhead.)
> > 
> > And all three grace-period kthreads are blocked apparently in their top-level loops (though inlining and all that).  There are quite a few preemptions ("72738.702815: rcu_preempt_task: rcu_preempt"), but they are all blocking the next grace period (29041008), not the current one (29041004).  And the "rcu_unlock_preempted_task" trace records flag the current grace-period sequence number as 29041004, which means that there is no grace period in progress, that is, RCU is idle.
> > 
> > Which explains why there is no RCU CPU stall warning -- after all, if there is no grace period in flight, it is not possible to stall that non-existent grace period.
> > 
> > That also could explain why increasing the priority of the grace-period kthreads gets things going again.  There have been a great number of requests for a new grace period (for example, "rcu_future_grace_period:
> > rcu_preempt 29041004 29041008 0 0 3 Startleaf"), so as soon as the grace-period kthread wakes up, a new grace period will start.
> > 
> > Except that the rcu_preempt task says "I" rather than "R", as you noted in an earlier email.
> > 
> > And there should have been multiple attempts to wake up the grace-period kthread, because there are lots of callbacks queued as in 136,045 of them ("rcu_callback: rcu_preempt rhp=0000000066f735c9 func=file_free_rcu 2811/136045").  Which is of course why you are seeing the OOM.
> > 
> > So the question becomes "Why is the grace-period kthread being awakened so many times, but not actually waking up?"  In the past, there was a scheduler bug that could cause that, but that was -way- before the v4.19 that you are running.  More recently, there have been timer-related problems, but those only happened while a grace period was active, and where also long before v4.19.
> > 
> > Hmmm...  One possibility is that you have somehow managed to invoke
> > call_rcu() with interrupts disabled, which would in turn disable the extra wakeups that RCU sends when it sees excessive numbers of callbacks.
> > Except that in that case, boosting the priority wouldn't help.  Besides, the scheduling-clock interrupt should also check for this, and should push things forward if need be.
> > 
> > If RCU managed to put all of its callbacks into the RCU_NEXT_READY_TAIL bucket on all CPUs, that would defeat the wakeup-if-no-grace-period checks (RCU is supposed to have started the relevant grace period before putting callbacks into that bucket).  But that cannot be the case here, because new callbacks are being enqueued throughout, and these would then trigger RCU's start-a-new-grace-period checks.
> > 
> > But it would be good to confirm that this is actually working like I would expect it to.  Could you please add scheduler wakeup to your tracing, if possible, only displaying those sent to the rcu_preempt task?
> > 
> > 							Thanx, Paul
> > 
> > > -----Original Message-----
> > > From: Paul E. McKenney <paulmck@linux.ibm.com>
> > > Sent: Thursday, December 6, 2018 1:45 AM
> > > To: He, Bo <bo.he@intel.com>
> > > Cc: Steven Rostedt <rostedt@goodmis.org>; 
> > > linux-kernel@vger.kernel.org; josh@joshtriplett.org; 
> > > mathieu.desnoyers@efficios.com; jiangshanlai@gmail.com; Zhang, Jun 
> > > <jun.zhang@intel.com>; Xiao, Jin <jin.xiao@intel.com>; Zhang, Yanmin 
> > > <yanmin.zhang@intel.com>; Bai, Jie A <jie.a.bai@intel.com>
> > > Subject: Re: rcu_preempt caused oom
> > > 
> > > On Wed, Dec 05, 2018 at 08:42:54AM +0000, He, Bo wrote:
> > > > I double checked the .config, we don't enable CONFIG_NO_HZ_FULL .
> > > > Our previous logs can dump all the task backtrace, and kthread (the rcu_preempt, rcu_sched, and rcu_bh tasks) are all in "I" state not in "R" state, my understandings are if it's the side-effect of causing RCU's kthreads to be run at SCHED_FIFO priority 1, the kthreads should be in R state.
> > > 
> > > Hmmm...  Well, the tasks could in theory be waiting on a blocking mutex.
> > > But in practice the grace-period kthreads wait on events, so that makes no sense.
> > > 
> > > Is it possible for you to dump out the grace-period kthread's stack, 
> > > for example, with sysreq-t?  (Steve might know a better way to do 
> > > this.)
> > > 
> > > > I will do more experiments and keep you update once we have more findings:
> > > > 1. set the kthread priority to SCHED_FIFO without CONFIG_RCU_BOOST and see if the issue can reproduce.
> > > 
> > > That sounds like a most excellent experiment!
> > > 
> > > > 2. check more ftrace to double confirm why there is no trace_rcu_quiescent_state_report and most of the trace_rcu_grace_period are in "AccWaitCB".
> > > 
> > > As noted earlier, to see something interesting, you will need to start the ftrace before the grace period starts.  This would probably mean having ftrace running before starting the test.  Starting the ftrace after the hang commences is unlikely to produce useful information.
> > > 
> > > 							Thanx, Paul
> > > 
> > > > -----Original Message-----
> > > > From: Paul E. McKenney <paulmck@linux.ibm.com>
> > > > Sent: Wednesday, December 5, 2018 3:50 AM
> > > > To: He, Bo <bo.he@intel.com>
> > > > Cc: Steven Rostedt <rostedt@goodmis.org>; 
> > > > linux-kernel@vger.kernel.org; josh@joshtriplett.org; 
> > > > mathieu.desnoyers@efficios.com; jiangshanlai@gmail.com; Zhang, Jun 
> > > > <jun.zhang@intel.com>; Xiao, Jin <jin.xiao@intel.com>; Zhang, Yanmin 
> > > > <yanmin.zhang@intel.com>; Bai, Jie A <jie.a.bai@intel.com>
> > > > Subject: Re: rcu_preempt caused oom
> > > > 
> > > > On Tue, Dec 04, 2018 at 07:50:04AM +0000, He, Bo wrote:
> > > > > Hi, Paul:
> > > > > the enclosed is the log trigger the 120s hung_task_panic without other debug patches, the hung task is blocked at __wait_rcu_gp, it means the rcu_cpu_stall can't detect the scenario:
> > > > > echo 1 > /proc/sys/kernel/panic_on_rcu_stall
> > > > > echo 7 > /sys/module/rcupdate/parameters/rcu_cpu_stall_timeout
> > > > 
> > > > Not necessarily.  If there is an RCU CPU stall warning, blocking 
> > > > within
> > > > __wait_rcu_gp() is expected behavior.  It is possible that the problem is that although the grace period is completing as required, the callbacks are not being invoked in a timely fashion.  And that could happen if you had CONFIG_NO_HZ_FULL and a bunch of nohz_full CPUs, or, alternatively, callback offloading enabled.  But I don't see these in your previous emails.  Another possible cause is that the grace-period kthread is being delayed, so that the grace period never starts.  This seems unlikely, but it is the only thing thus far that matches the symptoms.
> > > > 
> > > > CONFIG_RCU_BOOST=y has the side-effect of causing RCU's kthreads to be run at SCHED_FIFO priority 1, and that would help in the case where RCU's grace-period kthread (the rcu_preempt, rcu_sched, and rcu_bh tasks, all of which execute in the rcu_gp_kthread() function) was being starved of CPU time.
> > > > 
> > > > Does that sound likely?
> > > > 
> > > > 							Thanx, Paul
> > > > 
> > > > > -----Original Message-----
> > > > > From: Paul E. McKenney <paulmck@linux.ibm.com>
> > > > > Sent: Monday, December 3, 2018 9:57 PM
> > > > > To: He, Bo <bo.he@intel.com>
> > > > > Cc: Steven Rostedt <rostedt@goodmis.org>; 
> > > > > linux-kernel@vger.kernel.org; josh@joshtriplett.org; 
> > > > > mathieu.desnoyers@efficios.com; jiangshanlai@gmail.com; Zhang, Jun 
> > > > > <jun.zhang@intel.com>; Xiao, Jin <jin.xiao@intel.com>; Zhang, 
> > > > > Yanmin <yanmin.zhang@intel.com>
> > > > > Subject: Re: rcu_preempt caused oom
> > > > > 
> > > > > On Mon, Dec 03, 2018 at 07:44:03AM +0000, He, Bo wrote:
> > > > > > Thanks, we have run the test for the whole weekend and not reproduce the issue,  so we confirm the CONFIG_RCU_BOOST can fix the issue.
> > > > > 
> > > > > Very good, that is encouraging.  Perhaps I should think about making CONFIG_RCU_BOOST=y the default for CONFIG_PREEMPT in mainline, at least for architectures for which rt_mutexes are implemented.
> > > > > 
> > > > > > We have enabled the rcupdate.rcu_cpu_stall_timeout=7 and also set panic on rcu stall and will see if we can see the panic, will keep you posed with the test results.
> > > > > > echo 1 > /proc/sys/kernel/panic_on_rcu_stall
> > > > > 
> > > > > Looking forward to seeing what is going on!  Of course, to reproduce, you will need to again build with CONFIG_RCU_BOOST=n.
> > > > > 
> > > > > 							Thanx, Paul
> > > > > 
> > > > > > -----Original Message-----
> > > > > > From: Paul E. McKenney <paulmck@linux.ibm.com>
> > > > > > Sent: Saturday, December 1, 2018 12:49 AM
> > > > > > To: He, Bo <bo.he@intel.com>
> > > > > > Cc: Steven Rostedt <rostedt@goodmis.org>; 
> > > > > > linux-kernel@vger.kernel.org; josh@joshtriplett.org; 
> > > > > > mathieu.desnoyers@efficios.com; jiangshanlai@gmail.com; Zhang, 
> > > > > > Jun <jun.zhang@intel.com>; Xiao, Jin <jin.xiao@intel.com>; 
> > > > > > Zhang, Yanmin <yanmin.zhang@intel.com>
> > > > > > Subject: Re: rcu_preempt caused oom
> > > > > > 
> > > > > > On Fri, Nov 30, 2018 at 03:18:58PM +0000, He, Bo wrote:
> > > > > > > Here is the kernel cmdline:
> > > > > > 
> > > > > > Thank you!
> > > > > > 
> > > > > > > Kernel command line: androidboot.acpio_idx=0
> > > > > > > androidboot.bootloader=efiwrapper-02_03-userdebug_kernelflinge
> > > > > > > r-
> > > > > > > 06
> > > > > > > _0
> > > > > > > 3- userdebug androidboot.diskbus=00.0 
> > > > > > > androidboot.verifiedbootstate=green
> > > > > > > androidboot.bootreason=power-on 
> > > > > > > androidboot.serialno=R1J56L6006a7bb
> > > > > > > g_ffs.iSerialNumber=R1J56L6006a7bb no_timer_check noxsaves 
> > > > > > > reboot_panic=p,w i915.hpd_sense_invert=0x7 mem=2G nokaslr 
> > > > > > > nopti ftrace_dump_on_oops trace_buf_size=1024K intel_iommu=off 
> > > > > > > gpt
> > > > > > > loglevel=4 androidboot.hardware=gordon_peak 
> > > > > > > firmware_class.path=/vendor/firmware relative_sleep_states=1
> > > > > > > enforcing=0 androidboot.selinux=permissive cpu_init_udelay=10
> > > > > > > androidboot.android_dt_dir=/sys/bus/platform/devices/ANDR0001:
> > > > > > > 00 /p ro pe rties/android/ pstore.backend=ramoops
> > > > > > > memmap=0x1400000$0x50000000
> > > > > > > ramoops.mem_address=0x50000000 ramoops.mem_size=0x1400000
> > > > > > > ramoops.record_size=0x4000 ramoops.console_size=0x1000000
> > > > > > > ramoops.ftrace_size=0x10000 ramoops.dump_oops=1 vga=current
> > > > > > > i915.modeset=1 drm.atomic=1 i915.nuclear_pageflip=1 
> > > > > > > drm.vblankoffdelay=
> > > > > > 
> > > > > > And no sign of any suppression of RCU CPU stall warnings.  Hmmm...
> > > > > > It does take more than 21 seconds to OOM?  Or do things happen faster than that?  If they do happen faster than that, then on approach would be to add something like this to the kernel command line:
> > > > > > 
> > > > > > 	rcupdate.rcu_cpu_stall_timeout=7
> > > > > > 
> > > > > > This would set the stall timeout to seven seconds.  Note that timeouts less than three seconds are silently interpreted as three seconds.
> > > > > > 
> > > > > > 							Thanx, Paul
> > > > > > 
> > > > > > > -----Original Message-----
> > > > > > > From: Steven Rostedt <rostedt@goodmis.org>
> > > > > > > Sent: Friday, November 30, 2018 11:17 PM
> > > > > > > To: Paul E. McKenney <paulmck@linux.ibm.com>
> > > > > > > Cc: He, Bo <bo.he@intel.com>; linux-kernel@vger.kernel.org; 
> > > > > > > josh@joshtriplett.org; mathieu.desnoyers@efficios.com; 
> > > > > > > jiangshanlai@gmail.com; Zhang, Jun <jun.zhang@intel.com>; 
> > > > > > > Xiao, Jin <jin.xiao@intel.com>; Zhang, Yanmin 
> > > > > > > <yanmin.zhang@intel.com>
> > > > > > > Subject: Re: rcu_preempt caused oom
> > > > > > > 
> > > > > > > On Fri, 30 Nov 2018 06:43:17 -0800 "Paul E. McKenney" 
> > > > > > > <paulmck@linux.ibm.com> wrote:
> > > > > > > 
> > > > > > > > Could you please send me your list of kernel boot parameters?  
> > > > > > > > They usually appear near the start of your console output.
> > > > > > > 
> > > > > > > Or just: cat /proc/cmdline
> > > > > > > 
> > > > > > > -- Steve
> > > > > > > 
> > > > > > 
> > > > > 
> > > > 
> > > > 
> > > 
> > 
> > 
> 
> 

