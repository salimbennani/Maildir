Return-Path: <qemu-devel-bounces+like.xu=linux.intel.com@nongnu.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 07 Dec 2018 08:43:04 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga004.jf.intel.com (orsmga004.jf.intel.com [10.7.209.38])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 3F34B58014B
	for <like.xu@linux.intel.com>; Thu,  6 Dec 2018 01:23:30 -0800 (PST)
Received: from fmsmga103.fm.intel.com ([10.1.193.90])
  by orsmga004-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 06 Dec 2018 01:23:30 -0800
IronPort-PHdr: =?us-ascii?q?9a23=3AxnF3gxM/t15bTFyhDjAl6mtUPXoX/o7sNwtQ0KIM?=
 =?us-ascii?q?zox0I/r+rarrMEGX3/hxlliBBdydt6oUzbKO+4nbGkU4qa6bt34DdJEeHzQksu?=
 =?us-ascii?q?4x2zIaPcieFEfgJ+TrZSFpVO5LVVti4m3peRMNQJW2aFLduGC94iAPERvjKwV1?=
 =?us-ascii?q?Ov71GonPhMiryuy+4ZLebxlLiTanfb9+MAi9oBnMuMURnYZsMLs6xAHTontPde?=
 =?us-ascii?q?RWxGdoKkyWkh3h+Mq+/4Nt/jpJtf45+MFOTav1f6IjTbxFFzsmKHw65NfqtRbY?=
 =?us-ascii?q?UwSC4GYXX3gMnRpJBwjF6wz6Xov0vyDnuOdxxDWWMMvrRr0vRz+s87lkRwPpiC?=
 =?us-ascii?q?cfNj427mfXitBrjKlGpB6tvgFzz5LIbI2QMvd1Y6HTcs4ARWdZQMhRWSxPDICy?=
 =?us-ascii?q?YYQBAOUOP/pXoYbgqVsWrxawBwahCP7hxzNUmHD2xrY30/gjHAzcwAAsA9wDvX?=
 =?us-ascii?q?bSod7oNKkSS+e1zKzQwDvfcfxW3Df845XQfB44rv+CW697fM3LyUYxEgPEjk+c?=
 =?us-ascii?q?qYriPzyL0uQAqHOU7+56Wu2ylWErsg5xoiKoxscxkonFnJ4aylfB9Shgxos+ON?=
 =?us-ascii?q?62SFZjbNK6DJddtDuWO5ZrTs4hWW1kpig3x70ctZKmfiUG0IkryhzcZvCdbYSE?=
 =?us-ascii?q?/hHuWPyMLTp5nn5odqyzihCv+ka60OL8TNO70FNSoypFjNbMsncN2gTX6siGUf?=
 =?us-ascii?q?t94lyh1SyA1wDV9+FIO0c0lbDUK5I5w74wkIQcsVjbEyPohEn7j7Waelg59uWr?=
 =?us-ascii?q?8ejrfLvrq5+GO4NpiAzyKqEulda+AeQ8PAgORW+b+eGk2b3640L5RahKguQrna?=
 =?us-ascii?q?bHrpDVO8AbqreiDA9Sz4Yj7QqwACm90NgfmXkHLVFFdwydg4nmJlHDOPT4Dfa5?=
 =?us-ascii?q?g1SxnzZn3fHGPrv9AprTKnjPiqvufbF460NHzgozytZf551SCrEcOv7zXVXxtN?=
 =?us-ascii?q?PAAh8jLwO02/rnCMl61o4GXWKPA6yZP73IvV6H++IiOO2MZI4TuDbgJPkp/f/u?=
 =?us-ascii?q?jXklmVADeamlx4cYaHe9HqcuHkOCfHC5gssdCXxY+U06Tff2kxuEVjhcYWv0WL?=
 =?us-ascii?q?gzoTQyCYajBIGEQZixgbuHx2CiE5hLI2xLFF2IQkrubJiODvIFaSaOJZ14nzkZ?=
 =?us-ascii?q?ELSsVYIlkAujrRL30KZPKO3S9SsF85X5249u+ufRmBouoCFyFNmXyGqXTmt5zV?=
 =?us-ascii?q?8PEiY72b06rUFjx1Or169+jPpFU9tJ6KBnSAA/YKTRyv0yLffffkqVZNqMWRCg?=
 =?us-ascii?q?RdiiKTs1R948htAUZFtlXd6li0aQjGKRH7YJmunTV9QP+aXG0i20fp4lxg=3D?=
 =?us-ascii?q?=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0D6AwBD6QhchxHrdtBkgheBMSqBDwOBJ?=
 =?us-ascii?q?oN6iHiLLoFgCCWOQYkMgXYPAQEYAxGHVSI2Bw0BAwEBAQEBAQIBEwEBAQoLCQg?=
 =?us-ascii?q?bDiMMgjYFAgMaAQaCWwEBAQECAQECIAQRCAEBBAopAQICAQECBgEBChUDAgIiB?=
 =?us-ascii?q?AICAwEbFQEFARwTBgIBAQGCUUsBgXkIAQMBmhE8ih1wfDOCdgEBBYJDhGUIEnm?=
 =?us-ascii?q?Jd4EcgVc/gREnDIJfgwWFAIJXiTIEhkiQXQcCHIIIhF+KOR6BW02HOBAmhxqYW?=
 =?us-ascii?q?wIEAgQFAgUPIYEsB4F/cFCCbAmCHheDSoEEiVBxgQeIZYF3AQE?=
X-IPAS-Result: =?us-ascii?q?A0D6AwBD6QhchxHrdtBkgheBMSqBDwOBJoN6iHiLLoFgCCW?=
 =?us-ascii?q?OQYkMgXYPAQEYAxGHVSI2Bw0BAwEBAQEBAQIBEwEBAQoLCQgbDiMMgjYFAgMaA?=
 =?us-ascii?q?QaCWwEBAQECAQECIAQRCAEBBAopAQICAQECBgEBChUDAgIiBAICAwEbFQEFARw?=
 =?us-ascii?q?TBgIBAQGCUUsBgXkIAQMBmhE8ih1wfDOCdgEBBYJDhGUIEnmJd4EcgVc/gREnD?=
 =?us-ascii?q?IJfgwWFAIJXiTIEhkiQXQcCHIIIhF+KOR6BW02HOBAmhxqYWwIEAgQFAgUPIYE?=
 =?us-ascii?q?sB4F/cFCCbAmCHheDSoEEiVBxgQeIZYF3AQE?=
X-IronPort-AV: E=Sophos;i="5.56,321,1539673200"; 
   d="scan'208";a="54777116"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from lists.gnu.org ([208.118.235.17])
  by mtab.intel.com with ESMTP/TLS/AES256-SHA; 06 Dec 2018 01:23:29 -0800
Received: from localhost ([::1]:39735 helo=lists.gnu.org)
	by lists.gnu.org with esmtp (Exim 4.71)
	(envelope-from <qemu-devel-bounces+like.xu=linux.intel.com@nongnu.org>)
	id 1gUpsW-0000CL-B1
	for like.xu@linux.intel.com; Thu, 06 Dec 2018 04:23:28 -0500
Received: from eggs.gnu.org ([2001:4830:134:3::10]:58502)
	by lists.gnu.org with esmtp (Exim 4.71)
	(envelope-from <remy.noel@blade-group.com>) id 1gUpsM-0000C8-DF
	for qemu-devel@nongnu.org; Thu, 06 Dec 2018 04:23:19 -0500
Received: from Debian-exim by eggs.gnu.org with spam-scanned (Exim 4.71)
	(envelope-from <remy.noel@blade-group.com>) id 1gUpsH-0004JD-9d
	for qemu-devel@nongnu.org; Thu, 06 Dec 2018 04:23:18 -0500
Received: from mail-wm1-x32a.google.com ([2a00:1450:4864:20::32a]:38835)
	by eggs.gnu.org with esmtps (TLS1.0:RSA_AES_128_CBC_SHA1:16)
	(Exim 4.71) (envelope-from <remy.noel@blade-group.com>)
	id 1gUpsG-0004Iq-To
	for qemu-devel@nongnu.org; Thu, 06 Dec 2018 04:23:13 -0500
Received: by mail-wm1-x32a.google.com with SMTP id m22so231947wml.3
	for <qemu-devel@nongnu.org>; Thu, 06 Dec 2018 01:23:12 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=blade-group.com; s=google;
	h=subject:to:cc:references:from:message-id:date:user-agent
	:mime-version:in-reply-to:content-transfer-encoding:content-language;
	bh=VAGHiPJlBYn1XyahyinjTHtX+q6rLsvi/26de+cSh4g=;
	b=YfIlVfXIPWMSoLAXytfgXhD+eCQ+v3aRInYB7A3e3khBIlPRfsDne8zYZVDtpusohZ
	vaIIY/wOhjLvI+ikSggtvfbJqsHeCCqS3bC61yhz4bZDVL6N/tPuVVboGs3bp+2BmMs2
	OmnS7gZmqWBJYCGhzk8+h2sA0rUxLpTLFbIg1IWL4yuh2KIiAcOESt4QD7dnuGP072NW
	EXPir+PeuYp42YEo4qDbevGv3tPSoqRYX1Pb3Khh6ofse7czKLVHBormeelbms8EAwZQ
	XjSBfud4wO4MhBlgvTZbbE3CfMUwL904XDZSG5i2LcL3XDiSBAPG3PC5Mx0EMIPKw8pj
	+Q9Q==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:subject:to:cc:references:from:message-id:date
	:user-agent:mime-version:in-reply-to:content-transfer-encoding
	:content-language;
	bh=VAGHiPJlBYn1XyahyinjTHtX+q6rLsvi/26de+cSh4g=;
	b=T8MZjXTd0WDP7XNIIccCGE8cHWrfLeeLev4iTW0g5Tz9qhHnPOpDyQC8zredPfQG1a
	gBIdy+jUBsCEKCSi5w/+4KjXfKxp0VtxuN3DiI6NwRN93DJ6wwHOkRkG3lmmXrPQBfuz
	m5ZmxGAWUV/kkuI8eR1G8x/Ana8sAZDyGa6Cegt7pKVqqXD746oMpmYVhKpcqY0C8dMd
	aaFGwV3hvUv6HOMjHhpzlYZ8mMA7BflS6tPVchJgfgeAbPhXDpMNt2zB3LnFR55Bl8Vx
	gFZTuU/P3TNBVfINmc4f2mlm5wZe+H/46stTWP3r0do43Cq1UV+aKiOIU+OSGqiN6wLk
	srow==
X-Gm-Message-State: AA+aEWYii0+TUozi4szcF5vJScy4ShxfqafxrMUB5SGeytXIv/aUKdKk
	6GxcjkFheT43TArBRj27wY/gEg==
X-Google-Smtp-Source: AFSGD/XMAhRbrEzGurPjs4tIW0Cdd67m3YUhx/t/SXQF6ynEVKq6q9iC8F1n9de6yslfUq1X6HrW9Q==
X-Received: by 2002:a1c:b455:: with SMTP id d82mr19609393wmf.78.1544088191484; 
	Thu, 06 Dec 2018 01:23:11 -0800 (PST)
Received: from [192.168.1.40] ([178.208.16.32])
	by smtp.gmail.com with ESMTPSA id x81sm283171wmg.17.2018.12.06.01.23.10
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Thu, 06 Dec 2018 01:23:10 -0800 (PST)
To: qemu-devel@nongnu.org
References: <20181116190211.17622-1-remy.noel@blade-group.com>
	<20181116190211.17622-3-remy.noel@blade-group.com>
From: Remy NOEL <remy.noel@blade-group.com>
Message-ID: <b7b35303-6043-6830-3b55-2129eb69e2bc@blade-group.com>
Date: Thu, 6 Dec 2018 10:23:29 +0100
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101
	Thunderbird/60.3.1
MIME-Version: 1.0
In-Reply-To: <20181116190211.17622-3-remy.noel@blade-group.com>
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit
Content-Language: en-US-large
X-detected-operating-system: by eggs.gnu.org: Genre and OS details not
	recognized.
X-Received-From: 2a00:1450:4864:20::32a
Subject: Re: [Qemu-devel] util/aio-posix: Use RCU for handler insertion.
X-BeenThere: qemu-devel@nongnu.org
X-Mailman-Version: 2.1.21
Precedence: list
List-Id: <qemu-devel.nongnu.org>
List-Unsubscribe: <https://lists.nongnu.org/mailman/options/qemu-devel>,
	<mailto:qemu-devel-request@nongnu.org?subject=unsubscribe>
List-Archive: <http://lists.nongnu.org/archive/html/qemu-devel/>
List-Post: <mailto:qemu-devel@nongnu.org>
List-Help: <mailto:qemu-devel-request@nongnu.org?subject=help>
List-Subscribe: <https://lists.nongnu.org/mailman/listinfo/qemu-devel>,
	<mailto:qemu-devel-request@nongnu.org?subject=subscribe>
Cc: Stefan Weil <sw@weilnetz.de>, Fam Zheng <famz@redhat.com>,
	"open list:Block I/O path" <qemu-block@nongnu.org>,
	Stefan Hajnoczi <stefanha@redhat.com>
Errors-To: qemu-devel-bounces+like.xu=linux.intel.com@nongnu.org
Sender: "Qemu-devel" <qemu-devel-bounces+like.xu=linux.intel.com@nongnu.org>

I did some tests and noticed the second and third patch to incur some 
performance loss (on a scenario using virtio device)

I will therefore resubmit just the first patch alone.

On 11/16/18 8:02 PM, remy.noel@blade-group.com wrote:
> From: Remy Noel <remy.noel@blade-group.com>
>
> get rid of the delete attribute.
>
> We still need to get rid of the context list lock.
>
> Signed-off-by: Remy Noel <remy.noel@blade-group.com>
> ---
>   util/aio-posix.c | 75 ++++++++++++++++++++++--------------------------
>   util/aio-win32.c | 43 ++++++++++-----------------
>   2 files changed, 49 insertions(+), 69 deletions(-)
>
> diff --git a/util/aio-posix.c b/util/aio-posix.c
> index b34d97292a..83db3f65f4 100644
> --- a/util/aio-posix.c
> +++ b/util/aio-posix.c
> @@ -16,6 +16,7 @@
>   #include "qemu/osdep.h"
>   #include "qemu-common.h"
>   #include "block/block.h"
> +#include "qemu/rcu.h"
>   #include "qemu/rcu_queue.h"
>   #include "qemu/sockets.h"
>   #include "qemu/cutils.h"
> @@ -26,13 +27,14 @@
>   
>   struct AioHandler
>   {
> +    struct rcu_head rcu;
> +
>       GPollFD pfd;
>       IOHandler *io_read;
>       IOHandler *io_write;
>       AioPollFn *io_poll;
>       IOHandler *io_poll_begin;
>       IOHandler *io_poll_end;
> -    int deleted;
>       void *opaque;
>       bool is_external;
>       QLIST_ENTRY(AioHandler) node;
> @@ -65,19 +67,25 @@ static bool aio_epoll_try_enable(AioContext *ctx)
>   {
>       AioHandler *node;
>       struct epoll_event event;
> +    int r = 0;
> +
>   
> +    rcu_read_lock();
>       QLIST_FOREACH_RCU(node, &ctx->aio_handlers, node) {
> -        int r;
> -        if (node->deleted || !node->pfd.events) {
> +        if (!node->pfd.events) {
>               continue;
>           }
>           event.events = epoll_events_from_pfd(node->pfd.events);
>           event.data.ptr = node;
>           r = epoll_ctl(ctx->epollfd, EPOLL_CTL_ADD, node->pfd.fd, &event);
>           if (r) {
> -            return false;
> +            break;
>           }
>       }
> +    rcu_read_unlock();
> +    if (r) {
> +        return false;
> +    }
>       ctx->epoll_enabled = true;
>       return true;
>   }
> @@ -193,14 +201,13 @@ static AioHandler *find_aio_handler(AioContext *ctx, int fd)
>   
>       QLIST_FOREACH(node, &ctx->aio_handlers, node) {
>           if (node->pfd.fd == fd)
> -            if (!node->deleted)
> -                return node;
> +            return node;
>       }
>   
>       return NULL;
>   }
>   
> -static bool aio_remove_fd_handler(AioContext *ctx, AioHandler *node)
> +static void aio_remove_fd_handler(AioContext *ctx, AioHandler *node)
>   {
>       /* If the GSource is in the process of being destroyed then
>        * g_source_remove_poll() causes an assertion failure.  Skip
> @@ -210,19 +217,7 @@ static bool aio_remove_fd_handler(AioContext *ctx, AioHandler *node)
>       if (!g_source_is_destroyed(&ctx->source)) {
>           g_source_remove_poll(&ctx->source, &node->pfd);
>       }
> -
> -    /* If a read is in progress, just mark the node as deleted */
> -    if (qemu_lockcnt_count(&ctx->list_lock)) {
> -        node->deleted = 1;
> -        node->pfd.revents = 0;
> -        return false;
> -    }
> -    /* Otherwise, delete it for real.  We can't just mark it as
> -     * deleted because deleted nodes are only cleaned up while
> -     * no one is walking the handlers list.
> -     */
> -    QLIST_REMOVE(node, node);
> -    return true;
> +    QLIST_REMOVE_RCU(node, node);
>   }
>   
>   void aio_set_fd_handler(AioContext *ctx,
> @@ -249,7 +244,8 @@ void aio_set_fd_handler(AioContext *ctx,
>               qemu_lockcnt_unlock(&ctx->list_lock);
>               return;
>           }
> -        deleted = aio_remove_fd_handler(ctx, node);
> +        aio_remove_fd_handler(ctx, node);
> +        deleted = true;
>           poll_disable_change = -!node->io_poll;
>       } else {
>           poll_disable_change = !io_poll - (node && !node->io_poll);
> @@ -269,7 +265,8 @@ void aio_set_fd_handler(AioContext *ctx,
>           if (is_new) {
>               new_node->pfd.fd = fd;
>           } else {
> -            deleted = aio_remove_fd_handler(ctx, node);
> +            aio_remove_fd_handler(ctx, node);
> +            deleted = true;
>               new_node->pfd = node->pfd;
>           }
>           g_source_add_poll(&ctx->source, &new_node->pfd);
> @@ -296,7 +293,7 @@ void aio_set_fd_handler(AioContext *ctx,
>       aio_notify(ctx);
>   
>       if (deleted) {
> -        g_free(node);
> +        g_free_rcu(node, rcu);
>       }
>   }
>   
> @@ -345,13 +342,10 @@ static void poll_set_started(AioContext *ctx, bool started)
>       ctx->poll_started = started;
>   
>       qemu_lockcnt_inc(&ctx->list_lock);
> +    rcu_read_lock();
>       QLIST_FOREACH_RCU(node, &ctx->aio_handlers, node) {
>           IOHandler *fn;
>   
> -        if (node->deleted) {
> -            continue;
> -        }
> -
>           if (started) {
>               fn = node->io_poll_begin;
>           } else {
> @@ -362,6 +356,7 @@ static void poll_set_started(AioContext *ctx, bool started)
>               fn(node->opaque);
>           }
>       }
> +    rcu_read_unlock();
>       qemu_lockcnt_dec(&ctx->list_lock);
>   }
>   
> @@ -385,6 +380,7 @@ bool aio_pending(AioContext *ctx)
>        */
>       qemu_lockcnt_inc(&ctx->list_lock);
>   
> +    rcu_read_lock();
>       QLIST_FOREACH_RCU(node, &ctx->aio_handlers, node) {
>           int revents;
>   
> @@ -400,6 +396,7 @@ bool aio_pending(AioContext *ctx)
>               break;
>           }
>       }
> +    rcu_read_unlock();
>       qemu_lockcnt_dec(&ctx->list_lock);
>   
>       return result;
> @@ -410,14 +407,14 @@ static bool aio_dispatch_handlers(AioContext *ctx)
>       AioHandler *node, *tmp;
>       bool progress = false;
>   
> +    rcu_read_lock();
>       QLIST_FOREACH_SAFE_RCU(node, &ctx->aio_handlers, node, tmp) {
>           int revents;
>   
>           revents = node->pfd.revents & node->pfd.events;
>           node->pfd.revents = 0;
>   
> -        if (!node->deleted &&
> -            (revents & (G_IO_IN | G_IO_HUP | G_IO_ERR)) &&
> +        if ((revents & (G_IO_IN | G_IO_HUP | G_IO_ERR)) &&
>               aio_node_check(ctx, node->is_external) &&
>               node->io_read) {
>               node->io_read(node->opaque);
> @@ -427,22 +424,14 @@ static bool aio_dispatch_handlers(AioContext *ctx)
>                   progress = true;
>               }
>           }
> -        if (!node->deleted &&
> -            (revents & (G_IO_OUT | G_IO_ERR)) &&
> +        if ((revents & (G_IO_OUT | G_IO_ERR)) &&
>               aio_node_check(ctx, node->is_external) &&
>               node->io_write) {
>               node->io_write(node->opaque);
>               progress = true;
>           }
> -
> -        if (node->deleted) {
> -            if (qemu_lockcnt_dec_if_lock(&ctx->list_lock)) {
> -                QLIST_REMOVE(node, node);
> -                g_free(node);
> -                qemu_lockcnt_inc_and_unlock(&ctx->list_lock);
> -            }
> -        }
>       }
> +    rcu_read_unlock();
>   
>       return progress;
>   }
> @@ -508,8 +497,9 @@ static bool run_poll_handlers_once(AioContext *ctx, int64_t *timeout)
>       bool progress = false;
>       AioHandler *node;
>   
> +    rcu_read_lock();
>       QLIST_FOREACH_RCU(node, &ctx->aio_handlers, node) {
> -        if (!node->deleted && node->io_poll &&
> +        if (node->io_poll &&
>               aio_node_check(ctx, node->is_external) &&
>               node->io_poll(node->opaque)) {
>               *timeout = 0;
> @@ -520,6 +510,7 @@ static bool run_poll_handlers_once(AioContext *ctx, int64_t *timeout)
>   
>           /* Caller handles freeing deleted nodes.  Don't do it here. */
>       }
> +    rcu_read_unlock();
>   
>       return progress;
>   }
> @@ -637,12 +628,14 @@ bool aio_poll(AioContext *ctx, bool blocking)
>           /* fill pollfds */
>   
>           if (!aio_epoll_enabled(ctx)) {
> +            rcu_read_lock();
>               QLIST_FOREACH_RCU(node, &ctx->aio_handlers, node) {
> -                if (!node->deleted && node->pfd.events
> +                if (node->pfd.events
>                       && aio_node_check(ctx, node->is_external)) {
>                       add_pollfd(node);
>                   }
>               }
> +            rcu_read_unlock();
>           }
>   
>           /* wait until next event */
> diff --git a/util/aio-win32.c b/util/aio-win32.c
> index 00e38cdd9f..d7c694e5ac 100644
> --- a/util/aio-win32.c
> +++ b/util/aio-win32.c
> @@ -29,7 +29,6 @@ struct AioHandler {
>       IOHandler *io_write;
>       EventNotifierHandler *io_notify;
>       GPollFD pfd;
> -    int deleted;
>       void *opaque;
>       bool is_external;
>       QLIST_ENTRY(AioHandler) node;
> @@ -37,18 +36,8 @@ struct AioHandler {
>   
>   static void aio_remove_fd_handler(AioContext *ctx, AioHandler *node)
>   {
> -    /* If aio_poll is in progress, just mark the node as deleted */
> -    if (qemu_lockcnt_count(&ctx->list_lock)) {
> -        node->deleted = 1;
> -        node->pfd.revents = 0;
> -    } else {
> -        /* Otherwise, delete it for real.  We can't just mark it as
> -         * deleted because deleted nodes are only cleaned up after
> -         * releasing the list_lock.
> -         */
> -        QLIST_REMOVE(node, node);
> -        g_free(node);
> -    }
> +    QLIST_REMOVE_RCU(node, node);
> +    g_free_rcu(node);
>   }
>   
>   void aio_set_fd_handler(AioContext *ctx,
> @@ -64,7 +53,7 @@ void aio_set_fd_handler(AioContext *ctx,
>   
>       qemu_lockcnt_lock(&ctx->list_lock);
>       QLIST_FOREACH(node, &ctx->aio_handlers, node) {
> -        if (node->pfd.fd == fd && !node->deleted) {
> +        if (node->pfd.fd == fd) {
>               break;
>           }
>       }
> @@ -136,7 +125,7 @@ void aio_set_event_notifier(AioContext *ctx,
>   
>       qemu_lockcnt_lock(&ctx->list_lock);
>       QLIST_FOREACH(node, &ctx->aio_handlers, node) {
> -        if (node->e == e && !node->deleted) {
> +        if (node->e == e) {
>               break;
>           }
>       }
> @@ -188,6 +177,7 @@ bool aio_prepare(AioContext *ctx)
>        * called while we're walking.
>        */
>       qemu_lockcnt_inc(&ctx->list_lock);
> +    rcu_read_lock();
>   
>       /* fill fd sets */
>       FD_ZERO(&rfds);
> @@ -215,7 +205,7 @@ bool aio_prepare(AioContext *ctx)
>               }
>           }
>       }
> -
> +    rcu_read_unlock();
>       qemu_lockcnt_dec(&ctx->list_lock);
>       return have_select_revents;
>   }
> @@ -230,6 +220,7 @@ bool aio_pending(AioContext *ctx)
>        * called while we're walking.
>        */
>       qemu_lockcnt_inc(&ctx->list_lock);
> +    rcu_read_lock();
>       QLIST_FOREACH_RCU(node, &ctx->aio_handlers, node) {
>           if (node->pfd.revents && node->io_notify) {
>               result = true;
> @@ -246,6 +237,7 @@ bool aio_pending(AioContext *ctx)
>           }
>       }
>   
> +    rcu_read_unlock();
>       qemu_lockcnt_dec(&ctx->list_lock);
>       return result;
>   }
> @@ -256,6 +248,7 @@ static bool aio_dispatch_handlers(AioContext *ctx, HANDLE event)
>       bool progress = false;
>       AioHandler *tmp;
>   
> +    rcu_read_lock();
>       /*
>        * We have to walk very carefully in case aio_set_fd_handler is
>        * called while we're walking.
> @@ -263,8 +256,7 @@ static bool aio_dispatch_handlers(AioContext *ctx, HANDLE event)
>       QLIST_FOREACH_SAFE_RCU(node, &ctx->aio_handlers, node, tmp) {
>           int revents = node->pfd.revents;
>   
> -        if (!node->deleted &&
> -            (revents || event_notifier_get_handle(node->e) == event) &&
> +        if ((revents || event_notifier_get_handle(node->e) == event) &&
>               node->io_notify) {
>               node->pfd.revents = 0;
>               node->io_notify(node->e);
> @@ -275,8 +267,7 @@ static bool aio_dispatch_handlers(AioContext *ctx, HANDLE event)
>               }
>           }
>   
> -        if (!node->deleted &&
> -            (node->io_read || node->io_write)) {
> +        if ((node->io_read || node->io_write)) {
>               node->pfd.revents = 0;
>               if ((revents & G_IO_IN) && node->io_read) {
>                   node->io_read(node->opaque);
> @@ -297,14 +288,8 @@ static bool aio_dispatch_handlers(AioContext *ctx, HANDLE event)
>               }
>           }
>   
> -        if (node->deleted) {
> -            if (qemu_lockcnt_dec_if_lock(&ctx->list_lock)) {
> -                QLIST_REMOVE(node, node);
> -                g_free(node);
> -                qemu_lockcnt_inc_and_unlock(&ctx->list_lock);
> -            }
> -        }
>       }
> +    rcu_read_unlock();
>   
>       return progress;
>   }
> @@ -344,12 +329,14 @@ bool aio_poll(AioContext *ctx, bool blocking)
>   
>       /* fill fd sets */
>       count = 0;
> +    rcu_read_lock();
>       QLIST_FOREACH_RCU(node, &ctx->aio_handlers, node) {
> -        if (!node->deleted && node->io_notify
> +        if (node->io_notify
>               && aio_node_check(ctx, node->is_external)) {
>               events[count++] = event_notifier_get_handle(node->e);
>           }
>       }
> +    rcu_read_unlock();
>   
>       first = true;
>   

