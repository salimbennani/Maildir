Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from pop3.zju.edu.cn (124.160.105.205:110) by
  likexu-MOBL1.ccr.corp.intel.com with POP3; 21 Nov 2018 00:42:03 -0000
Received: from icoremail.net (unknown [209.85.214.169])
	by mail-app3 (Coremail) with SMTP id cC_KCgDHH8JecvRbBdSiAQ--.16921S3;
	Wed, 21 Nov 2018 04:45:19 +0800 (CST)
Received: from mail-pl1-f169.google.com (unknown [209.85.214.169])
	by mx2.icoremail.net (Coremail) with SMTP id AQAAfwAX+kVbcvRbstpfAA--.4056S3;
	Wed, 21 Nov 2018 04:45:16 +0800 (CST)
Received: by mail-pl1-f169.google.com with SMTP id u6so2005453plm.8
        for <xuliker@zju.edu.cn>; Tue, 20 Nov 2018 12:45:16 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:delivered-to:dkim-signature:date:from:to:cc
         :subject:message-id:references:mime-version:content-disposition
         :in-reply-to:user-agent:sender:precedence:list-id;
        bh=Wbfxnty6r01cTHG1D7CDCmF9dra0Fg9suHKL4jcE16A=;
        b=rdE3dfbGfxy2+6h4RnFGhnkhRJJSuQaG0hEvbyhwGI3BJ8DJzfZy7tN3Rc9JbsW3Z4
         IC/2ClaR61Hmtj1BTCw2zrq5Ss2WZBbO5pxLEUBWJWCY2E7qduPlLsXFvCT2qxkzE4sJ
         WlwDhogYUmUU2yegjXEB9ebqHAAoPMx0SqYCG/VJp4F5BXKBp5T9sfU8nN8ZfJ63Acx3
         U0tkTt/WkhUJU1Qy2OyKPsrUU4WaPKo5IRG0CaSKjIIUFdbolWA5/avjDUZegBMctbZV
         0hJWmOKi68Jo7duKdINj0aTYiqM2PdUuQn6YCS8mtkyXa2vjFLcihNg3l8LTGKItr+Dy
         CchA==
X-Gm-Message-State: AGRZ1gK/4c7HnqJvYIs/LS/qdenN1/KYOoAZbWc0fMkcViHmr4LljEqy
	r1QQ2+ls99WQy7SlwnaWX5s2kRfySNMMvrkOZohnozHH2nrI1jE=
X-Received: by 2002:aa7:87ce:: with SMTP id i14mr3773028pfo.20.1542746715681;
        Tue, 20 Nov 2018 12:45:15 -0800 (PST)
X-Forwarded-To: xuliker@zju.edu.cn
X-Forwarded-For: liker.xu@gmail.com xuliker@zju.edu.cn
Delivered-To: liker.xu@gmail.com
Received: by 2002:a17:90a:d106:0:0:0:0 with SMTP id l6-v6csp1099897pju;
        Tue, 20 Nov 2018 12:45:14 -0800 (PST)
X-Google-Smtp-Source: AFSGD/WXx9ac3g11+WMLQiWLTo9kSIGotUPF+DqZPOGTG/Ywj3QfovvdTZ1hGT+ekDnLZhpQQqU2
X-Received: by 2002:a63:1766:: with SMTP id 38mr3219269pgx.299.1542746714494;
        Tue, 20 Nov 2018 12:45:14 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1542746714; cv=none;
        d=google.com; s=arc-20160816;
        b=XdZGRpO7HEWvW60Bq0xzLQcV9bCwjtIXQ5StgqlGQUDuwKp5gmodmrhGk7y4wmEs67
         vcj5bW5pwWYxUvCKqiI6e73HAvV3HkKg9K/VkWVijmQM3SzyRCHObf4Q5G/y4cbfuyWy
         xYu8Jd3Xcj92VZv2T+okfIoPKXddqyvYZaL6UaO2d++67Hj9952PdEp+uGO07pxiaq0K
         3GB59WqbmtkL2m/2JgH15ox/BKwrfvt/rbcExubdbj9z21Q9uUZFE6IRJpGUFgAMdpv3
         +/oLdCX6cm8wxkqk4Usps0CWX8yH8MW2EfHUTWOfQkGqeizBTnuTMKtyeGjfHVOYo1Qd
         ewKw==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=list-id:precedence:sender:user-agent:in-reply-to
         :content-disposition:mime-version:references:message-id:subject:cc
         :to:from:date:dkim-signature;
        bh=Wbfxnty6r01cTHG1D7CDCmF9dra0Fg9suHKL4jcE16A=;
        b=l8GCXfZak/Ou+VEgJXCLYEyd6+xk2sMft4JSaposRkTNUVw5QMAkKaMzkZHvia3oYi
         qbIjr1loB/ra9LucsBdfNAPWTiUv3s1avO9NQROiqriBYpocpvtqX9jzkKodYpi00uzo
         5tWy01fWVa6cP3cJrPXKIG/KgpHENnoRDbJTi1XSdBJsKZnZHNCQeAv1OWcNUM8jD9Xx
         5q2j6ZwgtV56hXIH6v82rPZv3pTCNy8o2F8Rs19RZVahqBWzdhPBkneaLRzuMTs8DwIw
         dJDtrDbWCEKWRDYogqJVMqxDVc8XP7FxT+TD/vPe91YdbqyDo6ItXg86tPRjq1+RoJx6
         bASg==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass header.i=@joelfernandes.org header.s=google header.b=wvyEHrLn;
       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org
Received: from vger.kernel.org (vger.kernel.org. [209.132.180.67])
        by mx.google.com with ESMTP id v6si37416953pgv.277.2018.11.20.12.45.00;
        Tue, 20 Nov 2018 12:45:14 -0800 (PST)
Received-SPF: pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) client-ip=209.132.180.67;
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1727224AbeKUHNr (ORCPT <rfc822;yv9200@gmail.com> + 99 others);
        Wed, 21 Nov 2018 02:13:47 -0500
Received: from mail-pl1-f195.google.com ([209.85.214.195]:33612 "EHLO
        mail-pl1-f195.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1725917AbeKUHNr (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Wed, 21 Nov 2018 02:13:47 -0500
Received: by mail-pl1-f195.google.com with SMTP id z23so2016622plo.0
        for <linux-kernel@vger.kernel.org>; Tue, 20 Nov 2018 12:42:46 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=joelfernandes.org; s=google;
        h=date:from:to:cc:subject:message-id:references:mime-version
         :content-disposition:in-reply-to:user-agent;
        bh=Wbfxnty6r01cTHG1D7CDCmF9dra0Fg9suHKL4jcE16A=;
        b=wvyEHrLnz/ToFDudjOH8g3NMxf+PkEaUxBIOnUpVOP6RMRutxTaHGOFx7/Mv8atHN/
         4z5gSnm4vrRjd9m0hvG/Ea4rflHDCUTFUpvisUE6IYPuhGhxHlqJi9X9moxTfR7KFU7u
         Ry2p/Xn46CczBF49Y774Ov3B5jwyUDfUsCqqE=
X-Received: by 2002:a63:77ce:: with SMTP id s197mr3255254pgc.89.1542746565507;
        Tue, 20 Nov 2018 12:42:45 -0800 (PST)
Received: from localhost ([2620:0:1000:1601:3aef:314f:b9ea:889f])
        by smtp.gmail.com with ESMTPSA id h17-v6sm62766019pfj.125.2018.11.20.12.42.44
        (version=TLS1_2 cipher=ECDHE-RSA-CHACHA20-POLY1305 bits=256/256);
        Tue, 20 Nov 2018 12:42:44 -0800 (PST)
Date: Tue, 20 Nov 2018 12:42:43 -0800
From: Joel Fernandes <joel@joelfernandes.org>
To: "Paul E. McKenney" <paulmck@linux.ibm.com>
Cc: linux-kernel@vger.kernel.org, josh@joshtriplett.org,
        rostedt@goodmis.org, mathieu.desnoyers@efficios.com,
        jiangshanlai@gmail.com
Subject: Re: dyntick-idle CPU and node's qsmask
Message-ID: <20181120204243.GA22801@google.com>
References: <20181110214659.GA96924@google.com>
 <20181110230436.GL4170@linux.ibm.com>
 <20181111030925.GA182908@google.com>
 <20181111042210.GN4170@linux.ibm.com>
 <20181111180916.GA25327@google.com>
 <20181111183618.GY4170@linux.ibm.com>
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: <20181111183618.GY4170@linux.ibm.com>
User-Agent: Mutt/1.10.1 (2018-07-13)
Sender: liker.xu+caf_=xuliker=zju.edu.cn@gmail.com
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-CM-TRANSID: AQAAfwAX+kVbcvRbstpfAA--.4056S3
Authentication-Results: mail-app3; spf=pass smtp.mail=liker.xu+caf_=xu
	liker=zju.edu.cn@gmail.com;
X-Coremail-Antispam: 1UD129KBjvJXoW3AFyDtr1fuF43JrW5Ww18AFb_yoW3GrWfpF
	WDK3W5Krs5Jr1Ikwsavw4UXFyFyr13XFZxXFy5Kryqy3s8KF1Sgr17tF45uFy3Wr4Iy342
	vr45tFy7u3WqyaDanT9S1TB71UUUUUUqnTZGkaVYY2UrUUUU0bIjqfuFe4nvWSU8nxnvy2
	9KBjDU0xBIdaVrnRJUUUPIb7Iv0xC_Kw4lb4IE77IF4wAFF20E14v26r1j6r4UM7CIcVAF
	z4kK6r1j6r18M28lY4IEw2IIxxk0rwA2F7IY1VAKz4vEj48ve4kI8wA2z4x0Y4vE2Ix0cI
	8IcVAFwI0_Ar0_tr1l84ACjcxK6xIIjxv20xvEc7CjxVAFwI0_Cr0_Gr1UM28EF7xvwVC2
	z280aVAFwI0_Cr1j6rxdM28EF7xvwVC2z280aVCY1x0267AKxVWxJr0_GcWle2I262IYc4
	CY6c8Ij28IcVAaY2xG8wAqx4xG64xvF2IEw4CE5I8CrVC2j2WlYx0E2Ix0cI8IcVAFwI0_
	Jrv_JF1lYx0Ex4A2jsIE14v26r4j6F4UMcvjeVCFs4IE7xkEbVWUJVW8JwACjcxG0xvEwI
	xGrwCjxxvEa2IrMxkF7I0Ew4C26cxK6c8Ij28IcwCY1Ik26cxK6xAEc7vF6xCjj44lc2xS
	Y4AK6IIF6F1lc2IjII80xcxEwVAKI48JMxvI42IY6xIIjxv20xvE14v26ryj6F1UMxvI42
	IY6xIIjxv20xvEc7CjxVAFwI0_Gr0_Cr1lcIIF0xvEx4A2jsIE14v26r4UJVWxJr1lcIIF
	0xvEx4A2jsIEc7CjxVAFwI0_Gr1j6F4UJwCF04k20xvY0x0EwIxGrwCF04k20xvEw4C26c
	xK6c8Ij28IcwCF72vE77IF4wCFx2IqxVCFs4IE7xkEbVWUJVW8JwC20s026c02F40E14v2
	6r1j6r18MI8I3I0E7480Y4vE14v26r106r1rMI8E67AF67kF1VAFwI0_JF0_Jw1lIxkGc2
	Ij64vIr41lIxAIcVCF04k26cxKx2IYs7xG6r4j6FyUYxBIdaVFxhVjvjDU0xZFpf9x07bL
	BMNUUUUU=

On Sun, Nov 11, 2018 at 10:36:18AM -0800, Paul E. McKenney wrote:
> On Sun, Nov 11, 2018 at 10:09:16AM -0800, Joel Fernandes wrote:
> > On Sat, Nov 10, 2018 at 08:22:10PM -0800, Paul E. McKenney wrote:
> > > On Sat, Nov 10, 2018 at 07:09:25PM -0800, Joel Fernandes wrote:
> > > > On Sat, Nov 10, 2018 at 03:04:36PM -0800, Paul E. McKenney wrote:
> > > > > On Sat, Nov 10, 2018 at 01:46:59PM -0800, Joel Fernandes wrote:
> > > > > > Hi Paul and everyone,
> > > > > > 
> > > > > > I was tracing/studying the RCU code today in paul/dev branch and noticed that
> > > > > > for dyntick-idle CPUs, the RCU GP thread is clearing the rnp->qsmask
> > > > > > corresponding to the leaf node for the idle CPU, and reporting a QS on their
> > > > > > behalf.
> > > > > > 
> > > > > > rcu_sched-10    [003]    40.008039: rcu_fqs:              rcu_sched 792 0 dti
> > > > > > rcu_sched-10    [003]    40.008039: rcu_fqs:              rcu_sched 801 2 dti
> > > > > > rcu_sched-10    [003]    40.008041: rcu_quiescent_state_report: rcu_sched 805 5>0 0 0 3 0
> > > > > > 
> > > > > > That's all good but I was wondering if we can do better for the idle CPUs if
> > > > > > we can some how not set the qsmask of the node in the first place. Then no
> > > > > > reporting would be needed of quiescent state is needed for idle CPUs right?
> > > > > > And we would also not need to acquire the rnp lock I think.
> > > > > > 
> > > > > > At least for a single node tree RCU system, it seems that would avoid needing
> > > > > > to acquire the lock without complications. Anyway let me know your thoughts
> > > > > > and happy to discuss this at the hallways of the LPC as well for folks
> > > > > > attending :)
> > > > > 
> > > > > We could, but that would require consulting the rcu_data structure for
> > > > > each CPU while initializing the grace period, thus increasing the number
> > > > > of cache misses during grace-period initialization and also shortly after
> > > > > for any non-idle CPUs.  This seems backwards on busy systems where each
> > > > 
> > > > When I traced, it appears to me that rcu_data structure of a remote CPU was
> > > > being consulted anyway by the rcu_sched thread. So it seems like such cache
> > > > miss would happen anyway whether it is during grace-period initialization or
> > > > during the fqs stage? I guess I'm trying to say, the consultation of remote
> > > > CPU's rcu_data happens anyway.
> > > 
> > > Hmmm...
> > > 
> > > The rcu_gp_init() function does access an rcu_data structure, but it is
> > > that of the current CPU, so shouldn't involve a communications cache miss,
> > > at least not in the common case.
> > > 
> > > Or are you seeing these cross-CPU rcu_data accesses in rcu_gp_fqs() or
> > > functions that it calls?  In that case, please see below.
> > 
> > Yes, it was rcu_implicit_dynticks_qs called from rcu_gp_fqs.
> > 
> > > > > CPU will with high probability report its own quiescent state before three
> > > > > jiffies pass, in which case the cache misses on the rcu_data structures
> > > > > would be wasted motion.
> > > > 
> > > > If all the CPUs are busy and reporting their QS themselves, then I think the
> > > > qsmask is likely 0 so then rcu_implicit_dynticks_qs (called from
> > > > force_qs_rnp) wouldn't be called and so there would no cache misses on
> > > > rcu_data right?
> > > 
> > > Yes, but assuming that all CPUs report their quiescent states before
> > > the first call to rcu_gp_fqs().  One exception is when some CPU is
> > > looping in the kernel for many milliseconds without passing through a
> > > quiescent state.  This is because for recent kernels, cond_resched()
> > > is not a quiescent state until the grace period is something like 100
> > > milliseconds old.  (For older kernels, cond_resched() was never an RCU
> > > quiescent state unless it actually scheduled.)
> > > 
> > > Why wait 100 milliseconds?  Because otherwise the increase in
> > > cond_resched() overhead shows up all too well, causing 0day test robot
> > > to complain bitterly.  Besides, I would expect that in the common case,
> > > CPUs would be executing usermode code.
> > 
> > Makes sense. I was also wondering about this other thing you mentioned about
> > waiting for 3 jiffies before reporting the idle CPU's quiescent state. Does
> > that mean that even if a single CPU is dyntick-idle for a long period of
> > time, then the minimum grace period duration would be atleast 3 jiffies? In
> > our mobile embedded devices, jiffies is set to 3.33ms (HZ=300) to keep power
> > consumption low. Not that I'm saying its an issue or anything (since IIUC if
> > someone wants shorter grace periods, they should just use expedited GPs), but
> > it sounds like it would be shorter GP if we just set the qsmask early on some
> > how and we can manage the overhead of doing so.
> 
> First, there is some autotuning of the delay based on HZ:
> 
> #define RCU_JIFFIES_TILL_FORCE_QS (1 + (HZ > 250) + (HZ > 500))
> 
> So at HZ=300, you should be seeing a two-jiffy delay rather than the
> usual HZ=1000 three-jiffy delay.  Of course, this means that the delay
> is 6.67ms rather than the usual 3ms, but the theory is that lower HZ
> rates often mean slower instruction execution and thus a desire for
> lower RCU overhead.  There is further autotuning based on number of
> CPUs, but this does not kick in until you have 256 CPUs on your system,
> and I bet that smartphones aren't there yet.  Nevertheless, check out
> RCU_JIFFIES_FQS_DIV for more info on this.
> 
> But you can always override this autotuning using the following kernel
> boot paramters:
> 
> rcutree.jiffies_till_first_fqs
> rcutree.jiffies_till_next_fqs

Slightly related, I was just going through your patch in the dev branch "doc:
Now jiffies_till_sched_qs solicits from cond_resched()".

If I understand correctly, what you're trying to do is set
rcu_data.rcu_urgent_qs if you've not heard from the CPU long enough from
rcu_implicit_dynticks_qs.

Then in the other paths, you are reading this value and similuating a dyntick
idle transition even though you may not be really going into dyntick-idle.
Actually in the scheduler-tick, you are also using it to set NEED_RESCHED
appropriately.

Did I get it right so far?

I was thinking if we could simplify rcu_note_context_switch (the parts that
call rcu_momentary_dyntick_idle), if we did the following in
rcu_implicit_dynticks_qs.

Since we already call rcu_qs in rcu_note_context_switch, that would clear the
rdp->cpu_no_qs flag. Then there should be no need to call
rcu_momentary_dyntick_idle from rcu_note_context switch.

I think this would simplify cond_resched as well.  Could this avoid the need
for having an rcu_all_qs at all? Hopefully I didn't some Tasks-RCU corner cases..

Basically for some background, I was thinking can we simplify the code that
calls "rcu_momentary_dyntick_idle" since we already register a qs in other
ways (like by resetting cpu_no_qs).

I should probably start drawing some pictures to make sense of everything,
but do let me know if I have a point ;-) Thanks for your time.

- Joel

diff --git a/kernel/rcu/tree.c b/kernel/rcu/tree.c
index c818e0c91a81..5aa0259c014d 100644
--- a/kernel/rcu/tree.c
+++ b/kernel/rcu/tree.c
@@ -1063,7 +1063,7 @@ static int rcu_implicit_dynticks_qs(struct rcu_data *rdp)
 	 * read-side critical section that started before the beginning
 	 * of the current RCU grace period.
 	 */
-	if (rcu_dynticks_in_eqs_since(rdp, rdp->dynticks_snap)) {
+	if (rcu_dynticks_in_eqs_since(rdp, rdp->dynticks_snap) || !rdp->cpu_no_qs.b.norm) {
 		trace_rcu_fqs(rcu_state.name, rdp->gp_seq, rdp->cpu, TPS("dti"));
 		rcu_gpnum_ovf(rnp, rdp);
 		return 1;
