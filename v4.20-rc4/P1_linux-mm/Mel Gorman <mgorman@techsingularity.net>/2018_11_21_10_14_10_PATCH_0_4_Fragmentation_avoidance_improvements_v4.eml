Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from pop3.zju.edu.cn (61.164.42.155:110) by
  likexu-MOBL1.ccr.corp.intel.com with POP3; 21 Nov 2018 11:01:12 -0000
Received: from icoremail.net (unknown [209.85.214.180])
	by mail-app2 (Coremail) with SMTP id by_KCgDXv35jMPVbHAnHAQ--.58916S3;
	Wed, 21 Nov 2018 18:16:03 +0800 (CST)
Received: from mail-pl1-f180.google.com (unknown [209.85.214.180])
	by mx2.icoremail.net (Coremail) with SMTP id AQAAfwDHpkJhMPVbgNFiAA--.10926S3;
	Wed, 21 Nov 2018 18:16:02 +0800 (CST)
Received: by mail-pl1-f180.google.com with SMTP id b5-v6so4865008pla.6
        for <xuliker@zju.edu.cn>; Wed, 21 Nov 2018 02:16:02 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-original-authentication-results:x-gm-message-state:delivered-to
         :from:to:cc:subject:date:message-id:sender:precedence:list-id;
        bh=Y2FZjECEj5TfOfSIsupvejN8XNh6scPMKEwOMqnBax8=;
        b=B5S4er1tfubYuDgiZotMC+e2otcip0HmT9UiZcTpnNaN8m6KPgRlUk98lOpnzI/SHZ
         ObZoQuTQ1K8ke9kTku+iPo09sImTm+JwpJnNQECUOR5GCSzIWxoQJ/M5uQJeUzOFK/Rh
         64Reh/OGwYKUz8PFYUGDh2TPGns2Fi9eMXN39yjB0i5wD1MPcklPIvQrt+leHKOEZIPi
         Fp4Og/UMBIDoocLOr/t8I8W0902ALpCQivawh9YAOReOSnBX+GyuqBwTFY7pcbwOHacE
         oS7dBuMfE8V4klf+FrgXL4FBYDzUOjWgzLJW7fBwz/sqsXhcXAR+N/wovNsWALXl1roo
         2u7g==
X-Original-Authentication-Results: mx.google.com;       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org
X-Gm-Message-State: AGRZ1gJ1bau86ktlr5XrNB1PSyDjtL3zwUEl2OPYKs+K/o3Yz31f/tOE
	1VHDAdVQ50/RnbCZ8yzWz36we9ZvTC2oWHdBLsIM5Qc32t+A7Lo=
X-Received: by 2002:a62:3687:: with SMTP id d129-v6mr6199915pfa.56.1542795361749;
        Wed, 21 Nov 2018 02:16:01 -0800 (PST)
X-Forwarded-To: xuliker@zju.edu.cn
X-Forwarded-For: liker.xu@gmail.com xuliker@zju.edu.cn
Delivered-To: liker.xu@gmail.com
Received: by 2002:a17:90a:d106:0:0:0:0 with SMTP id l6-v6csp1751241pju;
        Wed, 21 Nov 2018 02:16:00 -0800 (PST)
X-Google-Smtp-Source: AFSGD/Vs6Zn7GKTO1ix+sQk2qfzKgBup/+rDYo/DzP1EPyrht0CvcHzgpAirGoyUVRdgtkDx2TEa
X-Received: by 2002:a17:902:bd92:: with SMTP id q18mr6177448pls.167.1542795360909;
        Wed, 21 Nov 2018 02:16:00 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1542795360; cv=none;
        d=google.com; s=arc-20160816;
        b=uIQ53QPNSgWHwsMtRS9H/9ImDof+QvD75YSRenpR/CtKbKUAbZLdcTM9iGAtP68agF
         dJGIFvAX91vkuek10K8RCRkNjV+HONh/LQY+6swKCmZ3jK/fvPqANxPJ/4otTG2Ga8F8
         Ctm/2pGZLQlr+72pl38qhp32f88JP/ux6UJl/94l1DGWxMVGu4qoIFkiyNz1dyDZOND+
         mdkAhV1c1FOMu9j1tfNPhIUZQKUj6sjgqXwOx78IIfXRn/rzZXNLe5AmwDnn71+E4r+m
         H8iKoSAvzixy0HrHalqQF7tAcLt46PZzOaVFnvPmCJoDhNdKkhCVnHpyrkqxUIWRSHbF
         3qew==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=list-id:precedence:sender:message-id:date:subject:cc:to:from;
        bh=Y2FZjECEj5TfOfSIsupvejN8XNh6scPMKEwOMqnBax8=;
        b=MoxqTouporN09hYZZ7SbhSy/4tYHssajYPu3k52umJBZE/PMWFCs+LhnqENyfSxFW9
         hShG7JOA4YLSpAbG9BiFiAWLODzE3zokgGlS/uweQ7ZiabPFw1reIvwc6TPQ+OjvUIMW
         cdYMMjjfOAyydyXTcdUGk1ySJmzPUwSaWYm+AMfwbTYQBYcZ5Emtq9fmQQkT+91Xj4w1
         OyZJ6jglKm1rUz5WGOE6NUXf+IDDKRJGiZx/gayL7HfwuhvtXYt2LJU2Pa/SNUxoHTK9
         DdiwtItKZrfbFoFjAWO15VxdRvsJL6gjHxT/+zQL3zsLbobp82F9uGpz+qVlA2WGdCTU
         1MBg==
ARC-Authentication-Results: i=1; mx.google.com;
       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org
Received: from vger.kernel.org (vger.kernel.org. [209.132.180.67])
        by mx.google.com with ESMTP id u9si19480662plk.61.2018.11.21.02.15.46;
        Wed, 21 Nov 2018 02:16:00 -0800 (PST)
Received-SPF: pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) client-ip=209.132.180.67;
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1729937AbeKUUsq (ORCPT <rfc822;nullbytes00@gmail.com>
        + 99 others); Wed, 21 Nov 2018 15:48:46 -0500
Received: from outbound-smtp26.blacknight.com ([81.17.249.194]:36514 "EHLO
        outbound-smtp26.blacknight.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S1729661AbeKUUsG (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Wed, 21 Nov 2018 15:48:06 -0500
Received: from mail.blacknight.com (pemlinmail01.blacknight.ie [81.17.254.10])
        by outbound-smtp26.blacknight.com (Postfix) with ESMTPS id F178EB8709
        for <linux-kernel@vger.kernel.org>; Wed, 21 Nov 2018 10:14:14 +0000 (GMT)
Received: (qmail 19847 invoked from network); 21 Nov 2018 10:14:14 -0000
Received: from unknown (HELO stampy.163woodhaven.lan) (mgorman@techsingularity.net@[37.228.229.69])
  by 81.17.254.9 with ESMTPA; 21 Nov 2018 10:14:14 -0000
From: Mel Gorman <mgorman@techsingularity.net>
To: Linux-MM <linux-mm@kvack.org>
Cc: Andrew Morton <akpm@linux-foundation.org>,
        Vlastimil Babka <vbabka@suse.cz>,
        David Rientjes <rientjes@google.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        Zi Yan <zi.yan@cs.rutgers.edu>,
        Michal Hocko <mhocko@kernel.org>,
        LKML <linux-kernel@vger.kernel.org>,
        Mel Gorman <mgorman@techsingularity.net>
Subject: [PATCH 0/4] Fragmentation avoidance improvements v4
Date: Wed, 21 Nov 2018 10:14:10 +0000
Message-Id: <20181121101414.21301-1-mgorman@techsingularity.net>
X-Mailer: git-send-email 2.16.4
Sender: liker.xu+caf_=xuliker=zju.edu.cn@gmail.com
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-CM-TRANSID: AQAAfwDHpkJhMPVbgNFiAA--.10926S3
Authentication-Results: mail-app2; spf=pass smtp.mail=liker.xu+caf_=xu
	liker=zju.edu.cn@gmail.com;
X-Coremail-Antispam: 1UD129KBjvJXoW3Xw1ktr13Zw1fAr13AFy7Jrb_yoW7Xw18pF
	WrJr15tFn5J3WxuwnxZw4xW34fCa18GFWUXrnIgry0y3ZxurnYyrWftr1jvFyUArZ7A3Wj
	q3y5t3W3CrsrZ37anT9S1TB71UUUUUUqnTZGkaVYY2UrUUUU0bIjqfuFe4nvWSU8nxnvy2
	9KBjDU0xBIdaVrnRJUUUm0b7Iv0xC_Kw4lb4IE77IF4wAFF20E14v26r1j6r4UM7CIcVAF
	z4kK6r1j6r18M28lY4IEw2IIxxk0rwA2F7IY1VAKz4vEj48ve4kI8wA2z4x0Y4vE2Ix0cI
	8IcVAFwI0_Ar0_tr1l84ACjcxK6xIIjxv20xvEc7CjxVAFwI0_Cr0_Gr1UM28EF7xvwVC2
	z280aVAFwI0_GcCE3s1l84ACjcxK6I8E87Iv6xkF7I0E14v26rxl6s0DM2AIxVAIcxkEcV
	Aq07x20xvEncxIr21l5I8CrVACY4xI64kE6c02F40Ex7xfMcIj6xIIjxv20xvE14v26r1Y
	6r17McIj6I8E87Iv67AKxVWxJVW8Jr1lOx8S6xCaFVCjc4AY6r1j6r4UM4x0Y48IcxkI7V
	AKI48JMx02cVCv0xWlc7CjxVAKzI0EY4vE52x082I5MxkFs20EbsIEYx1lc7Ca8VAvwVCF
	zxkY4VCF77xAMxkIecxEwVCI4VW8twCY0x0Ix7I2Y4AK64vIr41lcIIF0xvE2Ix0cI8IcV
	AFwI0_Ar0_tr1lcIIF0xvE2Ix0cI8IcVCY1x0267AKxVW8JVWxJwCYIxAIcVC2z280aVAF
	wI0_Cr1j6rxdMxvI42IY6I8E87Iv6xkF7I0E14v26F4UJVW0owCF04k20xvY0x0EwIxGrw
	CF04k20xvEw4C26cxK6c8Ij28IcwCF72vE77IF4wCFx2IqxVCFs4IE7xkEbVWUJVW8JwC2
	0s026c02F40E14v26r1j6r18MI8I3I0E7480Y4vE14v26r106r1rMI8E67AF67kF1VAFwI
	0_Jw0_GFylIxkGc2Ij64vIr41lIxAIcVCF04k26cxKx2IYs7xG6r4j6FyUYxBIdaVFxhVj
	vjDU0xZFpf9x07bljj9UUUUU=

No major change from v3 really, mostly resending to see if there is any
review reaction. It's rebased but a partial test indicated that the
behaviour is similar to the previous baseline

Changelog since v3
o Rebase to 4.20-rc3
o Remove a stupid warning from the last patch

Changelog since v2
o Drop patch 5 as it was borderline
o Decrease timeout when stalling on fragmentation events

Changelog since v1
o Rebase to v4.20-rc1 for the THP __GFP_THISNODE patch in particular
o Add tracepoint to record fragmentation stall durations
o Add vmstat event to record that a fragmentation stall occurred
o Stalls now alter watermark boosting
o Stalls occur only when the allocation is about to fail

It has been noted before that fragmentation avoidance (aka
anti-fragmentation) is not perfect. Given sufficient time or an adverse
workload, memory gets fragmented and the long-term success of high-order
allocations degrades. This series defines an adverse workload, a definition
of external fragmentation events (including serious) ones and a series
that reduces the level of those fragmentation events.

The details of the workload and the consequences are described in more
detail in the changelogs. However, from patch 1, this is a high-level
summary of the adverse workload. The exact details are found in the
mmtests implementation.

The broad details of the workload are as follows;

1. Create an XFS filesystem (not specified in the configuration but done
   as part of the testing for this patch)
2. Start 4 fio threads that write a number of 64K files inefficiently.
   Inefficiently means that files are created on first access and not
   created in advance (fio parameterr create_on_open=1) and fallocate
   is not used (fallocate=none). With multiple IO issuers this creates
   a mix of slab and page cache allocations over time. The total size
   of the files is 150% physical memory so that the slabs and page cache
   pages get mixed
3. Warm up a number of fio read-only threads accessing the same files
   created in step 2. This part runs for the same length of time it
   took to create the files. It'll fault back in old data and further
   interleave slab and page cache allocations. As it's now low on
   memory due to step 2, fragmentation occurs as pageblocks get
   stolen.
4. While step 3 is still running, start a process that tries to allocate
   75% of memory as huge pages with a number of threads. The number of
   threads is based on a (NR_CPUS_SOCKET - NR_FIO_THREADS)/4 to avoid THP
   threads contending with fio, any other threads or forcing cross-NUMA
   scheduling. Note that the test has not been used on a machine with less
   than 8 cores. The benchmark records whether huge pages were allocated
   and what the fault latency was in microseconds
5. Measure the number of events potentially causing external fragmentation,
   the fault latency and the huge page allocation success rate.
6. Cleanup

Overall the series reduces external fragmentation causing events by over 95%
on 1 and 2 socket machines, which in turn impacts high-order allocation
success rates over the long term. There are differences in latencies and
high-order allocation success rates. Latencies are a mixed bag as they
are vulnerable to exact system state and whether allocations succeeded so
they are treated as a secondary metric.

Patch 1 uses lower zones if they are populated and have free memory
	instead of fragmenting a higher zone. It's special cased to
	handle a Normal->DMA32 fallback with the reasons explained
	in the changelog.

Patch 2+3 boosts watermarks temporarily when an external fragmentation
	event occurs. kswapd wakes to reclaim a small amount of old memory
	and then wakes kcompactd on completion to recover the system
	slightly. This introduces some overhead in the slowpath. The level
	of boosting can be tuned or disabled depending on the tolerance
	for fragmentation vs allocation latency.

Patch 4 is more heavy handed. In the event of a movable allocation
	request that can stall, it'll wake kswapd as in patch 3.  However,
	if the expected fragmentation event is serious then the request
	will stall briefly on pfmemalloc_wait until kswapd completes
	light reclaim work and retry the allocation without stalling.
	This can avoid the fragmentation event entirely in some cases.
	The definition of a serious fragmentation event can be tuned
	or disabled.

The bulk of the improvement in fragmentation avoidance is from patches
1-3 (94-97% reduction in fragmentation events for an adverse workload on
both a 1-socket and 2-socket machine). The primary benefit of patch 4 is
the increase in THP success rates and the fact it reduces fragmentation
events to almost negligible levels with the option of eliminating them.

 Documentation/sysctl/vm.txt   |  42 ++++++++
 include/linux/mm.h            |   2 +
 include/linux/mmzone.h        |  14 ++-
 include/linux/vm_event_item.h |   1 +
 include/trace/events/kmem.h   |  21 ++++
 kernel/sysctl.c               |  18 ++++
 mm/compaction.c               |   2 +-
 mm/internal.h                 |  14 ++-
 mm/page_alloc.c               | 238 ++++++++++++++++++++++++++++++++++++++----
 mm/vmscan.c                   | 123 ++++++++++++++++++++--
 mm/vmstat.c                   |   1 +
 11 files changed, 436 insertions(+), 40 deletions(-)

-- 
2.16.4
