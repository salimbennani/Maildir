Return-Path: <qemu-devel-bounces+like.xu=linux.intel.com@nongnu.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by i7-8700 with POP3-SSL;
  12 Dec 2018 19:45:13 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga008.jf.intel.com (orsmga008.jf.intel.com [10.7.209.65])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id CCBD35807A4
	for <like.xu@linux.intel.com>; Wed, 12 Dec 2018 03:17:11 -0800 (PST)
Received: from orsmga103.jf.intel.com ([10.7.208.35])
  by orsmga008-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 12 Dec 2018 03:17:11 -0800
IronPort-PHdr: =?us-ascii?q?9a23=3ARBrgeh2UTWfXyE0hsmDT+DRfVm0co7zxezQtwd8Z?=
 =?us-ascii?q?sesWKPXxwZ3uMQTl6Ol3ixeRBMOHs6IC07KempujcFRI2YyGvnEGfc4EfD4+ou?=
 =?us-ascii?q?JSoTYdBtWYA1bwNv/gYn9yNs1DUFh44yPzahANS47xaFLIv3K98yMZFAnhOgpp?=
 =?us-ascii?q?POT1HZPZg9iq2+yo9JDffwZFiCChbb9uMR67sRjfus4KjIV4N60/0AHJonxGe+?=
 =?us-ascii?q?RXwWNnO1eelAvi68mz4ZBu7T1et+ou+MBcX6r6eb84TaFDAzQ9L281/szrugLd?=
 =?us-ascii?q?QgaJ+3ART38ZkhtMAwjC8RH6QpL8uTb0u+ZhxCWXO9D9QKsqUjq+8ahkVB7oiD?=
 =?us-ascii?q?8GNzEn9mHXltdwh79frB64uhBz35LYbISTOfFjfK3SYMkaSHJBUMhPWSJPAYSy?=
 =?us-ascii?q?YIkBD+UOIelWspH9qlkMoxaxGAWhCv/jxSFThnLtwa02z/4sHR3c0QA8A94Dtm?=
 =?us-ascii?q?nfotXvNKcVVOC41LPGwi/eb/NSxDzz5pXIcgo7of6WW7JwbdfaxE43Fwzfk1WQ?=
 =?us-ascii?q?rZbpMC6I1uQXqWiU9exgVf60hmE7qgFxviKvxsYji4XTmo0VzVXE+Dx/zY0oJt?=
 =?us-ascii?q?O4UFZ2bcC4HJZTrS2WKpZ6T8A4T212tis3yqcKtYO5cSQS1ZgqxgDTZ+aZf4SW?=
 =?us-ascii?q?/x7vTvudLDd5iX5/eL+yhxC/+lW6xOLmTMm7ylNKozJFktbSsnAN0ATe6syGSv?=
 =?us-ascii?q?tm4kehwiyD1w/V6uFZO0w0krDbK5E5zr4xkJocr1jDEzfolEnqj6KabFgo9vWr?=
 =?us-ascii?q?5uj9fLnrqJ+RO5Vphgz8Kqgun9awAeU8MggARWib/uG82aX6/ULnRbVKk+Q6nb?=
 =?us-ascii?q?THv5DEO8sbore1DBRS0oY+7RawEymp0M8fkXkDLVJFewyIg5LmOlHTOP34Cfa/?=
 =?us-ascii?q?g1KxkDZk3fzGP7vhAonTIXjHirvuYbF960tHxQo1ytBf4Z1UCrccIP7pXU/xrt?=
 =?us-ascii?q?PYAgcjMwOo2+bnFMl91oQGVGKLA6+ZM73dvUWH5+IyOOSMYI4VuDDgK/kq/fLu?=
 =?us-ascii?q?jHk5mUMDcqmtx5cYdHe4HvE1a3ifemfm19cdDX8R7E15SO3xlEbEVzlVaHCvGa?=
 =?us-ascii?q?Um6XY+AYOiCI7FAYe1nL2G2jz8B5BTeyVKB06BFSTVcZ6ZUaIJYSOWPsgzizEB?=
 =?us-ascii?q?SP2tRpEs0VS0uRbnxqF7BuzT/CIeqNTkztci/PDZlxw56WlpCd+A2XqGVWB+kz?=
 =?us-ascii?q?A0QGpixa16vAlxx0mO1YB+hPpXE8EV4OlGBENyYYfRyvE/AdT1UQHpeNCPR1C7?=
 =?us-ascii?q?BNK8Dmd1BoYhztlLb0tjFtGKihHYwzHsE7ITj6aMBpE/7uTbxXendOhnzHOT7K?=
 =?us-ascii?q?A8ixEKX9FVPGu6zvpn/hXXQYLUiV+QkbqCfqUAxi/dsmyEyDzd7wljTAdsXPCd?=
 =?us-ascii?q?DjgkbUzMoIG8vxuaQg=3D=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AUAAB97RBcmBHrdtBjHAEBAQQBAQcEA?=
 =?us-ascii?q?QGBUQcBAQsBgTCCYowUX4szgg18lleBcxQYFIc7IjQJDQEDAQEBAQEBAgETAQE?=
 =?us-ascii?q?BAQEICwsGGw4vgjYFAgMaAQaCXAMDAQIkHwopAwMBAgYBAR8pCAMBUwYBEgWDH?=
 =?us-ascii?q?IICAQSmXDOFQIRwh32EP4FXP4ERgl2LEQKJPYFxlVkHAoIiBI8vAhaRRId8gSq?=
 =?us-ascii?q?QE4FGgg4zGiNQgmyCJxeOHUExgQeLMIF3AQE?=
X-IPAS-Result: =?us-ascii?q?A0AUAAB97RBcmBHrdtBjHAEBAQQBAQcEAQGBUQcBAQsBgTC?=
 =?us-ascii?q?CYowUX4szgg18lleBcxQYFIc7IjQJDQEDAQEBAQEBAgETAQEBAQEICwsGGw4vg?=
 =?us-ascii?q?jYFAgMaAQaCXAMDAQIkHwopAwMBAgYBAR8pCAMBUwYBEgWDHIICAQSmXDOFQIR?=
 =?us-ascii?q?wh32EP4FXP4ERgl2LEQKJPYFxlVkHAoIiBI8vAhaRRId8gSqQE4FGgg4zGiNQg?=
 =?us-ascii?q?myCJxeOHUExgQeLMIF3AQE?=
X-IronPort-AV: E=Sophos;i="5.56,344,1539673200"; 
   d="scan'208";a="56020130"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from lists.gnu.org ([208.118.235.17])
  by mtab.intel.com with ESMTP/TLS/AES256-SHA; 12 Dec 2018 03:17:11 -0800
Received: from localhost ([::1]:43883 helo=lists.gnu.org)
	by lists.gnu.org with esmtp (Exim 4.71)
	(envelope-from <qemu-devel-bounces+like.xu=linux.intel.com@nongnu.org>)
	id 1gX2Vq-00072P-Bq
	for like.xu@linux.intel.com; Wed, 12 Dec 2018 06:17:10 -0500
Received: from eggs.gnu.org ([2001:4830:134:3::10]:34965)
	by lists.gnu.org with esmtp (Exim 4.71)
	(envelope-from <prvs=877c75a5a=Paul.Durrant@citrix.com>)
	id 1gX2VJ-0006p8-Aq
	for qemu-devel@nongnu.org; Wed, 12 Dec 2018 06:16:38 -0500
Received: from Debian-exim by eggs.gnu.org with spam-scanned (Exim 4.71)
	(envelope-from <prvs=877c75a5a=Paul.Durrant@citrix.com>)
	id 1gX2VI-0003BZ-6R
	for qemu-devel@nongnu.org; Wed, 12 Dec 2018 06:16:37 -0500
Received: from smtp03.citrix.com ([162.221.156.55]:43645)
	by eggs.gnu.org with esmtps (TLS1.0:DHE_RSA_AES_256_CBC_SHA1:32)
	(Exim 4.71) (envelope-from <prvs=877c75a5a=Paul.Durrant@citrix.com>)
	id 1gX2VD-0002eF-1W; Wed, 12 Dec 2018 06:16:31 -0500
X-IronPort-AV: E=Sophos;i="5.56,344,1539648000"; d="scan'208";a="73098899"
From: Paul Durrant <paul.durrant@citrix.com>
To: <qemu-devel@nongnu.org>, <qemu-block@nongnu.org>,
	<xen-devel@lists.xenproject.org>
Date: Wed, 12 Dec 2018 11:16:25 +0000
Message-ID: <1544613386-22045-3-git-send-email-paul.durrant@citrix.com>
X-Mailer: git-send-email 2.1.4
In-Reply-To: <1544613386-22045-1-git-send-email-paul.durrant@citrix.com>
References: <1544613386-22045-1-git-send-email-paul.durrant@citrix.com>
MIME-Version: 1.0
Content-Type: text/plain
X-detected-operating-system: by eggs.gnu.org: Genre and OS details not
	recognized.
X-Received-From: 162.221.156.55
Subject: [Qemu-devel] [PATCH v3 2/3] xen-block: improve response latency
X-BeenThere: qemu-devel@nongnu.org
X-Mailman-Version: 2.1.21
Precedence: list
List-Id: <qemu-devel.nongnu.org>
List-Unsubscribe: <https://lists.nongnu.org/mailman/options/qemu-devel>,
	<mailto:qemu-devel-request@nongnu.org?subject=unsubscribe>
List-Archive: <http://lists.nongnu.org/archive/html/qemu-devel/>
List-Post: <mailto:qemu-devel@nongnu.org>
List-Help: <mailto:qemu-devel-request@nongnu.org?subject=help>
List-Subscribe: <https://lists.nongnu.org/mailman/listinfo/qemu-devel>,
	<mailto:qemu-devel-request@nongnu.org?subject=subscribe>
Cc: Kevin Wolf <kwolf@redhat.com>, Stefano Stabellini <sstabellini@kernel.org>,
	Tim Smith <tim.smith@citrix.com>, Max Reitz <mreitz@redhat.com>,
	Paul Durrant <paul.durrant@citrix.com>,
	Stefan Hajnoczi <stefanha@redhat.com>,
	Anthony Perard <anthony.perard@citrix.com>
Errors-To: qemu-devel-bounces+like.xu=linux.intel.com@nongnu.org
Sender: "Qemu-devel" <qemu-devel-bounces+like.xu=linux.intel.com@nongnu.org>

From: Tim Smith <tim.smith@citrix.com>

If the I/O ring is full, the guest cannot send any more requests
until some responses are sent. Only sending all available responses
just before checking for new work does not leave much time for the
guest to supply new work, so this will cause stalls if the ring gets
full. Also, not completing reads as soon as possible adds latency
to the guest.

To alleviate that, complete IO requests as soon as they come back.
xen_block_send_response() already returns a value indicating whether
a notify should be sent, which is all the batching we need.

Signed-off-by: Tim Smith <tim.smith@citrix.com>

Re-based and commit comment adjusted.

Signed-off-by: Paul Durrant <paul.durrant@citrix.com>
---
Cc: Stefan Hajnoczi <stefanha@redhat.com>
Cc: Stefano Stabellini <sstabellini@kernel.org>
Cc: Anthony Perard <anthony.perard@citrix.com>
Cc: Kevin Wolf <kwolf@redhat.com>
Cc: Max Reitz <mreitz@redhat.com>
---
 hw/block/dataplane/xen-block.c | 56 ++++++++++++++----------------------------
 1 file changed, 18 insertions(+), 38 deletions(-)

diff --git a/hw/block/dataplane/xen-block.c b/hw/block/dataplane/xen-block.c
index db17ab5..b4ff2e3 100644
--- a/hw/block/dataplane/xen-block.c
+++ b/hw/block/dataplane/xen-block.c
@@ -55,11 +55,9 @@ struct XenBlockDataPlane {
     blkif_back_rings_t rings;
     int more_work;
     QLIST_HEAD(inflight_head, XenBlockRequest) inflight;
-    QLIST_HEAD(finished_head, XenBlockRequest) finished;
     QLIST_HEAD(freelist_head, XenBlockRequest) freelist;
     int requests_total;
     int requests_inflight;
-    int requests_finished;
     unsigned int max_requests;
     BlockBackend *blk;
     QEMUBH *bh;
@@ -116,12 +114,10 @@ static void xen_block_finish_request(XenBlockRequest *request)
     XenBlockDataPlane *dataplane = request->dataplane;
 
     QLIST_REMOVE(request, list);
-    QLIST_INSERT_HEAD(&dataplane->finished, request, list);
     dataplane->requests_inflight--;
-    dataplane->requests_finished++;
 }
 
-static void xen_block_release_request(XenBlockRequest *request, bool finish)
+static void xen_block_release_request(XenBlockRequest *request)
 {
     XenBlockDataPlane *dataplane = request->dataplane;
 
@@ -129,11 +125,7 @@ static void xen_block_release_request(XenBlockRequest *request, bool finish)
     reset_request(request);
     request->dataplane = dataplane;
     QLIST_INSERT_HEAD(&dataplane->freelist, request, list);
-    if (finish) {
-        dataplane->requests_finished--;
-    } else {
-        dataplane->requests_inflight--;
-    }
+    dataplane->requests_inflight--;
 }
 
 /*
@@ -248,6 +240,7 @@ static int xen_block_copy_request(XenBlockRequest *request)
 }
 
 static int xen_block_do_aio(XenBlockRequest *request);
+static int xen_block_send_response(XenBlockRequest *request);
 
 static void xen_block_complete_aio(void *opaque, int ret)
 {
@@ -312,6 +305,18 @@ static void xen_block_complete_aio(void *opaque, int ret)
     default:
         break;
     }
+    if (xen_block_send_response(request)) {
+        Error *local_err = NULL;
+
+        xen_device_notify_event_channel(dataplane->xendev,
+                                        dataplane->event_channel,
+                                        &local_err);
+        if (local_err) {
+            error_report_err(local_err);
+        }
+    }
+    xen_block_release_request(request);
+
     qemu_bh_schedule(dataplane->bh);
 
 done:
@@ -419,7 +424,7 @@ err:
     return -1;
 }
 
-static int xen_block_send_response_one(XenBlockRequest *request)
+static int xen_block_send_response(XenBlockRequest *request)
 {
     XenBlockDataPlane *dataplane = request->dataplane;
     int send_notify = 0;
@@ -474,29 +479,6 @@ static int xen_block_send_response_one(XenBlockRequest *request)
     return send_notify;
 }
 
-/* walk finished list, send outstanding responses, free requests */
-static void xen_block_send_response_all(XenBlockDataPlane *dataplane)
-{
-    XenBlockRequest *request;
-    int send_notify = 0;
-
-    while (!QLIST_EMPTY(&dataplane->finished)) {
-        request = QLIST_FIRST(&dataplane->finished);
-        send_notify += xen_block_send_response_one(request);
-        xen_block_release_request(request, true);
-    }
-    if (send_notify) {
-        Error *local_err = NULL;
-
-        xen_device_notify_event_channel(dataplane->xendev,
-                                        dataplane->event_channel,
-                                        &local_err);
-        if (local_err) {
-            error_report_err(local_err);
-        }
-    }
-}
-
 static int xen_block_get_request(XenBlockDataPlane *dataplane,
                                  XenBlockRequest *request, RING_IDX rc)
 {
@@ -547,7 +529,6 @@ static void xen_block_handle_requests(XenBlockDataPlane *dataplane)
     rp = dataplane->rings.common.sring->req_prod;
     xen_rmb(); /* Ensure we see queued requests up to 'rp'. */
 
-    xen_block_send_response_all(dataplane);
     /*
      * If there was more than IO_PLUG_THRESHOLD requests in flight
      * when we got here, this is an indication that there the bottleneck
@@ -591,7 +572,7 @@ static void xen_block_handle_requests(XenBlockDataPlane *dataplane)
                 break;
             };
 
-            if (xen_block_send_response_one(request)) {
+            if (xen_block_send_response(request)) {
                 Error *local_err = NULL;
 
                 xen_device_notify_event_channel(dataplane->xendev,
@@ -601,7 +582,7 @@ static void xen_block_handle_requests(XenBlockDataPlane *dataplane)
                     error_report_err(local_err);
                 }
             }
-            xen_block_release_request(request, false);
+            xen_block_release_request(request);
             continue;
         }
 
@@ -657,7 +638,6 @@ XenBlockDataPlane *xen_block_dataplane_create(XenDevice *xendev,
     dataplane->file_size = blk_getlength(dataplane->blk);
 
     QLIST_INIT(&dataplane->inflight);
-    QLIST_INIT(&dataplane->finished);
     QLIST_INIT(&dataplane->freelist);
 
     if (iothread) {
-- 
2.1.4


