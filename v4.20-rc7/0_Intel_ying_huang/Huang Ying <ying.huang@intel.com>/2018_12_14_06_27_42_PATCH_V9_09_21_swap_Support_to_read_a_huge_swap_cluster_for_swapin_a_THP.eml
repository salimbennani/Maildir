Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by i7-8700 with POP3-SSL;
  14 Dec 2018 20:24:48 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from fmsmga005.fm.intel.com (fmsmga005.fm.intel.com [10.253.24.32])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 88C0458079D;
	Thu, 13 Dec 2018 22:28:13 -0800 (PST)
Received: from fmsmga103.fm.intel.com ([10.1.193.90])
  by fmsmga005-1.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 13 Dec 2018 22:28:12 -0800
X-SG-BADATTACHMENTNOREPLY: True
IronPort-PHdr: =?us-ascii?q?9a23=3AvefFiRAEPe8G3PwWy37iUyQJP3N1i/DPJgcQr6Af?=
 =?us-ascii?q?oPdwSP7/osqwAkXT6L1XgUPTWs2DsrQY07qQ6/iocFdDyK7JiGoFfp1IWk1Nou?=
 =?us-ascii?q?QttCtkPvS4D1bmJuXhdS0wEZcKflZk+3amLRodQ56mNBXdrXKo8DEdBAj0OxZr?=
 =?us-ascii?q?KeTpAI7SiNm82/yv95HJbAhEmDmwbaluIBmqsA7cqtQYjYx+J6gr1xDHuGFIe+?=
 =?us-ascii?q?NYxWNpIVKcgRPx7dqu8ZBg7ipdpesv+9ZPXqvmcas4S6dYDCk9PGAu+MLrrxjD?=
 =?us-ascii?q?QhCR6XYaT24bjwBHAwnB7BH9Q5fxri73vfdz1SWGIcH7S60/VC+85Kl3VhDnlC?=
 =?us-ascii?q?YHNyY48G7JjMxwkLlbqw+lqxBm3oLYfJ2ZOP94c6jAf90VWHBBU95RWSJfH42y?=
 =?us-ascii?q?YYgBAe0DMuZWtIn9v1kDoACiCQWwHu7j1iVFimPq0aA8zu8vERvG3AslH98Wsn?=
 =?us-ascii?q?rUsc/6NKEdUeuozqbIzDPDb/xL0jr66InIcxYhof6WUbJwbMre008vFwzeg1WR?=
 =?us-ascii?q?r4zlIy2a1uAXv2eH6OpgUPuihmg6oA9yujii3tkghpXNi44P11zJ+zt1zJwoKd?=
 =?us-ascii?q?C7VEJ3e9+pHZlIuy2HOYZ7TdkuT3xmtSs10LEKpJC2cSkQxJg62xLTd/qKeJWS?=
 =?us-ascii?q?7B35TuaeOzJ4iWpleL2hgxay9lCtyvPzVsaqylZGtClFncfWtnALyRPT7tKLSv?=
 =?us-ascii?q?xn/keuwTqP1gbT5f9YIU0si6bXN5oszqQtmpcdr0jPBDL6lUbqgKOMd0gp+PCk?=
 =?us-ascii?q?6+H9bbXnop+cOZV0igb7Mqk2nsy/AOI4MhUBXmSC+uSzyqfj/UvnT7VOl/E2la?=
 =?us-ascii?q?fYsJbEKsQBvaO5HQBV3Zg56xqlDDepzs4YnX8ZI1JBYh6HiJLpO17WLPDiEfi/?=
 =?us-ascii?q?m0iskCtsx/3eOr3hA5bNIWbZnLbuYLZw8EpcyAs1zdBC6JNYELABIPTvWkDvsN?=
 =?us-ascii?q?zUFAM2Mwuxw+z/EtVyypseWX6TAq+eKK7SsUWH5uMzI+aWY48Zojb9K+U/6P7o?=
 =?us-ascii?q?gn80glsdfaiv3ZsKZ3G0BPVmI0OFYXXyhtcNC3sFvg07TObykl2NTSZTZ2quX6?=
 =?us-ascii?q?I7/jw7CoWmApnZSoCuh7yB2iG7HppNa2BCC1CMF2rodoqeV/cNbiKSPtFukjge?=
 =?us-ascii?q?Wbe9TI8h0AmktBXmxLp/MurU5ioYuIr529hu5+3TkhIy+SZuD8uH0WGAVGV0nm?=
 =?us-ascii?q?IORz8r06Fzu019ylGf0admh/xUD8Bc5/RMUg0iL57T0/R6C8zuWgLGZtqJS0yp?=
 =?us-ascii?q?QtO8DTE1T9I+2dkOY0lmFtWmjxDD2TeqArAPm7yKApw07rzT33zrK8lhzHbG0b?=
 =?us-ascii?q?Erj0M6TctXKW2mmql/+hDOCIHTjUWWibymdaQG0y7L72eM02yOsEZcUA5zVKXF?=
 =?us-ascii?q?WWsSZk/XrdT/+0PDQKWiCbUhMgtd18GCLrFGZcHujVVDXP3jIsjRY3qtm2esAh?=
 =?us-ascii?q?aF3q6DY5D0e2oDxindCFILkwYI/XmYMwgzHSOho2PYDDxzGlPjeULs8e9iqHyl?=
 =?us-ascii?q?Sk841R2Fb0pk17Ct4B4ameScS+8P3rIDoCohtzR0HFO639LKC9qBpxBtfLlGbd?=
 =?us-ascii?q?M6+ldH0WPZtwpyPpG7K6Bih1gecxl4vk/01hV3DJlAntYurH8w0AVyLqeY2ktb?=
 =?us-ascii?q?dzyExZDwJqHXKm7q8R+1b67ZxF7f38iW+6sV8/s4tkjssxuvFkoh9HVnzcJY03?=
 =?us-ascii?q?+d5pXMEQoTXoj9Ukcx9xhmub7aZjMx6J/T1X1pKaO0qCPN28o1BOs5zRatZ9de?=
 =?us-ascii?q?ML6eGADuCcEbBsiuJ/Ysm1imdR8EOOFS9KgpP8KpbfeG2airPPp+kzKil2hI/I?=
 =?us-ascii?q?d90keU/SpmVuHIx4oFw+2f3gafTTf8jUuuvdrtmY9ZYjEeBGy/xjb+BI5Qf6F9?=
 =?us-ascii?q?YZwECWOzLMKp3NV+gJjtVmVc9F6iAVMGxcCodQCTb1z7wQ1fy0AXrWa7liu/yj?=
 =?us-ascii?q?x+iyspobaH3CzS3+TicwIKO25KRGV4jVbgO5O0j80cXEWzawgplR2l5Vv1xqRB?=
 =?us-ascii?q?paR/KXXTTllMfyTsM25iVa6wvKKYY8FT8JMorTlXUOOkbFCYULH9uRga0yDkH2?=
 =?us-ascii?q?dE3zA0bTKqupb4nxx8lm2dKm1+rH7YecF22Bfe68bQRf9X3joaWiZ4jSPbCUS7?=
 =?us-ascii?q?P9ms5d+UjYvMsviiV2K9UZ1eaSnqwpmHtCeh521qABu/kuu3mt3mFwg6zCD628?=
 =?us-ascii?q?NrVSXOsBbzfI3r276mPuJge0liHEX85NZiGoFijoswg4kd2XsAiZWU53YHkXrz?=
 =?us-ascii?q?Ps9d2aLxd3cNQT8Lw9jI4AnqwkFjL3SJx57nWXWZ2Mdue966YmYO0CIn889KEL?=
 =?us-ascii?q?uU7KBDnSZtp1q4rAHRbuJnkjYT1/Qu82IajP8TuAU20CqdGL8SEFJcPSztkRSI?=
 =?us-ascii?q?8t+/oL9WZGapbbi/yk5+kcq9A7GFpwFWQGz5dYs6HS9s8sV/N0rB0GH06oHhYt?=
 =?us-ascii?q?XRbMgcthuJkxfblOhVKYk8lv4LhSphJGL8smcpy+89jRxyw566uJKLJHlq/KK8?=
 =?us-ascii?q?GhRYLCH6Z9sP+jHxiqZTht2Z0JqoHpVlBzUHRp/oTe+zHTIWtPTnMRuOETImpn?=
 =?us-ascii?q?eaH7rfARGQ6EN8o33TFJCrMmmdJGMFwtV6WBmdOEtfjRgWXDogm542CBuqyNb9?=
 =?us-ascii?q?cEtj5TAR+134qgZKyu1yLBnySWPfpAauajcpR5mTNhtW7gde50jLNcyS9P58Hy?=
 =?us-ascii?q?Zd/pe5tgyCNnSbZxhUDWEOQkGFB1fjPqS35dnd6eeYAPCyL+DJYbWIpuxTTPOI?=
 =?us-ascii?q?xZOp0ot78DeALMSPPn9+D/Il3kpPR2x2G8Pcmz8XUSwYizrNb9KHpBe74iB3rt?=
 =?us-ascii?q?6w8PXoWALs5IuDEbhSMc9o+xC5n6iDLfOQhD1iJDZc15MMw2LIybcF0F4TjSFu?=
 =?us-ascii?q?ayeiEbAauSHRS6LQn7ddDwQHZCNrKMtI86U80xFXNs7Gkdz1zKB3j/4vB1dBVF?=
 =?us-ascii?q?zsgcWpZc0MI2GgO1LLHkeLNLKaJTLVx8H7e7+zSbpVjO9MrR2/pS6bE1P/PjSE?=
 =?us-ascii?q?jzTpSxGvMeRWgC6HJhBRpIG9fQhrCWX4SNLmaxu7MMJ4jDEswL00gG/KOnAYMT?=
 =?us-ascii?q?Rmb0xNqbiQ5ztCgvpjA2xB8mZlLe6clieZ9ebYLJMWsfptAitsluNa4G41y71a?=
 =?us-ascii?q?7CxfQPx1mS3SrsNhol28k+mPzCZnXwRKqjpRmI2LukBiau3l8cxpX3re8Q1FyG?=
 =?us-ascii?q?KKFxMM749sCcXqk6RRzMXf0az0NTFO+s7V+s1aANLbfpGpKn0kZDr0ETjbRDoE?=
 =?us-ascii?q?SzHjYXDegUMbkOyb8HK9r54mp5yqk50LHOwIHGcpH+8XXxw2VOcJJ416C3Z9ye?=
 =?us-ascii?q?aW?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AAAABGTBNch0O0hNFbCBoBAQEBAQIBA?=
 =?us-ascii?q?QEBBwIBAQEBgVEFAQEBAQsBg2snjBWMEYIhl0GBJAVNERgTAYdFIjQJDQEDAQE?=
 =?us-ascii?q?BAQEBAgETAQEBCA0JCCkvgjYkAYJiAwMBAiRSBgkBARg4A1QGEwWDHIIBBadtM?=
 =?us-ascii?q?4ovh32EQYIWgRGCZIUKhgACiRgfgX2FElKPewcCkU4LGF9+iEaHJwGZUIFGbYE?=
 =?us-ascii?q?hMxojgzyCJxeOKjIBATEBgQQBAYxlAQE?=
X-IPAS-Result: =?us-ascii?q?A0AAAABGTBNch0O0hNFbCBoBAQEBAQIBAQEBBwIBAQEBgVE?=
 =?us-ascii?q?FAQEBAQsBg2snjBWMEYIhl0GBJAVNERgTAYdFIjQJDQEDAQEBAQEBAgETAQEBC?=
 =?us-ascii?q?A0JCCkvgjYkAYJiAwMBAiRSBgkBARg4A1QGEwWDHIIBBadtM4ovh32EQYIWgRG?=
 =?us-ascii?q?CZIUKhgACiRgfgX2FElKPewcCkU4LGF9+iEaHJwGZUIFGbYEhMxojgzyCJxeOK?=
 =?us-ascii?q?jIBATEBgQQBAYxlAQE?=
X-IronPort-AV: E=Sophos;i="5.56,351,1539673200"; 
   d="scan'208";a="55877461"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from vger.kernel.org ([209.132.180.67])
  by mtab.intel.com with ESMTP; 13 Dec 2018 22:28:11 -0800
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1728151AbeLNG2I (ORCPT <rfc822;like.xu@linux.intel.com>
        + 23 others); Fri, 14 Dec 2018 01:28:08 -0500
Received: from mga07.intel.com ([134.134.136.100]:52278 "EHLO mga07.intel.com"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1727942AbeLNG2H (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Fri, 14 Dec 2018 01:28:07 -0500
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from fmsmga002.fm.intel.com ([10.253.24.26])
  by orsmga105.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 13 Dec 2018 22:28:06 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.56,351,1539673200"; 
   d="scan'208";a="125840917"
Received: from yhuang-mobile.sh.intel.com ([10.239.197.226])
  by fmsmga002.fm.intel.com with ESMTP; 13 Dec 2018 22:28:03 -0800
From: Huang Ying <ying.huang@intel.com>
To: Andrew Morton <akpm@linux-foundation.org>
Cc: linux-mm@kvack.org, linux-kernel@vger.kernel.org,
        Huang Ying <ying.huang@intel.com>,
        "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>,
        Andrea Arcangeli <aarcange@redhat.com>,
        Michal Hocko <mhocko@kernel.org>,
        Johannes Weiner <hannes@cmpxchg.org>,
        Shaohua Li <shli@kernel.org>, Hugh Dickins <hughd@google.com>,
        Minchan Kim <minchan@kernel.org>,
        Rik van Riel <riel@redhat.com>,
        Dave Hansen <dave.hansen@linux.intel.com>,
        Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>,
        Zi Yan <zi.yan@cs.rutgers.edu>,
        Daniel Jordan <daniel.m.jordan@oracle.com>
Subject: [PATCH -V9 09/21] swap: Support to read a huge swap cluster for swapin a THP
Date: Fri, 14 Dec 2018 14:27:42 +0800
Message-Id: <20181214062754.13723-10-ying.huang@intel.com>
X-Mailer: git-send-email 2.18.1
In-Reply-To: <20181214062754.13723-1-ying.huang@intel.com>
References: <20181214062754.13723-1-ying.huang@intel.com>
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org

To swapin a THP in one piece, we need to read a huge swap cluster from
the swap device.  This patch revised the __read_swap_cache_async() and
its callers and callees to support this.  If __read_swap_cache_async()
find the swap cluster of the specified swap entry is huge, it will try
to allocate a THP, add it into the swap cache.  So later the contents
of the huge swap cluster can be read into the THP.

Signed-off-by: "Huang, Ying" <ying.huang@intel.com>
Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
Cc: Andrea Arcangeli <aarcange@redhat.com>
Cc: Michal Hocko <mhocko@kernel.org>
Cc: Johannes Weiner <hannes@cmpxchg.org>
Cc: Shaohua Li <shli@kernel.org>
Cc: Hugh Dickins <hughd@google.com>
Cc: Minchan Kim <minchan@kernel.org>
Cc: Rik van Riel <riel@redhat.com>
Cc: Dave Hansen <dave.hansen@linux.intel.com>
Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Cc: Zi Yan <zi.yan@cs.rutgers.edu>
Cc: Daniel Jordan <daniel.m.jordan@oracle.com>
---
 include/linux/huge_mm.h |  6 +++++
 include/linux/swap.h    |  4 +--
 mm/huge_memory.c        |  4 +--
 mm/swap_state.c         | 60 ++++++++++++++++++++++++++++++++---------
 mm/swapfile.c           |  9 ++++---
 5 files changed, 64 insertions(+), 19 deletions(-)

diff --git a/include/linux/huge_mm.h b/include/linux/huge_mm.h
index 1c0fda003d6a..72f2617d336b 100644
--- a/include/linux/huge_mm.h
+++ b/include/linux/huge_mm.h
@@ -250,6 +250,7 @@ static inline bool thp_migration_supported(void)
 	return IS_ENABLED(CONFIG_ARCH_ENABLE_THP_MIGRATION);
 }
 
+gfp_t alloc_hugepage_direct_gfpmask(struct vm_area_struct *vma);
 #else /* CONFIG_TRANSPARENT_HUGEPAGE */
 #define HPAGE_PMD_SHIFT ({ BUILD_BUG(); 0; })
 #define HPAGE_PMD_MASK ({ BUILD_BUG(); 0; })
@@ -363,6 +364,11 @@ static inline bool thp_migration_supported(void)
 {
 	return false;
 }
+
+static inline gfp_t alloc_hugepage_direct_gfpmask(struct vm_area_struct *vma)
+{
+	return 0;
+}
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 
 #endif /* _LINUX_HUGE_MM_H */
diff --git a/include/linux/swap.h b/include/linux/swap.h
index 441da4a832a6..4bd532c9315e 100644
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@ -462,7 +462,7 @@ extern sector_t map_swap_page(struct page *, struct block_device **);
 extern sector_t swapdev_block(int, pgoff_t);
 extern int page_swapcount(struct page *);
 extern int __swap_count(swp_entry_t entry);
-extern int __swp_swapcount(swp_entry_t entry);
+extern int __swp_swapcount(swp_entry_t entry, int *entry_size);
 extern int swp_swapcount(swp_entry_t entry);
 extern struct swap_info_struct *page_swap_info(struct page *);
 extern struct swap_info_struct *swp_swap_info(swp_entry_t entry);
@@ -590,7 +590,7 @@ static inline int __swap_count(swp_entry_t entry)
 	return 0;
 }
 
-static inline int __swp_swapcount(swp_entry_t entry)
+static inline int __swp_swapcount(swp_entry_t entry, int *entry_size)
 {
 	return 0;
 }
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index fc31fc1ae0b3..1cec1eec340e 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -629,9 +629,9 @@ static vm_fault_t __do_huge_pmd_anonymous_page(struct vm_fault *vmf,
  *	    available
  * never: never stall for any thp allocation
  */
-static inline gfp_t alloc_hugepage_direct_gfpmask(struct vm_area_struct *vma)
+gfp_t alloc_hugepage_direct_gfpmask(struct vm_area_struct *vma)
 {
-	const bool vma_madvised = !!(vma->vm_flags & VM_HUGEPAGE);
+	const bool vma_madvised = vma ? !!(vma->vm_flags & VM_HUGEPAGE) : false;
 
 	/* Always do synchronous compaction */
 	if (test_bit(TRANSPARENT_HUGEPAGE_DEFRAG_DIRECT_FLAG, &transparent_hugepage_flags))
diff --git a/mm/swap_state.c b/mm/swap_state.c
index 97831166994a..5e761bb6e354 100644
--- a/mm/swap_state.c
+++ b/mm/swap_state.c
@@ -361,7 +361,9 @@ struct page *__read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,
 {
 	struct page *found_page = NULL, *new_page = NULL;
 	struct swap_info_struct *si;
-	int err;
+	int err, entry_size = 1;
+	swp_entry_t hentry;
+
 	*new_page_allocated = false;
 
 	do {
@@ -387,14 +389,41 @@ struct page *__read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,
 		 * as SWAP_HAS_CACHE.  That's done in later part of code or
 		 * else swap_off will be aborted if we return NULL.
 		 */
-		if (!__swp_swapcount(entry) && swap_slot_cache_enabled)
+		if (!__swp_swapcount(entry, &entry_size) &&
+		    swap_slot_cache_enabled)
 			break;
 
 		/*
 		 * Get a new page to read into from swap.
 		 */
-		if (!new_page) {
-			new_page = alloc_page_vma(gfp_mask, vma, addr);
+		if (!new_page ||
+		    (IS_ENABLED(CONFIG_THP_SWAP) &&
+		     hpage_nr_pages(new_page) != entry_size)) {
+			if (new_page)
+				put_page(new_page);
+			if (IS_ENABLED(CONFIG_THP_SWAP) &&
+			    entry_size == HPAGE_PMD_NR) {
+				gfp_t gfp;
+
+				gfp = alloc_hugepage_direct_gfpmask(vma);
+				/*
+				 * Make sure huge page allocation flags are
+				 * compatible with that of normal page
+				 */
+				VM_WARN_ONCE(gfp_mask & ~(gfp | __GFP_RECLAIM),
+					     "ignoring gfp_mask bits: %x",
+					     gfp_mask & ~(gfp | __GFP_RECLAIM));
+				new_page = alloc_hugepage_vma(gfp, vma, addr,
+                                                               HPAGE_PMD_ORDER);
+				if (new_page)
+					prep_transhuge_page(new_page);
+				hentry = swp_entry(swp_type(entry),
+						   round_down(swp_offset(entry),
+							      HPAGE_PMD_NR));
+			} else {
+				new_page = alloc_page_vma(gfp_mask, vma, addr);
+				hentry = entry;
+			}
 			if (!new_page)
 				break;		/* Out of memory */
 		}
@@ -402,7 +431,7 @@ struct page *__read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,
 		/*
 		 * Swap entry may have been freed since our caller observed it.
 		 */
-		err = swapcache_prepare(entry, 1);
+		err = swapcache_prepare(hentry, entry_size);
 		if (err == -EEXIST) {
 			/*
 			 * We might race against get_swap_page() and stumble
@@ -411,18 +440,24 @@ struct page *__read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,
 			 */
 			cond_resched();
 			continue;
+		} else if (err == -ENOTDIR) {
+			/* huge swap cluster has been split under us */
+			continue;
 		} else if (err)		/* swp entry is obsolete ? */
 			break;
 
 		/* May fail (-ENOMEM) if XArray node allocation failed. */
 		__SetPageLocked(new_page);
 		__SetPageSwapBacked(new_page);
-		err = add_to_swap_cache(new_page, entry, gfp_mask & GFP_KERNEL);
+		err = add_to_swap_cache(new_page, hentry, gfp_mask & GFP_KERNEL);
 		if (likely(!err)) {
 			/* Initiate read into locked page */
 			SetPageWorkingset(new_page);
 			lru_cache_add_anon(new_page);
 			*new_page_allocated = true;
+			if (IS_ENABLED(CONFIG_THP_SWAP))
+				new_page += swp_offset(entry) &
+					(entry_size - 1);
 			return new_page;
 		}
 		__ClearPageLocked(new_page);
@@ -430,7 +465,7 @@ struct page *__read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,
 		 * add_to_swap_cache() doesn't return -EEXIST, so we can safely
 		 * clear SWAP_HAS_CACHE flag.
 		 */
-		put_swap_page(new_page, entry);
+		put_swap_page(new_page, hentry);
 	} while (err != -ENOMEM);
 
 	if (new_page)
@@ -452,7 +487,7 @@ struct page *read_swap_cache_async(swp_entry_t entry, gfp_t gfp_mask,
 			vma, addr, &page_was_allocated);
 
 	if (page_was_allocated)
-		swap_readpage(retpage, do_poll);
+		swap_readpage(compound_head(retpage), do_poll);
 
 	return retpage;
 }
@@ -571,8 +606,9 @@ struct page *swap_cluster_readahead(swp_entry_t entry, gfp_t gfp_mask,
 		if (!page)
 			continue;
 		if (page_allocated) {
-			swap_readpage(page, false);
-			if (offset != entry_offset) {
+			swap_readpage(compound_head(page), false);
+			if (offset != entry_offset &&
+			    !PageTransCompound(page)) {
 				SetPageReadahead(page);
 				count_vm_event(SWAP_RA);
 			}
@@ -733,8 +769,8 @@ static struct page *swap_vma_readahead(swp_entry_t fentry, gfp_t gfp_mask,
 		if (!page)
 			continue;
 		if (page_allocated) {
-			swap_readpage(page, false);
-			if (i != ra_info.offset) {
+			swap_readpage(compound_head(page), false);
+			if (i != ra_info.offset && !PageTransCompound(page)) {
 				SetPageReadahead(page);
 				count_vm_event(SWAP_RA);
 			}
diff --git a/mm/swapfile.c b/mm/swapfile.c
index c59cc2ca7c2c..e27fe24a1f41 100644
--- a/mm/swapfile.c
+++ b/mm/swapfile.c
@@ -1542,7 +1542,8 @@ int __swap_count(swp_entry_t entry)
 	return count;
 }
 
-static int swap_swapcount(struct swap_info_struct *si, swp_entry_t entry)
+static int swap_swapcount(struct swap_info_struct *si, swp_entry_t entry,
+			  int *entry_size)
 {
 	int count = 0;
 	pgoff_t offset = swp_offset(entry);
@@ -1550,6 +1551,8 @@ static int swap_swapcount(struct swap_info_struct *si, swp_entry_t entry)
 
 	ci = lock_cluster_or_swap_info(si, offset);
 	count = swap_count(si->swap_map[offset]);
+	if (entry_size)
+		*entry_size = ci && cluster_is_huge(ci) ? SWAPFILE_CLUSTER : 1;
 	unlock_cluster_or_swap_info(si, ci);
 	return count;
 }
@@ -1559,14 +1562,14 @@ static int swap_swapcount(struct swap_info_struct *si, swp_entry_t entry)
  * This does not give an exact answer when swap count is continued,
  * but does include the high COUNT_CONTINUED flag to allow for that.
  */
-int __swp_swapcount(swp_entry_t entry)
+int __swp_swapcount(swp_entry_t entry, int *entry_size)
 {
 	int count = 0;
 	struct swap_info_struct *si;
 
 	si = get_swap_device(entry);
 	if (si) {
-		count = swap_swapcount(si, entry);
+		count = swap_swapcount(si, entry, entry_size);
 		put_swap_device(si);
 	}
 	return count;
-- 
2.18.1

