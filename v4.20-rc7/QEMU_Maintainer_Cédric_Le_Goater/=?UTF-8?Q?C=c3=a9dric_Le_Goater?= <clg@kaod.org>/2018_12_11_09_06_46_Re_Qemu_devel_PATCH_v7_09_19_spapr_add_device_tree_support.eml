Return-Path: <qemu-devel-bounces+like.xu=linux.intel.com@nongnu.org>
Delivered-To: unknown
Received: from linux.intel.com (10.54.29.200:995) by likexu-workstation with
  POP3-SSL; 11 Dec 2018 19:47:55 -0000
X-Original-To: like.xu@linux.intel.com
Delivered-To: like.xu@linux.intel.com
Received: from orsmga005.jf.intel.com (orsmga005.jf.intel.com [10.7.209.41])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
	(No client certificate requested)
	by linux.intel.com (Postfix) with ESMTPS id 2C24E58028E
	for <like.xu@linux.intel.com>; Tue, 11 Dec 2018 01:07:30 -0800 (PST)
Received: from fmsmga101.fm.intel.com ([10.1.193.65])
  by orsmga005-1.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384; 11 Dec 2018 01:07:29 -0800
IronPort-PHdr: =?us-ascii?q?9a23=3AMVks8BPX4VTV48YcvSwl6mtUPXoX/o7sNwtQ0KIM?=
 =?us-ascii?q?zox0I/rzrarrMEGX3/hxlliBBdydt6oUzbKO+4nbGkU4qa6bt34DdJEeHzQksu?=
 =?us-ascii?q?4x2zIaPcieFEfgJ+TrZSFpVO5LVVti4m3peRMNQJW2aFLduGC94iAPERvjKwV1?=
 =?us-ascii?q?Ov71GonPhMiryuy+4ZLebxlLiTanfb9+MAi9oBnMuMURnYZsMLs6xAHTontPde?=
 =?us-ascii?q?RWxGdoKkyWkh3h+Mq+/4Nt/jpJtf45+MFOTav1f6IjTbxFFzsmKHw65NfqtRbY?=
 =?us-ascii?q?UwSC4GYXX3gMnRpJBwjF6wz6Xov0vyDnuOdxxDWWMMvrRr0vRz+s87lkRwPpiC?=
 =?us-ascii?q?cfNj427mfXitBrjKlGpB6tvgFzz5LIbI2QMvd1Y6HTcs4ARWdZQMhRWSxPDICy?=
 =?us-ascii?q?YYQBAOUOP/pXoYbgqVsWrxawBwahCP7hxzNUmHD2xrY30/g4EQzcwAAsA9wDvX?=
 =?us-ascii?q?TSod7oNKkSS+e1zKzQwDjddfRZxC396InUfR85p/+DQ6p/f8vWyUY1CwPKkE+Q?=
 =?us-ascii?q?opHiMjyJ0uQNvHOW7+l6WuKolmErsQZxoiKgxso1jITCm4Ebykjc+Cln3Io4Js?=
 =?us-ascii?q?e0RFNmbdOnCpdcqS+XOotsTs8/QWxkoCI3xqEctZO4eCUG0okryhreZvCdboSE?=
 =?us-ascii?q?/w7vWeCMKjlinn1lYqiwhxOq/Eig1OL8Us603U5OripEidnMqmsN2wbc6siBV/?=
 =?us-ascii?q?tx5ECh2SyA1wzL6+FEJ147lbbDJpI/3rI9lYAfvVneEiL1gkn6kqGbe0U+9uS1?=
 =?us-ascii?q?6enrerDmqYWdN49whAH+KKMumsmnDOQhLAcOWnWb9f2h27L94032XrFKguQtna?=
 =?us-ascii?q?nerZDaI9gUqbCiAwNS1oYj6hW/ACm83NUXgHkKNFZFeBOBj4j0NFDCOvH4DfGj?=
 =?us-ascii?q?g1uylDdn3ezJPrrkApjWKHjDl7Hhfbl7605B0gYzyspf551MBrEbPP3zQlPxtM?=
 =?us-ascii?q?DfDhIhNwy0wuXnB8tn2oIRRGKCGauZMKLUsV+V6eMjOeiMZIkJuDnjL/gp/eLh?=
 =?us-ascii?q?jXg8mQxVQK+ywJFCaGykBu80ZAKdYGHwmZEHFmEFuBd4S/blz1iLUDpWbnD1WL?=
 =?us-ascii?q?og5zY9E8W/AILeA4ygnrGFjxq9BYBcM2VPC1SQFiXxeoCZHvsBdi+WZ9VsiyEJ?=
 =?us-ascii?q?TqSJTYgn2hez8gjgxO18M+DW9yYE4I/lz8V//ObJlBs/pgBzWuiUzWCBB051lH?=
 =?us-ascii?q?gPQXdi0KlhoEBVxVaJyq9+xfBCEsRZ6vpTFAY2c5fBmb9UEdf3DylAcs2IQ2GC?=
 =?us-ascii?q?T+KJBjgtQ8h5l9QKbG5+EtKuyBfZ0Hz5UPcui7WXCclsoern1H/rKpM4ki6e2Q?=
 =?us-ascii?q?=3D=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A0AmAAAyfQ9cmBHrdtBkHAEBAQQBAQcEA?=
 =?us-ascii?q?QGBUgYBAQsBgTAqgjiMc4sxgWAtfJZWFIFfFBgUhECDOCI1CA0BAwEBAQEBAQI?=
 =?us-ascii?q?BEwEBAQEBCAsLBhsOL4I2BQIDGgEGglsBAQEBAgEBAiQfCikDAgEBAgYBAQoYF?=
 =?us-ascii?q?QcKCAMBORoGDQYCAQEBglFLgXoIAQMBpiszhUCDYIENjDoRBoF/gREngW1JNYQ?=
 =?us-ascii?q?8HxAWP4UcAokWUpcPCYpEhwYGGJE/gwKWMYFIA4IJTTAIO4JsgicXEm0BAwSNF?=
 =?us-ascii?q?3GBBAOIGFWBdwEB?=
X-IPAS-Result: =?us-ascii?q?A0AmAAAyfQ9cmBHrdtBkHAEBAQQBAQcEAQGBUgYBAQsBgTA?=
 =?us-ascii?q?qgjiMc4sxgWAtfJZWFIFfFBgUhECDOCI1CA0BAwEBAQEBAQIBEwEBAQEBCAsLB?=
 =?us-ascii?q?hsOL4I2BQIDGgEGglsBAQEBAgEBAiQfCikDAgEBAgYBAQoYFQcKCAMBORoGDQY?=
 =?us-ascii?q?CAQEBglFLgXoIAQMBpiszhUCDYIENjDoRBoF/gREngW1JNYQ8HxAWP4UcAokWU?=
 =?us-ascii?q?pcPCYpEhwYGGJE/gwKWMYFIA4IJTTAIO4JsgicXEm0BAwSNF3GBBAOIGFWBdwE?=
 =?us-ascii?q?B?=
X-IronPort-AV: E=Sophos;i="5.56,341,1539673200"; 
   d="scan'208";a="66248764"
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from lists.gnu.org ([208.118.235.17])
  by mga01b.intel.com with ESMTP/TLS/AES256-SHA; 11 Dec 2018 01:07:28 -0800
Received: from localhost ([::1]:36681 helo=lists.gnu.org)
	by lists.gnu.org with esmtp (Exim 4.71)
	(envelope-from <qemu-devel-bounces+like.xu=linux.intel.com@nongnu.org>)
	id 1gWe0m-0007lc-6r
	for like.xu@linux.intel.com; Tue, 11 Dec 2018 04:07:28 -0500
Received: from eggs.gnu.org ([2001:4830:134:3::10]:53799)
	by lists.gnu.org with esmtp (Exim 4.71)
	(envelope-from <clg@kaod.org>) id 1gWe0J-0007fW-2P
	for qemu-devel@nongnu.org; Tue, 11 Dec 2018 04:07:00 -0500
Received: from Debian-exim by eggs.gnu.org with spam-scanned (Exim 4.71)
	(envelope-from <clg@kaod.org>) id 1gWe0D-0005U5-V7
	for qemu-devel@nongnu.org; Tue, 11 Dec 2018 04:06:59 -0500
Received: from 20.mo3.mail-out.ovh.net ([178.33.47.94]:39437)
	by eggs.gnu.org with esmtps (TLS1.0:DHE_RSA_AES_256_CBC_SHA1:32)
	(Exim 4.71) (envelope-from <clg@kaod.org>) id 1gWe0D-0005Si-L6
	for qemu-devel@nongnu.org; Tue, 11 Dec 2018 04:06:53 -0500
Received: from player750.ha.ovh.net (unknown [10.109.143.232])
	by mo3.mail-out.ovh.net (Postfix) with ESMTP id 71E211ED789
	for <qemu-devel@nongnu.org>; Tue, 11 Dec 2018 10:06:51 +0100 (CET)
Received: from kaod.org (lfbn-1-10605-110.w90-89.abo.wanadoo.fr
	[90.89.196.110]) (Authenticated sender: postmaster@kaod.org)
	by player750.ha.ovh.net (Postfix) with ESMTPSA id DFFD7CE4A73;
	Tue, 11 Dec 2018 09:06:46 +0000 (UTC)
To: David Gibson <david@gibson.dropbear.id.au>
References: <20181209194610.29727-1-clg@kaod.org>
	<20181209194610.29727-10-clg@kaod.org>
	<20181210063919.GU4261@umbus.fritz.box>
	<dc1104f1-0809-e7df-b145-e7faa2255fe6@kaod.org>
	<20181211003857.GY4261@umbus.fritz.box>
From: =?UTF-8?Q?C=c3=a9dric_Le_Goater?= <clg@kaod.org>
Message-ID: <4070f6b9-a4e3-6fd0-f6d7-45f33b869426@kaod.org>
Date: Tue, 11 Dec 2018 10:06:46 +0100
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101
	Thunderbird/60.3.1
MIME-Version: 1.0
In-Reply-To: <20181211003857.GY4261@umbus.fritz.box>
Content-Type: text/plain; charset=windows-1252
Content-Language: en-US
X-Ovh-Tracer-Id: 9091360276872334193
X-VR-SPAMSTATE: OK
X-VR-SPAMSCORE: -100
X-VR-SPAMCAUSE: gggruggvucftvghtrhhoucdtuddrgedtkedrudegiedguddvlecutefuodetggdotefrodftvfcurfhrohhfihhlvgemucfqggfjpdevjffgvefmvefgnecuuegrihhlohhuthemucehtddtnecusecvtfgvtghiphhivghnthhsucdlqddutddtmd
Content-Transfer-Encoding: quoted-printable
X-detected-operating-system: by eggs.gnu.org: GNU/Linux 2.2.x-3.x [generic]
	[fuzzy]
X-Received-From: 178.33.47.94
Subject: Re: [Qemu-devel] [PATCH v7 09/19] spapr: add device tree support
 for the XIVE exploitation mode
X-BeenThere: qemu-devel@nongnu.org
X-Mailman-Version: 2.1.21
Precedence: list
List-Id: <qemu-devel.nongnu.org>
List-Unsubscribe: <https://lists.nongnu.org/mailman/options/qemu-devel>,
	<mailto:qemu-devel-request@nongnu.org?subject=unsubscribe>
List-Archive: <http://lists.nongnu.org/archive/html/qemu-devel/>
List-Post: <mailto:qemu-devel@nongnu.org>
List-Help: <mailto:qemu-devel-request@nongnu.org?subject=help>
List-Subscribe: <https://lists.nongnu.org/mailman/listinfo/qemu-devel>,
	<mailto:qemu-devel-request@nongnu.org?subject=subscribe>
Cc: qemu-ppc@nongnu.org, qemu-devel@nongnu.org
Errors-To: qemu-devel-bounces+like.xu=linux.intel.com@nongnu.org
Sender: "Qemu-devel" <qemu-devel-bounces+like.xu=linux.intel.com@nongnu.org>

On 12/11/18 1:38 AM, David Gibson wrote:
> On Mon, Dec 10, 2018 at 08:53:17AM +0100, C=E9dric Le Goater wrote:
>> On 12/10/18 7:39 AM, David Gibson wrote:
>>> On Sun, Dec 09, 2018 at 08:46:00PM +0100, C=E9dric Le Goater wrote:
>>>> The XIVE interface for the guest is described in the device tree und=
er
>>>> the "interrupt-controller" node. A couple of new properties are
>>>> specific to XIVE :
>>>>
>>>>  - "reg"
>>>>
>>>>    contains the base address and size of the thread interrupt
>>>>    managnement areas (TIMA), for the User level and for the Guest OS
>>>>    level. Only the Guest OS level is taken into account today.
>>>>
>>>>  - "ibm,xive-eq-sizes"
>>>>
>>>>    the size of the event queues. One cell per size supported, contai=
ns
>>>>    log2 of size, in ascending order.
>>>>
>>>>  - "ibm,xive-lisn-ranges"
>>>>
>>>>    the IRQ interrupt number ranges assigned to the guest for the IPI=
s.
>>>>
>>>> and also under the root node :
>>>>
>>>>  - "ibm,plat-res-int-priorities"
>>>>
>>>>    contains a list of priorities that the hypervisor has reserved fo=
r
>>>>    its own use. OPAL uses the priority 7 queue to automatically
>>>>    escalate interrupts for all other queues (DD2.X POWER9). So only
>>>>    priorities [0..6] are allowed for the guest.
>>>>
>>>> Extend the sPAPR IRQ backend with a new handler to populate the DT
>>>> with the appropriate "interrupt-controller" node.
>>>>
>>>> Signed-off-by: C=E9dric Le Goater <clg@kaod.org>
>>>> ---
>>>>  include/hw/ppc/spapr_irq.h  |  2 ++
>>>>  include/hw/ppc/spapr_xive.h |  2 ++
>>>>  include/hw/ppc/xics.h       |  4 +--
>>>>  hw/intc/spapr_xive.c        | 64 ++++++++++++++++++++++++++++++++++=
+++
>>>>  hw/intc/xics_spapr.c        |  3 +-
>>>>  hw/ppc/spapr.c              |  3 +-
>>>>  hw/ppc/spapr_irq.c          |  3 ++
>>>>  7 files changed, 77 insertions(+), 4 deletions(-)
>>>>
>>>> diff --git a/include/hw/ppc/spapr_irq.h b/include/hw/ppc/spapr_irq.h
>>>> index 23cdb51b879e..e51e9f052f63 100644
>>>> --- a/include/hw/ppc/spapr_irq.h
>>>> +++ b/include/hw/ppc/spapr_irq.h
>>>> @@ -39,6 +39,8 @@ typedef struct sPAPRIrq {
>>>>      void (*free)(sPAPRMachineState *spapr, int irq, int num);
>>>>      qemu_irq (*qirq)(sPAPRMachineState *spapr, int irq);
>>>>      void (*print_info)(sPAPRMachineState *spapr, Monitor *mon);
>>>> +    void (*dt_populate)(sPAPRMachineState *spapr, uint32_t nr_serve=
rs,
>>>> +                        void *fdt, uint32_t phandle);
>>>>  } sPAPRIrq;
>>>> =20
>>>>  extern sPAPRIrq spapr_irq_xics;
>>>> diff --git a/include/hw/ppc/spapr_xive.h b/include/hw/ppc/spapr_xive=
.h
>>>> index 9506a8f4d10a..728a5e8dc163 100644
>>>> --- a/include/hw/ppc/spapr_xive.h
>>>> +++ b/include/hw/ppc/spapr_xive.h
>>>> @@ -45,5 +45,7 @@ qemu_irq spapr_xive_qirq(sPAPRXive *xive, uint32_t=
 lisn);
>>>>  typedef struct sPAPRMachineState sPAPRMachineState;
>>>> =20
>>>>  void spapr_xive_hcall_init(sPAPRMachineState *spapr);
>>>> +void spapr_dt_xive(sPAPRMachineState *spapr, uint32_t nr_servers, v=
oid *fdt,
>>>> +                   uint32_t phandle);
>>>> =20
>>>>  #endif /* PPC_SPAPR_XIVE_H */
>>>> diff --git a/include/hw/ppc/xics.h b/include/hw/ppc/xics.h
>>>> index 9958443d1984..14afda198cdb 100644
>>>> --- a/include/hw/ppc/xics.h
>>>> +++ b/include/hw/ppc/xics.h
>>>> @@ -181,8 +181,6 @@ typedef struct XICSFabricClass {
>>>>      ICPState *(*icp_get)(XICSFabric *xi, int server);
>>>>  } XICSFabricClass;
>>>> =20
>>>> -void spapr_dt_xics(int nr_servers, void *fdt, uint32_t phandle);
>>>> -
>>>>  ICPState *xics_icp_get(XICSFabric *xi, int server);
>>>> =20
>>>>  /* Internal XICS interfaces */
>>>> @@ -204,6 +202,8 @@ void icp_resend(ICPState *ss);
>>>> =20
>>>>  typedef struct sPAPRMachineState sPAPRMachineState;
>>>> =20
>>>> +void spapr_dt_xics(sPAPRMachineState *spapr, uint32_t nr_servers, v=
oid *fdt,
>>>> +                   uint32_t phandle);
>>>>  int xics_kvm_init(sPAPRMachineState *spapr, Error **errp);
>>>>  void xics_spapr_init(sPAPRMachineState *spapr);
>>>> =20
>>>> diff --git a/hw/intc/spapr_xive.c b/hw/intc/spapr_xive.c
>>>> index 982ac6e17051..a6d854b07690 100644
>>>> --- a/hw/intc/spapr_xive.c
>>>> +++ b/hw/intc/spapr_xive.c
>>>> @@ -14,6 +14,7 @@
>>>>  #include "target/ppc/cpu.h"
>>>>  #include "sysemu/cpus.h"
>>>>  #include "monitor/monitor.h"
>>>> +#include "hw/ppc/fdt.h"
>>>>  #include "hw/ppc/spapr.h"
>>>>  #include "hw/ppc/spapr_xive.h"
>>>>  #include "hw/ppc/xive.h"
>>>> @@ -1381,3 +1382,66 @@ void spapr_xive_hcall_init(sPAPRMachineState =
*spapr)
>>>>      spapr_register_hypercall(H_INT_SYNC, h_int_sync);
>>>>      spapr_register_hypercall(H_INT_RESET, h_int_reset);
>>>>  }
>>>> +
>>>> +void spapr_dt_xive(sPAPRMachineState *spapr, uint32_t nr_servers, v=
oid *fdt,
>>>> +                   uint32_t phandle)
>>>> +{
>>>> +    sPAPRXive *xive =3D spapr->xive;
>>>> +    int node;
>>>> +    uint64_t timas[2 * 2];
>>>> +    /* Interrupt number ranges for the IPIs */
>>>> +    uint32_t lisn_ranges[] =3D {
>>>> +        cpu_to_be32(0),
>>>> +        cpu_to_be32(nr_servers),
>>>> +    };
>>>> +    uint32_t eq_sizes[] =3D {
>>>> +        cpu_to_be32(12), /* 4K */
>>>> +        cpu_to_be32(16), /* 64K */
>>>> +        cpu_to_be32(21), /* 2M */
>>>> +        cpu_to_be32(24), /* 16M */
>>>
>>> For KVM, are we going to need to clamp this list based on the
>>> pagesizes the guest can use?
>>
>> I would say so. Is there a KVM service for that ?
>=20
> I'm not sure what you mean by a KVM service here.

I meant a routine giving a list of the possible backing pagesizes.
=20
>> Today, the OS scans the list and picks the size fitting its current=20
>> PAGE_SIZE/SHIFT. But I suppose it would be better to advertise=20
>> only the page sizes that QEMU/KVM supports. Or should we play safe=20
>> and only export : 4K, 64K ?=20
>=20
> So, I'm guessing the limitation here is that when we use the KVM XIVE
> acceleration, the EQ will need to be host-contiguous? =20

The EQ should be a single page (from the specs). So we don't have=20
a contiguity problem.=20

> So, we can't have EQs larger than the backing pagesize for guest=20
> memory.
>
 > We try to avoid having guest visible differences based on whether
> we're using KVM or not, so we don't want to change this list depending
> on whether KVM is active etc. - or on whether we're backing the guest
> with hugepages.
>=20
> We could reuse the cap-hpt-max-page-size machine parameter which also
> relates to backing page size, but that might be confusing since it's
> explicitly about HPT only whereas this limitation would apply to RPT
> guests too, IIUC.
>=20
> I think our best bet is probably to limit to 64kiB queues
> unconditionally. =20

OK. That's the default. We can refine this list of page sizes later on
when we introduce KVM support if needed.

> AIUI, that still gives us 8192 slots in the queue,

The entries are 32bits. So that's 16384 entries.=20

> which is enough to store one of every possible irq, which should be
> enough.

yes.=20

I don't think Linux measures how much entries are dequeued at once.
It would give us a feel of how much pressure these EQs can sustain.

C.
=20
> There's still an issue if we have a 4kiB pagesize host kernel, but we
> could just disable kernel_irqchip in that situation, since we don't
> expect people to be using that much in practice.
>=20
>>>> +    };
>>>> +    /* The following array is in sync with the reserved priorities
>>>> +     * defined by the 'spapr_xive_priority_is_reserved' routine.
>>>> +     */
>>>> +    uint32_t plat_res_int_priorities[] =3D {
>>>> +        cpu_to_be32(7),    /* start */
>>>> +        cpu_to_be32(0xf8), /* count */
>>>> +    };
>>>> +    gchar *nodename;
>>>> +
>>>> +    /* Thread Interrupt Management Area : User (ring 3) and OS (rin=
g 2) */
>>>> +    timas[0] =3D cpu_to_be64(xive->tm_base +
>>>> +                           XIVE_TM_USER_PAGE * (1ull << TM_SHIFT));
>>>> +    timas[1] =3D cpu_to_be64(1ull << TM_SHIFT);
>>>> +    timas[2] =3D cpu_to_be64(xive->tm_base +
>>>> +                           XIVE_TM_OS_PAGE * (1ull << TM_SHIFT));
>>>> +    timas[3] =3D cpu_to_be64(1ull << TM_SHIFT);
>>>
>>> So this gives the MMIO address of the TIMA. =20
>>
>> Yes. It is considered to be a fixed "address"=20
>>
>>> Where does the guest get the MMIO address for the ESB pages from?
>>
>> with the hcall H_INT_GET_SOURCE_INFO.
>=20
> Ah, right.
>=20


