Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from pop3.zju.edu.cn (61.164.42.155:110) by
  likexu-MOBL1.ccr.corp.intel.com with POP3; 11 Nov 2018 15:02:35 -0000
Received: from icoremail.net (unknown [209.85.214.172])
	by mail-app2 (Coremail) with SMTP id by_KCgDHH+YXnOZbpsZtAQ--.33217S3;
	Sat, 10 Nov 2018 16:51:35 +0800 (CST)
Received: from mail-pl1-f172.google.com (unknown [209.85.214.172])
	by mx2.icoremail.net (Coremail) with SMTP id AQAAfwBX0EwPnOZbSjomAA--.10215S3;
	Sat, 10 Nov 2018 16:51:27 +0800 (CST)
Received: by mail-pl1-f172.google.com with SMTP id g59-v6so2004546plb.10
        for <xuliker@zju.edu.cn>; Sat, 10 Nov 2018 00:51:27 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:delivered-to:dkim-signature:from:to:cc:subject
         :date:message-id:in-reply-to:references:mime-version
         :content-transfer-encoding:sender:precedence:list-id;
        bh=byoN6dpxnS7IRdKUDH2AwtnUODkweYTe+330yUFTr7M=;
        b=Gzj6qFaDyKkRGOWYgseyLTguFcD8ndqnRRm2+1xLUPe3zpMpVaQuWNg6WkJVODnT82
         DiCSZXZCbNz+7ubxsEvGVZVsj5PqakerCOZBuDUM2WTXgG+joSFJ+y0u2JCmlxrBvznV
         9O951GR7NOC1SIf4Mo7nX+SYIq2pFBh/WBz8wqYhwKB4EPObfCfWlsjwCJ41HJ570VYE
         HWtNYrS7wkzkTl8fZDm2rwXX1CEq1Kbf5rnQ8I/502KHjHrV3CTbxzRvLA/EOyAOlkVc
         dhZlQ9wDMXhTdUYAOvzV+uX6y66Zg6GAPYB4plHy+9F4NjPhN62aaNQ7v29tash4478B
         IOLQ==
X-Gm-Message-State: AGRZ1gLyC+j6O85d+O7hAM7UCHVfptkXLxIY3xZEArr4jOYmS1GRWouH
	3bN1SgG3Qb7nldeLDxhv+IocBWT7gi+Aa74VXpchcqHkgKYXniM=
X-Received: by 2002:a17:902:887:: with SMTP id 7-v6mr4122333pll.283.1541839887326;
        Sat, 10 Nov 2018 00:51:27 -0800 (PST)
X-Forwarded-To: xuliker@zju.edu.cn
X-Forwarded-For: liker.xu@gmail.com xuliker@zju.edu.cn
Delivered-To: liker.xu@gmail.com
Received: by 2002:a17:90a:c304:0:0:0:0 with SMTP id g4-v6csp891929pjt;
        Sat, 10 Nov 2018 00:51:26 -0800 (PST)
X-Google-Smtp-Source: AJdET5fsvG7aYpHE8KCW/BEbm0FCv5/e6qNsUl2qVl/4Ubf4CWprAqH5Olvo8mjh2rz6e7XwW9/a
X-Received: by 2002:a63:504d:: with SMTP id q13mr10510791pgl.319.1541839886502;
        Sat, 10 Nov 2018 00:51:26 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1541839886; cv=none;
        d=google.com; s=arc-20160816;
        b=njBO9NwC91m1ffsq8p+Tp01GSLcavDjXDZ0knmKe9DvqqdTSex3mgdaQLD8nb8BRxI
         +iPIbZcfYtBzin4W2d3RagRt9/0iK/ZQPD8XnyrRAPmUzw6Pn1yyhB4lvqFmXlJ+M2KF
         HK/WMcMkxi/WFte5TbwzcArb4aNwoxZBBjP4djdaYsisXTclOjkxIcJ1f35TfjDWDhga
         pm0HvetCG50pz9eG6Cu4qTdTbDjLu9iZ+pkxkYVC7rf5ahZDtqLX+1gmNS+X4mdQJgnh
         Ch+A0l4PbBiGNmPeFRIM5oDUMcc0BHwLwxqGu22oQw4nv1MKw25EIIMeCCt1QRhr0DMQ
         UzGQ==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=list-id:precedence:sender:content-transfer-encoding:mime-version
         :references:in-reply-to:message-id:date:subject:cc:to:from
         :dkim-signature;
        bh=byoN6dpxnS7IRdKUDH2AwtnUODkweYTe+330yUFTr7M=;
        b=P81EC9OnxI1/diQ2M4KKvH6gu0jHPzfXF0i71m/TcOk3fDdMvejiLPi/yBq02cxsNV
         c6Yp35GTQekgmbSHGjumg/+kgglkDZq6+nEHsSzzFEGi7KhQ7BBepG06T+RH9nk5W0rU
         Gprk/rMX6sglN7jx7yE3TPJodAVV9TtFdrAkbgmqbuhbyfDYaY/YXxMZM8tbM+8BA0gX
         uc9gPiUur/o5E+nAKcEENsH+m9vjj7m46jLDrBKnYo5lKz01XQQEdMtUb9em9DEYq5ym
         hi3oOu+gcbw9mYsc2D6XHyGfFloiuAhWXTQ7FOTfLfBBDKi7xDYd05xGm4llUk5bqary
         mjIA==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass header.i=@gmail.com header.s=20161025 header.b=ajwrcVRx;
       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org;
       dmarc=pass (p=NONE sp=QUARANTINE dis=NONE) header.from=gmail.com
Received: from vger.kernel.org (vger.kernel.org. [209.132.180.67])
        by mx.google.com with ESMTP id b3si9147980pgh.496.2018.11.10.00.51.12;
        Sat, 10 Nov 2018 00:51:26 -0800 (PST)
Received-SPF: pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) client-ip=209.132.180.67;
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1729034AbeKJSfM (ORCPT <rfc822;tu.k.phung@gmail.com>
        + 99 others); Sat, 10 Nov 2018 13:35:12 -0500
Received: from mail-ot1-f66.google.com ([209.85.210.66]:40900 "EHLO
        mail-ot1-f66.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1728791AbeKJSfM (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Sat, 10 Nov 2018 13:35:12 -0500
Received: by mail-ot1-f66.google.com with SMTP id s5so3827611oth.7;
        Sat, 10 Nov 2018 00:50:58 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=from:to:cc:subject:date:message-id:in-reply-to:references
         :mime-version:content-transfer-encoding;
        bh=byoN6dpxnS7IRdKUDH2AwtnUODkweYTe+330yUFTr7M=;
        b=ajwrcVRxs4El5+P9ZBzXqQSnZXvSJZXJ0XPA/y88FiBtBqDx8SJzEezYRSTApnfI+/
         iqOZyPx7wRadvTHkFjAzGMey7xzu4kZwLs1oI0dQ2nwGbSl8NX5bvcJGvNzrd+dtT64I
         CX4rui3+pIPAeVs51W5NomjzeWR+heJCV4eZIhEU5QFUu/nI+rOwE+ao9FZ3q1g68+ok
         QBRtVaUDfMK5Ra09GOATElnRZQSKHIdoIWRoaro0zDLa/LRBFNtXXCMGBJO+T4W4aF5u
         hXYjC18rgL/kwMDnfl5oJPjoUkjaZvvs3NosDVZtKR5CrjqPeUTdN+ooydbjQVr9Mcw6
         l9PQ==
X-Received: by 2002:a9d:7059:: with SMTP id x25mr7671591otj.35.1541839858309;
        Sat, 10 Nov 2018 00:50:58 -0800 (PST)
Received: from sandstorm.nvidia.com ([2600:1700:43b0:3120:feaa:14ff:fe9e:34cb])
        by smtp.gmail.com with ESMTPSA id c7-v6sm3908683oia.58.2018.11.10.00.50.56
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Sat, 10 Nov 2018 00:50:57 -0800 (PST)
From: john.hubbard@gmail.com
X-Google-Original-From: jhubbard@nvidia.com
To: linux-mm@kvack.org
Cc: Andrew Morton <akpm@linux-foundation.org>,
        LKML <linux-kernel@vger.kernel.org>,
        linux-rdma <linux-rdma@vger.kernel.org>,
        linux-fsdevel@vger.kernel.org, John Hubbard <jhubbard@nvidia.com>,
        Matthew Wilcox <willy@infradead.org>,
        Michal Hocko <mhocko@kernel.org>,
        Christopher Lameter <cl@linux.com>,
        Jason Gunthorpe <jgg@ziepe.ca>,
        Dan Williams <dan.j.williams@intel.com>,
        Jan Kara <jack@suse.cz>, Al Viro <viro@zeniv.linux.org.uk>,
        Jerome Glisse <jglisse@redhat.com>,
        Christoph Hellwig <hch@infradead.org>,
        Ralph Campbell <rcampbell@nvidia.com>
Subject: [PATCH v2 2/6] mm: introduce put_user_page*(), placeholder versions
Date: Sat, 10 Nov 2018 00:50:37 -0800
Message-Id: <20181110085041.10071-3-jhubbard@nvidia.com>
X-Mailer: git-send-email 2.19.1
In-Reply-To: <20181110085041.10071-1-jhubbard@nvidia.com>
References: <20181110085041.10071-1-jhubbard@nvidia.com>
MIME-Version: 1.0
X-NVConfidentiality: public
Content-Transfer-Encoding: 8bit
Sender: liker.xu+caf_=xuliker=zju.edu.cn@gmail.com
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-CM-TRANSID: AQAAfwBX0EwPnOZbSjomAA--.10215S3
Authentication-Results: mail-app2; spf=pass smtp.mail=liker.xu+caf_=xu
	liker=zju.edu.cn@gmail.com;
X-Coremail-Antispam: 1UD129KBjvJXoW3Xry7XF45trW5uF1kXr4fXwb_yoW7KFy8pF
	47Jw13Aw17Gr9Iyr93C3WDur1Sgw1rW3yrtFyxJ34rAw15tF92vF1Iyw1fAr1DKr18AFZ8
	JFWjkrnYkFs8u3DanT9S1TB71UUUUUUqnTZGkaVYY2UrUUUU0bIjqfuFe4nvWSU8nxnvy2
	9KBjDU0xBIdaVrnRJUUUm0b7Iv0xC_Kw4lb4IE77IF4wAFF20E14v26r1j6r4UM7CIcVAF
	z4kK6r1j6r18M28lY4IEw2IIxxk0rwA2F7IY1VAKz4vEj48ve4kI8wA2z4x0Y4vE2Ix0cI
	8IcVAFwI0_Xr0_Ar1l84ACjcxK6xIIjxv20xvEc7CjxVAFwI0_Gr0_Cr1l84ACjcxK6I8E
	87Iv67AKxVW0oVCq3wA2z4x0Y4vEx4A2jsIEc7CjxVAFwI0_GcCE3s1le2I262IYc4CY6c
	8Ij28IcVAaY2xG8wAqx4xG64xvF2IEw4CE5I8CrVC2j2WlYx0E2Ix0cI8IcVAFwI0_JrI_
	JrylYx0Ex4A2jsIE14v26r4j6F4UMcvjeVCFs4IE7xkEbVWUJVW8JwACjcxG0xvY0x0EwI
	xGrwAKzVCY07xG64k0F24l7I0Y6sxI4wCY1x0264kExVAvwVAq07x20xylc7Ca8VAvwVCF
	zxkY4VCF77xAMxkIecxEwVCI4VW8ZwCY0x0Ix7I2Y4AK64vIr41lcIIF0xvE2Ix0cI8IcV
	AFwI0_JFI_Gr1lcIIF0xvE2Ix0cI8IcVCY1x0267AKxVWUJVW8JwCYIxAIcVC2z280aVAF
	wI0_Cr1j6rxdMxvI42IY6I8E87Iv6xkF7I0E14v26F4UJVW0owCF04k20xvY0x0EwIxGrw
	CF04k20xvEw4C26cxK6c8Ij28IcwCF72vE77IF4wCFx2IqxVCFs4IE7xkEbVWUJVW8JwC2
	0s026c02F40E14v26r1j6r18MI8I3I0E7480Y4vE14v26r106r1rMI8E67AF67kF1VAFwI
	0_GFv_WrylIxkGc2Ij64vIr41lIxAIcVCF04k26cxKx2IYs7xG6r1j6r1xYxBIdaVFxhVj
	vjDU0xZFpf9x07bLsj8UUUUU=

From: John Hubbard <jhubbard@nvidia.com>

Introduces put_user_page(), which simply calls put_page().
This provides a way to update all get_user_pages*() callers,
so that they call put_user_page(), instead of put_page().

Also introduces put_user_pages(), and a few dirty/locked variations,
as a replacement for release_pages(), and also as a replacement
for open-coded loops that release multiple pages.
These may be used for subsequent performance improvements,
via batching of pages to be released.

This is the first step of fixing the problem described in [1]. The steps
are:

1) (This patch): provide put_user_page*() routines, intended to be used
   for releasing pages that were pinned via get_user_pages*().

2) Convert all of the call sites for get_user_pages*(), to
   invoke put_user_page*(), instead of put_page(). This involves dozens of
   call sites, and will take some time.

3) After (2) is complete, use get_user_pages*() and put_user_page*() to
   implement tracking of these pages. This tracking will be separate from
   the existing struct page refcounting.

4) Use the tracking and identification of these pages, to implement
   special handling (especially in writeback paths) when the pages are
   backed by a filesystem. Again, [1] provides details as to why that is
   desirable.

[1] https://lwn.net/Articles/753027/ : "The Trouble with get_user_pages()"

Cc: Matthew Wilcox <willy@infradead.org>
Cc: Michal Hocko <mhocko@kernel.org>
Cc: Christopher Lameter <cl@linux.com>
Cc: Jason Gunthorpe <jgg@ziepe.ca>
Cc: Dan Williams <dan.j.williams@intel.com>
Cc: Jan Kara <jack@suse.cz>
Cc: Al Viro <viro@zeniv.linux.org.uk>
Cc: Jerome Glisse <jglisse@redhat.com>
Cc: Christoph Hellwig <hch@infradead.org>
Cc: Ralph Campbell <rcampbell@nvidia.com>

Reviewed-by: Jan Kara <jack@suse.cz>
Signed-off-by: John Hubbard <jhubbard@nvidia.com>
---
 include/linux/mm.h | 20 ++++++++++++
 mm/swap.c          | 80 ++++++++++++++++++++++++++++++++++++++++++++++
 2 files changed, 100 insertions(+)

diff --git a/include/linux/mm.h b/include/linux/mm.h
index 5411de93a363..09fbb2c81aba 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -963,6 +963,26 @@ static inline void put_page(struct page *page)
 		__put_page(page);
 }
 
+/*
+ * put_user_page() - release a page that had previously been acquired via
+ * a call to one of the get_user_pages*() functions.
+ *
+ * Pages that were pinned via get_user_pages*() must be released via
+ * either put_user_page(), or one of the put_user_pages*() routines
+ * below. This is so that eventually, pages that are pinned via
+ * get_user_pages*() can be separately tracked and uniquely handled. In
+ * particular, interactions with RDMA and filesystems need special
+ * handling.
+ */
+static inline void put_user_page(struct page *page)
+{
+	put_page(page);
+}
+
+void put_user_pages_dirty(struct page **pages, unsigned long npages);
+void put_user_pages_dirty_lock(struct page **pages, unsigned long npages);
+void put_user_pages(struct page **pages, unsigned long npages);
+
 #if defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)
 #define SECTION_IN_PAGE_FLAGS
 #endif
diff --git a/mm/swap.c b/mm/swap.c
index aa483719922e..bb8c32595e5f 100644
--- a/mm/swap.c
+++ b/mm/swap.c
@@ -133,6 +133,86 @@ void put_pages_list(struct list_head *pages)
 }
 EXPORT_SYMBOL(put_pages_list);
 
+typedef int (*set_dirty_func)(struct page *page);
+
+static void __put_user_pages_dirty(struct page **pages,
+				   unsigned long npages,
+				   set_dirty_func sdf)
+{
+	unsigned long index;
+
+	for (index = 0; index < npages; index++) {
+		struct page *page = compound_head(pages[index]);
+
+		if (!PageDirty(page))
+			sdf(page);
+
+		put_user_page(page);
+	}
+}
+
+/*
+ * put_user_pages_dirty() - for each page in the @pages array, make
+ * that page (or its head page, if a compound page) dirty, if it was
+ * previously listed as clean. Then, release the page using
+ * put_user_page().
+ *
+ * Please see the put_user_page() documentation for details.
+ *
+ * set_page_dirty(), which does not lock the page, is used here.
+ * Therefore, it is the caller's responsibility to ensure that this is
+ * safe. If not, then put_user_pages_dirty_lock() should be called instead.
+ *
+ * @pages:  array of pages to be marked dirty and released.
+ * @npages: number of pages in the @pages array.
+ *
+ */
+void put_user_pages_dirty(struct page **pages, unsigned long npages)
+{
+	__put_user_pages_dirty(pages, npages, set_page_dirty);
+}
+EXPORT_SYMBOL(put_user_pages_dirty);
+
+/*
+ * put_user_pages_dirty_lock() - for each page in the @pages array, make
+ * that page (or its head page, if a compound page) dirty, if it was
+ * previously listed as clean. Then, release the page using
+ * put_user_page().
+ *
+ * Please see the put_user_page() documentation for details.
+ *
+ * This is just like put_user_pages_dirty(), except that it invokes
+ * set_page_dirty_lock(), instead of set_page_dirty().
+ *
+ * @pages:  array of pages to be marked dirty and released.
+ * @npages: number of pages in the @pages array.
+ *
+ */
+void put_user_pages_dirty_lock(struct page **pages, unsigned long npages)
+{
+	__put_user_pages_dirty(pages, npages, set_page_dirty_lock);
+}
+EXPORT_SYMBOL(put_user_pages_dirty_lock);
+
+/*
+ * put_user_pages() - for each page in the @pages array, release the page
+ * using put_user_page().
+ *
+ * Please see the put_user_page() documentation for details.
+ *
+ * @pages:  array of pages to be marked dirty and released.
+ * @npages: number of pages in the @pages array.
+ *
+ */
+void put_user_pages(struct page **pages, unsigned long npages)
+{
+	unsigned long index;
+
+	for (index = 0; index < npages; index++)
+		put_user_page(pages[index]);
+}
+EXPORT_SYMBOL(put_user_pages);
+
 /*
  * get_kernel_pages() - pin kernel pages in memory
  * @kiov:	An array of struct kvec structures
-- 
2.19.1
