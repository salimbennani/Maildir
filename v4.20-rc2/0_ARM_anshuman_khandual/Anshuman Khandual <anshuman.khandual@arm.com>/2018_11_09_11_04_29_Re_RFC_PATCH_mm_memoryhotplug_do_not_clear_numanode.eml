Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from pop3.zju.edu.cn (61.164.42.155:110) by
  likexu-MOBL1.ccr.corp.intel.com with POP3; 10 Nov 2018 00:52:03 -0000
Received: from icoremail.net (unknown [209.85.214.171])
	by mail-app2 (Coremail) with SMTP id by_KCgD3_wsOauVb7EBnAQ--.31382S3;
	Fri, 09 Nov 2018 19:05:50 +0800 (CST)
Received: from mail-pl1-f171.google.com (unknown [209.85.214.171])
	by mx2.icoremail.net (Coremail) with SMTP id AQAAfwC3kT0MauVbezsiAA--.1731S3;
	Fri, 09 Nov 2018 19:05:48 +0800 (CST)
Received: by mail-pl1-f171.google.com with SMTP id p16-v6so800661plr.8
        for <xuliker@zju.edu.cn>; Fri, 09 Nov 2018 03:05:48 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-original-authentication-results:x-gm-message-state:delivered-to
         :subject:to:cc:references:from:message-id:date:user-agent
         :mime-version:in-reply-to:content-language:content-transfer-encoding
         :sender:precedence:list-id;
        bh=jjb5MsvE8JHevr1eLbhw7WqU20roCvzR5k0D7+uQ+ls=;
        b=HTnQjJoTTpC2yv8oIxZaHY9GOyO0Dn6cq1reMs/+g/F1fyFD/HQZpClKIdiSx0nq8X
         EqFKGz79lnZhwRM1x1UdcEMO6cGy+A5QTXDb5EUXLX/5C2B/EdLBDvn4lILYAEuePJt0
         /bnnb59PhIgoDKYTPTuN/Qpf7RxoKD3jkSN41CKCXOOizr7FHVMqKvFbX9pGhFLXcHef
         wEHmPzhMfJgwGB7YcJG1Wv44nBUg8i2ICeycaREMnSTmMiHAT0IqNYutEbMG8at5W6pV
         gX+pYQcmdGZjLIQrkt9PuX5XP9daI95Ewim1zWt3FubPOIpOkkGVyudNDnwc2ZyXeYuv
         7Vbw==
X-Original-Authentication-Results: mx.google.com;       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org
X-Gm-Message-State: AGRZ1gLtLrF+/t/VMfkMPO3/mBhm6yxFiQv7h96s1EZuIdPO6HctuV1N
	8UNpb1bck8twEV/Nk1arFyJMVSeu0JI81WT1H6/MpFntxc8Oc3Vezw==
X-Received: by 2002:a17:902:887:: with SMTP id 7-v6mr375998pll.283.1541761548470;
        Fri, 09 Nov 2018 03:05:48 -0800 (PST)
X-Forwarded-To: xuliker@zju.edu.cn
X-Forwarded-For: liker.xu@gmail.com xuliker@zju.edu.cn
Delivered-To: liker.xu@gmail.com
Received: by 2002:a17:90a:c304:0:0:0:0 with SMTP id g4-v6csp62507pjt;
        Fri, 9 Nov 2018 03:05:47 -0800 (PST)
X-Google-Smtp-Source: AJdET5dc1xl78tMaXdapgY4vuRkzZO1/nDtvDGvEgKo3A46nwAhKvZUNdQMomWz5Hc6xExmt+PZY
X-Received: by 2002:a63:5f95:: with SMTP id t143mr7129763pgb.395.1541761547462;
        Fri, 09 Nov 2018 03:05:47 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1541761547; cv=none;
        d=google.com; s=arc-20160816;
        b=MlbcilnyeJ4pkkj9jnucKHWffCxF4Q/le390PyRC+cC8TKxyEcTUSHjenlZ0TbZEg5
         J7gatZvdlLLHQJTkqB98Y+pOkxYKWs/Q0RQ1a/l9yllp5tcVZgiVRnBh7Au9w0DwP1qP
         zrajSiZ+oUPhLaxOqZ2Ef1WctAQIn+yC7qlJ1Y07hbNS6hWir8qW3xrSMVs8TqTT9OVt
         8yFUBqa/jw+ZGsjDvw9U4LE5B3wN5rGDli6ffJyLh5eUSSZBW1scq4eE8W6EM/1bQy2x
         W8LXOG3apXh1QokCWuE5PbPo0dwedmogKBCLy3TVKalIeFEBS4X+V4iA/JAYL92GZYqc
         q4jw==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=list-id:precedence:sender:content-transfer-encoding
         :content-language:in-reply-to:mime-version:user-agent:date
         :message-id:from:references:cc:to:subject;
        bh=jjb5MsvE8JHevr1eLbhw7WqU20roCvzR5k0D7+uQ+ls=;
        b=vXvCizHTcsQt+jd/oPEjWHCzUuQXIb/sNjYgbsykz8Sp8ML5mi0l8iXvhxl6oXRtyW
         XLj+JrcBsDLQl1eTgLErgrQlQIeZPoD1i3VlaXIh6bilQb4t+dCj9Po3Ox8o04sDJi8k
         EKD5MUWjWR1WFeLgqwO7RY9fGUNsKJbAfmGcjlA9JXK2nRZkRrP746Wk6u6kgE6uRHcP
         BxE7OF/DJq3LSwhVEsA87gl5N4qTrPbSlAwKarkm5XiueS5s1VP0XcYTDwALMV89dyoO
         F5fuIsgTN4tgJvp/FiYa+MZQr89X/8on21Pn8Gsg6ibz9FTjrqgezrC+Ma71EAHDMjDS
         Vxqw==
ARC-Authentication-Results: i=1; mx.google.com;
       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org
Received: from vger.kernel.org (vger.kernel.org. [209.132.180.67])
        by mx.google.com with ESMTP id o22-v6si7407497pfk.50.2018.11.09.03.05.13;
        Fri, 09 Nov 2018 03:05:47 -0800 (PST)
Received-SPF: pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) client-ip=209.132.180.67;
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1728170AbeKIUok (ORCPT <rfc822;changseok.bae@gmail.com>
        + 99 others); Fri, 9 Nov 2018 15:44:40 -0500
Received: from foss.arm.com ([217.140.101.70]:57276 "EHLO foss.arm.com"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1727532AbeKIUok (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Fri, 9 Nov 2018 15:44:40 -0500
Received: from usa-sjc-imap-foss1.foss.arm.com (unknown [10.72.51.249])
        by usa-sjc-mx-foss1.foss.arm.com (Postfix) with ESMTP id CE33AA78;
        Fri,  9 Nov 2018 03:04:33 -0800 (PST)
Received: from [10.162.0.72] (p8cg001049571a15.blr.arm.com [10.162.0.72])
        by usa-sjc-imap-foss1.foss.arm.com (Postfix) with ESMTPSA id C58013F718;
        Fri,  9 Nov 2018 03:04:31 -0800 (PST)
Subject: Re: [RFC PATCH] mm, memory_hotplug: do not clear numa_node
 association after hot_remove
To: Michal Hocko <mhocko@kernel.org>
Cc: linux-mm@kvack.org, Andrew Morton <akpm@linux-foundation.org>,
        Oscar Salvador <OSalvador@suse.com>,
        LKML <linux-kernel@vger.kernel.org>,
        Miroslav Benes <mbenes@suse.cz>,
        Vlastimil Babka <vbabka@suse.cz>
References: <20181108100413.966-1-mhocko@kernel.org>
 <20181108102917.GV27423@dhcp22.suse.cz>
 <048c04ae-7394-d03f-813e-42acdc965dd2@arm.com>
 <20181109075914.GD18390@dhcp22.suse.cz>
From: Anshuman Khandual <anshuman.khandual@arm.com>
Message-ID: <f9dd3dd0-3b20-446f-a131-70180fb733bf@arm.com>
Date: Fri, 9 Nov 2018 16:34:29 +0530
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko/20100101
 Thunderbird/52.9.1
MIME-Version: 1.0
In-Reply-To: <20181109075914.GD18390@dhcp22.suse.cz>
Content-Type: text/plain; charset=utf-8
Content-Language: en-US
Content-Transfer-Encoding: 7bit
Sender: liker.xu+caf_=xuliker=zju.edu.cn@gmail.com
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-CM-TRANSID: AQAAfwC3kT0MauVbezsiAA--.1731S3
Authentication-Results: mail-app2; spf=pass smtp.mail=liker.xu+caf_=xu
	liker=zju.edu.cn@gmail.com;
X-Coremail-Antispam: 1UD129KBjvJXoW3JFyUZw15WrWktF4fAryxKrg_yoWxGw4Upr
	WUXFWYkr4ktr18Cr10y345GryUKr1xAay7Xr1xWryUZFn8Gr1xJr17Jr4UCryDJr18uF17
	trs8Xw4Iqr15CaUanT9S1TB71UUUUUUqnTZGkaVYY2UrUUUU0bIjqfuFe4nvWSU8nxnvy2
	9KBjDU0xBIdaVrnRJUUUm2b7Iv0xC_Kw4lb4IE77IF4wAFF20E14v26r1j6r4UM7CIcVAF
	z4kK6r1j6r18M28lY4IEw2IIxxk0rwA2F7IY1VAKz4vEj48ve4kI8wA2z4x0Y4vE2Ix0cI
	8IcVAFwI0_Ar0_tr1l84ACjcxK6xIIjxv20xvEc7CjxVAFwI0_Cr0_Gr1UM28EF7xvwVC2
	z280aVAFwI0_GcCE3s1l84ACjcxK6I8E87Iv6xkF7I0E14v26rxl6s0DM2AIxVAIcxkEcV
	Aq07x20xvEncxIr21l5I8CrVACY4xI64kE6c02F40Ex7xfMcIj6xIIjxv20xvE14v26r10
	6r15McIj6I8E87Iv67AKxVW8JVWxJwAm72CE4IkC6x0Yz7v_Jr0_Gr1lF7xvr2IY64vIr4
	1l7I0Y6sxI4wCYjI0SjxkI62AI1cAE67vIY487MxkF7I0Ew4C26cxK6c8Ij28IcwCY1Ik2
	6cxK6xAEc7vF6xCjj44lc2xSY4AK6IIF6r4kMxkI7II2jI8vz4vEwIxGrwCYIxAIcVC0I7
	IYx2IY67AKxVW8JVW5JwCYIxAIcVC0I7IYx2IY6xkF7I0E14v26r4j6F4UMxvI42IY6I8E
	87Iv67AKxVW0oVCq3wCYIxAIcVC2z280aVCY1x0267AKxVW0oVCq3wCF04k20xvY0x0EwI
	xGrwCF04k20xvEw4C26cxK6c8Ij28IcwCF72vE77IF4wCFx2IqxVCFs4IE7xkEbVWUJVW8
	JwC20s026c02F40E14v26r1j6r18MI8I3I0E7480Y4vE14v26r106r1rMI8E67AF67kF1V
	AFwI0_Jw0_GFylIxkGc2Ij64vIr41lIxAIcVCF04k26cxKx2IYs7xG6r4j6FyUYxBIdaVF
	xhVjvjDU0xZFpf9x07bQqXdUUUUU=



On 11/09/2018 01:29 PM, Michal Hocko wrote:
> On Fri 09-11-18 09:12:09, Anshuman Khandual wrote:
>>
>>
>> On 11/08/2018 03:59 PM, Michal Hocko wrote:
>>> [Removing Wen Congyang and Tang Chen from the CC list because their
>>>  emails bounce. It seems that we will never learn about their motivation]
>>>
>>> On Thu 08-11-18 11:04:13, Michal Hocko wrote:
>>>> From: Michal Hocko <mhocko@suse.com>
>>>>
>>>> Per-cpu numa_node provides a default node for each possible cpu. The
>>>> association gets initialized during the boot when the architecture
>>>> specific code explores cpu->NUMA affinity. When the whole NUMA node is
>>>> removed though we are clearing this association
>>>>
>>>> try_offline_node
>>>>   check_and_unmap_cpu_on_node
>>>>     unmap_cpu_on_node
>>>>       numa_clear_node
>>>>         numa_set_node(cpu, NUMA_NO_NODE)
>>>>
>>>> This means that whoever calls cpu_to_node for a cpu associated with such
>>>> a node will get NUMA_NO_NODE. This is problematic for two reasons. First
>>>> it is fragile because __alloc_pages_node would simply blow up on an
>>>> out-of-bound access. We have encountered this when loading kvm module
>>>> BUG: unable to handle kernel paging request at 00000000000021c0
>>>> IP: [<ffffffff8119ccb3>] __alloc_pages_nodemask+0x93/0xb70
>>>> PGD 800000ffe853e067 PUD 7336bbc067 PMD 0
>>>> Oops: 0000 [#1] SMP
>>>> [...]
>>>> CPU: 88 PID: 1223749 Comm: modprobe Tainted: G        W          4.4.156-94.64-default #1
>>>> task: ffff88727eff1880 ti: ffff887354490000 task.ti: ffff887354490000
>>>> RIP: 0010:[<ffffffff8119ccb3>]  [<ffffffff8119ccb3>] __alloc_pages_nodemask+0x93/0xb70
>>>> RSP: 0018:ffff887354493b40  EFLAGS: 00010202
>>>> RAX: 00000000000021c0 RBX: 0000000000000000 RCX: 0000000000000000
>>>> RDX: 0000000000000000 RSI: 0000000000000002 RDI: 00000000014000c0
>>>> RBP: 00000000014000c0 R08: ffffffffffffffff R09: 0000000000000000
>>>> R10: ffff88fffc89e790 R11: 0000000000014000 R12: 0000000000000101
>>>> R13: ffffffffa0772cd4 R14: ffffffffa0769ac0 R15: 0000000000000000
>>>> FS:  00007fdf2f2f1700(0000) GS:ffff88fffc880000(0000) knlGS:0000000000000000
>>>> CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
>>>> CR2: 00000000000021c0 CR3: 00000077205ee000 CR4: 0000000000360670
>>>> DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
>>>> DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
>>>> Stack:
>>>>  0000000000000086 014000c014d20400 ffff887354493bb8 ffff882614d20f4c
>>>>  0000000000000000 0000000000000046 0000000000000046 ffffffff810ac0c9
>>>>  ffff88ffe78c0000 ffffffff0000009f ffffe8ffe82d3500 ffff88ff8ac55000
>>>> Call Trace:
>>>>  [<ffffffffa07476cd>] alloc_vmcs_cpu+0x3d/0x90 [kvm_intel]
>>>>  [<ffffffffa0772c0c>] hardware_setup+0x781/0x849 [kvm_intel]
>>>>  [<ffffffffa04a1c58>] kvm_arch_hardware_setup+0x28/0x190 [kvm]
>>>>  [<ffffffffa04856fc>] kvm_init+0x7c/0x2d0 [kvm]
>>>>  [<ffffffffa0772cf2>] vmx_init+0x1e/0x32c [kvm_intel]
>>>>  [<ffffffff8100213a>] do_one_initcall+0xca/0x1f0
>>>>  [<ffffffff81193886>] do_init_module+0x5a/0x1d7
>>>>  [<ffffffff81112083>] load_module+0x1393/0x1c90
>>>>  [<ffffffff81112b30>] SYSC_finit_module+0x70/0xa0
>>>>  [<ffffffff8161cbc3>] entry_SYSCALL_64_fastpath+0x1e/0xb7
>>>> DWARF2 unwinder stuck at entry_SYSCALL_64_fastpath+0x1e/0xb7
>>>>
>>>> on an older kernel but the code is basically the same in the current
>>>> Linus tree as well. alloc_vmcs_cpu could use alloc_pages_nodemask which
>>>> would recognize NUMA_NO_NODE and use alloc_pages_node which would translate
>>>> it to numa_mem_id but that is wrong as well because it would use a cpu
>>>> affinity of the local CPU which might be quite far from the original node.
>>
>> But then the original node is getting/already off-lined. The allocation is
>> going to come from a different node. alloc_pages_node() at least steer the
>> allocation alway from VM_BUG_ON() because of NUMA_NO_NODE by replacing it
>> with numa_mem_id().
>>
>> If node fallback order is important for this allocation then could not it
>> use __alloc_pages_nodemask() directly giving preference for its zonelist
>> node and nodemask. Just curious.
> 
> How does the caller get the right node to allocate from? We do have the
> proper zone list for the offline node so why not use it?
I get your point. NODE_DATA() for the off lined node is still around and
so does the proper zone list for allocation, so why the caller should work
around the problem by building it's preferred nodemask_t etc. No problem,
I was just curious.

> 
>>>> It is also reasonable to expect that cpu_to_node will provide a sane value
>>>> and there might be many more callers like that.
>>
>> AFAICS there are two choices here. Either mark them NUMA_NO_NODE for all
>> cpus of a node going offline or keep the existing mapping in case the node
>> comes back again.
> 
> Or update the mapping to the closeses node. I have chosen to keep the
> mapping because it is the easiest and the most natural one.

Agreed.

> 
>>>> The second problem is that __register_one_node relies on cpu_to_node
>>>> to properly associate cpus back to the node when it is onlined. We do
>>>> not want to lose that link as there is no arch independent way to get it
>>>> from the early boot time AFAICS.
>>
>> Retaining the links seems to be right unless unmap_cpu_on_node() is sort
>> of a weak callback letting arch to decide.
>>
>>>>
>>>> Drop the whole check_and_unmap_cpu_on_node machinery and keep the
>>>> association to fix both issues. The NODE_DATA(nid) is not deallocated
>> Though retaining the link is a problem in itself but the allocation related
>> crash could be solved by exploring __alloc_pages_nodemask() options.
> 
> Yes that is the case but this looks like a very fragile fix to me. If
> you are getting a node number from cpu_to_node then you shouldn't really
> think about obscurities like which allocation function to use, right?
> You should just get a valid node number.

Probably not worth it for the caller.

> 
> Do you see any problems with the patch as is?

No, this patch does remove an erroneous node-cpu map update which help solve
a real crash.
