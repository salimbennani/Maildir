Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from pop3.zju.edu.cn (61.164.42.155:110) by
  likexu-MOBL1.ccr.corp.intel.com with POP3; 10 Nov 2018 00:50:34 -0000
Received: from icoremail.net (unknown [209.85.214.179])
	by mail-app2 (Coremail) with SMTP id by_KCgDnX9+BPuVbkidmAQ--.31173S3;
	Fri, 09 Nov 2018 16:00:02 +0800 (CST)
Received: from mail-pl1-f179.google.com (unknown [209.85.214.179])
	by mx2.icoremail.net (Coremail) with SMTP id AQAAfwCHb0t_PuVbC2ghAA--.106S3;
	Fri, 09 Nov 2018 15:59:59 +0800 (CST)
Received: by mail-pl1-f179.google.com with SMTP id w24-v6so589929plq.3
        for <xuliker@zju.edu.cn>; Thu, 08 Nov 2018 23:59:59 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-original-authentication-results:x-gm-message-state:delivered-to
         :date:from:to:cc:subject:message-id:references:mime-version
         :content-disposition:in-reply-to:user-agent:sender:precedence
         :list-id;
        bh=5eaWJGqFz9Qi2XxscxT+skbzd+KU+Ck5KTYCAqeeADc=;
        b=mNpNKfLbZ++Xc35Avrwpp0eGQyoO3k/su77mcMtHCUWAVS0MJuI2+HgbzWzGdFIltO
         G7MH4bGx8mwWs7/Yz0Y7AtuTPDxDSHQFHSHH48UNk4wz/rEnjIz+NIrVtyxkfLblCGAR
         MyPcBX4yvPDINeD+t9A6FpjHjydFSxY3PYi65pzDYYv3/LrQNu7cTHPlYMCyVs0Coy2m
         noKoQTvIcjzjaxzcrdYPJ5PdXykFsCJ25TPo0Pj8Ej+5NHTbQpiD24AUwRFvcUFmK3AS
         kAbJn/9XgQimHfQeG9iMSMNKyaNRp+o8cTPtSIY4bYQD9WC7mp2DLWIlegOG1b8MC3zf
         nJVg==
X-Original-Authentication-Results: mx.google.com;       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org;       dmarc=fail (p=NONE sp=NONE dis=NONE) header.from=kernel.org
X-Gm-Message-State: AGRZ1gLB7I04iECbtg7+nhrjGpd5Oa1wIzO8U8I3TKS7QPBzqqY//Ztv
	qgE/gLipnkOGRhxiv83cR6QggmTSE/K9+G2bYFVtzzYYxxzoYF2Viw==
X-Received: by 2002:a17:902:166:: with SMTP id 93-v6mr7742013plb.68.1541750399043;
        Thu, 08 Nov 2018 23:59:59 -0800 (PST)
X-Forwarded-To: xuliker@zju.edu.cn
X-Forwarded-For: liker.xu@gmail.com xuliker@zju.edu.cn
Delivered-To: liker.xu@gmail.com
Received: by 2002:a17:90a:c304:0:0:0:0 with SMTP id g4-v6csp1221488pjt;
        Thu, 8 Nov 2018 23:59:58 -0800 (PST)
X-Google-Smtp-Source: AJdET5czdC2EX3Ny4Eks/6ZumNJCImh9CxxF0lopFG5iANhyun72IrqQOsfbkZ7A256HDqm5OwgW
X-Received: by 2002:a65:66ce:: with SMTP id c14mr6558668pgw.450.1541750398243;
        Thu, 08 Nov 2018 23:59:58 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1541750398; cv=none;
        d=google.com; s=arc-20160816;
        b=zrbo1ibIkZT9zdv62N4tmjOjWxoC//+OnH829Hj/SppJgf1xazL3q9J87bdR/1Bz8x
         3VClusnlaMufZI0nFvsXtnF3q/4WncWplKIg6lid2brVzhHqU2+O88Ypb95XMBoDXiNr
         hCiSohlrCv6alwn86wCjNKXFF3YZYSb3U0Wf3ahBLx+CPr4CCJQOQ2E2jxVE3K/BDaOn
         bA88lOVGHC9CTk1QXe46xfxpRHwWlXv5c/7EWavWCFP/rvkixY1ni3lbED5adV/2QKjV
         iP481PaKqrV4pXqONu2dnPtPym/qwHABiImkGuJqQ2qG/gAR9bd3y6Nxa5iFablacjhW
         qoBg==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=list-id:precedence:sender:user-agent:in-reply-to
         :content-disposition:mime-version:references:message-id:subject:cc
         :to:from:date;
        bh=5eaWJGqFz9Qi2XxscxT+skbzd+KU+Ck5KTYCAqeeADc=;
        b=wISmb2TCMsgk4tv9dAsdpTrysDfwu9z/G6okgX1eH/kBiwG/Q5iUYZWhAEcmnlK/Un
         RHUBJL++WFE41CRXHWjtlHAqL3JYIuBPMbuPRzuovwySA4XVpqldO0HjsPEDuUIdhx5p
         IoBV0UjcEqyiGD8y9Rx9alfzIK908epl8gR13VSRWMLtbbujlGdUK24Zc648eYVoDGvp
         i3VdiTLGCtuEKdLCLwXp8vUIE+A3BHqydgzeYS9yZax91DeOAQVrIXpppmpyVOT4YfRw
         ZRsqWqbXQ9qagjtwAs0goBun8ldG0jBNOMwPwOubp2ghEAZMoaYA2t0hFYBaIbJcRYeK
         p5HQ==
ARC-Authentication-Results: i=1; mx.google.com;
       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org;
       dmarc=fail (p=NONE sp=NONE dis=NONE) header.from=kernel.org
Received: from vger.kernel.org (vger.kernel.org. [209.132.180.67])
        by mx.google.com with ESMTP id gn20si2469869plb.273.2018.11.08.23.59.43;
        Thu, 08 Nov 2018 23:59:58 -0800 (PST)
Received-SPF: pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) client-ip=209.132.180.67;
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1728378AbeKIRin (ORCPT <rfc822;changseok.bae@gmail.com>
        + 99 others); Fri, 9 Nov 2018 12:38:43 -0500
Received: from mx2.suse.de ([195.135.220.15]:40748 "EHLO mx1.suse.de"
        rhost-flags-OK-OK-OK-FAIL) by vger.kernel.org with ESMTP
        id S1728088AbeKIRin (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Fri, 9 Nov 2018 12:38:43 -0500
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay2.suse.de (unknown [195.135.220.254])
        by mx1.suse.de (Postfix) with ESMTP id F1CCDACE5;
        Fri,  9 Nov 2018 07:59:16 +0000 (UTC)
Date: Fri, 9 Nov 2018 08:59:14 +0100
From: Michal Hocko <mhocko@kernel.org>
To: Anshuman Khandual <anshuman.khandual@arm.com>
Cc: linux-mm@kvack.org, Andrew Morton <akpm@linux-foundation.org>,
        Oscar Salvador <OSalvador@suse.com>,
        LKML <linux-kernel@vger.kernel.org>,
        Miroslav Benes <mbenes@suse.cz>,
        Vlastimil Babka <vbabka@suse.cz>
Subject: Re: [RFC PATCH] mm, memory_hotplug: do not clear numa_node
 association after hot_remove
Message-ID: <20181109075914.GD18390@dhcp22.suse.cz>
References: <20181108100413.966-1-mhocko@kernel.org>
 <20181108102917.GV27423@dhcp22.suse.cz>
 <048c04ae-7394-d03f-813e-42acdc965dd2@arm.com>
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: <048c04ae-7394-d03f-813e-42acdc965dd2@arm.com>
User-Agent: Mutt/1.10.1 (2018-07-13)
Sender: liker.xu+caf_=xuliker=zju.edu.cn@gmail.com
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-CM-TRANSID: AQAAfwCHb0t_PuVbC2ghAA--.106S3
Authentication-Results: mail-app2; spf=pass smtp.mail=liker.xu+caf_=xu
	liker=zju.edu.cn@gmail.com;
X-Coremail-Antispam: 1UD129KBjvJXoWxtw4kXw4xZr4UCFW3Ary5Arb_yoW7uF4rpr
	WjqFWYkr4ktr18Cr18t3s8tryUKr4xAay7Xr1Sgry8ZFn8Gw1xtr1UJr4UCryDJr18ZF17
	trs0qw4Igr15CaUanT9S1TB71UUUUUUqnTZGkaVYY2UrUUUU0bIjqfuFe4nvWSU8nxnvy2
	9KBjDU0xBIdaVrnRJUUUBEb7Iv0xC_Kw4lb4IE77IF4wAFF20E14v26r1j6r4UM7CIcVAF
	z4kK6r1j6r18M28lY4IEw2IIxxk0rwA2F7IY1VAKz4vEj48ve4kI8wA2z4x0Y4vE2Ix0cI
	8IcVAFwI0_Ar0_tr1l84ACjcxK6xIIjxv20xvEc7CjxVAFwI0_Gr0_Cr1l84ACjcxK6I8E
	87Iv67AKxVW0oVCq3wA2z4x0Y4vEx4A2jsIEc7CjxVAFwI0_GcCE3s1le2I262IYc4CY6c
	8Ij28IcVAaY2xG8wAqx4xG64xvF2IEw4CE5I8CrVC2j2WlYx0E2Ix0cI8IcVAFwI0_Jr0_
	Jr4lYx0Ex4A2jsIE14v26r4j6F4UMcvjeVCFs4IE7xkEbVWUJVW8JwACjcxG0xvEwIxGrw
	CjxxvEa2IrMxkF7I0Ew4C26cxK6c8Ij28IcwCY1Ik26cxK6xAEc7vF6xCjj44lc2xSY4AK
	6IIF6r43MxkI7II2jI8vz4vEwIxGrwCYIxAIcVC0I7IYx2IY67AKxVW8JVW5JwCYIxAIcV
	C0I7IYx2IY6xkF7I0E14v26r4j6F4UMxvI42IY6I8E87Iv67AKxVW0oVCq3wCYIxAIcVC2
	z280aVCY1x0267AKxVW0oVCq3wCF04k20xvY0x0EwIxGrwCF04k20xvEw4C26cxK6c8Ij2
	8IcwCF72vE77IF4wCFx2IqxVCFs4IE7xkEbVWUJVW8JwC20s026c02F40E14v26r1j6r18
	MI8I3I0E7480Y4vE14v26r106r1rMI8E67AF67kF1VAFwI0_Jw0_GFylIxkGc2Ij64vIr4
	1lIxAIcVCF04k26cxKx2IYs7xG6r4j6FyUYxBIdaVFxhVjvjDU0xZFpf9x07bpMa5UUUUU
	=

On Fri 09-11-18 09:12:09, Anshuman Khandual wrote:
> 
> 
> On 11/08/2018 03:59 PM, Michal Hocko wrote:
> > [Removing Wen Congyang and Tang Chen from the CC list because their
> >  emails bounce. It seems that we will never learn about their motivation]
> > 
> > On Thu 08-11-18 11:04:13, Michal Hocko wrote:
> >> From: Michal Hocko <mhocko@suse.com>
> >>
> >> Per-cpu numa_node provides a default node for each possible cpu. The
> >> association gets initialized during the boot when the architecture
> >> specific code explores cpu->NUMA affinity. When the whole NUMA node is
> >> removed though we are clearing this association
> >>
> >> try_offline_node
> >>   check_and_unmap_cpu_on_node
> >>     unmap_cpu_on_node
> >>       numa_clear_node
> >>         numa_set_node(cpu, NUMA_NO_NODE)
> >>
> >> This means that whoever calls cpu_to_node for a cpu associated with such
> >> a node will get NUMA_NO_NODE. This is problematic for two reasons. First
> >> it is fragile because __alloc_pages_node would simply blow up on an
> >> out-of-bound access. We have encountered this when loading kvm module
> >> BUG: unable to handle kernel paging request at 00000000000021c0
> >> IP: [<ffffffff8119ccb3>] __alloc_pages_nodemask+0x93/0xb70
> >> PGD 800000ffe853e067 PUD 7336bbc067 PMD 0
> >> Oops: 0000 [#1] SMP
> >> [...]
> >> CPU: 88 PID: 1223749 Comm: modprobe Tainted: G        W          4.4.156-94.64-default #1
> >> task: ffff88727eff1880 ti: ffff887354490000 task.ti: ffff887354490000
> >> RIP: 0010:[<ffffffff8119ccb3>]  [<ffffffff8119ccb3>] __alloc_pages_nodemask+0x93/0xb70
> >> RSP: 0018:ffff887354493b40  EFLAGS: 00010202
> >> RAX: 00000000000021c0 RBX: 0000000000000000 RCX: 0000000000000000
> >> RDX: 0000000000000000 RSI: 0000000000000002 RDI: 00000000014000c0
> >> RBP: 00000000014000c0 R08: ffffffffffffffff R09: 0000000000000000
> >> R10: ffff88fffc89e790 R11: 0000000000014000 R12: 0000000000000101
> >> R13: ffffffffa0772cd4 R14: ffffffffa0769ac0 R15: 0000000000000000
> >> FS:  00007fdf2f2f1700(0000) GS:ffff88fffc880000(0000) knlGS:0000000000000000
> >> CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
> >> CR2: 00000000000021c0 CR3: 00000077205ee000 CR4: 0000000000360670
> >> DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
> >> DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
> >> Stack:
> >>  0000000000000086 014000c014d20400 ffff887354493bb8 ffff882614d20f4c
> >>  0000000000000000 0000000000000046 0000000000000046 ffffffff810ac0c9
> >>  ffff88ffe78c0000 ffffffff0000009f ffffe8ffe82d3500 ffff88ff8ac55000
> >> Call Trace:
> >>  [<ffffffffa07476cd>] alloc_vmcs_cpu+0x3d/0x90 [kvm_intel]
> >>  [<ffffffffa0772c0c>] hardware_setup+0x781/0x849 [kvm_intel]
> >>  [<ffffffffa04a1c58>] kvm_arch_hardware_setup+0x28/0x190 [kvm]
> >>  [<ffffffffa04856fc>] kvm_init+0x7c/0x2d0 [kvm]
> >>  [<ffffffffa0772cf2>] vmx_init+0x1e/0x32c [kvm_intel]
> >>  [<ffffffff8100213a>] do_one_initcall+0xca/0x1f0
> >>  [<ffffffff81193886>] do_init_module+0x5a/0x1d7
> >>  [<ffffffff81112083>] load_module+0x1393/0x1c90
> >>  [<ffffffff81112b30>] SYSC_finit_module+0x70/0xa0
> >>  [<ffffffff8161cbc3>] entry_SYSCALL_64_fastpath+0x1e/0xb7
> >> DWARF2 unwinder stuck at entry_SYSCALL_64_fastpath+0x1e/0xb7
> >>
> >> on an older kernel but the code is basically the same in the current
> >> Linus tree as well. alloc_vmcs_cpu could use alloc_pages_nodemask which
> >> would recognize NUMA_NO_NODE and use alloc_pages_node which would translate
> >> it to numa_mem_id but that is wrong as well because it would use a cpu
> >> affinity of the local CPU which might be quite far from the original node.
> 
> But then the original node is getting/already off-lined. The allocation is
> going to come from a different node. alloc_pages_node() at least steer the
> allocation alway from VM_BUG_ON() because of NUMA_NO_NODE by replacing it
> with numa_mem_id().
> 
> If node fallback order is important for this allocation then could not it
> use __alloc_pages_nodemask() directly giving preference for its zonelist
> node and nodemask. Just curious.

How does the caller get the right node to allocate from? We do have the
proper zone list for the offline node so why not use it?

> >> It is also reasonable to expect that cpu_to_node will provide a sane value
> >> and there might be many more callers like that.
> 
> AFAICS there are two choices here. Either mark them NUMA_NO_NODE for all
> cpus of a node going offline or keep the existing mapping in case the node
> comes back again.

Or update the mapping to the closeses node. I have chosen to keep the
mapping because it is the easiest and the most natural one.

> >> The second problem is that __register_one_node relies on cpu_to_node
> >> to properly associate cpus back to the node when it is onlined. We do
> >> not want to lose that link as there is no arch independent way to get it
> >> from the early boot time AFAICS.
> 
> Retaining the links seems to be right unless unmap_cpu_on_node() is sort
> of a weak callback letting arch to decide.
> 
> >>
> >> Drop the whole check_and_unmap_cpu_on_node machinery and keep the
> >> association to fix both issues. The NODE_DATA(nid) is not deallocated
> Though retaining the link is a problem in itself but the allocation related
> crash could be solved by exploring __alloc_pages_nodemask() options.

Yes that is the case but this looks like a very fragile fix to me. If
you are getting a node number from cpu_to_node then you shouldn't really
think about obscurities like which allocation function to use, right?
You should just get a valid node number.

Do you see any problems with the patch as is?
-- 
Michal Hocko
SUSE Labs
