Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from pop3.zju.edu.cn (124.160.105.205:110) by
  likexu-MOBL1.ccr.corp.intel.com with POP3; 09 Nov 2018 05:30:50 -0000
Received: from icoremail.net (unknown [209.85.214.182])
	by mail-app2 (Coremail) with SMTP id by_KCgDHHyZBAuVbPrRkAQ--.31001S3;
	Fri, 09 Nov 2018 11:42:58 +0800 (CST)
Received: from mail-pl1-f182.google.com (unknown [209.85.214.182])
	by mx2.icoremail.net (Coremail) with SMTP id AQAAfwB38Ew_AuVbt1AgAA--.9032S3;
	Fri, 09 Nov 2018 11:42:55 +0800 (CST)
Received: by mail-pl1-f182.google.com with SMTP id w22-v6so305708plk.0
        for <xuliker@zju.edu.cn>; Thu, 08 Nov 2018 19:42:55 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-original-authentication-results:x-gm-message-state:delivered-to
         :subject:to:cc:references:from:message-id:date:user-agent
         :mime-version:in-reply-to:content-language:content-transfer-encoding
         :sender:precedence:list-id;
        bh=x3RBPynOnpN7RnQ4S44xVs5HneX2TTEMNMGxBoZ2FYQ=;
        b=BsD2sGcpF/9LAGm5iMfksx9TRyj4Hu4+i5VaGgFS8/OEX+AXy9IA51z/4PYgUL51x8
         0mLTKVhVXlUxFRPuR422JRvzOp4LIbfmPF59KP6sQIPClupxMMjrATD6RjX8mwFD8mvv
         qC1HilkLqm1AWZsqtzR8utfYZi1lieHweQ2gOXOdbxLPG+U6oylrU5Mt8xxoOopT7zSF
         2p2+r7HZeeEaaEmkh943n5VcE85976C6vq+5N1gKV6iQ4iTLPsh2vjCWbYaAkULEm+kp
         BZAHCPTn3qlCw0PP/yuPLn5FgJ6NzCwF6hX/mL3GESeQwrDUbcWZHx/yZE0YkwO6La5A
         L5Sg==
X-Original-Authentication-Results: mx.google.com;       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org
X-Gm-Message-State: AGRZ1gIBOHZt5iL0/3tLQn7H+JPZxnM+m8lJkS1NV2W4yV3joMSonjSX
	v+PwAi0CaOBy/qEuuiFQmS0K+BaiSDGHFM4GmEl9SMrDbh0g2kNhkg==
X-Received: by 2002:a17:902:560f:: with SMTP id h15-v6mr4080536pli.160.1541734975407;
        Thu, 08 Nov 2018 19:42:55 -0800 (PST)
X-Forwarded-To: xuliker@zju.edu.cn
X-Forwarded-For: liker.xu@gmail.com xuliker@zju.edu.cn
Delivered-To: liker.xu@gmail.com
Received: by 2002:a17:90a:c304:0:0:0:0 with SMTP id g4-v6csp1042320pjt;
        Thu, 8 Nov 2018 19:42:54 -0800 (PST)
X-Google-Smtp-Source: AJdET5ckxdufajtyE13j0gYykDLSLh9LYs55FOKGOOCHVDmMUSei85FPFRmxS7VcnQlW86fxwM/m
X-Received: by 2002:a63:4745:: with SMTP id w5mr6210154pgk.377.1541734974532;
        Thu, 08 Nov 2018 19:42:54 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1541734974; cv=none;
        d=google.com; s=arc-20160816;
        b=yaq8Oiub+gj8tGlZFGuF6KTjyo+n8Nhn6ilk+yoRn3i2fR6FWBFy7j2ZPXLQhFOOzP
         +paEV99oF16CQRMfdw+IImGCu6nylhg/jPwhuFB74KlSpD2jHrxx5fiUCKzCpBxOHnoe
         aPSGcuJvIiW+sBwIPxWXqh53ik1U+4Ly4AEnRGdUht1Gx7r3WqMgCifcCK/UB2++KRis
         EKyBkuP22tOOrN+A+BVMHHCKJJqyvyVVQf9r1SYH6wZn4TK4OfK5Rfo8bXa61xEMI3fI
         ErUcgM3RHoJMMc/6eAG3fUagcxBEK/r+7cU2WJldktLEA6ErnWOxOT1mPivdVx2Xyn7q
         JfGg==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=list-id:precedence:sender:content-transfer-encoding
         :content-language:in-reply-to:mime-version:user-agent:date
         :message-id:from:references:cc:to:subject;
        bh=x3RBPynOnpN7RnQ4S44xVs5HneX2TTEMNMGxBoZ2FYQ=;
        b=E7n/vXkxyVo7//Hki41JuOGg1VVnOPx3ag6jVbFuQ4y6envTYt7e/WTNzgU4qb7Ky0
         6/iH20CFIVJ/s3aaGu6gqlsEkbq2PEW9E81jxPhyQff8P6vXXlBdDb2CJ64UtoX7SHs6
         tpe9po/e3tL4raiJ0dfs0uMk4DznmcJ4PVJ7ZGKC12KIfvBLX0zuMh1zW2vMLo4f2Tpe
         PNhlGYMbFXFtM+qp/StzRWP/bXC8io1riS/4mSoR+43I2NfEwBseHDakNAtLTa4RH7Hd
         xnh1vJQ1VJsDLLm15oZNWwXW4HDpPNXD1r214XyLUCNaE629VD9PMsIcmwxoYPV5lgtL
         K6Vg==
ARC-Authentication-Results: i=1; mx.google.com;
       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org
Received: from vger.kernel.org (vger.kernel.org. [209.132.180.67])
        by mx.google.com with ESMTP id r3-v6si5797094pgr.252.2018.11.08.19.42.39;
        Thu, 08 Nov 2018 19:42:54 -0800 (PST)
Received-SPF: pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) client-ip=209.132.180.67;
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1727736AbeKINUz (ORCPT <rfc822;tu.k.phung@gmail.com>
        + 99 others); Fri, 9 Nov 2018 08:20:55 -0500
Received: from foss.arm.com ([217.140.101.70]:52840 "EHLO foss.arm.com"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S1727391AbeKINUz (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Fri, 9 Nov 2018 08:20:55 -0500
Received: from usa-sjc-imap-foss1.foss.arm.com (unknown [10.72.51.249])
        by usa-sjc-mx-foss1.foss.arm.com (Postfix) with ESMTP id 66F5EEBD;
        Thu,  8 Nov 2018 19:42:14 -0800 (PST)
Received: from [10.162.0.72] (p8cg001049571a15.blr.arm.com [10.162.0.72])
        by usa-sjc-imap-foss1.foss.arm.com (Postfix) with ESMTPSA id 5BA0E3F5CF;
        Thu,  8 Nov 2018 19:42:12 -0800 (PST)
Subject: Re: [RFC PATCH] mm, memory_hotplug: do not clear numa_node
 association after hot_remove
To: Michal Hocko <mhocko@kernel.org>, linux-mm@kvack.org
Cc: Andrew Morton <akpm@linux-foundation.org>,
        Oscar Salvador <OSalvador@suse.com>,
        LKML <linux-kernel@vger.kernel.org>,
        Miroslav Benes <mbenes@suse.cz>,
        Vlastimil Babka <vbabka@suse.cz>
References: <20181108100413.966-1-mhocko@kernel.org>
 <20181108102917.GV27423@dhcp22.suse.cz>
From: Anshuman Khandual <anshuman.khandual@arm.com>
Message-ID: <048c04ae-7394-d03f-813e-42acdc965dd2@arm.com>
Date: Fri, 9 Nov 2018 09:12:09 +0530
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko/20100101
 Thunderbird/52.9.1
MIME-Version: 1.0
In-Reply-To: <20181108102917.GV27423@dhcp22.suse.cz>
Content-Type: text/plain; charset=utf-8
Content-Language: en-US
Content-Transfer-Encoding: 7bit
Sender: liker.xu+caf_=xuliker=zju.edu.cn@gmail.com
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-CM-TRANSID: AQAAfwB38Ew_AuVbt1AgAA--.9032S3
Authentication-Results: mail-app2; spf=pass smtp.mail=liker.xu+caf_=xu
	liker=zju.edu.cn@gmail.com;
X-Coremail-Antispam: 1UD129KBjvJXoW3WFWDuF1kJw15Wry3Cr15Arb_yoW7Xw1Dpr
	WjgrW5Cr4ktr1UCr18t345Gry8Gr4xAay3Jr1xWrykZF15Gr1xtr1UJr4UJryDJr4rXF17
	trs8ta1Igr15CaUanT9S1TB71UUUUUUqnTZGkaVYY2UrUUUU0bIjqfuFe4nvWSU8nxnvy2
	9KBjDU0xBIdaVrnRJUUUB2b7Iv0xC_Kw4lb4IE77IF4wAFF20E14v26r1j6r4UM7CIcVAF
	z4kK6r1j6r18M28lY4IEw2IIxxk0rwA2F7IY1VAKz4vEj48ve4kI8wA2z4x0Y4vE2Ix0cI
	8IcVAFwI0_Xr0_Ar1l84ACjcxK6xIIjxv20xvEc7CjxVAFwI0_Gr0_Cr1l84ACjcxK6I8E
	87Iv67AKxVW0oVCq3wA2z4x0Y4vEx4A2jsIEc7CjxVAFwI0_GcCE3s1le2I262IYc4CY6c
	8Ij28IcVAaY2xG8wAqx4xG64xvF2IEw4CE5I8CrVC2j2WlYx0E2Ix0cI8IcVAFwI0_Jr0_
	Jr4lYx0Ex4A2jsIE14v26r4j6F4UMcvjeVCFs4IE7xkEbVWUJVW8JwACjcxG0xvEwIxGrw
	CjxxvEa2IrMxk0xIA0c2IEe2xFo4CEbIxvr21lc7CjxVAKzI0EY4vE52x082I5MxkI7II2
	jI8vz4vEwIxGrwCYIxAIcVC0I7IYx2IY67AKxVWUJVWUCwCYIxAIcVC0I7IYx2IY6xkF7I
	0E14v26r1j6r4UMxvI42IY6I8E87Iv67AKxVW0oVCq3wCYIxAIcVC2z280aVCY1x0267AK
	xVW0oVCq3wCF04k20xvY0x0EwIxGrwCF04k20xvEw4C26cxK6c8Ij28IcwCF72vE77IF4w
	CFx2IqxVCFs4IE7xkEbVWUJVW8JwC20s026c02F40E14v26r1j6r18MI8I3I0E7480Y4vE
	14v26r106r1rMI8E67AF67kF1VAFwI0_Jw0_GFylIxkGc2Ij64vIr41lIxAIcVCF04k26c
	xKx2IYs7xG6Fyj6rWUJbIYCTnIWIevJa73UjIFyTuYvjxUBSfODUUUU



On 11/08/2018 03:59 PM, Michal Hocko wrote:
> [Removing Wen Congyang and Tang Chen from the CC list because their
>  emails bounce. It seems that we will never learn about their motivation]
> 
> On Thu 08-11-18 11:04:13, Michal Hocko wrote:
>> From: Michal Hocko <mhocko@suse.com>
>>
>> Per-cpu numa_node provides a default node for each possible cpu. The
>> association gets initialized during the boot when the architecture
>> specific code explores cpu->NUMA affinity. When the whole NUMA node is
>> removed though we are clearing this association
>>
>> try_offline_node
>>   check_and_unmap_cpu_on_node
>>     unmap_cpu_on_node
>>       numa_clear_node
>>         numa_set_node(cpu, NUMA_NO_NODE)
>>
>> This means that whoever calls cpu_to_node for a cpu associated with such
>> a node will get NUMA_NO_NODE. This is problematic for two reasons. First
>> it is fragile because __alloc_pages_node would simply blow up on an
>> out-of-bound access. We have encountered this when loading kvm module
>> BUG: unable to handle kernel paging request at 00000000000021c0
>> IP: [<ffffffff8119ccb3>] __alloc_pages_nodemask+0x93/0xb70
>> PGD 800000ffe853e067 PUD 7336bbc067 PMD 0
>> Oops: 0000 [#1] SMP
>> [...]
>> CPU: 88 PID: 1223749 Comm: modprobe Tainted: G        W          4.4.156-94.64-default #1
>> task: ffff88727eff1880 ti: ffff887354490000 task.ti: ffff887354490000
>> RIP: 0010:[<ffffffff8119ccb3>]  [<ffffffff8119ccb3>] __alloc_pages_nodemask+0x93/0xb70
>> RSP: 0018:ffff887354493b40  EFLAGS: 00010202
>> RAX: 00000000000021c0 RBX: 0000000000000000 RCX: 0000000000000000
>> RDX: 0000000000000000 RSI: 0000000000000002 RDI: 00000000014000c0
>> RBP: 00000000014000c0 R08: ffffffffffffffff R09: 0000000000000000
>> R10: ffff88fffc89e790 R11: 0000000000014000 R12: 0000000000000101
>> R13: ffffffffa0772cd4 R14: ffffffffa0769ac0 R15: 0000000000000000
>> FS:  00007fdf2f2f1700(0000) GS:ffff88fffc880000(0000) knlGS:0000000000000000
>> CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
>> CR2: 00000000000021c0 CR3: 00000077205ee000 CR4: 0000000000360670
>> DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
>> DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
>> Stack:
>>  0000000000000086 014000c014d20400 ffff887354493bb8 ffff882614d20f4c
>>  0000000000000000 0000000000000046 0000000000000046 ffffffff810ac0c9
>>  ffff88ffe78c0000 ffffffff0000009f ffffe8ffe82d3500 ffff88ff8ac55000
>> Call Trace:
>>  [<ffffffffa07476cd>] alloc_vmcs_cpu+0x3d/0x90 [kvm_intel]
>>  [<ffffffffa0772c0c>] hardware_setup+0x781/0x849 [kvm_intel]
>>  [<ffffffffa04a1c58>] kvm_arch_hardware_setup+0x28/0x190 [kvm]
>>  [<ffffffffa04856fc>] kvm_init+0x7c/0x2d0 [kvm]
>>  [<ffffffffa0772cf2>] vmx_init+0x1e/0x32c [kvm_intel]
>>  [<ffffffff8100213a>] do_one_initcall+0xca/0x1f0
>>  [<ffffffff81193886>] do_init_module+0x5a/0x1d7
>>  [<ffffffff81112083>] load_module+0x1393/0x1c90
>>  [<ffffffff81112b30>] SYSC_finit_module+0x70/0xa0
>>  [<ffffffff8161cbc3>] entry_SYSCALL_64_fastpath+0x1e/0xb7
>> DWARF2 unwinder stuck at entry_SYSCALL_64_fastpath+0x1e/0xb7
>>
>> on an older kernel but the code is basically the same in the current
>> Linus tree as well. alloc_vmcs_cpu could use alloc_pages_nodemask which
>> would recognize NUMA_NO_NODE and use alloc_pages_node which would translate
>> it to numa_mem_id but that is wrong as well because it would use a cpu
>> affinity of the local CPU which might be quite far from the original node.

But then the original node is getting/already off-lined. The allocation is
going to come from a different node. alloc_pages_node() at least steer the
allocation alway from VM_BUG_ON() because of NUMA_NO_NODE by replacing it
with numa_mem_id().

If node fallback order is important for this allocation then could not it
use __alloc_pages_nodemask() directly giving preference for its zonelist
node and nodemask. Just curious.

>> It is also reasonable to expect that cpu_to_node will provide a sane value
>> and there might be many more callers like that.

AFAICS there are two choices here. Either mark them NUMA_NO_NODE for all
cpus of a node going offline or keep the existing mapping in case the node
comes back again.

>>
>> The second problem is that __register_one_node relies on cpu_to_node
>> to properly associate cpus back to the node when it is onlined. We do
>> not want to lose that link as there is no arch independent way to get it
>> from the early boot time AFAICS.

Retaining the links seems to be right unless unmap_cpu_on_node() is sort
of a weak callback letting arch to decide.

>>
>> Drop the whole check_and_unmap_cpu_on_node machinery and keep the
>> association to fix both issues. The NODE_DATA(nid) is not deallocated
Though retaining the link is a problem in itself but the allocation related
crash could be solved by exploring __alloc_pages_nodemask() options.

>> so it will stay in place and if anybody wants to allocate from that node
>> then a fallback node will be used.

Right, NODE_DATA(nid) is an advantage of retaining the link.
