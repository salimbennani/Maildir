Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from pop3.zju.edu.cn (61.164.42.155:110) by
  likexu-MOBL1.ccr.corp.intel.com with POP3; 11 Nov 2018 15:02:57 -0000
Received: from icoremail.net (unknown [209.85.214.177])
	by mail-app2 (Coremail) with SMTP id by_KCgDHH7bU2+ZbyAxvAQ--.33974S3;
	Sat, 10 Nov 2018 21:23:33 +0800 (CST)
Received: from mail-pl1-f177.google.com (unknown [209.85.214.177])
	by mx2.icoremail.net (Coremail) with SMTP id AQAAfwBXNUHS2+Zbtv4mAA--.12829S3;
	Sat, 10 Nov 2018 21:23:30 +0800 (CST)
Received: by mail-pl1-f177.google.com with SMTP id n4-v6so2205362plp.2
        for <xuliker@zju.edu.cn>; Sat, 10 Nov 2018 05:23:30 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-original-authentication-results:x-gm-message-state:delivered-to
         :date:from:to:cc:subject:message-id:references:mime-version
         :content-disposition:in-reply-to:user-agent:sender:precedence
         :list-id;
        bh=xE5V8mLZIBCRM6qUY3vRgN2EMQYXAEAXzWzJk7Xe7e4=;
        b=fzYrQkxX2J5WwgHwuqH0OhduBXvt5PohiboysXkGrfbakzG79l1/iNiJi+WhQWcCQr
         R/kzh9herafqUkZOaPWhWrFAmpXAUG1DTxxkEf0DCv2Bi+H8yAr+bz8Y9ulSy7NHkaYo
         k97r8L6Rdr4jYCxSgdfIHg0jXm7kmiYWn0fzGUhuUEUp57qrLSv6aBcaXkyycnvu2Tmw
         quhjGrB/lcdbomhC0IS9Q/DAURZF76TMW2cIi/KyB6h5xOc/k0GX6OEzi5rpZdyLvDad
         544+/ftbAj/kJ6+3c978hz+MMWLllv0+Ljd9bohFGXOK4ojn876k6B7yo0WqriMgLhzJ
         ACpQ==
X-Original-Authentication-Results: mx.google.com;       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org
X-Gm-Message-State: AGRZ1gLBbztfJuW6Scy2SH/2YtmrT4+1JLsZ9QoBGTuHAx3u/fP82le/
	IV1f6XwbBnOGjt9Z+iz0d/XaPWsamoKWVyRVDOArfouoJBULsCs=
X-Received: by 2002:a17:902:6686:: with SMTP id e6-v6mr12747566plk.173.1541856209763;
        Sat, 10 Nov 2018 05:23:29 -0800 (PST)
X-Forwarded-To: xuliker@zju.edu.cn
X-Forwarded-For: liker.xu@gmail.com xuliker@zju.edu.cn
Delivered-To: liker.xu@gmail.com
Received: by 2002:a17:90a:c304:0:0:0:0 with SMTP id g4-v6csp1099290pjt;
        Sat, 10 Nov 2018 05:23:28 -0800 (PST)
X-Google-Smtp-Source: AJdET5ebGmqzbkXY+E8VXcwq0er3yKHNisU+uzx3Y0CIxod8A/VhFWCrb6H4JjIA7bYl0x3BrjvV
X-Received: by 2002:a17:902:9043:: with SMTP id w3-v6mr12425134plz.32.1541856208449;
        Sat, 10 Nov 2018 05:23:28 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1541856208; cv=none;
        d=google.com; s=arc-20160816;
        b=rUwZGt759fwN01WOiYDo/d05D8runQCbMz7qHowpcE6jW90KFEAKIpSKCOwE6WtIpF
         vyE9qkVjHtVr0PuvVss+0RybYzZjOrnajJ96Ceaqik4z1ivKhR9foF/ZyQJxaA9b50oR
         1MzeYYJDCd6ivaHiyhjhVWgcjxAMQyiKdalKaAVtYv9xvS7hHn7KtgMEZRstHpdMSbgM
         iqkNRoFbyo8pXk+fih4zcZFXG37PuY90ciMmqpnnrzToSj1FYa8598VBQwpGeinwarSL
         Z/dcwiEFKsHeHsjlx0HGUXa9N++UFeyiRH/7EXkiM5guO5+5vdPz2rLlDXw90k1ghvih
         FjSg==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=list-id:precedence:sender:user-agent:in-reply-to
         :content-disposition:mime-version:references:message-id:subject:cc
         :to:from:date;
        bh=xE5V8mLZIBCRM6qUY3vRgN2EMQYXAEAXzWzJk7Xe7e4=;
        b=Z6TIoBfyy/CRcMuCwH7sDf2YzQZHXsx4RakvxiC519R/nvtCiz/MNZz8G++XqsOjiF
         WnF68h6BvvURwuBKZqFTa7SMx8fowfmGYephhVCrOTaADTlLrtOxNSYtFqyGTj74oS7X
         txJbdJx/Pgkb9ZaVoEAtjga1RU015R6qklQaTJBb+YiL+4N19z2SVHHs5l7H4de39lqz
         feuas7aSNajZEAgxhvK5z2H4gLcnCqNJ7UXw2A59eJYKdZHH5zcbCsJz0ifufSp40cyj
         UVfyWNHQwR7zOIQ6Jr6ZsSas/Kt2bGMQBktOHzOBuy7SZ1sPn8tscydDDBVeEwnVQKFD
         TZFQ==
ARC-Authentication-Results: i=1; mx.google.com;
       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org
Received: from vger.kernel.org (vger.kernel.org. [209.132.180.67])
        by mx.google.com with ESMTP id k71si9959156pgd.351.2018.11.10.05.23.00;
        Sat, 10 Nov 2018 05:23:28 -0800 (PST)
Received-SPF: pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) client-ip=209.132.180.67;
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1729061AbeKJXH4 (ORCPT <rfc822;tu.k.phung@gmail.com>
        + 99 others); Sat, 10 Nov 2018 18:07:56 -0500
Received: from outbound-smtp08.blacknight.com ([46.22.139.13]:41495 "EHLO
        outbound-smtp08.blacknight.com" rhost-flags-OK-OK-OK-OK)
        by vger.kernel.org with ESMTP id S1728988AbeKJXH4 (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Sat, 10 Nov 2018 18:07:56 -0500
Received: from mail.blacknight.com (pemlinmail05.blacknight.ie [81.17.254.26])
        by outbound-smtp08.blacknight.com (Postfix) with ESMTPS id D6DED1C1CA1
        for <linux-kernel@vger.kernel.org>; Sat, 10 Nov 2018 13:22:51 +0000 (GMT)
Received: (qmail 1113 invoked from network); 10 Nov 2018 13:22:51 -0000
Received: from unknown (HELO techsingularity.net) (mgorman@techsingularity.net@[37.228.229.69])
  by 81.17.254.9 with ESMTPSA (AES256-SHA encrypted, authenticated); 10 Nov 2018 13:22:51 -0000
Date: Sat, 10 Nov 2018 13:22:49 +0000
From: Mel Gorman <mgorman@techsingularity.net>
To: Andrea Arcangeli <aarcange@redhat.com>
Cc: "Kirill A. Shutemov" <kirill@shutemov.name>,
        Anthony Yznaga <anthony.yznaga@oracle.com>, linux-mm@kvack.org,
        linux-kernel@vger.kernel.org, aneesh.kumar@linux.ibm.com,
        akpm@linux-foundation.org, jglisse@redhat.com,
        khandual@linux.vnet.ibm.com, kirill.shutemov@linux.intel.com,
        mhocko@kernel.org, minchan@kernel.org, peterz@infradead.org,
        rientjes@google.com, vbabka@suse.cz, willy@infradead.org,
        ying.huang@intel.com, nitingupta910@gmail.com
Subject: Re: [RFC PATCH] mm: thp: implement THP reservations for anonymous
 memory
Message-ID: <20181110132249.GH23260@techsingularity.net>
References: <1541746138-6706-1-git-send-email-anthony.yznaga@oracle.com>
 <20181109121318.3f3ou56ceegrqhcp@kshutemo-mobl1>
 <20181109195150.GA24747@redhat.com>
MIME-Version: 1.0
Content-Type: text/plain; charset=iso-8859-15
Content-Disposition: inline
In-Reply-To: <20181109195150.GA24747@redhat.com>
User-Agent: Mutt/1.10.1 (2018-07-13)
Sender: liker.xu+caf_=xuliker=zju.edu.cn@gmail.com
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-CM-TRANSID: AQAAfwBXNUHS2+Zbtv4mAA--.12829S3
Authentication-Results: mail-app2; spf=pass smtp.mail=liker.xu+caf_=xu
	liker=zju.edu.cn@gmail.com;
X-Coremail-Antispam: 1UD129KBjvJXoWxKF1kJryxuF4ruF13XFWUurg_yoW7ury5pF
	W5Kwn0k3Z7JF4DAr1I9wn7JryFkayfJFW5Jr9Ygw1kZ3s8u3Z2vr17ta45Za4xWrs5Aa1j
	vrWjv34DZF4DZaDanT9S1TB71UUUUUUqnTZGkaVYY2UrUUUU0bIjqfuFe4nvWSU8nxnvy2
	9KBjDU0xBIdaVrnRJUUUPEb7Iv0xC_Kw4lb4IE77IF4wAFF20E14v26r1j6r4UM7CIcVAF
	z4kK6r1j6r18M28lY4IEw2IIxxk0rwA2F7IY1VAKz4vEj48ve4kI8wA2z4x0Y4vE2Ix0cI
	8IcVAFwI0_Xr0_Ar1l84ACjcxK6xIIjxv20xvEc7CjxVAFwI0_Gr0_Cr1l84ACjcxK6I8E
	87Iv67AKxVW0oVCq3wA2z4x0Y4vEx4A2jsIEc7CjxVAFwI0_GcCE3s1le2I262IYc4CY6c
	8Ij28IcVAaY2xG8wAqx4xG64xvF2IEw4CE5I8CrVC2j2WlYx0E2Ix0cI8IcVAFwI0_Jr0_
	Jr4lYx0Ex4A2jsIE14v26r4j6F4UMcvjeVCFs4IE7xkEbVWUJVW8JwACjcxG0xvEwIxGrw
	CjxxvEa2IrMxkF7I0Ew4C26cxK6c8Ij28IcwCY1Ik267vv6cIUMxkFs20EY4vE44CYbxCE
	4x80FwCY02Avz4vEIxC_Xr4lc2IjII80xcxEwVAKI48JMxvI42IY6xIIjxv20xvE14v26r
	1j6r1xMxvI42IY6xIIjxv20xvEc7CjxVAFwI0_Jr0_Gr1lcIIF0xvEx4A2jsIE14v26F4U
	JVW0owCYIxAIcVC2z280aVCY1x0267AKxVWxJr0_GcWl42xK82IYc2Ij64vIr41l42xK82
	IY64kExVAvwVAq07x20xyl4x8a6x804xWl4I8I3I0E4IkC6x0Yz7v_Jr0_Gr1lx2IqxVAq
	x4xG67AKxVWUJVWUGwC20s026x8GjcxK67AKxVWUGVWUWwC2zVAF1VAY17CE14v26r4a6r
	W5MIIYrxkI7VAKI48JMIIF0xvE42xK8VAvwI8IcIk0rVWrJr0_WFyUJbIYCTnIWIevJa73
	UjIFyTuYvjxUl0edUUUUU

On Fri, Nov 09, 2018 at 02:51:50PM -0500, Andrea Arcangeli wrote:
> On Fri, Nov 09, 2018 at 03:13:18PM +0300, Kirill A. Shutemov wrote:
> > I haven't yet read the patch in details, but I'm skeptical about the
> > approach in general for few reasons:
> > 
> > - PTE page table retracting to replace it with huge PMD entry requires
> >   down_write(mmap_sem). It makes the approach not practical for many
> >   multi-threaded workloads.
> > 
> >   I don't see a way to avoid exclusive lock here. I will be glad to
> >   be proved otherwise.
> > 
> > - The promotion will also require TLB flush which might be prohibitively
> >   slow on big machines.
> > 
> > - Short living processes will fail to benefit from THP with the policy,
> >   even with plenty of free memory in the system: no time to promote to THP
> >   or, with synchronous promotion, cost will overweight the benefit.
> > 
> > The goal to reduce memory overhead of THP is admirable, but we need to be
> > careful not to kill THP benefit itself. The approach will reduce number of
> > THP mapped in the system and/or shift their allocation to later stage of
> > process lifetime.
> > 
> > The only way I see it can be useful is if it will be possible to apply the
> > policy on per-VMA basis. It will be very useful for malloc()
> > implementations, for instance. But as a global policy it's no-go to me.
> 
> I'm also skeptical about this: the current design is quite
> intentional. It's not a bug but a feature that we're not doing the
> promotion.
> 

Understood. I think with two people with extensive THP experience being
skeptical about this, we should take a step back before Anthony spends
too much more time on this. It would be a shame to work extensively on
a series just to have it rejected.

> Part of the tradeoff with THP is to use more RAM to save CPU, when you
> use less RAM you're inherently already wasting some CPU just for the
> reservation management and you don't get the immediate TLB benefit
> anymore either.
> 

This is true, there is a gap where there is no THP benefit. The big
question is how many workloads, if any, suffer as a result of premature
reclaim due to sparse references of the address space consuming too much
memory. Anthony, do you have any benchmarks in mind? I don't because the
HPC workloads I'm aware of are usually sized to fit in memory regardless
of THP use.

> And if you're in the camp that is concerned about the use of more RAM
> or/and about the higher latency of COW faults, I'm afraid the
> intermediate solution will be still slower than the already available
> MADV_NOHUGEPAGE or enabled=madvise.
> 

Does that not prevent huge page usage? Maybe you can spell it out a bit
better. What is the set of system calls an application should make to
not use huge pages either for the address space or on a per-VMA basis
and defer to kcompactd? I know that can be tuned globally but that's not
quite the same thing given that multiple applications or containers can
be running with different requirements.

> Now about the implementation: the whole point of the reservation
> complexity is to skip the khugepaged copy, so it can collapse in
> place. Is skipping the copy worth it? Isn't the big cost the IPI
> anyway to avoid leaving two simultaneous TLB mappings of different
> granularity?
> 

Not necessarily. With THP anon in the simple case, it might be just a
single thread and kcompact so that's one IPI (kcompactd flushes local and
one IPI to the CPU the thread was running on assuming it's not migrating
excessively). It would scale up with the number of threads but I suspect
the main cost is the actual copying, page table manipulation and the
locking required.

> So if you are ok to copy the memory that you promote to THP, you'd
> just need a global THP mode to avoid allocating THP even when they're
> available during the page fault (while still allowing khugepaged to
> collapse hugepages in the background), and then reduce max_ptes_none
> to get the desired promotion ratio.
> 

As an aside, a universal benefit would be looking at reducing the time
to allocate the necessary huge page as we know that can be excessive. It
would be ortogonal to this series.

> > <SNIP>
> >
> > Prove me wrong with performance data. :)
> 
> Same here.
> 

Could you and Kirill outline what sort of workloads you would consider
acceptable for evaluating this series? One would assume it covers at
least the following, potentially with a number of workloads.

1. Evaluate the collapse and copying costs (probing the entire time
   spent in collapse_huge_page might do it)
2. Evaluate mmap_sem hold time during hugepage collapse
3. Estimate excessive RAM use due to unnecessary THP usage
4. Estimate the slowdown due to delayed THP usage 

1 and 2 would indicate how much time is lost due to not using
reservations. That potentially goes in the direction of simply making
this faster -- fragmentation reduction (posted but unreviewed), faster
compaction searches, better page isolation during compaction to
avoid free pages being reused before an order-9 is free.

3 should be straight-forward but 4 would be the hardest to evaluate
because it would have to be determimed if 4 is offset by improvements to
1-3. If 1-3 is improved enough, it might remove the motivation for the
series entirely.

In other words, if we agree on a workload in advance, it might bring
this the right direction and not accidentally throw Anthony down a hole
working on a series that never gets ack'd.

I'm not necessarily the best person to answer because my natural inclination
after the fragmentation series would be to keep using thpfiosacle
(from the fragmentation avoidance series) and work on improving the THP
allocation success rates and reduce latencies. I've tunnel vision on that
for the moment.

Thanks.

-- 
Mel Gorman
SUSE Labs
