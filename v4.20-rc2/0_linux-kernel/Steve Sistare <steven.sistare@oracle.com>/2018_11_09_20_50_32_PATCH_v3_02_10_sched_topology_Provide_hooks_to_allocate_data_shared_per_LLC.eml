Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from pop3.zju.edu.cn (61.164.42.155:110) by
  likexu-MOBL1.ccr.corp.intel.com with POP3; 10 Nov 2018 00:52:56 -0000
Received: from icoremail.net (unknown [209.85.210.180])
	by mail-app2 (Coremail) with SMTP id by_KCgDnX9N0heVbHexnAQ--.31661S3;
	Fri, 09 Nov 2018 21:02:46 +0800 (CST)
Received: from mail-pf1-f180.google.com (unknown [209.85.210.180])
	by mx2.icoremail.net (Coremail) with SMTP id AQAAfwAnBEByheVbtLAiAA--.3640S3;
	Fri, 09 Nov 2018 21:02:43 +0800 (CST)
Received: by mail-pf1-f180.google.com with SMTP id y18-v6so919744pfn.1
        for <xuliker@zju.edu.cn>; Fri, 09 Nov 2018 05:02:43 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:delivered-to:dkim-signature:from:to:cc:subject
         :date:message-id:in-reply-to:references:sender:precedence:list-id;
        bh=9Rm/MPxBp8VM3wK/vjrE1VQBOiiXTlrUMLjtKqaRBD0=;
        b=HeCKwZy026wLaDgI/vc0tQ6cylLgoRaiBVy92aFMVIlJN7NeV35W4ip21XnuC2yumv
         KmB0mgk7QdgCr+zu+mMlOZumonL+C5oQL601s/x0JvxMZ5VsJMgDAmXisE0RAYvk6lZ6
         OZEvAFxbctgyahPramxEr2pZt6CAO/d4ieNDgLwauAwdm26P70DUBIWuU0R94wMcXfPo
         gWYbUmypB30GpTidd0dSE2nIr64Rod+hsLN7mewGIy0g9ZdBJcW9tQ+6Fu6YJGsKaWmr
         HTpNCC6AiOY0Eh3U6LDAzZzWYMhIia3JIDG2R6cb46F5/KhMVJWSCvTXEhX6hbZWL4tu
         V18Q==
X-Gm-Message-State: AGRZ1gImt1RQRupiFaYzmrFVk0gq8bMaGoPKcv8tfqm+qCdXiL3kYBBG
	l7vLoxNxFKKqSrEyEhYZMuJO9icV5y7spedwxXI2538JlQbijgE6Fw==
X-Received: by 2002:a63:fd53:: with SMTP id m19mr7485728pgj.340.1541768562738;
        Fri, 09 Nov 2018 05:02:42 -0800 (PST)
X-Forwarded-To: xuliker@zju.edu.cn
X-Forwarded-For: liker.xu@gmail.com xuliker@zju.edu.cn
Delivered-To: liker.xu@gmail.com
Received: by 2002:a17:90a:c304:0:0:0:0 with SMTP id g4-v6csp171125pjt;
        Fri, 9 Nov 2018 05:02:41 -0800 (PST)
X-Google-Smtp-Source: AJdET5eyuCQC4MSHs3X2jlSEv183OvpDZ9n07oldYvNAsvYE7OISfp6hzIjPhO/RhRAsjwtVcP8/
X-Received: by 2002:a63:63c3:: with SMTP id x186mr7366228pgb.330.1541768561809;
        Fri, 09 Nov 2018 05:02:41 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1541768561; cv=none;
        d=google.com; s=arc-20160816;
        b=Iu3UPjyKjEz/0sk36jMYxGZ1cWCH216Nx7SLyE9yQj+mj8GZrPYdBuzDd3/MAbJfg0
         q5j4XCiuSFeM9+J/nyR79HAfHzYoRc5L7JZGPyZ1vqKpBdyMeYucEGGsQlbVQcWAyA1w
         b3D6ozXMxeeX3wET7nF4yu387KYQ4Rac5TjTQFNzpdv4j4DSKD9oAR+AmibvjIqyVANK
         E1DE90ALbKq1EEO61pcIdZQIWBF4/dyb80kUiqdBhZyQ+fgbQX4ONYv+WKG1gS/SOqW4
         ZS01Zv0lE+UkvOvcEFvZCKU0VfbMtE0D5kUsdrQhndeKKs7SL71+qt2Bko9s1QRWeR84
         bu/w==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=list-id:precedence:sender:references:in-reply-to:message-id:date
         :subject:cc:to:from:dkim-signature;
        bh=9Rm/MPxBp8VM3wK/vjrE1VQBOiiXTlrUMLjtKqaRBD0=;
        b=joG3C5cvshcB/OsX2/2OkZO2GqLHO7B5uhJwYbKTqt1yY1K9tuXRyqvF0cXKRgUGd0
         EIvl25A8ayuDb827hqZvN+wvtcEfzYVX/PD+m5bNRp6Wm8kqE2m2lLVX11Yc/rPSQzSX
         63ORs1E720huv91chFvQpVRCi6CLFM2wRLdF1zlcYWhx1HBlmPhu3BtLsYETsjTdco88
         g/xYty2+D4zBxVeYfPUiKpst6MYp+PdmS8Ef3tYcRk8qy+cc3AJV9NoTZD9QtnfEDSVZ
         JudbBxVHqVXadOrYIMhId59y/X53Rf4/kErTfsZan3W+OLysR5cu/713WHYlu61scrfd
         nhpw==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass header.i=@oracle.com header.s=corp-2018-07-02 header.b=VeUUEFHz;
       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org;
       dmarc=pass (p=NONE sp=NONE dis=NONE) header.from=oracle.com
Received: from vger.kernel.org (vger.kernel.org. [209.132.180.67])
        by mx.google.com with ESMTP id d32-v6si8068240pla.405.2018.11.09.05.02.23;
        Fri, 09 Nov 2018 05:02:41 -0800 (PST)
Received-SPF: pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) client-ip=209.132.180.67;
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1728341AbeKIWlx (ORCPT <rfc822;tu.k.phung@gmail.com>
        + 99 others); Fri, 9 Nov 2018 17:41:53 -0500
Received: from userp2130.oracle.com ([156.151.31.86]:58676 "EHLO
        userp2130.oracle.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1727735AbeKIWlw (ORCPT
        <rfc822;linux-kernel@vger.kernel.org>);
        Fri, 9 Nov 2018 17:41:52 -0500
Received: from pps.filterd (userp2130.oracle.com [127.0.0.1])
        by userp2130.oracle.com (8.16.0.22/8.16.0.22) with SMTP id wA9Cx8Kk111325;
        Fri, 9 Nov 2018 13:00:51 GMT
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=oracle.com; h=from : to : cc :
 subject : date : message-id : in-reply-to : references; s=corp-2018-07-02;
 bh=9Rm/MPxBp8VM3wK/vjrE1VQBOiiXTlrUMLjtKqaRBD0=;
 b=VeUUEFHzv5TXgGqZ1z3e0ZA+Ek+KFM6vZQFcFWjBP3cbpi1v+8E1PhOW8fkNCVbLIQzN
 ndIF6DU6MHePHkilrvqpyaooX7s1uRFmq1JVKs/F86E7/qjRrU2aTE7UqowqgG9RhoCo
 UZP6eRisI5e3NCzEFc2l2TenQDh7GNsqU19EHxynYrq/0aPvuT8Ki7G1H/4GKRzXOtC3
 qsisQtrQhOSBrzmh+/0EhrKDrvTUZwm9a8X7T10bgCPVvhqeV+gzyQM3V0sggwpHpfl1
 0Y/2ZrJrJI/Tc6cwZYFns/bfM9U3jtJ2qoJCWyYx1LvlkkwZlDCcx+xHHk/TjyNfOoOG /g== 
Received: from aserv0022.oracle.com (aserv0022.oracle.com [141.146.126.234])
        by userp2130.oracle.com with ESMTP id 2nh33uex4a-1
        (version=TLSv1.2 cipher=ECDHE-RSA-AES256-GCM-SHA384 bits=256 verify=OK);
        Fri, 09 Nov 2018 13:00:51 +0000
Received: from aserv0122.oracle.com (aserv0122.oracle.com [141.146.126.236])
        by aserv0022.oracle.com (8.14.4/8.14.4) with ESMTP id wA9D0jAU018227
        (version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256 verify=OK);
        Fri, 9 Nov 2018 13:00:45 GMT
Received: from abhmp0017.oracle.com (abhmp0017.oracle.com [141.146.116.23])
        by aserv0122.oracle.com (8.14.4/8.14.4) with ESMTP id wA9D076F020412;
        Fri, 9 Nov 2018 13:00:44 GMT
Received: from ca-dev63.us.oracle.com (/10.211.8.221)
        by default (Oracle Beehive Gateway v4.0)
        with ESMTP ; Fri, 09 Nov 2018 05:00:07 -0800
From: Steve Sistare <steven.sistare@oracle.com>
To: mingo@redhat.com, peterz@infradead.org
Cc: subhra.mazumdar@oracle.com, dhaval.giani@oracle.com,
        daniel.m.jordan@oracle.com, pavel.tatashin@microsoft.com,
        matt@codeblueprint.co.uk, umgwanakikbuti@gmail.com,
        riel@redhat.com, jbacik@fb.com, juri.lelli@redhat.com,
        valentin.schneider@arm.com, vincent.guittot@linaro.org,
        quentin.perret@arm.com, steven.sistare@oracle.com,
        linux-kernel@vger.kernel.org
Subject: [PATCH v3 02/10] sched/topology: Provide hooks to allocate data shared per LLC
Date: Fri,  9 Nov 2018 04:50:32 -0800
Message-Id: <1541767840-93588-3-git-send-email-steven.sistare@oracle.com>
X-Mailer: git-send-email 1.8.3.1
In-Reply-To: <1541767840-93588-1-git-send-email-steven.sistare@oracle.com>
References: <1541767840-93588-1-git-send-email-steven.sistare@oracle.com>
X-Proofpoint-Virus-Version: vendor=nai engine=5900 definitions=9071 signatures=668683
X-Proofpoint-Spam-Details: rule=notspam policy=default score=0 suspectscore=2 malwarescore=0
 phishscore=0 bulkscore=0 spamscore=0 mlxscore=0 mlxlogscore=999
 adultscore=0 classifier=spam adjust=0 reason=mlx scancount=1
 engine=8.0.1-1807170000 definitions=main-1811090121
Sender: liker.xu+caf_=xuliker=zju.edu.cn@gmail.com
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-CM-TRANSID: AQAAfwAnBEByheVbtLAiAA--.3640S3
Authentication-Results: mail-app2; spf=pass smtp.mail=liker.xu+caf_=xu
	liker=zju.edu.cn@gmail.com;
X-Coremail-Antispam: 1UD129KBjvJXoWxur4DurWfJFWxJw48Zr1kZrb_yoW5KFykpF
	4qqw45ZayYqry7X3y3C34qgryYk3s2q34Ig3y5u3s3Ar9rJrykWF1kZFy3uFs8urWkAa42
	yrnIg3W7WF4qyFJanT9S1TB71UUUUUUqnTZGkaVYY2UrUUUUjbIjqfuFe4nvWSU8nxnvy2
	9KBjDU0xBIdaVrnRJUUUBab7Iv0xC_KF4lb4IE77IF4wAFF20E14v26r1j6r4UM7CIcVAF
	z4kK6r1j6r18M28lY4IEw2IIxxk0rwA2F7IY1VAKz4vEj48ve4kI8wA2z4x0Y4vE2Ix0cI
	8IcVAFwI0_Ar0_tr1l84ACjcxK6xIIjxv20xvEc7CjxVAFwI0_Cr0_Gr1UM28EF7xvwVC2
	z280aVAFwI0_GcCE3s1l84ACjcxK6I8E87Iv6xkF7I0E14v26rxl6s0DM2AIxVAIcxkEcV
	Aq07x20xvEncxIr21l5I8CrVACY4xI64kE6c02F40Ex7xfMcIj6xIIjxv20xvE14v26r1j
	6r18McIj6I8E87Iv67AKxVW8JVWxJwAm72CE4IkC6x0Yz7v_Jr0_Gr1lF7xvr2IYc2Ij64
	vIr41l7I0Y6sxI4wCY1x0264kExVAvwVAq07x20xylc7Ca8VAvwVCFzxkY4VCF77xAMxkI
	7II2jI8vz4vEwIxGrwCYIxAIcVC0I7IYx2IY67AKxVW8JVW5JwCYIxAIcVC0I7IYx2IY6x
	kF7I0E14v26r4j6F4UMxvI42IY6I8E87Iv67AKxVW0oVCq3wCYIxAIcVC2z280aVCY1x02
	67AKxVW0oVCq3wCF04k20xvY0x0EwIxGrwCF04k20xvEw4C26cxK6c8Ij28IcwCF72vE77
	IF4wCFx2IqxVCFs4IE7xkEbVWUJVW8JwC20s026c02F40E14v26r1j6r18MI8I3I0E7480
	Y4vE14v26r106r1rMI8E67AF67kF1VAFwI0_GFv_WrylIxkGc2Ij64vIr41lIxAIcVCF04
	k26cxKx2IYs7xG6r4j6FyUYxBIdaVFxhVjvjDU0xZFpf9x07jMyxiUUUUU=

Add functions sd_llc_alloc_all() and sd_llc_free_all() to allocate and
free data pointed to by struct sched_domain_shared at the last-level-cache
domain.  sd_llc_alloc_all() is called after the SD hierarchy is known, to
eliminate the unnecessary allocations that would occur if we instead
allocated in __sdt_alloc() and then figured out which shared nodes are
redundant.

Signed-off-by: Steve Sistare <steven.sistare@oracle.com>
---
 kernel/sched/topology.c | 75 ++++++++++++++++++++++++++++++++++++++++++++++++-
 1 file changed, 74 insertions(+), 1 deletion(-)

diff --git a/kernel/sched/topology.c b/kernel/sched/topology.c
index 8d7f15b..3e72ce0 100644
--- a/kernel/sched/topology.c
+++ b/kernel/sched/topology.c
@@ -10,6 +10,12 @@
 static cpumask_var_t sched_domains_tmpmask;
 static cpumask_var_t sched_domains_tmpmask2;
 
+struct s_data;
+static int sd_llc_alloc(struct sched_domain *sd);
+static void sd_llc_free(struct sched_domain *sd);
+static int sd_llc_alloc_all(const struct cpumask *cpu_map, struct s_data *d);
+static void sd_llc_free_all(const struct cpumask *cpu_map);
+
 #ifdef CONFIG_SCHED_DEBUG
 
 static int __init sched_debug_setup(char *str)
@@ -361,8 +367,10 @@ static void destroy_sched_domain(struct sched_domain *sd)
 	 */
 	free_sched_groups(sd->groups, 1);
 
-	if (sd->shared && atomic_dec_and_test(&sd->shared->ref))
+	if (sd->shared && atomic_dec_and_test(&sd->shared->ref)) {
+		sd_llc_free(sd);
 		kfree(sd->shared);
+	}
 	kfree(sd);
 }
 
@@ -996,6 +1004,7 @@ static void __free_domain_allocs(struct s_data *d, enum s_alloc what,
 		free_percpu(d->sd);
 		/* Fall through */
 	case sa_sd_storage:
+		sd_llc_free_all(cpu_map);
 		__sdt_free(cpu_map);
 		/* Fall through */
 	case sa_none:
@@ -1610,6 +1619,62 @@ static void __sdt_free(const struct cpumask *cpu_map)
 	}
 }
 
+static int sd_llc_alloc(struct sched_domain *sd)
+{
+	/* Allocate sd->shared data here. Empty for now. */
+
+	return 0;
+}
+
+static void sd_llc_free(struct sched_domain *sd)
+{
+	struct sched_domain_shared *sds = sd->shared;
+
+	if (!sds)
+		return;
+
+	/* Free data here. Empty for now. */
+}
+
+static int sd_llc_alloc_all(const struct cpumask *cpu_map, struct s_data *d)
+{
+	struct sched_domain *sd, *hsd;
+	int i;
+
+	for_each_cpu(i, cpu_map) {
+		/* Find highest domain that shares resources */
+		hsd = NULL;
+		for (sd = *per_cpu_ptr(d->sd, i); sd; sd = sd->parent) {
+			if (!(sd->flags & SD_SHARE_PKG_RESOURCES))
+				break;
+			hsd = sd;
+		}
+		if (hsd && sd_llc_alloc(hsd))
+			return 1;
+	}
+
+	return 0;
+}
+
+static void sd_llc_free_all(const struct cpumask *cpu_map)
+{
+	struct sched_domain_topology_level *tl;
+	struct sched_domain *sd;
+	struct sd_data *sdd;
+	int j;
+
+	for_each_sd_topology(tl) {
+		sdd = &tl->data;
+		if (!sdd)
+			continue;
+		for_each_cpu(j, cpu_map) {
+			sd = *per_cpu_ptr(sdd->sd, j);
+			if (sd)
+				sd_llc_free(sd);
+		}
+	}
+}
+
 static struct sched_domain *build_sched_domain(struct sched_domain_topology_level *tl,
 		const struct cpumask *cpu_map, struct sched_domain_attr *attr,
 		struct sched_domain *child, int dflags, int cpu)
@@ -1769,6 +1834,14 @@ static struct sched_domain *build_sched_domain(struct sched_domain_topology_leve
 		}
 	}
 
+	/*
+	 * Allocate shared sd data at last level cache.  Must be done after
+	 * domains are built above, but before the data is used in
+	 * cpu_attach_domain and descendants below.
+	 */
+	if (sd_llc_alloc_all(cpu_map, &d))
+		goto error;
+
 	/* Attach the domains */
 	rcu_read_lock();
 	for_each_cpu(i, cpu_map) {
-- 
1.8.3.1
