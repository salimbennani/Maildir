Return-Path: <linux-kernel-owner@vger.kernel.org>
Delivered-To: unknown
Received: from pop3.zju.edu.cn (124.160.105.205:110) by
  likexu-MOBL1.ccr.corp.intel.com with POP3; 12 Nov 2018 00:44:42 -0000
Received: from icoremail.net (unknown [209.85.215.182])
	by mail-app2 (Coremail) with SMTP id by_KCgCn31xvvOhbYrV4AQ--.36654S3;
	Mon, 12 Nov 2018 07:34:08 +0800 (CST)
Received: from mail-pg1-f182.google.com (unknown [209.85.215.182])
	by mx2.icoremail.net (Coremail) with SMTP id AQAAfwBXNUFtvOhbDK8uAA--.26500S3;
	Mon, 12 Nov 2018 07:34:05 +0800 (CST)
Received: by mail-pg1-f182.google.com with SMTP id r9-v6so3187229pgv.6
        for <xuliker@zju.edu.cn>; Sun, 11 Nov 2018 15:34:05 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:delivered-to:dkim-signature:from:to:cc:subject
         :date:message-id:in-reply-to:references:user-agent:mime-version
         :content-transfer-encoding:sender:precedence:list-id;
        bh=VnEvY/U1igddedSAfKW7wUakjtpEHwdPXAzganVdTbA=;
        b=jZy5ehr9MCJql9nebd6iuUhcAEsb+2RF5IyYllccQ35tyBDWo33ijgTpxhRtEGuXGm
         HfZ2tYx9NFuS2Oc7foegtN6woaL9jT6D8hyaMFCZMR4QPoYID/q/aPIHF7qpHUGOjEJ+
         xMXCOyCfHMxDUaNTH1CtWfWF5NmAsFITTFsBMhDFi6LJfFTPeYL6hem1JqhPbAv3dZDt
         N33jV9+ALMlEK4ThiqfO/CSEjG6z4leL9YbyQIXWT6ylM81dDcT5GfKRiCx+sSPwoXhS
         i4Dv/6OJ/2gDW4/V847ZXMEH5JgPKQZbxTfMU/b91+lAUv3bUzAaOF/vgCaG/LEkzwf8
         BjbQ==
X-Gm-Message-State: AGRZ1gKb0T03VYVdglR0G8LaubML4vizTb7paNHbI/bhwIzFADoCimn2
	1mE3IdRVGYDNBwKWiq6aiImk6X9T6bR8f/CbAxDYVGQI4t8Cviw=
X-Received: by 2002:a63:2c0e:: with SMTP id s14mr15832202pgs.132.1541979244963;
        Sun, 11 Nov 2018 15:34:04 -0800 (PST)
X-Forwarded-To: xuliker@zju.edu.cn
X-Forwarded-For: liker.xu@gmail.com xuliker@zju.edu.cn
Delivered-To: liker.xu@gmail.com
Received: by 2002:a17:90a:c304:0:0:0:0 with SMTP id g4-v6csp2565725pjt;
        Sun, 11 Nov 2018 15:34:04 -0800 (PST)
X-Google-Smtp-Source: AJdET5chaA4XG7MG5Ts3z0hMBFimNbjvsG+Zwu83YctpwALjejruH7EI80Y6n713NfiDdD3kHGv7
X-Received: by 2002:a62:2e46:: with SMTP id u67-v6mr1330936pfu.236.1541979244115;
        Sun, 11 Nov 2018 15:34:04 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1541979244; cv=none;
        d=google.com; s=arc-20160816;
        b=vnJDE3BYp/Rqe5wkzLG1a+DhGRS5CFgfqzvphikcZXqnob/DmdRBiY1XhmSSogdxc9
         i9AA/icqClwlF1YBUyVa9BCIhp6JdzY1jmHOLNli+MRlwr61kdvo9rK37ns41xvV3ojE
         0fP1DHAGqrRf/Sk8Tv//lqIgkKTMaFHYNZfiC63SPVgLDZNL8ZzttI9rP9xOQcca47It
         VvP3Q8S/KPPMzqYck9Gc9x+LAMGuO51Mf/ofA4IiAF6aAs2HhZ65Gb1iQzxR2bnh+G0L
         fnCXKaGOZ+fM/OAxkKMWwk4jZ3gvJKN1eFe8/ltLxfR3YoppBRnRXyM5UOad83/vNACf
         K5mA==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=list-id:precedence:sender:content-transfer-encoding:mime-version
         :user-agent:references:in-reply-to:message-id:date:subject:cc:to
         :from:dkim-signature;
        bh=VnEvY/U1igddedSAfKW7wUakjtpEHwdPXAzganVdTbA=;
        b=m6JiLUsS1mIADva163DeJvbffv2dW9iKqSG83EiVmKLtMcMvh5Gd9D1c4aH4IdvoMK
         DGOdlbTOWZ/CknIKIBdnmJfcUPWbhMESUSLPa5dFm0jjhQ4a2ujyevQDnm/DVUyiiIE7
         7sO0EmGz7krAP9e+l4IkeHpDxjyOx4KximIYxvk5LDtmf3C4KvwbmJWblW7TBE+WfXoE
         o6xf7mNtHY/XD30KnDOYmPj8I+UyKRaKffh53OlMOihq26z+yrYzLDiud9zaNQczNcxn
         HLjzzeIsWW3b23aSGk7UaqBxc3f3lUHqT0Ni63eqZAe/McemBDUMAjk3vtd9JDQk8cH4
         +N9w==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass header.i=@kernel.org header.s=default header.b=HdeNUVay;
       spf=pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) smtp.mailfrom=linux-kernel-owner@vger.kernel.org
Received: from vger.kernel.org (vger.kernel.org. [209.132.180.67])
        by mx.google.com with ESMTP id v15-v6si15720326pfd.125.2018.11.11.15.33.49;
        Sun, 11 Nov 2018 15:34:04 -0800 (PST)
Received-SPF: pass (google.com: best guess record for domain of linux-kernel-owner@vger.kernel.org designates 209.132.180.67 as permitted sender) client-ip=209.132.180.67;
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S2388455AbeKLIUL (ORCPT <rfc822;berg.ding@gmail.com> + 99 others);
        Mon, 12 Nov 2018 03:20:11 -0500
Received: from mail.kernel.org ([198.145.29.99]:43840 "EHLO mail.kernel.org"
        rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
        id S2388390AbeKLIUJ (ORCPT <rfc822;linux-kernel@vger.kernel.org>);
        Mon, 12 Nov 2018 03:20:09 -0500
Received: from localhost (unknown [206.108.79.134])
        (using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
        (No client certificate requested)
        by mail.kernel.org (Postfix) with ESMTPSA id D0F45216FD;
        Sun, 11 Nov 2018 22:30:11 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=kernel.org;
        s=default; t=1541975411;
        bh=786esyUBKOyQw849EvrCadjJIXjM4xY/ae7rSS3OmrQ=;
        h=From:To:Cc:Subject:Date:In-Reply-To:References:From;
        b=HdeNUVay/ipCh7On4DA51NTTywr5qC1N+DzjStohGmI7iftGlPUvvFWeoLzD8WUhI
         fyCuzIY20jiJBcFUnxToMuXBTvR2WRHDxMEbeQay8GwtOLcTtpyxAk0uzP1nPiUAe8
         4pS1nLFDdQVWF71D+oWKP5xafwIgk/F/Tgx1tqF0=
From: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
To: linux-kernel@vger.kernel.org
Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>,
        stable@vger.kernel.org, Bart Van Assche <bvanassche@acm.org>,
        Sagi Grimberg <sagi@grimberg.me>,
        Christoph Hellwig <hch@lst.de>, Sasha Levin <sashal@kernel.org>
Subject: [PATCH 4.18 090/350] nvmet-rdma: use a private workqueue for delete
Date: Sun, 11 Nov 2018 14:19:14 -0800
Message-Id: <20181111221710.879282379@linuxfoundation.org>
X-Mailer: git-send-email 2.19.1
In-Reply-To: <20181111221707.043394111@linuxfoundation.org>
References: <20181111221707.043394111@linuxfoundation.org>
User-Agent: quilt/0.65
X-stable: review
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
Sender: liker.xu+caf_=xuliker=zju.edu.cn@gmail.com
Precedence: bulk
List-ID: <linux-kernel.vger.kernel.org>
X-Mailing-List: linux-kernel@vger.kernel.org
X-CM-TRANSID: AQAAfwBXNUFtvOhbDK8uAA--.26500S3
Authentication-Results: mail-app2; spf=pass smtp.mail=liker.xu+caf_=xu
	liker=zju.edu.cn@gmail.com;
X-Coremail-Antispam: 1UD129KBjvJXoWxKrW3tF4DuF1DCw1UJFWxCrg_yoW7Aw45pF
	9ag34xArZ7trWjkr4aya1UXrykua90vr4UAr1xtwn7JF4Yyr9IqayDt3W0qFyUGr9xZrW0
	kF4Utr4Sy3W3A37anT9S1TB71UUUUUUqnTZGkaVYY2UrUUUU0bIjqfuFe4nvWSU8nxnvy2
	9KBjDU0xBIdaVrnRJUUUPIb7Iv0xC_Kw4lb4IE77IF4wAFF20E14v26r1j6r4UM7CIcVAF
	z4kK6r1j6r18M28lY4IEw2IIxxk0rwA2F7IY1VAKz4vEj48ve4kI8wA2z4x0Y4vE2Ix0cI
	8IcVAFwI0_tr0E3s1l84ACjcxK6xIIjxv20xvEc7CjxVAFwI0_Gr1j6F4UJwA2z4x0Y4vE
	x4A2jsIE14v26rxl6s0DM28EF7xvwVC2z280aVCY1x0267AKxVW0oVCq3wAS0I0E0xvYzx
	vE52x082IY62kv0487Mc02F40EFcxC0VAKzVAqx4xG6I80ewAv7VC0I7IYx2IY67AKxVWU
	AVWUtwAv7VC2z280aVAFwI0_Gr0_Cr1lOx8S6xCaFVCjc4AY6r1j6r4UM4x0Y48IcxkI7V
	AKI48JMx02cVCv0xWlc7CjxVAKzI0EY4vE52x082I5MxkFs20EY4vE44CYbxCE4x80FwCY
	02Avz4vEIxC_uwCY0x0Ix7I2Y4AK64vIr41lcIIF0xvE2Ix0cI8IcVAFwI0_tr0E3s1lcI
	IF0xvE2Ix0cI8IcVCY1x0267AKxVW8Jr0_Cr1UMxvI42IY6I8E87Iv67AKxVW0oVCq3wCY
	IxAIcVC2z280aVCY1x0267AKxVW0oVCq3wCF04k20xvY0x0EwIxGrwCF04k20xvEw4C26c
	xK6c8Ij28IcwCF72vE77IF4wCFx2IqxVCFs4IE7xkEbVWUJVW8JwC20s026c02F40E14v2
	6r1j6r18MI8I3I0E7480Y4vE14v26r106r1rMI8E67AF67kF1VAFwI0_Jw0_GFylIxkGc2
	Ij64vIr41lIxAIcVCF04k26cxKx2IYs7xG6r1I6r4UYxBIdaVFxhVjvjDU0xZFpf9x07bL
	xRgUUUUU=

4.18-stable review patch.  If anyone has any objections, please let me know.

------------------

From: Sagi Grimberg <sagi@grimberg.me>

[ Upstream commit 2acf70ade79d26b97611a8df52eb22aa33814cd4 ]

Queue deletion is done asynchronous when the last reference on the queue
is dropped.  Thus, in order to make sure we don't over allocate under a
connect/disconnect storm, we let queue deletion complete before making
forward progress.

However, given that we flush the system_wq from rdma_cm context which
runs from a workqueue context, we can have a circular locking complaint
[1]. Fix that by using a private workqueue for queue deletion.

[1]:
======================================================
WARNING: possible circular locking dependency detected
4.19.0-rc4-dbg+ #3 Not tainted
------------------------------------------------------
kworker/5:0/39 is trying to acquire lock:
00000000a10b6db9 (&id_priv->handler_mutex){+.+.}, at: rdma_destroy_id+0x6f/0x440 [rdma_cm]

but task is already holding lock:
00000000331b4e2c ((work_completion)(&queue->release_work)){+.+.}, at: process_one_work+0x3ed/0xa20

which lock already depends on the new lock.

the existing dependency chain (in reverse order) is:

-> #3 ((work_completion)(&queue->release_work)){+.+.}:
       process_one_work+0x474/0xa20
       worker_thread+0x63/0x5a0
       kthread+0x1cf/0x1f0
       ret_from_fork+0x24/0x30

-> #2 ((wq_completion)"events"){+.+.}:
       flush_workqueue+0xf3/0x970
       nvmet_rdma_cm_handler+0x133d/0x1734 [nvmet_rdma]
       cma_ib_req_handler+0x72f/0xf90 [rdma_cm]
       cm_process_work+0x2e/0x110 [ib_cm]
       cm_req_handler+0x135b/0x1c30 [ib_cm]
       cm_work_handler+0x2b7/0x38cd [ib_cm]
       process_one_work+0x4ae/0xa20
nvmet_rdma:nvmet_rdma_cm_handler: nvmet_rdma: disconnected (10): status 0 id 0000000040357082
       worker_thread+0x63/0x5a0
       kthread+0x1cf/0x1f0
       ret_from_fork+0x24/0x30
nvme nvme0: Reconnecting in 10 seconds...

-> #1 (&id_priv->handler_mutex/1){+.+.}:
       __mutex_lock+0xfe/0xbe0
       mutex_lock_nested+0x1b/0x20
       cma_ib_req_handler+0x6aa/0xf90 [rdma_cm]
       cm_process_work+0x2e/0x110 [ib_cm]
       cm_req_handler+0x135b/0x1c30 [ib_cm]
       cm_work_handler+0x2b7/0x38cd [ib_cm]
       process_one_work+0x4ae/0xa20
       worker_thread+0x63/0x5a0
       kthread+0x1cf/0x1f0
       ret_from_fork+0x24/0x30

-> #0 (&id_priv->handler_mutex){+.+.}:
       lock_acquire+0xc5/0x200
       __mutex_lock+0xfe/0xbe0
       mutex_lock_nested+0x1b/0x20
       rdma_destroy_id+0x6f/0x440 [rdma_cm]
       nvmet_rdma_release_queue_work+0x8e/0x1b0 [nvmet_rdma]
       process_one_work+0x4ae/0xa20
       worker_thread+0x63/0x5a0
       kthread+0x1cf/0x1f0
       ret_from_fork+0x24/0x30

Fixes: 777dc82395de ("nvmet-rdma: occasionally flush ongoing controller teardown")
Reported-by: Bart Van Assche <bvanassche@acm.org>
Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
Tested-by: Bart Van Assche <bvanassche@acm.org>

Signed-off-by: Christoph Hellwig <hch@lst.de>

Signed-off-by: Sasha Levin <sashal@kernel.org>
Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
---
 drivers/nvme/target/rdma.c |   19 +++++++++++++++----
 1 file changed, 15 insertions(+), 4 deletions(-)

--- a/drivers/nvme/target/rdma.c
+++ b/drivers/nvme/target/rdma.c
@@ -119,6 +119,7 @@ struct nvmet_rdma_device {
 	struct list_head	entry;
 };
 
+struct workqueue_struct *nvmet_rdma_delete_wq;
 static bool nvmet_rdma_use_srq;
 module_param_named(use_srq, nvmet_rdma_use_srq, bool, 0444);
 MODULE_PARM_DESC(use_srq, "Use shared receive queue.");
@@ -1169,12 +1170,12 @@ static int nvmet_rdma_queue_connect(stru
 
 	if (queue->host_qid == 0) {
 		/* Let inflight controller teardown complete */
-		flush_scheduled_work();
+		flush_workqueue(nvmet_rdma_delete_wq);
 	}
 
 	ret = nvmet_rdma_cm_accept(cm_id, queue, &event->param.conn);
 	if (ret) {
-		schedule_work(&queue->release_work);
+		queue_work(nvmet_rdma_delete_wq, &queue->release_work);
 		/* Destroying rdma_cm id is not needed here */
 		return 0;
 	}
@@ -1239,7 +1240,7 @@ static void __nvmet_rdma_queue_disconnec
 
 	if (disconnect) {
 		rdma_disconnect(queue->cm_id);
-		schedule_work(&queue->release_work);
+		queue_work(nvmet_rdma_delete_wq, &queue->release_work);
 	}
 }
 
@@ -1269,7 +1270,7 @@ static void nvmet_rdma_queue_connect_fai
 	mutex_unlock(&nvmet_rdma_queue_mutex);
 
 	pr_err("failed to connect queue %d\n", queue->idx);
-	schedule_work(&queue->release_work);
+	queue_work(nvmet_rdma_delete_wq, &queue->release_work);
 }
 
 /**
@@ -1543,8 +1544,17 @@ static int __init nvmet_rdma_init(void)
 	if (ret)
 		goto err_ib_client;
 
+	nvmet_rdma_delete_wq = alloc_workqueue("nvmet-rdma-delete-wq",
+			WQ_UNBOUND | WQ_MEM_RECLAIM | WQ_SYSFS, 0);
+	if (!nvmet_rdma_delete_wq) {
+		ret = -ENOMEM;
+		goto err_unreg_transport;
+	}
+
 	return 0;
 
+err_unreg_transport:
+	nvmet_unregister_transport(&nvmet_rdma_ops);
 err_ib_client:
 	ib_unregister_client(&nvmet_rdma_ib_client);
 	return ret;
@@ -1552,6 +1562,7 @@ err_ib_client:
 
 static void __exit nvmet_rdma_exit(void)
 {
+	destroy_workqueue(nvmet_rdma_delete_wq);
 	nvmet_unregister_transport(&nvmet_rdma_ops);
 	ib_unregister_client(&nvmet_rdma_ib_client);
 	WARN_ON_ONCE(!list_empty(&nvmet_rdma_queue_list));

